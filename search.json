[
  {
    "objectID": "posts/2021-05-15-bootstrapping.html",
    "href": "posts/2021-05-15-bootstrapping.html",
    "title": "Bootstrapping in Python",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.power import NormalIndPower, TTestIndPower\nfrom scipy.stats import ttest_ind_from_stats\nimport numpy as np\nimport scipy\ndf = pd.read_csv('df_panel_fix.csv')\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\ndf=df_subset\ndf\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 7 columns\n# Add distributions by region\nimport matplotlib.pyplot as plt\n#fig, axes = plt.subplots(nrows=3, ncols=3)\n\ntest_cells = ['East China', 'North China']\nmetrics = ['gdp', 'fdi', 'it']\n\nfor test_cell in test_cells:\n    for metric in metrics:\n        df.loc[df[\"region\"] == test_cell].hist(column=[metric], bins=60)\n        print(test_cell)\n        print(metric)\n\nEast China\ngdp\nEast China\nfdi\nEast China\nit\nNorth China\ngdp\nNorth China\nfdi\nNorth China\nit\ndf.hist(column=['fdi'], bins=60)\n\narray([[&lt;AxesSubplot:title={'center':'fdi'}&gt;]], dtype=object)"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html#distributions-of-dependant-variables",
    "href": "posts/2021-05-15-bootstrapping.html#distributions-of-dependant-variables",
    "title": "Bootstrapping in Python",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[&lt;AxesSubplot:title={'center':'fdi'}&gt;]], dtype=object)\n\n\n\n\n\n\nsns.histplot(df['fdi'])\n\n&lt;AxesSubplot:xlabel='fdi', ylabel='Count'&gt;\n\n\n\n\n\n\nsns.displot(df['gdp'])\n\n\n\n\n\nsns.displot(df['fdi'])\n\n\n\n\n\nsns.displot(df['it'])\n\n\n\n\n\nsns.displot(df['specific'].dropna())\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[&lt;AxesSubplot:title={'center':'fdi'}&gt;]], dtype=object)"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2021-05-15-bootstrapping.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Bootstrapping in Python",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2021-05-15-bootstrapping.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Bootstrapping in Python",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are &gt; 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2021-05-15-bootstrapping.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2021-05-15-bootstrapping.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Bootstrapping in Python",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then &gt; 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])&gt;3].hist(column = ['gdp'])\n\narray([[&lt;AxesSubplot:title={'center':'gdp'}&gt;]], dtype=object)\n\n\n\n\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])&lt;3]\n\n\ndf_no_gdp_outliers\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n-0.521466\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n-0.464746\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n-0.421061\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n-0.383239\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n-0.340870\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n354\n2002\nEast China\nZhejiang\n8003.67\n307610\n1962633\n365437.0\n0.798274\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n1.178172\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n1.612181\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n2.007180\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n2.520929\n\n\n\n\n350 rows × 8 columns\n\n\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\narray([[&lt;AxesSubplot:title={'center':'gdp'}&gt;]], dtype=object)\n\n\n\n\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\n\n\n\n\n\n\nyear\nprovince\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\nregion\n\n\n\n\n\n\n\n\n\n\n\nEast China\n84\n84\n84\n84\n84\n84\n84\n\n\nNorth China\n48\n48\n48\n48\n48\n47\n48\n\n\nNortheast China\n36\n36\n36\n36\n36\n36\n36\n\n\nNorthwest China\n60\n60\n60\n60\n60\n60\n60\n\n\nSouth Central China\n72\n72\n72\n72\n72\n72\n72\n\n\nSouthwest China\n60\n60\n60\n60\n60\n57\n60\n\n\n\n\n\n\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n\n\n\n\n\n\nyear\nregion\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\nprovince\n\n\n\n\n\n\n\n\n\n\n\nAnhui\n12\n12\n12\n12\n12\n12\n12\n\n\nBeijing\n12\n12\n12\n12\n12\n12\n12\n\n\nChongqing\n12\n12\n12\n12\n12\n9\n12\n\n\nFujian\n12\n12\n12\n12\n12\n12\n12\n\n\nGansu\n12\n12\n12\n12\n12\n12\n12\n\n\nGuangdong\n12\n12\n12\n12\n12\n12\n12\n\n\nGuangxi\n12\n12\n12\n12\n12\n12\n12\n\n\nGuizhou\n12\n12\n12\n12\n12\n12\n12\n\n\nHainan\n12\n12\n12\n12\n12\n12\n12\n\n\nHebei\n12\n12\n12\n12\n12\n11\n12\n\n\nHeilongjiang\n12\n12\n12\n12\n12\n12\n12\n\n\nHenan\n12\n12\n12\n12\n12\n12\n12\n\n\nHubei\n12\n12\n12\n12\n12\n12\n12\n\n\nHunan\n12\n12\n12\n12\n12\n12\n12\n\n\nJiangsu\n12\n12\n12\n12\n12\n12\n12\n\n\nJiangxi\n12\n12\n12\n12\n12\n12\n12\n\n\nJilin\n12\n12\n12\n12\n12\n12\n12\n\n\nLiaoning\n12\n12\n12\n12\n12\n12\n12\n\n\nNingxia\n12\n12\n12\n12\n12\n12\n12\n\n\nQinghai\n12\n12\n12\n12\n12\n12\n12\n\n\nShaanxi\n12\n12\n12\n12\n12\n12\n12\n\n\nShandong\n12\n12\n12\n12\n12\n12\n12\n\n\nShanghai\n12\n12\n12\n12\n12\n12\n12\n\n\nShanxi\n12\n12\n12\n12\n12\n12\n12\n\n\nSichuan\n12\n12\n12\n12\n12\n12\n12\n\n\nTianjin\n12\n12\n12\n12\n12\n12\n12\n\n\nTibet\n12\n12\n12\n12\n12\n12\n12\n\n\nXinjiang\n12\n12\n12\n12\n12\n12\n12\n\n\nYunnan\n12\n12\n12\n12\n12\n12\n12\n\n\nZhejiang\n12\n12\n12\n12\n12\n12\n12\n\n\n\n\n\n\n\n\n#df_no_gdp_outliers.pivot_table(index='grouping column 1', columns='grouping column 2', values='aggregating column', aggfunc='sum')\n\n\n#pd.crosstab(df_no_gdp_outliers, 'year')\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\n\n\n\n\n\n\nregion\ngdp\nfdi\nit\n\n\n\n\n0\nEast China\n2093.30\n50661\n631930\n\n\n1\nEast China\n2347.32\n43443\n657860\n\n\n2\nEast China\n2542.96\n27673\n889463\n\n\n3\nEast China\n2712.34\n26131\n1227364\n\n\n4\nEast China\n2902.09\n31847\n1499110\n\n\n...\n...\n...\n...\n...\n\n\n354\nEast China\n8003.67\n307610\n1962633\n\n\n355\nEast China\n9705.02\n498055\n2261631\n\n\n356\nEast China\n11648.70\n668128\n3162299\n\n\n357\nEast China\n13417.68\n772000\n2370200\n\n\n358\nEast China\n15718.47\n888935\n2553268\n\n\n\n\n350 rows × 4 columns\n\n\n\n\ndef aggregate_and_ttest(dataset, groupby_feature='province', alpha=.05, test_cells = [0, 1]):\n    #Imports\n    from tqdm import tqdm\n    from scipy.stats import ttest_ind_from_stats\n\n    \n    metrics = ['gdp', 'fdi', 'it']\n    \n    feature_size = 'size'\n    feature_mean = 'mean'\n    feature_std = 'std'    \n\n    for metric in tqdm(metrics):\n        \n        #print(metric)\n        crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg(['size', 'mean', 'std'])\n        print(crosstab)\n        \n        treatment = crosstab.index[test_cells[0]]\n        control = crosstab.index[test_cells[1]]\n        \n        counts_control = crosstab.loc[control, feature_size]\n        counts_treatment = crosstab.loc[treatment, feature_size]\n\n        mean_control = crosstab.loc[control, feature_mean]\n        mean_treatment = crosstab.loc[treatment, feature_mean]\n\n        standard_deviation_control = crosstab.loc[control, feature_std]\n        standard_deviation_treatment = crosstab.loc[treatment, feature_std]\n        \n        t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control)\n        \n        #fstring to print the p value and t statistic\n        print(f\"The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} for the metric of {metric} is {t_statistic} and the p value is {p_value}.\")\n        \n        #f string to say of the comparison is significant at a given alpha level\n\n        if p_value &lt; alpha: \n            print(f'The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}') \n        else: \n            print(f'The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}')\n\n\naggregate_and_ttest(df_no_gdp_outliers, test_cells = [0,2])\n\n100%|██████████| 3/3 [00:00&lt;00:00, 73.44it/s]\n\n\n              size          mean          std\nprovince                                     \nAnhui           12   3905.870000  1657.186350\nBeijing         12   4673.453333  2585.218431\nChongqing       12   2477.712500  1073.374101\nFujian          12   4864.023333  2065.665290\nGansu           12   1397.832500   628.751284\nGuangdong        8  10564.827500  3076.928885\nGuangxi         12   2924.104167  1316.680079\nGuizhou         12   1422.010833   679.163186\nHainan          12    686.714167   277.167010\nHebei           12   6936.825000  3266.776349\nHeilongjiang    12   4041.241667  1531.676708\nHenan           12   7208.966667  3669.236184\nHubei           12   4772.503333  2121.833184\nHunan           12   4765.891667  2159.588877\nJiangsu         10   8880.142000  3069.858941\nJiangxi         12   2460.782500  1125.673920\nJilin           12   2274.854167   975.812431\nLiaoning        12   5231.135000  1988.700441\nNingxia         12    432.268333   224.934621\nQinghai         12    383.099167   194.618478\nShaanxi         12   2658.034167  1461.540671\nShandong         9   9093.784444  2952.172758\nShanghai        12   6432.454167  3049.477185\nShanxi          12   2817.210833  1531.856025\nSichuan         12   5377.790000  2412.985532\nTianjin         12   2528.665000  1367.201360\nTibet           12    170.426667    88.715089\nXinjiang        12   1828.896667   848.752092\nYunnan          12   2604.054167  1016.828525\nZhejiang        11   8264.008182  3870.124534\nThe t statistic of the comparison of the treatment test cell of Anhui compared to the control test cell of Chongqing for the metric of gdp is 2.505668475205307 and the p value is 0.020116468101911197.\nThe comparison between Anhui and Chongqing is statistically significant at the threshold of 0.05\n              size          mean            std\nprovince                                       \nAnhui           12  7.095308e+04   78371.990245\nBeijing         12  2.573693e+05  121078.451044\nChongqing       12  4.112783e+04   25850.251481\nFujian          12  3.744664e+05   65608.304198\nGansu           12  5.295500e+03    2941.514777\nGuangdong        8  1.117272e+06  137741.790514\nGuangxi         12  5.514783e+04   19725.422944\nGuizhou         12  5.812333e+03    3337.775071\nHainan          12  6.436600e+04   19972.968837\nHebei           12  1.322308e+05   56541.699667\nHeilongjiang    12  8.271933e+04   62818.132171\nHenan           12  9.442600e+04   78299.340203\nHubei           12  1.497132e+05   70692.266346\nHunan           12  1.321102e+05   86224.990870\nJiangsu         10  7.422869e+05  257982.284030\nJiangxi         12  1.037352e+05   94052.957802\nJilin           12  4.122658e+04   16166.473875\nLiaoning        12  2.859253e+05  152318.549543\nNingxia         12  3.950417e+03    3662.459319\nQinghai         12  1.098408e+04   12241.262884\nShaanxi         12  5.089258e+04   29331.097377\nShandong         9  3.943093e+05  219313.559118\nShanghai        12  5.082483e+05  166880.730080\nShanxi          12  3.862883e+04   32974.368539\nSichuan         12  6.219717e+04   39329.514938\nTianjin         12  2.501733e+05  119418.314501\nTibet           12  8.397500e+02     922.467750\nXinjiang        12  4.433083e+03    3630.847471\nYunnan          12  1.704833e+04    9213.888976\nZhejiang        11  3.704169e+05  286211.503281\nThe t statistic of the comparison of the treatment test cell of Anhui compared to the control test cell of Chongqing for the metric of fdi is 1.251953695962862 and the p value is 0.2237342006262051.\nThe comparison between Anhui and Chongqing is not statistically significant at the threshold of 0.05\n              size          mean           std\nprovince                                      \nAnhui           12  2.649674e+06  1.966030e+06\nBeijing         12  1.175965e+06  4.944598e+05\nChongqing       12  1.636146e+06  1.155977e+06\nFujian          12  1.274117e+06  6.641800e+05\nGansu           12  2.045347e+06  1.432134e+06\nGuangdong        8  2.269997e+06  9.845008e+05\nGuangxi         12  2.326539e+06  1.691126e+06\nGuizhou         12  2.132636e+06  1.553924e+06\nHainan          12  5.404872e+05  4.064191e+05\nHebei           12  2.944163e+06  2.160958e+06\nHeilongjiang    12  3.230451e+06  2.227509e+06\nHenan           12  3.671971e+06  2.987163e+06\nHubei           12  2.904660e+06  2.189358e+06\nHunan           12  3.215128e+06  2.351869e+06\nJiangsu         10  1.563339e+06  8.478538e+05\nJiangxi         12  1.760613e+06  1.208039e+06\nJilin           12  2.136635e+06  1.375026e+06\nLiaoning        12  2.628358e+06  1.563158e+06\nNingxia         12  7.872062e+05  5.531448e+05\nQinghai         12  9.600921e+05  6.721767e+05\nShaanxi         12  2.474031e+06  1.786697e+06\nShandong         9  1.965966e+06  8.946962e+05\nShanghai        12  1.569143e+06  6.996706e+05\nShanxi          12  1.983718e+06  1.491559e+06\nSichuan         12  4.016480e+06  2.923696e+06\nTianjin         12  8.310284e+05  4.641450e+05\nTibet           12  1.174176e+06  7.959846e+05\nXinjiang        12  2.251012e+06  1.612187e+06\nYunnan          12  3.165419e+06  1.652014e+06\nZhejiang        11  1.648032e+06  8.385722e+05\nThe t statistic of the comparison of the treatment test cell of Anhui compared to the control test cell of Chongqing for the metric of it is 1.5394289719091268 and the p value is 0.13796027793319976.\nThe comparison between Anhui and Chongqing is not statistically significant at the threshold of 0.05\n\n\n\nEastvNorth=pd.DataFrame()\nEastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\nEastvNorth\n\n100%|██████████| 3/3 [00:00&lt;00:00, 138.11it/s]\n\n\n                     size         mean          std\nregion                                             \nEast China             78  6070.604231  3500.372702\nNorth China            48  4239.038542  2866.705149\nNortheast China        36  3849.076944  1948.531835\nNorthwest China        60  1340.026167  1174.399739\nSouth Central China    68  4835.540882  3697.129915\nSouthwest China        60  2410.398833  2144.589994\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.0028085413359212334.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size           mean            std\nregion                                                 \nEast China             78  355577.897436  275635.866746\nNorth China            48  169600.583333  127011.475909\nNortheast China        36  136623.750000  142734.495232\nNorthwest China        60   15111.133333   22954.193559\nSouth Central China    68  218931.426471  339981.399823\nSouthwest China        60   25405.083333   31171.373876\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size          mean           std\nregion                                               \nEast China             78  1.775615e+06  1.153030e+06\nNorth China            48  1.733719e+06  1.548794e+06\nNortheast China        36  2.665148e+06  1.768442e+06\nNorthwest China        60  1.703538e+06  1.446408e+06\nSouth Central China    68  2.500962e+06  2.196436e+06\nSouthwest China        60  2.424971e+06  2.002198e+06\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372.\nThe comparison between East China and North China is not statistically significant at the threshold of 0.05\n\n\n\nimport numpy as np\nimport bootstrapped.bootstrap as bs\nimport bootstrapped.stats_functions as bs_stats\n\n\ntest_1=df_no_gdp_outliers[df_no_gdp_outliers['province']=='Beijing']\ntest=test_1['gdp'].to_numpy()\ntest\n\ncontrol_1=df_no_gdp_outliers[df_no_gdp_outliers['province']=='Shanxi']\ncontrol=control_1['gdp'].to_numpy()\ncontrol\n\narray([1292.11, 1476.  , 1611.08, 1667.1 , 1845.72, 2029.53, 2324.8 ,\n       2855.23, 3571.37, 4230.53, 4878.61, 6024.45])\n\n\n\nbins = np.linspace(0, 40, 20)\n\nplt.hist(control, label='Control')\nplt.hist(test, label='Test', color='orange')\nplt.title('Test/Ctrl Data')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7faf8596a190&gt;\n\n\n\n\n\n\nbs.bootstrap_ab(test, control, stat_func=bs_stats.sum, compare_func=bs_compare.percent_change)\n\n65.88937107712621    (-19.58634300490877, 124.01876332252021)\n\n\n\n# run an a/b test simulation considering the lengths of the series (sum)\n# consider the full 'volume' of values that are passed in\n\nprint(bs_compare.percent_change(test.sum(), control.sum()))\n\nprint(bs.bootstrap_ab(\n    test, \n    control, \n    stat_func=bs_stats.sum,\n    compare_func=bs_compare.percent_change\n))\n\n65.88937107712621\n65.88937107712621    (-20.064956167313596, 124.54556877143521)\n\n\n\n# run an a/b test simulation ignoring the lengths of the series (average)\n# just what is the 'typical' value\n# use percent change to compare test and control\n\nprint(bs_compare.difference(test.mean(), control.mean()))\n\n1856.2424999999998\n\n\n\nprint(bs.bootstrap_ab(test, control, bs_stats.mean, bs_compare.difference))\n\n1856.2424999999998    (218.25606250000146, 3411.9760624999994)"
  },
  {
    "objectID": "posts/2020-12-09-Health-Data-EDA.html",
    "href": "posts/2020-12-09-Health-Data-EDA.html",
    "title": "EDA on Healthcare Data",
    "section": "",
    "text": "Credit: code from https://towardsdatascience.com/step-by-step-exploratory-data-analysis-on-stroke-dataset-840aefea8739 and https://www.kaggle.com/lirilkumaramal/heart-stroke\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n30669\nMale\n3.0\n0\n0\nNo\nchildren\nRural\n95.12\n18.0\nNaN\n0\n\n\n1\n30468\nMale\n58.0\n1\n0\nYes\nPrivate\nUrban\n87.96\n39.2\nnever smoked\n0\n\n\n2\n16523\nFemale\n8.0\n0\n0\nNo\nPrivate\nUrban\n110.89\n17.6\nNaN\n0\n\n\n3\n56543\nFemale\n70.0\n0\n0\nYes\nPrivate\nRural\n69.04\n35.9\nformerly smoked\n0\n\n\n4\n46136\nMale\n14.0\n0\n0\nNo\nNever_worked\nRural\n161.28\n19.1\nNaN\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n43395\n56196\nFemale\n10.0\n0\n0\nNo\nchildren\nUrban\n58.64\n20.4\nnever smoked\n0\n\n\n43396\n5450\nFemale\n56.0\n0\n0\nYes\nGovt_job\nUrban\n213.61\n55.4\nformerly smoked\n0\n\n\n43397\n28375\nFemale\n82.0\n1\n0\nYes\nPrivate\nUrban\n91.94\n28.9\nformerly smoked\n0\n\n\n43398\n27973\nMale\n40.0\n0\n0\nYes\nPrivate\nUrban\n99.16\n33.2\nnever smoked\n0\n\n\n43399\n36271\nFemale\n82.0\n0\n0\nYes\nPrivate\nUrban\n79.48\n20.6\nnever smoked\n0\n\n\n\n\n43400 rows × 12 columns\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 43400 entries, 0 to 43399\nData columns (total 12 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   id                 43400 non-null  int64  \n 1   gender             43400 non-null  object \n 2   age                43400 non-null  float64\n 3   hypertension       43400 non-null  int64  \n 4   heart_disease      43400 non-null  int64  \n 5   ever_married       43400 non-null  object \n 6   work_type          43400 non-null  object \n 7   Residence_type     43400 non-null  object \n 8   avg_glucose_level  43400 non-null  float64\n 9   bmi                41938 non-null  float64\n 10  smoking_status     30108 non-null  object \n 11  stroke             43400 non-null  int64  \ndtypes: float64(3), int64(4), object(5)\nmemory usage: 4.0+ MB\n\n\n\ndf['stroke'].value_counts()\n\n0    42617\n1      783\nName: stroke, dtype: int64\n\n\n\n# labeled target is unbalanced\n\n\n# Drop the id column\ndf.drop(columns=['id'], inplace=True)\n\n\n# Showing records where patient suffered from stroke but had missing value in bmi attribute.\ndf[df['bmi'].isna() & df['stroke'] == 1]\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n81\nFemale\n61.0\n0\n0\nYes\nSelf-employed\nRural\n202.21\nNaN\nnever smoked\n1\n\n\n407\nFemale\n59.0\n0\n0\nYes\nPrivate\nRural\n76.15\nNaN\nNaN\n1\n\n\n747\nMale\n78.0\n0\n1\nYes\nPrivate\nUrban\n219.84\nNaN\nNaN\n1\n\n\n1139\nMale\n57.0\n0\n1\nNo\nGovt_job\nUrban\n217.08\nNaN\nNaN\n1\n\n\n1613\nMale\n58.0\n0\n0\nYes\nPrivate\nRural\n189.84\nNaN\nNaN\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n42530\nMale\n66.0\n0\n0\nYes\nSelf-employed\nUrban\n182.89\nNaN\nnever smoked\n1\n\n\n42839\nFemale\n67.0\n1\n0\nYes\nGovt_job\nUrban\n234.43\nNaN\nnever smoked\n1\n\n\n43007\nFemale\n69.0\n0\n1\nYes\nSelf-employed\nRural\n89.19\nNaN\nsmokes\n1\n\n\n43100\nMale\n67.0\n0\n0\nYes\nSelf-employed\nUrban\n136.79\nNaN\nsmokes\n1\n\n\n43339\nFemale\n76.0\n0\n0\nNo\nPrivate\nRural\n100.55\nNaN\nnever smoked\n1\n\n\n\n\n140 rows × 11 columns\n\n\n\n\n# Replace the missing values with mean of bmi attribute\ndf['bmi'].fillna(np.round(df['bmi'].mean(), 1), inplace = True)\n\n\n# Create a new category named 'not known'\ndf['smoking_status'].fillna('not known', inplace=True)\nprint(df['smoking_status'].value_counts())\n\nnever smoked       16053\nnot known          13292\nformerly smoked     7493\nsmokes              6562\nName: smoking_status, dtype: int64\n\n\n\n# Discretize with respective equal-width bin\ndf['age_binned'] = pd.cut(df['age'], np.arange(0, 91, 5))\ndf['avg_glucose_level_binned'] = pd.cut(df['avg_glucose_level'], np.arange(0, 301, 10))\ndf['bmi_binned'] = pd.cut(df['bmi'], np.arange(0, 101, 5))\n\nSettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['age_binned'] = pd.cut(df['age'], np.arange(0, 91, 5))\n&lt;ipython-input-23-b4c4cb89b1bd&gt;:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['avg_glucose_level_binned'] = pd.cut(df['avg_glucose_level'], np.arange(0, 301, 10))\n&lt;ipython-input-23-b4c4cb89b1bd&gt;:4: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  df['bmi_binned'] = pd.cut(df['bmi'], np.arange(0, 101, 5))\n\n\n\n# Create the correlation heatmap\nheatmap = sns.heatmap(df[['age_norm', 'avg_glucose_level_norm', 'bmi_norm']].corr(), vmin=-1, vmax=1, annot=True)\n# Create the title\nheatmap.set_title('Correlation Heatmap');\n\n\n\n\n\ndef get_stacked_bar_chart(column):\n    # Get the count of records by column and stroke    \n    df_pct = df.groupby([column, 'stroke'])['age'].count()\n    # Create proper DataFrame's format\n    df_pct = df_pct.unstack()    \n    return df_pct.plot.bar(stacked=True, figsize=(6,6), width=1);\n\n\ndef get_100_percent_stacked_bar_chart(column, width = 0.5):\n    # Get the count of records by column and stroke\n    df_breakdown = df.groupby([column, 'stroke'])['age'].count()\n    # Get the count of records by gender\n    df_total = df.groupby([column])['age'].count()\n    # Get the percentage for 100% stacked bar chart\n    df_pct = df_breakdown / df_total * 100\n    # Create proper DataFrame's format\n    df_pct = df_pct.unstack()\n    return df_pct.plot.bar(stacked=True, figsize=(6,6), width=width);\n\n\n# Age related to risk\nget_stacked_bar_chart('age_binned')\n\n&lt;AxesSubplot:xlabel='age_binned'&gt;\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('age_binned', width = 0.9)\n\n&lt;AxesSubplot:xlabel='age_binned'&gt;\n\n\n\n\n\n\nget_stacked_bar_chart('bmi_binned')\nget_100_percent_stacked_bar_chart('bmi_binned', width = 0.9)\n\n&lt;AxesSubplot:xlabel='bmi_binned'&gt;\n\n\n\n\n\n\n\n\n\nget_stacked_bar_chart('avg_glucose_level_binned')\nget_100_percent_stacked_bar_chart('avg_glucose_level_binned', width = 0.9)\n\n&lt;AxesSubplot:xlabel='avg_glucose_level_binned'&gt;\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('hypertension')\nget_100_percent_stacked_bar_chart('heart_disease')\n\n&lt;AxesSubplot:xlabel='heart_disease'&gt;\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('gender')\nget_100_percent_stacked_bar_chart('Residence_type')\n\n&lt;AxesSubplot:xlabel='Residence_type'&gt;\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('work_type')\ndf.groupby(['work_type'])[['age']].agg(['count', 'mean'])\n\n\n\n\n\n\n\n\nage\n\n\n\ncount\nmean\n\n\nwork_type\n\n\n\n\n\n\nGovt_job\n5438\n49.098750\n\n\nNever_worked\n177\n17.757062\n\n\nPrivate\n24827\n45.016837\n\n\nSelf-employed\n6793\n59.307817\n\n\nchildren\n6154\n6.698018\n\n\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('ever_married')\ndf.groupby(['ever_married'])[['age']].agg(['count', 'mean'])\n\n\n\n\n\n\n\n\nage\n\n\n\ncount\nmean\n\n\never_married\n\n\n\n\n\n\nNo\n15456\n21.237487\n\n\nYes\n27933\n53.829735\n\n\n\n\n\n\n\n\n\n\n\ng = sns.catplot(x=\"Residence_type\", hue=\"smoking_status\", col=\"work_type\",\n                data=df, kind=\"count\",\n                height=4, aspect=.7)\n\n\n\n\n\nmissingno.matrix(df, figsize = (30,5))\n\nNameError: name 'missingno' is not defined\n\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(25,7))\n\nfig.suptitle(\"Countplot for the dataset\", fontsize=35)\n\nsns.countplot(x=\"gender\", data=df,ax=ax1)\nsns.countplot(x=\"stroke\", data=df,ax=ax2)\nsns.countplot(x=\"ever_married\", data=df,ax=ax3)\nsns.countplot(x=\"hypertension\", data=df,ax=ax4)\n\n&lt;AxesSubplot:xlabel='hypertension', ylabel='count'&gt;\n\n\n\n\n\n\nsns.displot(x=\"age\", data=df, kind=\"kde\", hue=\"gender\", col=\"smoking_status\", row=\"Residence_type\")\n\n\n\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,7))\nfig.suptitle(\"Boxplot for Dataset\", fontsize=35)\n\nsns.boxplot(x=\"stroke\", y=\"avg_glucose_level\", data=df,ax=ax1)\nsns.boxplot(x=\"stroke\", y=\"bmi\", data=df,ax=ax2)\nsns.boxplot(x=\"stroke\", y=\"age\", data=df,ax=ax3)\n\n&lt;AxesSubplot:xlabel='stroke', ylabel='age'&gt;\n\n\n\n\n\n\n# Compute a correlation matrix and convert to long-form\ncorr_mat = df.corr(\"kendall\").stack().reset_index(name=\"correlation\")\n\n# Draw each cell as a scatter point with varying size and color\ng = sns.relplot(\n    data=corr_mat,\n    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n    palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n    height=5, sizes=(50, 250), size_norm=(-.2, .8),\n)\n\n# Tweak the figure to finalize\ng.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\ng.despine(left=True, bottom=True)\ng.ax.margins(0.25)\nfor label in g.ax.get_xticklabels():\n    label.set_rotation(90)\nfor artist in g.legend.legendHandles:\n    artist.set_edgecolor(\".1\")\n\n\n\n\n\nstrokes_temp_df=df\nstrokes_temp_df[['stroke','hypertension']] = df[['stroke','hypertension']].astype('int')\ncorr = strokes_temp_df.corr()\ncorr.style.background_gradient()\ncorr.style.background_gradient().set_precision(2)\n\n\n\n\n\n\nage\nhypertension\nheart_disease\navg_glucose_level\nbmi\nstroke\nage_norm\navg_glucose_level_norm\nbmi_norm\n\n\n\n\nage\n1.00\n0.27\n0.25\n0.24\n0.35\n0.16\n1.00\n0.24\n0.35\n\n\nhypertension\n0.27\n1.00\n0.12\n0.16\n0.15\n0.08\n0.27\n0.16\n0.15\n\n\nheart_disease\n0.25\n0.12\n1.00\n0.15\n0.05\n0.11\n0.25\n0.15\n0.05\n\n\navg_glucose_level\n0.24\n0.16\n0.15\n1.00\n0.18\n0.08\n0.24\n1.00\n0.18\n\n\nbmi\n0.35\n0.15\n0.05\n0.18\n1.00\n0.02\n0.35\n0.18\n1.00\n\n\nstroke\n0.16\n0.08\n0.11\n0.08\n0.02\n1.00\n0.16\n0.08\n0.02\n\n\nage_norm\n1.00\n0.27\n0.25\n0.24\n0.35\n0.16\n1.00\n0.24\n0.35\n\n\navg_glucose_level_norm\n0.24\n0.16\n0.15\n1.00\n0.18\n0.08\n0.24\n1.00\n0.18\n\n\nbmi_norm\n0.35\n0.15\n0.05\n0.18\n1.00\n0.02\n0.35\n0.18\n1.00"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-pipelines-example.html",
    "href": "posts/2021-05-31-sklearn-pipelines-example.html",
    "title": "sklearn-pipelines-example",
    "section": "",
    "text": "# code adapted from https://github.com/thomasjpfan/ml-workshop-intro\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-pipelines-example.html#make-pipeline",
    "href": "posts/2021-05-31-sklearn-pipelines-example.html#make-pipeline",
    "title": "sklearn-pipelines-example",
    "section": "Make pipeline!",
    "text": "Make pipeline!\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import KNeighborsRegressor\n\nknr = make_pipeline(\n    StandardScaler(), KNeighborsRegressor()\n)\nknr.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsregressor', KNeighborsRegressor())])StandardScalerStandardScaler()KNeighborsRegressorKNeighborsRegressor()\n\n\n\nknr.score(X_test, y_test)\n\n-0.16833369709565682\n\n\n\nfrom sklearn.preprocessing import PolynomialFeatures\n\nknr_select = make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(),\n    KNeighborsRegressor()\n)\nknr_select.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('polynomialfeatures', PolynomialFeatures()),\n                ('kneighborsregressor', KNeighborsRegressor())])StandardScalerStandardScaler()PolynomialFeaturesPolynomialFeatures()KNeighborsRegressorKNeighborsRegressor()\n\n\n\nknr_select.score(X_test, y_test)\n\n-0.1635984889204516\n\n\n\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer(as_frame=True)\n\nX, y = cancer.data, cancer.target\n\ny.value_counts()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y,\n                                                   random_state=0)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = make_pipeline(\n    StandardScaler(),\n    LogisticRegression(random_state=0)\n)\n\nlog_reg.fit(X_train, y_train)\n\nlog_reg.score(X_test, y_test)\n\nlog_reg_poly = make_pipeline(\n    StandardScaler(),\n    PolynomialFeatures(),\n    LogisticRegression(random_state=0)\n)\n\nlog_reg_poly.fit(X_train, y_train)\n\nlog_reg_poly.score(X_test, y_test)\n\n0.965034965034965"
  },
  {
    "objectID": "posts/2020-08-30-NLP with Pyspark.html",
    "href": "posts/2020-08-30-NLP with Pyspark.html",
    "title": "NLP with Pyspark",
    "section": "",
    "text": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks.\nfrom pyspark.ml.feature import Tokenizer, RegexTokenizer\nfrom pyspark.sql.functions import col, udf\nfrom pyspark.sql.types import IntegerType\nsentenceDataFrame = spark.createDataFrame([\n    (0, \"Hi I heard about Spark\"),\n    (1, \"I wish Java could use case classes\"),\n    (2, \"Logistic,regression,models,are,neat\")\n], [\"id\", \"sentence\"])\nsentenceDataFrame.show()\n\n\n+---+--------------------+\n id|            sentence|\n+---+--------------------+\n  0|Hi I heard about ...|\n  1|I wish Java could...|\n  2|Logistic,regressi...|\n+---+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-NLP with Pyspark.html#using-tokenizer-and-regextokenizer",
    "href": "posts/2020-08-30-NLP with Pyspark.html#using-tokenizer-and-regextokenizer",
    "title": "NLP with Pyspark",
    "section": "Using Tokenizer and RegexTokenizer",
    "text": "Using Tokenizer and RegexTokenizer\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n\nregexTokenizer = RegexTokenizer(inputCol=\"sentence\", outputCol=\"words\", pattern=\"\\\\W\")\n# alternatively, pattern=\"\\\\w+\", gaps(False)\n\ncountTokens = udf(lambda words: len(words), IntegerType())\n\ntokenized = tokenizer.transform(sentenceDataFrame)\ntokenized.select(\"sentence\", \"words\")\\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\nregexTokenized = regexTokenizer.transform(sentenceDataFrame)\nregexTokenized.select(\"sentence\", \"words\") \\\n    .withColumn(\"tokens\", countTokens(col(\"words\"))).show(truncate=False)\n\n\n+-----------------------------------+------------------------------------------+------+\nsentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\nHi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\nI wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\nLogistic,regression,models,are,neat|[logistic,regression,models,are,neat]     |1     |\n+-----------------------------------+------------------------------------------+------+\n\n+-----------------------------------+------------------------------------------+------+\nsentence                           |words                                     |tokens|\n+-----------------------------------+------------------------------------------+------+\nHi I heard about Spark             |[hi, i, heard, about, spark]              |5     |\nI wish Java could use case classes |[i, wish, java, could, use, case, classes]|7     |\nLogistic,regression,models,are,neat|[logistic, regression, models, are, neat] |5     |\n+-----------------------------------+------------------------------------------+------+"
  },
  {
    "objectID": "posts/2020-08-30-NLP with Pyspark.html#removing-stop-words",
    "href": "posts/2020-08-30-NLP with Pyspark.html#removing-stop-words",
    "title": "NLP with Pyspark",
    "section": "Removing Stop Words",
    "text": "Removing Stop Words\n\nfrom pyspark.ml.feature import StopWordsRemover\n\nsentenceData = spark.createDataFrame([\n    (0, [\"I\", \"saw\", \"the\", \"red\", \"balloon\"]),\n    (1, [\"Mary\", \"had\", \"a\", \"little\", \"lamb\"])\n], [\"id\", \"raw\"])\n\nremover = StopWordsRemover(inputCol=\"raw\", outputCol=\"filtered\")\nremover.transform(sentenceData).show(truncate=False)\n\n\n+---+----------------------------+--------------------+\nid |raw                         |filtered            |\n+---+----------------------------+--------------------+\n0  |[I, saw, the, red, balloon] |[saw, red, balloon] |\n1  |[Mary, had, a, little, lamb]|[Mary, little, lamb]|\n+---+----------------------------+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-NLP with Pyspark.html#n-grams",
    "href": "posts/2020-08-30-NLP with Pyspark.html#n-grams",
    "title": "NLP with Pyspark",
    "section": "n-grams",
    "text": "n-grams\n\nfrom pyspark.ml.feature import NGram\n\nwordDataFrame = spark.createDataFrame([\n    (0, [\"Hi\", \"I\", \"heard\", \"about\", \"Spark\"]),\n    (1, [\"I\", \"wish\", \"Java\", \"could\", \"use\", \"case\", \"classes\"]),\n    (2, [\"Logistic\", \"regression\", \"models\", \"are\", \"neat\"])\n], [\"id\", \"words\"])\n\nngram = NGram(n=2, inputCol=\"words\", outputCol=\"ngrams\")\n\nngramDataFrame = ngram.transform(wordDataFrame)\nngramDataFrame.select(\"ngrams\").show(truncate=False)\n\n\n+------------------------------------------------------------------+\nngrams                                                            |\n+------------------------------------------------------------------+\n[Hi I, I heard, heard about, about Spark]                         |\n[I wish, wish Java, Java could, could use, use case, case classes]|\n[Logistic regression, regression models, models are, are neat]    |\n+------------------------------------------------------------------+\n\n\n\n\n\nfrom pyspark.ml.feature import HashingTF, IDF, Tokenizer\n\nsentenceData = spark.createDataFrame([\n    (0.0, \"Hi I heard about Spark\"),\n    (0.0, \"I wish Java could use case classes\"),\n    (1.0, \"Logistic regression models are neat\")\n], [\"label\", \"sentence\"])\n\nsentenceData.show()\n\n\n+-----+--------------------+\nlabel|            sentence|\n+-----+--------------------+\n  0.0|Hi I heard about ...|\n  0.0|I wish Java could...|\n  1.0|Logistic regressi...|\n+-----+--------------------+\n\n\n\n\n\ntokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\nwordsData = tokenizer.transform(sentenceData)\nwordsData.show()\n\n\n+-----+--------------------+--------------------+\nlabel|            sentence|               words|\n+-----+--------------------+--------------------+\n  0.0|Hi I heard about ...|[hi, i, heard, ab...|\n  0.0|I wish Java could...|[i, wish, java, c...|\n  1.0|Logistic regressi...|[logistic, regres...|\n+-----+--------------------+--------------------+\n\n\n\n\n\nhashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\nfeaturizedData = hashingTF.transform(wordsData)\n# alternatively, CountVectorizer can also be used to get term frequency vectors\n\nidf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\nidfModel = idf.fit(featurizedData)\nrescaledData = idfModel.transform(featurizedData)\n\nrescaledData.select(\"label\", \"features\").show()\n\n\n+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(20,[6,8,13,16],[...|\n  0.0|(20,[0,2,7,13,15,...|\n  1.0|(20,[3,4,6,11,19]...|\n+-----+--------------------+"
  },
  {
    "objectID": "posts/2020-08-30-NLP with Pyspark.html#countvectorizer",
    "href": "posts/2020-08-30-NLP with Pyspark.html#countvectorizer",
    "title": "NLP with Pyspark",
    "section": "CountVectorizer",
    "text": "CountVectorizer\n\nfrom pyspark.ml.feature import CountVectorizer\n\n# Input data: Each row is a bag of words with a ID.\ndf = spark.createDataFrame([\n    (0, \"a b c\".split(\" \")),\n    (1, \"a b b c a\".split(\" \"))\n], [\"id\", \"words\"])\n\n# fit a CountVectorizerModel from the corpus.\ncv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n\nmodel = cv.fit(df)\n\nresult = model.transform(df)\nresult.show(truncate=False)\n\n\n+---+---------------+-------------------------+\nid |words          |features                 |\n+---+---------------+-------------------------+\n0  |[a, b, c]      |(3,[0,1,2],[1.0,1.0,1.0])|\n1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,2.0,1.0])|\n+---+---------------+-------------------------+\n\n\n\n\n\ndf = spark.read.load(\"/FileStore/tables/SMSSpamCollection\",\n                     format=\"csv\", sep=\"\\t\", inferSchema=\"true\", header=\"false\")\n\n\n\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: string (nullable = true)\n-- _c1: string (nullable = true)\n\n\n\n\n\ndata = df.withColumnRenamed('_c0','class').withColumnRenamed('_c1','text')\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)"
  },
  {
    "objectID": "posts/2020-08-30-NLP with Pyspark.html#clean-and-prepare-the-data",
    "href": "posts/2020-08-30-NLP with Pyspark.html#clean-and-prepare-the-data",
    "title": "NLP with Pyspark",
    "section": "Clean and Prepare the Data",
    "text": "Clean and Prepare the Data\n\nfrom pyspark.sql.functions import length\n\n\n\n\n\n\ndata = data.withColumn('length',length(data['text']))\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)\n-- length: integer (nullable = true)\n\n\n\n\n\n# Pretty Clear Difference\ndata.groupby('class').mean().show()\n\n\n+-----+-----------------+\nclass|      avg(length)|\n+-----+-----------------+\n  ham| 71.4545266210897|\n spam|138.6706827309237|\n+-----+-----------------+\n\n\n\n\n\nfrom pyspark.ml.feature import Tokenizer,StopWordsRemover, CountVectorizer,IDF,StringIndexer\n\ntokenizer = Tokenizer(inputCol=\"text\", outputCol=\"token_text\")\nstopremove = StopWordsRemover(inputCol='token_text',outputCol='stop_tokens')\ncount_vec = CountVectorizer(inputCol='stop_tokens',outputCol='c_vec')\nidf = IDF(inputCol=\"c_vec\", outputCol=\"tf_idf\")\nham_spam_to_num = StringIndexer(inputCol='class',outputCol='label')\n\n\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.linalg import Vector\n\n\n\n\n\n\nclean_up = VectorAssembler(inputCols=['tf_idf','length'],outputCol='features')\n\n\n\n\n\n\nNaive Bayes\n\nfrom pyspark.ml.classification import NaiveBayes\n\n\n\n\n\n\n# Use defaults\nnb = NaiveBayes()\n\n\n\n\n\n\n### Pipeline\n\n\nfrom pyspark.ml import Pipeline\n\n\n\n\n\n\ndata_prep_pipe = Pipeline(stages=[ham_spam_to_num,tokenizer,stopremove,count_vec,idf,clean_up])\n\n\n\n\n\n\ncleaner = data_prep_pipe.fit(data)\n\n\n\n\n\n\nclean_data = cleaner.transform(data)\n\n\n\n\n\n\n\nTraining and Evaluation\n\nclean_data = clean_data.select(['label','features'])\n\n\n\n\n\n\nclean_data.show()\n\n\n+-----+--------------------+\nlabel|            features|\n+-----+--------------------+\n  0.0|(13424,[7,11,31,6...|\n  0.0|(13424,[0,24,297,...|\n  1.0|(13424,[2,13,19,3...|\n  0.0|(13424,[0,70,80,1...|\n  0.0|(13424,[36,134,31...|\n  1.0|(13424,[10,60,139...|\n  0.0|(13424,[10,53,103...|\n  0.0|(13424,[125,184,4...|\n  1.0|(13424,[1,47,118,...|\n  1.0|(13424,[0,1,13,27...|\n  0.0|(13424,[18,43,120...|\n  1.0|(13424,[8,17,37,8...|\n  1.0|(13424,[13,30,47,...|\n  0.0|(13424,[39,96,217...|\n  0.0|(13424,[552,1697,...|\n  1.0|(13424,[30,109,11...|\n  0.0|(13424,[82,214,47...|\n  0.0|(13424,[0,2,49,13...|\n  0.0|(13424,[0,74,105,...|\n  1.0|(13424,[4,30,33,5...|\n+-----+--------------------+\nonly showing top 20 rows\n\n\n\n\n\n(training,testing) = clean_data.randomSplit([0.7,0.3])\n\n\n\n\n\n\nspam_predictor = nb.fit(training)\n\n\n\n\n\n\ndata.printSchema()\n\n\nroot\n-- class: string (nullable = true)\n-- text: string (nullable = true)\n-- length: integer (nullable = true)\n\n\n\n\n\ntest_results = spam_predictor.transform(testing)\n\n\n\n\n\n\ntest_results.show()\n\n\n+-----+--------------------+--------------------+--------------------+----------+\nlabel|            features|       rawPrediction|         probability|prediction|\n+-----+--------------------+--------------------+--------------------+----------+\n  0.0|(13424,[0,1,2,13,...|[-605.26168264963...|[1.0,7.3447866033...|       0.0|\n  0.0|(13424,[0,1,2,41,...|[-1063.2170425771...|[1.0,9.8700382552...|       0.0|\n  0.0|(13424,[0,1,3,9,1...|[-569.95657733189...|[1.0,1.4498595638...|       0.0|\n  0.0|(13424,[0,1,5,15,...|[-998.87457222776...|[1.0,5.4020023412...|       0.0|\n  0.0|(13424,[0,1,7,15,...|[-658.37986687391...|[1.0,2.6912246466...|       0.0|\n  0.0|(13424,[0,1,14,31...|[-217.18809411711...|[1.0,3.3892033063...|       0.0|\n  0.0|(13424,[0,1,14,78...|[-688.50251926938...|[1.0,8.6317783323...|       0.0|\n  0.0|(13424,[0,1,17,19...|[-809.51840544334...|[1.0,1.3686507989...|       0.0|\n  0.0|(13424,[0,1,27,35...|[-1472.6804140726...|[0.99999999999983...|       0.0|\n  0.0|(13424,[0,1,31,43...|[-341.31126583915...|[1.0,3.4983325940...|       0.0|\n  0.0|(13424,[0,1,46,17...|[-1137.4942938439...|[5.99448563047616...|       1.0|\n  0.0|(13424,[0,1,72,10...|[-704.77256939631...|[1.0,1.2592610663...|       0.0|\n  0.0|(13424,[0,1,874,1...|[-96.404593207515...|[0.99999996015865...|       0.0|\n  0.0|(13424,[0,1,874,1...|[-98.086094104500...|[0.99999996999685...|       0.0|\n  0.0|(13424,[0,2,3,4,6...|[-1289.3891411076...|[1.0,1.3408017664...|       0.0|\n  0.0|(13424,[0,2,3,5,6...|[-2561.6651406471...|[1.0,2.6887776075...|       0.0|\n  0.0|(13424,[0,2,3,5,3...|[-490.88944126371...|[1.0,9.6538338828...|       0.0|\n  0.0|(13424,[0,2,4,5,1...|[-2493.1672898653...|[1.0,9.4058507096...|       0.0|\n  0.0|(13424,[0,2,4,7,2...|[-517.23267032348...|[1.0,2.8915589432...|       0.0|\n  0.0|(13424,[0,2,4,8,2...|[-1402.5570102185...|[1.0,6.7531061115...|       0.0|\n+-----+--------------------+--------------------+--------------------+----------+\nonly showing top 20 rows\n\n\n\n\n\n## Evaluating Model Accuracy\n\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n\n\n\n\nacc_eval = MulticlassClassificationEvaluator()\nacc = acc_eval.evaluate(test_results)\nprint(\"Accuracy of model at predicting spam was: {}\".format(acc))\n\n\nAccuracy of model at predicting spam was: 0.9204435112848836"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html",
    "title": "CausalML Uplift Tree Visualization",
    "section": "",
    "text": "# Code from https://github.com/uber/causalml/tree/master/examples"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#introduction",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#introduction",
    "title": "CausalML Uplift Tree Visualization",
    "section": "Introduction",
    "text": "Introduction\nThis example notebooks illustrates how to visualize uplift trees for interpretation and diagnosis.\n\nSupported Models\nThese visualization functions work only for tree-based algorithms:\n\nUplift tree/random forests on KL divergence, Euclidean Distance, and Chi-Square\nUplift tree/random forests on Contextual Treatment Selection\n\nCurrently, they are NOT supporting Meta-learner algorithms\n\nS-learner\nT-learner\nX-learner\nR-learner\n\n\n\nSupported Usage\nThis notebook will show how to use visualization for:\n\nUplift Tree and Uplift Random Forest\n\nVisualize a trained uplift classification tree model\nVisualize an uplift tree in a trained uplift random forests\n\nTraining and Validation Data\n\nVisualize the validation tree: fill the trained uplift classification tree with validation (or testing) data, and show the statistics for both training data and validation data\n\nOne Treatment Group and Multiple Treatment Groups\n\nVisualize the case where there are one control group and one treatment group\nVisualize the case where there are one control group and multiple treatment groups"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#step-1-load-modules",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#step-1-load-modules",
    "title": "CausalML Uplift Tree Visualization",
    "section": "Step 1 Load Modules",
    "text": "Step 1 Load Modules\n\nLoad CausalML modules\n\nfrom causalml.dataset import make_uplift_classification\nfrom causalml.inference.tree import UpliftTreeClassifier, UpliftRandomForestClassifier\nfrom causalml.inference.tree import uplift_tree_string, uplift_tree_plot\n\n\n\nLoad standard modules\n\nimport numpy as np\nimport pandas as pd\nfrom IPython.display import Image\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-one-treatment-for-uplift-classification-tree",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-one-treatment-for-uplift-classification-tree",
    "title": "CausalML Uplift Tree Visualization",
    "section": "One Control + One Treatment for Uplift Classification Tree",
    "text": "One Control + One Treatment for Uplift Classification Tree\n\n# Data generation\ndf, x_names = make_uplift_classification()\n\n# Rename features for easy interpretation of visualization\nx_names_new = ['feature_%s'%(i) for i in range(len(x_names))]\nrename_dict = {x_names[i]:x_names_new[i] for i in range(len(x_names))}\ndf = df.rename(columns=rename_dict)\nx_names = x_names_new\n\ndf.head()\n\ndf = df[df['treatment_group_key'].isin(['control','treatment1'])]\n\n# Look at the conversion rate and sample size in each group\ndf.pivot_table(values='conversion',\n               index='treatment_group_key',\n               aggfunc=[np.mean, np.size],\n               margins=True)\n\n\n\n\n\n\n\n\nmean\nsize\n\n\n\nconversion\nconversion\n\n\ntreatment_group_key\n\n\n\n\n\n\ncontrol\n0.5110\n1000\n\n\ntreatment1\n0.5140\n1000\n\n\nAll\n0.5125\n2000\n\n\n\n\n\n\n\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftTreeClassifier(max_depth = 4, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Print uplift tree as a string\nresult = uplift_tree_string(uplift_model.fitted_uplift_tree, x_names)\n\nfeature_17 &gt;= -0.44234212654232735?\nyes -&gt; feature_10 &gt;= 1.020659213325515?\n        yes -&gt; {'treatment1': 0.606557, 'control': 0.381356}\n        no  -&gt; {'treatment1': 0.526786, 'control': 0.507812}\nno  -&gt; feature_9 &gt;= 0.8142773340486678?\n        yes -&gt; {'treatment1': 0.61, 'control': 0.459677}\n        no  -&gt; feature_4 &gt;= 0.280545459525536?\n                yes -&gt; {'treatment1': 0.41433, 'control': 0.552288}\n                no  -&gt; {'treatment1': 0.574803, 'control': 0.507042}\n\n\n\nRead the tree\n\nFirst line: node split condition\nimpurity: the value for the loss function\ntotal_sample: total sample size in this node\ngroup_sample: sample size by treatment group\nuplift score: the treatment effect between treatment and control (when there are multiple treatment groups, this is the maximum of the treatment effects)\nuplift p_value: the p_value for the treatment effect\nvalidation uplift score: when validation data is filled in the tree, this reflects the uplift score based on the - validation data. It can be compared with the uplift score (for training data) to check if there are over-fitting issue.\n\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\n\nVisualize Validation Tree: One Control + One Treatment for Uplift Classification Tree\nNote the validation uplift score will update.\n\n### Fill the trained tree with testing data set \n# The uplift score based on testing dataset is shown as validation uplift score in the tree nodes\nuplift_model.fill(X=df_test[x_names].values, treatment=df_test['treatment_group_key'].values, y=df_test['conversion'].values)\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\n\nVisualize a Tree in Random Forest\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftRandomForestClassifier(n_estimators=5, max_depth = 5, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Specify a tree in the random forest (the index can be any integer from 0 to n_estimators-1)\nuplift_tree = uplift_model.uplift_forest[0]\n# Print uplift tree as a string\nresult = uplift_tree_string(uplift_tree.fitted_uplift_tree, x_names)\n\nfeature_9 &gt;= 0.7626607142400706?\nyes -&gt; {'treatment1': 0.621795, 'control': 0.481707}\nno  -&gt; feature_12 &gt;= 0.5486596851987631?\n        yes -&gt; feature_4 &gt;= 0.9956888137470166?\n                yes -&gt; {'treatment1': 0.496815, 'control': 0.398773}\n                no  -&gt; {'treatment1': 0.483871, 'control': 0.551515}\n        no  -&gt; feature_9 &gt;= -0.15675867904794422?\n                yes -&gt; {'treatment1': 0.456376, 'control': 0.48538}\n                no  -&gt; {'treatment1': 0.40625, 'control': 0.625}\n\n\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_tree.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\nFill the tree with validation data\n\n### Fill the trained tree with testing data set \n# The uplift score based on testing dataset is shown as validation uplift score in the tree nodes\nuplift_tree.fill(X=df_test[x_names].values, treatment=df_test['treatment_group_key'].values, y=df_test['conversion'].values)\n\n# Plot uplift tree\ngraph = uplift_tree_plot(uplift_tree.fitted_uplift_tree,x_names)\nImage(graph.create_png())"
  },
  {
    "objectID": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-multiple-treatments",
    "href": "posts/2021-06-05-causalml-uplift_tree_visualization.html#one-control-multiple-treatments",
    "title": "CausalML Uplift Tree Visualization",
    "section": "One Control + Multiple Treatments",
    "text": "One Control + Multiple Treatments\n\n# Data generation\ndf, x_names = make_uplift_classification()\n# Look at the conversion rate and sample size in each group\ndf.pivot_table(values='conversion',\n               index='treatment_group_key',\n               aggfunc=[np.mean, np.size],\n               margins=True)\n\n\n\n\n\n\n\n\nmean\nsize\n\n\n\nconversion\nconversion\n\n\ntreatment_group_key\n\n\n\n\n\n\ncontrol\n0.511\n1000\n\n\ntreatment1\n0.514\n1000\n\n\ntreatment2\n0.559\n1000\n\n\ntreatment3\n0.600\n1000\n\n\nAll\n0.546\n4000\n\n\n\n\n\n\n\n\n# Split data to training and testing samples for model validation (next section)\ndf_train, df_test = train_test_split(df, test_size=0.2, random_state=111)\n\n# Train uplift tree\nuplift_model = UpliftTreeClassifier(max_depth = 3, min_samples_leaf = 200, min_samples_treatment = 50, n_reg = 100, evaluationFunction='KL', control_name='control')\n\nuplift_model.fit(df_train[x_names].values,\n                 treatment=df_train['treatment_group_key'].values,\n                 y=df_train['conversion'].values)\n\n\n# Plot uplift tree\n# The uplift score represents the best uplift score among all treatment effects\ngraph = uplift_tree_plot(uplift_model.fitted_uplift_tree,x_names)\nImage(graph.create_png())\n\n\n\n\n\nSave the Plot\n\n# Save the graph as pdf\ngraph.write_pdf(\"tbc.pdf\")\n# Save the graph as png\ngraph.write_png(\"tbc.png\")\n\nTrue"
  },
  {
    "objectID": "posts/2021-05-31-EDA_Sklearn_Examples.html",
    "href": "posts/2021-05-31-EDA_Sklearn_Examples.html",
    "title": "EDA with Sklearn examples",
    "section": "",
    "text": "code adapted from: https://github.com/thomasjpfan/ml-workshop-intro"
  },
  {
    "objectID": "posts/2021-05-31-EDA_Sklearn_Examples.html#generated-datasets",
    "href": "posts/2021-05-31-EDA_Sklearn_Examples.html#generated-datasets",
    "title": "EDA with Sklearn examples",
    "section": "Generated datasets",
    "text": "Generated datasets\n\n## Reading the dataset using pandas\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/CTG.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nFileName\nDate\nSegFile\nb\ne\nLBE\nLB\nAC\nFM\nUC\n...\nC\nD\nE\nAD\nDE\nLD\nFS\nSUSP\nCLASS\nNSP\n\n\n\n\n0\nVariab10.txt\n12/1/1996\nCTG0001.txt\n240.0\n357.0\n120.0\n120.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n9.0\n2.0\n\n\n1\nFmcs_1.txt\n5/3/1996\nCTG0002.txt\n5.0\n632.0\n132.0\n132.0\n4.0\n0.0\n4.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n2\nFmcs_1.txt\n5/3/1996\nCTG0003.txt\n177.0\n779.0\n133.0\n133.0\n2.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n3\nFmcs_1.txt\n5/3/1996\nCTG0004.txt\n411.0\n1192.0\n134.0\n134.0\n2.0\n0.0\n6.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n4\nFmcs_1.txt\n5/3/1996\nCTG0005.txt\n533.0\n1147.0\n132.0\n132.0\n4.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2124\nS8001045.dsp\n6/6/1998\nCTG2127.txt\n1576.0\n3049.0\n140.0\n140.0\n1.0\n0.0\n9.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n2.0\n\n\n2125\nS8001045.dsp\n6/6/1998\nCTG2128.txt\n2796.0\n3415.0\n142.0\n142.0\n1.0\n1.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n2126\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2127\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2128\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n564.0\n23.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2129 rows × 40 columns\n\n\n\n\n## Dropping the columns which we don't need\ndf=df.drop([\"FileName\",\"Date\",\"SegFile\",\"b\",\"e\"],axis=1)\ndf.head()\n\n\n\n\n\n\n\n\nLBE\nLB\nAC\nFM\nUC\nASTV\nMSTV\nALTV\nMLTV\nDL\n...\nC\nD\nE\nAD\nDE\nLD\nFS\nSUSP\nCLASS\nNSP\n\n\n\n\n0\n120.0\n120.0\n0.0\n0.0\n0.0\n73.0\n0.5\n43.0\n2.4\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n9.0\n2.0\n\n\n1\n132.0\n132.0\n4.0\n0.0\n4.0\n17.0\n2.1\n0.0\n10.4\n2.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n2\n133.0\n133.0\n2.0\n0.0\n5.0\n16.0\n2.1\n0.0\n13.4\n2.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n3\n134.0\n134.0\n2.0\n0.0\n6.0\n16.0\n2.4\n0.0\n23.0\n2.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n4\n132.0\n132.0\n4.0\n0.0\n5.0\n16.0\n2.4\n0.0\n19.9\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n1.0\n\n\n\n\n5 rows × 35 columns\n\n\n\n\ndf['C']\n\n0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n2124    0.0\n2125    0.0\n2126    NaN\n2127    NaN\n2128    NaN\nName: C, Length: 2129, dtype: float64\n\n\n\nX = df['C']\nX\n\n0       0.0\n1       0.0\n2       0.0\n3       0.0\n4       0.0\n       ... \n2124    0.0\n2125    0.0\n2126    NaN\n2127    NaN\n2128    NaN\nName: C, Length: 2129, dtype: float64\n\n\n\ny = df['NSP']\ny\n\n0       2.0\n1       1.0\n2       1.0\n3       1.0\n4       1.0\n       ... \n2124    2.0\n2125    1.0\n2126    NaN\n2127    NaN\n2128    NaN\nName: NSP, Length: 2129, dtype: float64\n\n\n\nimport seaborn as sns\nsns.set_theme(font_scale=1.5)\n\n\ndf.columns\n\nIndex(['LBE', 'LB', 'AC', 'FM', 'UC', 'ASTV', 'MSTV', 'ALTV', 'MLTV', 'DL',\n       'DS', 'DP', 'DR', 'Width', 'Min', 'Max', 'Nmax', 'Nzeros', 'Mode',\n       'Mean', 'Median', 'Variance', 'Tendency', 'A', 'B', 'C', 'D', 'E', 'AD',\n       'DE', 'LD', 'FS', 'SUSP', 'CLASS', 'NSP'],\n      dtype='object')\n\n\n\nsns.relplot(data=df, x='B', y='NSP', height=6);\n\n\n\n\n\nsns.displot(data=df, x='B', hue='NSP', kind='kde', aspect=2);\n\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\nsns.jointplot(data=df, x=\"B\", y=\"C\", height=10, hue='NSP');\n\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\nsns.jointplot(x=\"C\", y=\"B\", data=df, height=10, hue='NSP')\n\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)"
  },
  {
    "objectID": "posts/2020-11-12-Tensorflow_and_Keras.html#credit-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "href": "posts/2020-11-12-Tensorflow_and_Keras.html#credit-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "title": "Tensorflow and Keras",
    "section": "Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning",
    "text": "Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning\n\nTensorFlow Homepage\nTensorFlow GitHib\nTensorFlow Google Groups Support\nTensorFlow Google Groups Developer Discussion\nTensorFlow FAQ"
  },
  {
    "objectID": "posts/2020-11-12-Tensorflow_and_Keras.html#deep-learning-tools",
    "href": "posts/2020-11-12-Tensorflow_and_Keras.html#deep-learning-tools",
    "title": "Tensorflow and Keras",
    "section": "Deep Learning Tools",
    "text": "Deep Learning Tools\n\nTensorFlow - Google’s deep learning API. The focus of this class, along with Keras.\nKeras - Also by Google, higher level framework that allows the use of TensorFlow, MXNet and Theano interchangeably.\nPyTorch - PyTorch is an open source machine learning library based on the Torch library, used for applications such as computer vision and natural language processing. It is primarily developed by Facebook’s AI Research lab.\nMXNet Apache foundation’s deep learning API. Can be used through Keras.\nTorch is used by Google DeepMind, the Facebook AI Research Group, IBM, Yandex and the Idiap Research Institute. It has been used for some of the most advanced deep learning projects in the world. However, it requires the LUA** programming language. It is very advanced, but it is not mainstream. I have not worked with Torch (yet!).\nPaddlePaddle - Baidu’s deep learning API.\nDeeplearning4J - Java based. Supports all major platforms. GPU support in Java!\nComputational Network Toolkit (CNTK) - Microsoft. Support for Windows/Linux, command line only. Bindings for predictions for C#/Python. GPU support.\nH2O - Java based. Supports all major platforms. Limited support for computer vision. No GPU support.\nCommunicate with TensorFlow using Keras [Cite:franccois2017deep].\n\n\ntry:\n    %tensorflow_version 2.x\n    COLAB = True\n    print(\"Note: using Google CoLab\")\nexcept:\n    print(\"Note: not using Google CoLab\")\n    COLAB = False\n\nNote: using Google CoLab\n\n\n\nimport tensorflow as tf\n\n# Create a Constant op that produces a 1x2 matrix.  The op is\n# added as a node to the default graph.\n#\n# The value returned by the constructor represents the output\n# of the Constant op.\nmatrix1 = tf.constant([[3., 3.]])\n\n# Create another Constant that produces a 2x1 matrix.\nmatrix2 = tf.constant([[2.],[2.]])\n\n# Create a Matmul op that takes 'matrix1' and 'matrix2' as inputs.\n# The returned value, 'product', represents the result of the matrix\n# multiplication.\nproduct = tf.matmul(matrix1, matrix2)\n\nprint(product)\nprint(float(product))\n\ntf.Tensor([[12.]], shape=(1, 1), dtype=float32)\n12.0\n\n\n\nimport tensorflow as tf\n\nx = tf.Variable([1.0, 2.0])\na = tf.constant([3.0, 3.0])\n\n# Add an op to subtract 'a' from 'x'.  Run it and print the result\nsub = tf.subtract(x, a)\nprint(sub)\nprint(sub.numpy())\n# ==&gt; [-2. -1.]\n\ntf.Tensor([-2. -1.], shape=(2,), dtype=float32)\n[-2. -1.]\n\n\n\nx.assign([4.0, 6.0])\n\n&lt;tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([4., 6.], dtype=float32)&gt;\n\n\n\nsub = tf.subtract(x, a)\nprint(sub)\nprint(sub.numpy())\n\ntf.Tensor([1. 3.], shape=(2,), dtype=float32)\n[1. 3.]\n\n\n\n# Import libraries for simulation\nimport tensorflow as tf\nimport numpy as np\n\n# Imports for visualization\nimport PIL.Image\nfrom io import BytesIO\nfrom IPython.display import Image, display\n\ndef DisplayFractal(a, fmt='jpeg'):\n  \"\"\"Display an array of iteration counts as a\n     colorful picture of a fractal.\"\"\"\n  a_cyclic = (6.28*a/20.0).reshape(list(a.shape)+[1])\n  img = np.concatenate([10+20*np.cos(a_cyclic),\n                        30+50*np.sin(a_cyclic),\n                        155-80*np.cos(a_cyclic)], 2)\n  img[a==a.max()] = 0\n  a = img\n  a = np.uint8(np.clip(a, 0, 255))\n  f = BytesIO()\n  PIL.Image.fromarray(a).save(f, fmt)\n  display(Image(data=f.getvalue()))\n\n# Use NumPy to create a 2D array of complex numbers\n\nY, X = np.mgrid[-1.3:1.3:0.005, -2:1:0.005]\nZ = X+1j*Y\n\nxs = tf.constant(Z.astype(np.complex64))\nzs = tf.Variable(xs)\nns = tf.Variable(tf.zeros_like(xs, tf.float32))\n\n\n\n# Operation to update the zs and the iteration count.\n#\n# Note: We keep computing zs after they diverge! This\n#       is very wasteful! There are better, if a little\n#       less simple, ways to do this.\n#\nfor i in range(200):\n    # Compute the new values of z: z^2 + x\n    zs_ = zs*zs + xs\n\n    # Have we diverged with this new value?\n    not_diverged = tf.abs(zs_) &lt; 4\n\n    zs.assign(zs_),\n    ns.assign_add(tf.cast(not_diverged, tf.float32))\n    \nDisplayFractal(ns.numpy())\n\n\n\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nimport pandas as pd\nimport io\nimport os\nimport requests\nimport numpy as np\nfrom sklearn import metrics\n\ndf = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n    na_values=['NA', '?'])\n\ncars = df['name']\n\n# Handle missing value\ndf['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n\n# Pandas to Numpy\nx = df[['cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin']].values\ny = df['mpg'].values # regression\n\n# Build the neural network\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\nmodel.add(Dense(10, activation='relu')) # Hidden 2\nmodel.add(Dense(1)) # Output\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmodel.fit(x,y,verbose=2,epochs=100)\n\nEpoch 1/100\n13/13 - 0s - loss: 4580.9326\nEpoch 2/100\n13/13 - 0s - loss: 1225.3738\nEpoch 3/100\n13/13 - 0s - loss: 553.5983\nEpoch 4/100\n13/13 - 0s - loss: 376.0931\nEpoch 5/100\n13/13 - 0s - loss: 371.2755\nEpoch 6/100\n13/13 - 0s - loss: 355.6521\nEpoch 7/100\n13/13 - 0s - loss: 340.7242\nEpoch 8/100\n13/13 - 0s - loss: 332.5170\nEpoch 9/100\n13/13 - 0s - loss: 321.3474\nEpoch 10/100\n13/13 - 0s - loss: 309.0533\nEpoch 11/100\n13/13 - 0s - loss: 298.6037\nEpoch 12/100\n13/13 - 0s - loss: 290.2845\nEpoch 13/100\n13/13 - 0s - loss: 281.7734\nEpoch 14/100\n13/13 - 0s - loss: 267.6147\nEpoch 15/100\n13/13 - 0s - loss: 255.6712\nEpoch 16/100\n13/13 - 0s - loss: 245.0956\nEpoch 17/100\n13/13 - 0s - loss: 238.7114\nEpoch 18/100\n13/13 - 0s - loss: 228.2198\nEpoch 19/100\n13/13 - 0s - loss: 215.6643\nEpoch 20/100\n13/13 - 0s - loss: 206.8975\nEpoch 21/100\n13/13 - 0s - loss: 198.3939\nEpoch 22/100\n13/13 - 0s - loss: 189.5826\nEpoch 23/100\n13/13 - 0s - loss: 184.5812\nEpoch 24/100\n13/13 - 0s - loss: 173.4126\nEpoch 25/100\n13/13 - 0s - loss: 169.0896\nEpoch 26/100\n13/13 - 0s - loss: 158.8616\nEpoch 27/100\n13/13 - 0s - loss: 147.4314\nEpoch 28/100\n13/13 - 0s - loss: 140.2026\nEpoch 29/100\n13/13 - 0s - loss: 133.2955\nEpoch 30/100\n13/13 - 0s - loss: 128.1834\nEpoch 31/100\n13/13 - 0s - loss: 120.1640\nEpoch 32/100\n13/13 - 0s - loss: 114.6829\nEpoch 33/100\n13/13 - 0s - loss: 109.8683\nEpoch 34/100\n13/13 - 0s - loss: 105.6856\nEpoch 35/100\n13/13 - 0s - loss: 100.0182\nEpoch 36/100\n13/13 - 0s - loss: 93.3371\nEpoch 37/100\n13/13 - 0s - loss: 91.3530\nEpoch 38/100\n13/13 - 0s - loss: 85.7752\nEpoch 39/100\n13/13 - 0s - loss: 82.4184\nEpoch 40/100\n13/13 - 0s - loss: 76.0113\nEpoch 41/100\n13/13 - 0s - loss: 73.7212\nEpoch 42/100\n13/13 - 0s - loss: 70.6216\nEpoch 43/100\n13/13 - 0s - loss: 67.4598\nEpoch 44/100\n13/13 - 0s - loss: 64.0807\nEpoch 45/100\n13/13 - 0s - loss: 62.0253\nEpoch 46/100\n13/13 - 0s - loss: 59.2131\nEpoch 47/100\n13/13 - 0s - loss: 56.5928\nEpoch 48/100\n13/13 - 0s - loss: 56.3038\nEpoch 49/100\n13/13 - 0s - loss: 52.7268\nEpoch 50/100\n13/13 - 0s - loss: 50.3988\nEpoch 51/100\n13/13 - 0s - loss: 49.0474\nEpoch 52/100\n13/13 - 0s - loss: 47.9957\nEpoch 53/100\n13/13 - 0s - loss: 47.7353\nEpoch 54/100\n13/13 - 0s - loss: 44.7161\nEpoch 55/100\n13/13 - 0s - loss: 44.8755\nEpoch 56/100\n13/13 - 0s - loss: 43.3957\nEpoch 57/100\n13/13 - 0s - loss: 42.0779\nEpoch 58/100\n13/13 - 0s - loss: 41.9396\nEpoch 59/100\n13/13 - 0s - loss: 40.3681\nEpoch 60/100\n13/13 - 0s - loss: 41.1852\nEpoch 61/100\n13/13 - 0s - loss: 39.7117\nEpoch 62/100\n13/13 - 0s - loss: 41.0216\nEpoch 63/100\n13/13 - 0s - loss: 36.1499\nEpoch 64/100\n13/13 - 0s - loss: 35.3468\nEpoch 65/100\n13/13 - 0s - loss: 35.1042\nEpoch 66/100\n13/13 - 0s - loss: 34.2546\nEpoch 67/100\n13/13 - 0s - loss: 33.6177\nEpoch 68/100\n13/13 - 0s - loss: 32.9660\nEpoch 69/100\n13/13 - 0s - loss: 32.6077\nEpoch 70/100\n13/13 - 0s - loss: 32.2787\nEpoch 71/100\n13/13 - 0s - loss: 31.4921\nEpoch 72/100\n13/13 - 0s - loss: 30.9388\nEpoch 73/100\n13/13 - 0s - loss: 30.7971\nEpoch 74/100\n13/13 - 0s - loss: 30.1166\nEpoch 75/100\n13/13 - 0s - loss: 30.3033\nEpoch 76/100\n13/13 - 0s - loss: 29.7823\nEpoch 77/100\n13/13 - 0s - loss: 29.0691\nEpoch 78/100\n13/13 - 0s - loss: 29.2846\nEpoch 79/100\n13/13 - 0s - loss: 28.1004\nEpoch 80/100\n13/13 - 0s - loss: 28.0843\nEpoch 81/100\n13/13 - 0s - loss: 27.5363\nEpoch 82/100\n13/13 - 0s - loss: 27.3842\nEpoch 83/100\n13/13 - 0s - loss: 26.8047\nEpoch 84/100\n13/13 - 0s - loss: 26.2902\nEpoch 85/100\n13/13 - 0s - loss: 26.4791\nEpoch 86/100\n13/13 - 0s - loss: 25.9914\nEpoch 87/100\n13/13 - 0s - loss: 26.4426\nEpoch 88/100\n13/13 - 0s - loss: 25.3650\nEpoch 89/100\n13/13 - 0s - loss: 24.7668\nEpoch 90/100\n13/13 - 0s - loss: 25.0466\nEpoch 91/100\n13/13 - 0s - loss: 23.9254\nEpoch 92/100\n13/13 - 0s - loss: 24.7530\nEpoch 93/100\n13/13 - 0s - loss: 24.3067\nEpoch 94/100\n13/13 - 0s - loss: 25.2663\nEpoch 95/100\n13/13 - 0s - loss: 24.8965\nEpoch 96/100\n13/13 - 0s - loss: 24.3817\nEpoch 97/100\n13/13 - 0s - loss: 22.7545\nEpoch 98/100\n13/13 - 0s - loss: 23.2711\nEpoch 99/100\n13/13 - 0s - loss: 22.9325\nEpoch 100/100\n13/13 - 0s - loss: 22.9305\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7efe002f9fd0&gt;\n\n\n\npred = model.predict(x)\nprint(f\"Shape: {pred.shape}\")\nprint(pred[0:10])\n\nShape: (398, 1)\n[[14.227755]\n [13.352877]\n [13.366304]\n [14.042093]\n [13.76317 ]\n [12.621584]\n [11.604197]\n [11.754587]\n [12.065157]\n [11.800211]]\n\n\n\n# Measure RMSE error.  RMSE is common for regression.\nscore = np.sqrt(metrics.mean_squared_error(pred,y))\nprint(f\"Final score (RMSE): {score}\")\n\nFinal score (RMSE): 4.71485252486946\n\n\n\n# Sample predictions\nfor i in range(10):\n    print(f\"{i+1}. Car name: {cars[i]}, MPG: {y[i]}, \" \n          + \"predicted MPG: {pred[i]}\")\n\n1. Car name: chevrolet chevelle malibu, MPG: 18.0, predicted MPG: {pred[i]}\n2. Car name: buick skylark 320, MPG: 15.0, predicted MPG: {pred[i]}\n3. Car name: plymouth satellite, MPG: 18.0, predicted MPG: {pred[i]}\n4. Car name: amc rebel sst, MPG: 16.0, predicted MPG: {pred[i]}\n5. Car name: ford torino, MPG: 17.0, predicted MPG: {pred[i]}\n6. Car name: ford galaxie 500, MPG: 15.0, predicted MPG: {pred[i]}\n7. Car name: chevrolet impala, MPG: 14.0, predicted MPG: {pred[i]}\n8. Car name: plymouth fury iii, MPG: 14.0, predicted MPG: {pred[i]}\n9. Car name: pontiac catalina, MPG: 14.0, predicted MPG: {pred[i]}\n10. Car name: amc ambassador dpl, MPG: 15.0, predicted MPG: {pred[i]}\n\n\n\nimport pandas as pd\nimport io\nimport requests\nimport numpy as np\nfrom sklearn import metrics\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ndf = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n    na_values=['NA', '?'])\n\n# Convert to numpy - Classification\nx = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ndummies = pd.get_dummies(df['species']) # Classification\nspecies = dummies.columns\ny = dummies.values\n\n\n# Build neural network\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\nmodel.add(Dense(25, activation='relu')) # Hidden 2\nmodel.add(Dense(y.shape[1],activation='softmax')) # Output\n\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmodel.fit(x,y,verbose=2,epochs=100)\n\nEpoch 1/100\n5/5 - 0s - loss: 1.2044\nEpoch 2/100\n5/5 - 0s - loss: 1.0163\nEpoch 3/100\n5/5 - 0s - loss: 0.9398\nEpoch 4/100\n5/5 - 0s - loss: 0.9153\nEpoch 5/100\n5/5 - 0s - loss: 0.8853\nEpoch 6/100\n5/5 - 0s - loss: 0.8544\nEpoch 7/100\n5/5 - 0s - loss: 0.8274\nEpoch 8/100\n5/5 - 0s - loss: 0.8119\nEpoch 9/100\n5/5 - 0s - loss: 0.7936\nEpoch 10/100\n5/5 - 0s - loss: 0.7705\nEpoch 11/100\n5/5 - 0s - loss: 0.7500\nEpoch 12/100\n5/5 - 0s - loss: 0.7247\nEpoch 13/100\n5/5 - 0s - loss: 0.7005\nEpoch 14/100\n5/5 - 0s - loss: 0.6765\nEpoch 15/100\n5/5 - 0s - loss: 0.6503\nEpoch 16/100\n5/5 - 0s - loss: 0.6312\nEpoch 17/100\n5/5 - 0s - loss: 0.6034\nEpoch 18/100\n5/5 - 0s - loss: 0.5799\nEpoch 19/100\n5/5 - 0s - loss: 0.5612\nEpoch 20/100\n5/5 - 0s - loss: 0.5416\nEpoch 21/100\n5/5 - 0s - loss: 0.5222\nEpoch 22/100\n5/5 - 0s - loss: 0.5048\nEpoch 23/100\n5/5 - 0s - loss: 0.4886\nEpoch 24/100\n5/5 - 0s - loss: 0.4717\nEpoch 25/100\n5/5 - 0s - loss: 0.4600\nEpoch 26/100\n5/5 - 0s - loss: 0.4461\nEpoch 27/100\n5/5 - 0s - loss: 0.4323\nEpoch 28/100\n5/5 - 0s - loss: 0.4218\nEpoch 29/100\n5/5 - 0s - loss: 0.4096\nEpoch 30/100\n5/5 - 0s - loss: 0.3990\nEpoch 31/100\n5/5 - 0s - loss: 0.3883\nEpoch 32/100\n5/5 - 0s - loss: 0.3787\nEpoch 33/100\n5/5 - 0s - loss: 0.3684\nEpoch 34/100\n5/5 - 0s - loss: 0.3594\nEpoch 35/100\n5/5 - 0s - loss: 0.3510\nEpoch 36/100\n5/5 - 0s - loss: 0.3413\nEpoch 37/100\n5/5 - 0s - loss: 0.3341\nEpoch 38/100\n5/5 - 0s - loss: 0.3243\nEpoch 39/100\n5/5 - 0s - loss: 0.3164\nEpoch 40/100\n5/5 - 0s - loss: 0.3128\nEpoch 41/100\n5/5 - 0s - loss: 0.3011\nEpoch 42/100\n5/5 - 0s - loss: 0.2978\nEpoch 43/100\n5/5 - 0s - loss: 0.2863\nEpoch 44/100\n5/5 - 0s - loss: 0.2832\nEpoch 45/100\n5/5 - 0s - loss: 0.2735\nEpoch 46/100\n5/5 - 0s - loss: 0.2705\nEpoch 47/100\n5/5 - 0s - loss: 0.2627\nEpoch 48/100\n5/5 - 0s - loss: 0.2563\nEpoch 49/100\n5/5 - 0s - loss: 0.2499\nEpoch 50/100\n5/5 - 0s - loss: 0.2448\nEpoch 51/100\n5/5 - 0s - loss: 0.2381\nEpoch 52/100\n5/5 - 0s - loss: 0.2348\nEpoch 53/100\n5/5 - 0s - loss: 0.2301\nEpoch 54/100\n5/5 - 0s - loss: 0.2236\nEpoch 55/100\n5/5 - 0s - loss: 0.2187\nEpoch 56/100\n5/5 - 0s - loss: 0.2144\nEpoch 57/100\n5/5 - 0s - loss: 0.2082\nEpoch 58/100\n5/5 - 0s - loss: 0.2033\nEpoch 59/100\n5/5 - 0s - loss: 0.1978\nEpoch 60/100\n5/5 - 0s - loss: 0.1949\nEpoch 61/100\n5/5 - 0s - loss: 0.1858\nEpoch 62/100\n5/5 - 0s - loss: 0.1832\nEpoch 63/100\n5/5 - 0s - loss: 0.1779\nEpoch 64/100\n5/5 - 0s - loss: 0.1736\nEpoch 65/100\n5/5 - 0s - loss: 0.1689\nEpoch 66/100\n5/5 - 0s - loss: 0.1657\nEpoch 67/100\n5/5 - 0s - loss: 0.1655\nEpoch 68/100\n5/5 - 0s - loss: 0.1607\nEpoch 69/100\n5/5 - 0s - loss: 0.1587\nEpoch 70/100\n5/5 - 0s - loss: 0.1523\nEpoch 71/100\n5/5 - 0s - loss: 0.1498\nEpoch 72/100\n5/5 - 0s - loss: 0.1473\nEpoch 73/100\n5/5 - 0s - loss: 0.1452\nEpoch 74/100\n5/5 - 0s - loss: 0.1455\nEpoch 75/100\n5/5 - 0s - loss: 0.1390\nEpoch 76/100\n5/5 - 0s - loss: 0.1415\nEpoch 77/100\n5/5 - 0s - loss: 0.1332\nEpoch 78/100\n5/5 - 0s - loss: 0.1379\nEpoch 79/100\n5/5 - 0s - loss: 0.1297\nEpoch 80/100\n5/5 - 0s - loss: 0.1287\nEpoch 81/100\n5/5 - 0s - loss: 0.1269\nEpoch 82/100\n5/5 - 0s - loss: 0.1243\nEpoch 83/100\n5/5 - 0s - loss: 0.1234\nEpoch 84/100\n5/5 - 0s - loss: 0.1204\nEpoch 85/100\n5/5 - 0s - loss: 0.1196\nEpoch 86/100\n5/5 - 0s - loss: 0.1177\nEpoch 87/100\n5/5 - 0s - loss: 0.1153\nEpoch 88/100\n5/5 - 0s - loss: 0.1154\nEpoch 89/100\n5/5 - 0s - loss: 0.1138\nEpoch 90/100\n5/5 - 0s - loss: 0.1118\nEpoch 91/100\n5/5 - 0s - loss: 0.1101\nEpoch 92/100\n5/5 - 0s - loss: 0.1094\nEpoch 93/100\n5/5 - 0s - loss: 0.1087\nEpoch 94/100\n5/5 - 0s - loss: 0.1057\nEpoch 95/100\n5/5 - 0s - loss: 0.1066\nEpoch 96/100\n5/5 - 0s - loss: 0.1035\nEpoch 97/100\n5/5 - 0s - loss: 0.1027\nEpoch 98/100\n5/5 - 0s - loss: 0.1037\nEpoch 99/100\n5/5 - 0s - loss: 0.1016\nEpoch 100/100\n5/5 - 0s - loss: 0.1036\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x7efe41398b00&gt;\n\n\n\n# Print out number of species found:\n\nprint(species)\n\nIndex(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='object')\n\n\n\npred = model.predict(x)\nprint(f\"Shape: {pred.shape}\")\nprint(pred[0:10])\n\nShape: (150, 3)\n[[9.97697055e-01 2.15794984e-03 1.44956546e-04]\n [9.94137406e-01 5.50468592e-03 3.57910059e-04]\n [9.96191502e-01 3.52777750e-03 2.80687527e-04]\n [9.93117690e-01 6.42941520e-03 4.52894397e-04]\n [9.97973382e-01 1.89066969e-03 1.36019531e-04]\n [9.97510195e-01 2.37838831e-03 1.11383706e-04]\n [9.95967269e-01 3.74595844e-03 2.86734197e-04]\n [9.96544778e-01 3.24807805e-03 2.07129444e-04]\n [9.91251111e-01 8.11220054e-03 6.36695651e-04]\n [9.94891763e-01 4.79313312e-03 3.15106096e-04]]\n\n\n\nnp.set_printoptions(suppress=True)\n\n\nprint(y[0:10])\n\n[[1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]\n [1 0 0]]\n\n\n\npredict_classes = np.argmax(pred,axis=1)\nexpected_classes = np.argmax(y,axis=1)\nprint(f\"Predictions: {predict_classes}\")\nprint(f\"Expected: {expected_classes}\")\n\nPredictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 1 1 2 1\n 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\nExpected: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n 2 2]\n\n\n\nprint(species[predict_classes[1:10]])\n\nIndex(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n       'Iris-setosa'],\n      dtype='object')\n\n\n\nfrom sklearn.metrics import accuracy_score\n\ncorrect = accuracy_score(expected_classes,predict_classes)\nprint(f\"Accuracy: {correct}\")\n\nAccuracy: 0.98\n\n\n\nsample_flower = np.array( [[5.0,3.0,4.0,2.0]], dtype=float)\npred = model.predict(sample_flower)\nprint(pred)\npred = np.argmax(pred)\nprint(f\"Predict that {sample_flower} is: {species[pred]}\")\n\n[[0.00188068 0.5159181  0.48220128]]\nPredict that [[5. 3. 4. 2.]] is: Iris-versicolor\n\n\n\nsample_flower = np.array( [[5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8]],\\\n        dtype=float)\npred = model.predict(sample_flower)\nprint(pred)\npred = np.argmax(pred,axis=1)\nprint(f\"Predict that these two flowers {sample_flower} \")\nprint(f\"are: {species[pred]}\")\n\n[[0.00188067 0.51591814 0.4822012 ]\n [0.9928797  0.00682994 0.0002904 ]]\nPredict that these two flowers [[5.  3.  4.  2. ]\n [5.2 3.5 1.5 0.8]] \nare: Index(['Iris-versicolor', 'Iris-setosa'], dtype='object')"
  },
  {
    "objectID": "posts/2020-08-17-Pyspark-Group-By.html",
    "href": "posts/2020-08-17-Pyspark-Group-By.html",
    "title": "Group By and Aggregation with Pyspark",
    "section": "",
    "text": "“Group By and Aggregation with Pyspark”"
  },
  {
    "objectID": "posts/2020-08-17-Pyspark-Group-By.html#read-csv-and-inferschema",
    "href": "posts/2020-08-17-Pyspark-Group-By.html#read-csv-and-inferschema",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Read CSV and inferSchema",
    "text": "Read CSV and inferSchema\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import countDistinct, avg,stddev\n\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n\n\n\n\n_c0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\nAnhui\n147002.0\nnull\n1996\n2093.3\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\nAnhui\n151981.0\nnull\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\nAnhui\n174930.0\nnull\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\nAnhui\n285324.0\nnull\n1999\n2712.34\n26131\nnull\nnull\nnull\n1646891\nEast China\n1227364\n\n\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\n\n\n\n\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: double (nullable = true)\n-- general: double (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: double (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: double (nullable = true)\n-- rr: double (nullable = true)\n-- i: double (nullable = true)\n-- fr: string (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)"
  },
  {
    "objectID": "posts/2020-08-17-Pyspark-Group-By.html#using-groupby-for-averages-and-counts",
    "href": "posts/2020-08-17-Pyspark-Group-By.html#using-groupby-for-averages-and-counts",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Using groupBy for Averages and Counts",
    "text": "Using groupBy for Averages and Counts\n\ndf.groupBy(\"province\")\n\n\nOut[8]: &lt;pyspark.sql.group.GroupedData at 0x7f939a0aada0&gt;\n\n\n\ndf.groupBy(\"province\").mean().show()\n\n\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n    province|avg(_c0)|     avg(specific)|      avg(general)|avg(year)|          avg(gdp)|          avg(fdi)|            avg(rnr)|             avg(rr)|              avg(i)|           avg(it)|\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n   Guangdong|    65.5|1123328.0833333333|          312308.0|   2001.5|15358.781666666668|        1194950.25|0.011261261250000001|                 0.0|                 0.0|        3099014.25|\n       Hunan|   161.5| 824676.9166666666| 480788.3333333333|   2001.5| 4765.891666666666|         132110.25|                 0.0| 0.07291666666666667|                 0.0|         3215128.5|\n      Shanxi|   281.5| 577540.4166666666|          351680.0|   2001.5| 2817.210833333333|38628.833333333336|                 0.0|                 0.0|                 0.0|1983718.3333333333|\n       Tibet|   317.5|189219.91666666666|165365.33333333334|   2001.5|170.42666666666665|            839.75| 0.03030303033333333| 0.15583333333333335| 0.20278090583333333|1174175.5833333333|\n       Hubei|   149.5|         595463.25|          391326.5|   2001.5| 4772.503333333333|         149713.25|         0.045045045| 0.11386386375000002| 0.06230392158333333|        2904659.75|\n     Tianjin|   305.5| 76884.16666666667|          126636.0|   2001.5|2528.6650000000004|250173.33333333334|                 0.0|                 0.0|                 0.0| 831028.4166666666|\n     Beijing|    17.5| 581440.8333333334|          412825.0|   2001.5| 4673.453333333333|257369.33333333334|                 0.0|  0.3613053613636364| 0.29545454545454547|1175965.4166666667|\nHeilongjiang|   125.5|1037878.1666666666| 315925.3333333333|   2001.5| 4041.241666666667| 82719.33333333333|                 0.0|                 0.0| 0.03931203927272728|3230451.1666666665|\n    Liaoning|   209.5|        1111002.75|185280.83333333334|   2001.5| 5231.135000000001| 285925.3333333333| 0.11469534044444446|                 0.0|                null|2628358.4166666665|\n       Henan|   137.5| 955407.4166666666|          673392.5|   2001.5| 7208.966666666667|           94426.0|                 0.0|                0.04| 0.08602150533333335|3671970.6666666665|\n       Anhui|     5.5| 643984.1666666666|159698.83333333334|   2001.5|3905.8700000000003| 70953.08333333333|                 0.0|                 0.0| 0.08845208836363637|2649674.4166666665|\n    Xinjiang|   329.5| 345334.3333333333|          412906.0|   2001.5|1828.8966666666665| 4433.083333333333|                 0.0|                 0.0|                 0.0|         2251012.0|\n      Fujian|    41.5|246144.16666666666|140619.33333333334|   2001.5|4864.0233333333335| 374466.4166666667|  0.1366666666666667|0.049999999999999996| 0.09999999999999999|        1274116.75|\n     Jiangxi|   185.5| 592906.3333333334| 458268.6666666667|   2001.5|         2460.7825|         103735.25|                 0.0|  0.1491841490909091|0.042727272727272725|        1760613.25|\n       Jilin|   197.5|         711132.25|          348186.0|   2001.5|2274.8541666666665|41226.583333333336|                 0.0|                 0.0|                 0.0|2136634.9166666665|\n   Chongqing|    29.5| 561854.1111111111|          151201.4|   2001.5|         2477.7125|41127.833333333336| 0.09677419400000001|                 0.0|                 0.0|1636146.4166666667|\n     Shaanxi|   245.5| 387167.1666666667|          386760.5|   2001.5| 2658.034166666667|50892.583333333336|0.002840909090909091|                 0.0| 0.07386363636363637|2474031.4166666665|\n     Sichuan|   293.5|         1194640.5| 707032.8333333334|   2001.5|           5377.79|62197.166666666664| 0.00818181818181818| 0.00818181818181818|                 0.2|4016479.5833333335|\n      Yunnan|   341.5| 802151.1666666666|          200426.0|   2001.5| 2604.054166666667|17048.333333333332|                 0.0|                 0.0|                 0.0|3165418.9166666665|\n       Gansu|    53.5| 498930.9166666667| 382092.6666666667|   2001.5|1397.8325000000002|            5295.5| 0.11111111120000002|         0.088974359| 0.13038461533333334|         2045347.0|\n+------------+--------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy(\"reg\").mean().show()\n\n\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n                reg|          avg(_c0)|     avg(specific)|      avg(general)|avg(year)|          avg(gdp)|          avg(fdi)|            avg(rnr)|             avg(rr)|              avg(i)|           avg(it)|\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n    Southwest China|             214.3| 648086.8070175438|          327627.0|   2001.5|2410.3988333333336|25405.083333333332| 0.01764440930612245|0.053185448081632655| 0.13679739081632653|         2424971.4|\n         East China|183.78571428571428|517524.90476190473|230217.37142857144|   2001.5| 7126.732976190476|414659.03571428574| 0.08284508739240506| 0.05701117448101268| 0.09036240282278483|1949130.4761904762|\n    Northeast China|             177.5| 953337.7222222222|283130.72222222225|   2001.5| 3849.076944444444|         136623.75| 0.03686635942857143|                 0.0| 0.02275960168421053|2665148.1666666665|\n        North China|             179.5|506433.57446808513|334689.14285714284|   2001.5| 4239.038541666667|169600.58333333334|                 0.0| 0.15428824051724138| 0.11206896551724138|1733718.7291666667|\n    Northwest China|             216.7|324849.06666666665|293066.73333333334|   2001.5|1340.0261666666668|15111.133333333333|0.022847222240000003|0.033887245249999996|0.048179240615384616|        1703537.75|\nSouth Central China|             115.5| 690125.8333333334| 382414.8888888889|   2001.5| 5952.826944444445|281785.59722222225|0.014928879322033899| 0.07324349771186443| 0.06797753142372882|       2626299.875|\n+-------------------+------------------+------------------+------------------+---------+------------------+------------------+--------------------+--------------------+--------------------+------------------+\n\n\n\n\n\n# Count\ndf.groupBy(\"reg\").count().show()\n\n\n+-------------------+-----+\n                reg|count|\n+-------------------+-----+\n    Southwest China|   60|\n         East China|   84|\n    Northeast China|   36|\n        North China|   48|\n    Northwest China|   60|\nSouth Central China|   72|\n+-------------------+-----+\n\n\n\n\n\n# Max\ndf.groupBy(\"reg\").max().show()\n\n\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n                reg|max(_c0)|max(specific)|max(general)|max(year)|max(gdp)|max(fdi)|          max(rnr)|    max(rr)|             max(i)| max(it)|\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n    Southwest China|     347|    3937966.0|   1725100.0|     2007|10562.39|  149322|       0.181818182|       0.84|               0.75|10384846|\n         East China|     359|    2213991.0|   1272600.0|     2007|25776.91| 1743140|       1.214285714|       0.53|                0.6| 7040099|\n    Northeast China|     215|    3847672.0|   1046700.0|     2007| 9304.52|  598554|       0.516129032|        0.0|0.21621621600000002| 7968319|\n        North China|     311|    2981235.0|   1023453.0|     2007|13607.32|  527776|               0.0|0.794871795|                0.6| 7537692|\n    Northwest China|     335|    2669238.0|   1197400.0|     2007| 5757.29|  119516|0.5555555560000001|        0.5|               1.05| 6308151|\nSouth Central China|     167|    3860764.0|   1737800.0|     2007|31777.01| 1712603|        0.27027027|     0.4375| 0.6176470589999999|10533312|\n+-------------------+--------+-------------+------------+---------+--------+--------+------------------+-----------+-------------------+--------+\n\n\n\n\n\n# Min\ndf.groupBy(\"reg\").min().show()\n\n\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n                reg|min(_c0)|min(specific)|min(general)|min(year)|min(gdp)|min(fdi)|min(rnr)|min(rr)|min(i)|min(it)|\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n    Southwest China|      24|      18829.0|     18700.0|     1996|   64.98|       2|     0.0|    0.0|   0.0| 176802|\n         East China|       0|       8964.0|         0.0|     1996| 1169.73|   22724|     0.0|    0.0|   0.0| 489132|\n    Northeast China|     120|      80595.0|     19360.0|     1996| 1137.23|   19059|     0.0|    0.0|   0.0| 625471|\n        North China|      12|      35084.0|     32119.0|     1996| 1121.93|   13802|     0.0|    0.0|   0.0| 303992|\n    Northwest China|      48|      32088.0|      2990.0|     1996|  184.17|     247|     0.0|    0.0|   0.0| 178668|\nSouth Central China|      60|      54462.0|         0.0|     1996|  389.68|   29579|     0.0|    0.0|   0.0| 147897|\n+-------------------+--------+-------------+------------+---------+--------+--------+--------+-------+------+-------+\n\n\n\n\n\n# Sum\ndf.groupBy(\"reg\").sum().show()\n\n\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n                reg|sum(_c0)|sum(specific)|sum(general)|sum(year)|          sum(gdp)|sum(fdi)|          sum(rnr)|           sum(rr)|             sum(i)|  sum(it)|\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n    Southwest China|   12858|  3.6940948E7|   9501183.0|   120090|144623.93000000002| 1524305|       0.864576056|       2.606086956|         6.70307215|145498284|\n         East China|   15438|  4.3472092E7|   8057608.0|   168126|         598645.57|34831359|       6.544761904| 4.503882784000002|  7.138629823000002|163726960|\n    Northeast China|    6390|  3.4320158E7|   5096353.0|    72054|         138566.77| 4918455|       1.032258064|               0.0|0.43243243200000003| 95945334|\n        North China|    8616|  2.3802378E7|   7028472.0|    96072|         203473.85| 8140828|               0.0|       4.474358975|               3.25| 83218499|\n    Northwest China|   13002|  1.9490944E7|   8792002.0|   120090|          80401.57|  906668|1.1423611120000001|1.7621367529999998|        2.505320512|102212265|\nSouth Central China|    8316|   4.968906E7| 1.3766936E7|   144108|428603.54000000004|20288563|        0.88080388| 4.321366365000001|  4.010674354000001|189093591|\n+-------------------+--------+-------------+------------+---------+------------------+--------+------------------+------------------+-------------------+---------+\n\n\n\n\n\n# Max it across everything\ndf.agg({'specific':'max'}).show()\n\n\n+-------------+\nmax(specific)|\n+-------------+\n    3937966.0|\n+-------------+\n\n\n\n\n\ngrouped = df.groupBy(\"reg\")\ngrouped.agg({\"it\":'max'}).show()\n\n\n+-------------------+--------+\n                reg| max(it)|\n+-------------------+--------+\n    Southwest China|10384846|\n         East China| 7040099|\n    Northeast China| 7968319|\n        North China| 7537692|\n    Northwest China| 6308151|\nSouth Central China|10533312|\n+-------------------+--------+\n\n\n\n\n\ndf.select(countDistinct(\"reg\")).show()\n\n\n+-------------------+\ncount(DISTINCT reg)|\n+-------------------+\n                  6|\n+-------------------+\n\n\n\n\n\ndf.select(countDistinct(\"reg\").alias(\"Distinct Region\")).show()\n\n\n+---------------+\nDistinct Region|\n+---------------+\n              6|\n+---------------+\n\n\n\n\n\ndf.select(avg('specific')).show()\n\n\n+-----------------+\n    avg(specific)|\n+-----------------+\n583470.7303370787|\n+-----------------+\n\n\n\n\n\ndf.select(stddev(\"specific\")).show()\n\n\n+---------------------+\nstddev_samp(specific)|\n+---------------------+\n    654055.3290782663|\n+---------------------+"
  },
  {
    "objectID": "posts/2020-08-17-Pyspark-Group-By.html#choosing-significant-digits-with-format_number",
    "href": "posts/2020-08-17-Pyspark-Group-By.html#choosing-significant-digits-with-format_number",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Choosing Significant Digits with format_number",
    "text": "Choosing Significant Digits with format_number\n\nfrom pyspark.sql.functions import format_number\n\n\nspecific_std = df.select(stddev(\"specific\").alias('std'))\nspecific_std.show()\n\n\n+-----------------+\n              std|\n+-----------------+\n654055.3290782663|\n+-----------------+\n\n\n\n\n\nspecific_std.select(format_number('std',0)).show()\n\n\n+---------------------+\nformat_number(std, 0)|\n+---------------------+\n              654,055|\n+---------------------+"
  },
  {
    "objectID": "posts/2020-08-17-Pyspark-Group-By.html#using-orderby",
    "href": "posts/2020-08-17-Pyspark-Group-By.html#using-orderby",
    "title": "Group By and Aggregation with Pyspark",
    "section": "Using orderBy",
    "text": "Using orderBy\n\ndf.orderBy(\"specific\").show()\n\n\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\n_c0| province|specific|general|year|     gdp|   fdi|        rnr|  rr|   i|     fr|            reg|     it|\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\n 28|Chongqing|    null|   null|2000|  1791.0| 24436|       null|null|null|   null|Southwest China|1022148|\n109|    Hebei|    null|   null|1997| 3953.78|110064|       null|null|null|   null|    North China| 826734|\n 24|Chongqing|    null|   null|1996| 1315.12| 21878|       null|null|null|   null|Southwest China| 176802|\n 25|Chongqing|    null|   null|1997| 1509.75| 38675|       null|null|null|   null|Southwest China| 383402|\n268| Shanghai|  8964.0|   null|2000| 4771.17|316014|        0.0| 0.0|0.44|2224124|     East China|1212473|\n269| Shanghai|  9834.0|   null|2001| 5210.12|429159|        0.0| 0.0|0.44|2947285|     East China|1053917|\n312|    Tibet| 18829.0|   null|1996|   64.98|   679|0.181818182| 0.0| 0.0|  27801|Southwest China| 306114|\n270| Shanghai| 19985.0|   null|2002| 5741.03|427229|        0.0| 0.0|0.44|3380397|     East China|1572208|\n271| Shanghai| 23547.0|   null|2003| 6694.23|546849|        0.0|0.53| 0.0|4461153|     East China|2031496|\n313|    Tibet| 25185.0|   null|1997|   77.24|    63|0.181818182| 0.0| 0.0|  33787|Southwest China| 346368|\n273| Shanghai| 29943.0|   null|2005| 9247.66|685000|        0.0|0.53| 0.0|   null|     East China|2140461|\n272| Shanghai| 29943.0|   null|2004| 8072.83|654100|        0.0|0.53| 0.0|   null|     East China|2703643|\n216|  Ningxia| 32088.0|   null|1996|   202.9|  2826|       null|null|null|  90805|Northwest China| 178668|\n305|  Tianjin| 35084.0|   null|2001| 1919.09|213348|        0.0| 0.0| 0.0| 942763|    North China| 688810|\n228|  Qinghai| 37976.0|   null|1996|  184.17|   576|       null|null|null|  73260|Northwest China| 218361|\n302|  Tianjin| 39364.0|   null|1998|  1374.6|211361|       null|null|null| 540178|    North China| 361723|\n274| Shanghai| 42928.0|   null|2006|10572.24|710700|        0.0|0.53| 0.0|8175966|     East China|2239987|\n217|  Ningxia| 44267.0|   null|1997|  224.59|   671|       null|null|null| 102083|Northwest China| 195295|\n303|  Tianjin| 45463.0|   null|1999| 1500.95|176399|        0.0| 0.0| 0.0| 605662|    North China| 422522|\n314|    Tibet| 48197.0|   null|1998|    91.5|   481|        0.0|0.24| 0.0|   3810|Southwest China| 415547|\n+---+---------+--------+-------+----+--------+------+-----------+----+----+-------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.orderBy(df[\"specific\"].desc()).show()\n\n\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\n_c0|    province| specific|  general|year|     gdp|    fdi|        rnr|         rr|                  i|      fr|                reg|      it|\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\n299|     Sichuan|3937966.0|1725100.0|2007|10562.39| 149322|       null|       null|               null| 8508606|    Southwest China|10384846|\n143|       Henan|3860764.0|1737800.0|2007|15012.46| 306162|        0.0|        0.0|                0.0| 8620804|South Central China|10533312|\n131|Heilongjiang|3847672.0|1046700.0|2007|  7104.0| 208508|        0.0|        0.0|0.21621621600000002| 4404689|    Northeast China| 7968319|\n215|    Liaoning|3396397.0| 599600.0|2007| 9304.52| 598554|0.516129032|        0.0|               null|10826948|    Northeast China| 5502192|\n167|       Hunan|3156087.0|1329200.0|2007|  9439.6| 327051|        0.0|     0.4375|                0.0| 6065508|South Central China| 8340692|\n119|       Hebei|2981235.0| 694400.0|2007|13607.32| 241621|        0.0|        0.5|                0.0| 7891198|        North China| 7537692|\n155|       Hubei|2922784.0|1263500.0|2007|  9333.4| 276622|        0.0|0.111111111|                0.0| 5903552|South Central China| 7666512|\n251|     Shaanxi|2669238.0|1081000.0|2007| 5757.29| 119516|    0.03125|        0.0|             0.8125| 4752398|    Northwest China| 6308151|\n203|       Jilin|2663667.0|1016400.0|2007| 4275.12|  76064|        0.0|        0.0|                0.0| 3206892|    Northeast China| 4607955|\n347|      Yunnan|2482173.0| 564400.0|2007| 4772.52|  39453|        0.0|        0.0|                0.0| 4867146|    Southwest China| 6832541|\n298|     Sichuan|2225220.0|1187958.0|2006| 8690.24| 120819|        0.0|        0.0|               0.55| 4247403|    Southwest China| 7646885|\n 11|       Anhui|2213991.0| 178705.0|2007| 7360.92| 299892|        0.0|        0.0|        0.324324324| 4468640|         East China| 7040099|\n287|      Shanxi|2189020.0| 661200.0|2007| 6024.45| 134283|       null|       null|               null| 5978870|        North China| 5070166|\n263|    Shandong|2121243.0| 581800.0|2007|25776.91|1101159|        0.0|        0.0|                0.0|16753980|         East China| 6357869|\n191|     Jiangxi|2045869.0|1272600.0|2007| 4820.53| 280657|        0.0| 0.41025641|                0.0| 3898510|         East China| 4229821|\n 83|     Guangxi|2022957.0|1214100.0|2007| 5823.41|  68396|0.205128205|        0.0|0.23076923100000002| 4188265|South Central China| 6185600|\n142|       Henan|2018158.0|1131615.0|2006|12362.79| 184526|        0.0|        0.0|                0.0| 6212824|South Central China| 7601825|\n 59|       Gansu|2010553.0|1039400.0|2007| 2703.98|  11802|       null|        0.0|               1.05| 1909107|    Northwest China| 5111059|\n 95|     Guizhou|1956261.0|1239200.0|2007| 2884.11|  12651|        0.0|        0.0| 0.7105263159999999| 2851375|    Southwest China| 5639838|\n214|    Liaoning|1947031.0| 179893.0|2006| 8047.26| 359000|0.516129032|        0.0|               null| 6530236|    Northeast China| 4605917|\n+---+------------+---------+---------+----+--------+-------+-----------+-----------+-------------------+--------+-------------------+--------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-10-26-Working_with_sqlite3_dbs_in_jupyter_Soccer_Pred.html",
    "href": "posts/2020-10-26-Working_with_sqlite3_dbs_in_jupyter_Soccer_Pred.html",
    "title": "Working with sqlite databases in Jupyter for European Soccer Match Data",
    "section": "",
    "text": "This post includes code and notes from data-analysis-using-sql.\n\nimport sqlalchemy as db\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\n\nengine = db.create_engine('sqlite:///database.sqlite')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nconnection\n\n&lt;sqlalchemy.engine.base.Connection at 0x7fb9c3356780&gt;\n\n\n\nengine.execute(\"SELECT * FROM Country LIMIT 10\").fetchall()\n\n[(1, 'Belgium'),\n (1729, 'England'),\n (4769, 'France'),\n (7809, 'Germany'),\n (10257, 'Italy'),\n (13274, 'Netherlands'),\n (15722, 'Poland'),\n (17642, 'Portugal'),\n (19694, 'Scotland'),\n (21518, 'Spain')]\n\n\n\n%load_ext sql\n\nThe sql extension is already loaded. To reload it, use:\n  %reload_ext sql\n\n\n\n%sql sqlite:///database.sqlite\n\n\n%%sql\nSELECT *\nFROM Country\nLIMIT 10\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nname\n\n\n1\nBelgium\n\n\n1729\nEngland\n\n\n4769\nFrance\n\n\n7809\nGermany\n\n\n10257\nItaly\n\n\n13274\nNetherlands\n\n\n15722\nPoland\n\n\n17642\nPortugal\n\n\n19694\nScotland\n\n\n21518\nSpain\n\n\n\n\n\n\n%%sql\nSELECT id\n,name\nFROM Country\nWHERE name = \"England\"\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nname\n\n\n1729\nEngland\n\n\n\n\n\n\n%%sql\nSELECT * FROM League LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\ncountry_id\nname\n\n\n1\n1\nBelgium Jupiler League\n\n\n1729\n1729\nEngland Premier League\n\n\n4769\n4769\nFrance Ligue 1\n\n\n7809\n7809\nGermany 1. Bundesliga\n\n\n10257\n10257\nItaly Serie A\n\n\n13274\n13274\nNetherlands Eredivisie\n\n\n15722\n15722\nPoland Ekstraklasa\n\n\n17642\n17642\nPortugal Liga ZON Sagres\n\n\n19694\n19694\nScotland Premier League\n\n\n21518\n21518\nSpain LIGA BBVA\n\n\n\n\n\n\n%%sql\nSELECT * FROM Match LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\ncountry_id\nleague_id\nseason\nstage\ndate\nmatch_api_id\nhome_team_api_id\naway_team_api_id\nhome_team_goal\naway_team_goal\nhome_player_X1\nhome_player_X2\nhome_player_X3\nhome_player_X4\nhome_player_X5\nhome_player_X6\nhome_player_X7\nhome_player_X8\nhome_player_X9\nhome_player_X10\nhome_player_X11\naway_player_X1\naway_player_X2\naway_player_X3\naway_player_X4\naway_player_X5\naway_player_X6\naway_player_X7\naway_player_X8\naway_player_X9\naway_player_X10\naway_player_X11\nhome_player_Y1\nhome_player_Y2\nhome_player_Y3\nhome_player_Y4\nhome_player_Y5\nhome_player_Y6\nhome_player_Y7\nhome_player_Y8\nhome_player_Y9\nhome_player_Y10\nhome_player_Y11\naway_player_Y1\naway_player_Y2\naway_player_Y3\naway_player_Y4\naway_player_Y5\naway_player_Y6\naway_player_Y7\naway_player_Y8\naway_player_Y9\naway_player_Y10\naway_player_Y11\nhome_player_1\nhome_player_2\nhome_player_3\nhome_player_4\nhome_player_5\nhome_player_6\nhome_player_7\nhome_player_8\nhome_player_9\nhome_player_10\nhome_player_11\naway_player_1\naway_player_2\naway_player_3\naway_player_4\naway_player_5\naway_player_6\naway_player_7\naway_player_8\naway_player_9\naway_player_10\naway_player_11\ngoal\nshoton\nshotoff\nfoulcommit\ncard\ncross\ncorner\npossession\nB365H\nB365D\nB365A\nBWH\nBWD\nBWA\nIWH\nIWD\nIWA\nLBH\nLBD\nLBA\nPSH\nPSD\nPSA\nWHH\nWHD\nWHA\nSJH\nSJD\nSJA\nVCH\nVCD\nVCA\nGBH\nGBD\nGBA\nBSH\nBSD\nBSA\n\n\n1\n1\n1\n2008/2009\n1\n2008-08-17 00:00:00\n492473\n9987\n9993\n1\n1\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.73\n3.4\n5\n1.75\n3.35\n4.2\n1.85\n3.2\n3.5\n1.8\n3.3\n3.75\nNone\nNone\nNone\n1.7\n3.3\n4.33\n1.9\n3.3\n4\n1.65\n3.4\n4.5\n1.78\n3.25\n4\n1.73\n3.4\n4.2\n\n\n2\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492474\n10000\n9994\n0\n0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.95\n3.2\n3.6\n1.8\n3.3\n3.95\n1.9\n3.2\n3.5\n1.9\n3.2\n3.5\nNone\nNone\nNone\n1.83\n3.3\n3.6\n1.95\n3.3\n3.8\n2\n3.25\n3.25\n1.85\n3.25\n3.75\n1.91\n3.25\n3.6\n\n\n3\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492475\n9984\n8635\n0\n3\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2.38\n3.3\n2.75\n2.4\n3.3\n2.55\n2.6\n3.1\n2.3\n2.5\n3.2\n2.5\nNone\nNone\nNone\n2.5\n3.25\n2.4\n2.63\n3.3\n2.5\n2.35\n3.25\n2.65\n2.5\n3.2\n2.5\n2.3\n3.2\n2.75\n\n\n4\n1\n1\n2008/2009\n1\n2008-08-17 00:00:00\n492476\n9991\n9998\n5\n0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.44\n3.75\n7.5\n1.4\n4\n6.8\n1.4\n3.9\n6\n1.44\n3.6\n6.5\nNone\nNone\nNone\n1.44\n3.75\n6\n1.44\n4\n7.5\n1.45\n3.75\n6.5\n1.5\n3.75\n5.5\n1.44\n3.75\n6.5\n\n\n5\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492477\n7947\n9985\n1\n3\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n5\n3.5\n1.65\n5\n3.5\n1.6\n4\n3.3\n1.7\n4\n3.4\n1.72\nNone\nNone\nNone\n4.2\n3.4\n1.7\n4.5\n3.5\n1.73\n4.5\n3.4\n1.65\n4.5\n3.5\n1.65\n4.75\n3.3\n1.67\n\n\n6\n1\n1\n2008/2009\n1\n2008-09-24 00:00:00\n492478\n8203\n8342\n1\n1\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n4.75\n3.4\n1.67\n4.85\n3.4\n1.65\n3.7\n3.2\n1.8\n5\n3.25\n1.62\nNone\nNone\nNone\n4.2\n3.4\n1.7\n5.5\n3.75\n1.67\n4.35\n3.4\n1.7\n4.5\n3.4\n1.7\nNone\nNone\nNone\n\n\n7\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492479\n9999\n8571\n2\n2\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2.1\n3.2\n3.3\n2.05\n3.25\n3.15\n1.85\n3.2\n3.5\n1.83\n3.3\n3.6\nNone\nNone\nNone\n1.83\n3.3\n3.6\n1.91\n3.4\n3.6\n2.1\n3.25\n3\n1.85\n3.25\n3.75\n2.1\n3.25\n3.1\n\n\n8\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492480\n4049\n9996\n1\n2\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n3.2\n3.4\n2.2\n2.55\n3.3\n2.4\n2.4\n3.2\n2.4\n2.5\n3.2\n2.5\nNone\nNone\nNone\n2.7\n3.25\n2.25\n2.6\n3.4\n2.4\n2.8\n3.25\n2.25\n2.8\n3.2\n2.25\n2.88\n3.25\n2.2\n\n\n9\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492481\n10001\n9986\n1\n0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2.25\n3.25\n2.88\n2.3\n3.25\n2.7\n2.1\n3.1\n3\n2.25\n3.2\n2.75\nNone\nNone\nNone\n2.2\n3.25\n2.75\n2.2\n3.3\n3.1\n2.25\n3.25\n2.8\n2.2\n3.3\n2.8\n2.25\n3.2\n2.8\n\n\n10\n1\n1\n2008/2009\n10\n2008-11-01 00:00:00\n492564\n8342\n8571\n4\n1\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.3\n5.25\n9.5\n1.25\n5\n10\n1.3\n4.2\n8\n1.25\n4.5\n10\nNone\nNone\nNone\n1.35\n4.2\n7\n1.27\n5\n10\n1.3\n4.35\n8.5\n1.25\n5\n10\n1.29\n4.5\n9\n\n\n\n\n\n\n%%sql\nSELECT * FROM Player LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nplayer_api_id\nplayer_name\nplayer_fifa_api_id\nbirthday\nheight\nweight\n\n\n1\n505942\nAaron Appindangoye\n218353\n1992-02-29 00:00:00\n182.88\n187\n\n\n2\n155782\nAaron Cresswell\n189615\n1989-12-15 00:00:00\n170.18\n146\n\n\n3\n162549\nAaron Doran\n186170\n1991-05-13 00:00:00\n170.18\n163\n\n\n4\n30572\nAaron Galindo\n140161\n1982-05-08 00:00:00\n182.88\n198\n\n\n5\n23780\nAaron Hughes\n17725\n1979-11-08 00:00:00\n182.88\n154\n\n\n6\n27316\nAaron Hunt\n158138\n1986-09-04 00:00:00\n182.88\n161\n\n\n7\n564793\nAaron Kuhl\n221280\n1996-01-30 00:00:00\n172.72\n146\n\n\n8\n30895\nAaron Lennon\n152747\n1987-04-16 00:00:00\n165.1\n139\n\n\n9\n528212\nAaron Lennox\n206592\n1993-02-19 00:00:00\n190.5\n181\n\n\n10\n101042\nAaron Meijers\n188621\n1987-10-28 00:00:00\n175.26\n170\n\n\n\n\n\n\n%%sql\nSELECT * FROM Player_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nplayer_fifa_api_id\nplayer_api_id\ndate\noverall_rating\npotential\npreferred_foot\nattacking_work_rate\ndefensive_work_rate\ncrossing\nfinishing\nheading_accuracy\nshort_passing\nvolleys\ndribbling\ncurve\nfree_kick_accuracy\nlong_passing\nball_control\nacceleration\nsprint_speed\nagility\nreactions\nbalance\nshot_power\njumping\nstamina\nstrength\nlong_shots\naggression\ninterceptions\npositioning\nvision\npenalties\nmarking\nstanding_tackle\nsliding_tackle\ngk_diving\ngk_handling\ngk_kicking\ngk_positioning\ngk_reflexes\n\n\n1\n218353\n505942\n2016-02-18 00:00:00\n67\n71\nright\nmedium\nmedium\n49\n44\n71\n61\n44\n51\n45\n39\n64\n49\n60\n64\n59\n47\n65\n55\n58\n54\n76\n35\n71\n70\n45\n54\n48\n65\n69\n69\n6\n11\n10\n8\n8\n\n\n2\n218353\n505942\n2015-11-19 00:00:00\n67\n71\nright\nmedium\nmedium\n49\n44\n71\n61\n44\n51\n45\n39\n64\n49\n60\n64\n59\n47\n65\n55\n58\n54\n76\n35\n71\n70\n45\n54\n48\n65\n69\n69\n6\n11\n10\n8\n8\n\n\n3\n218353\n505942\n2015-09-21 00:00:00\n62\n66\nright\nmedium\nmedium\n49\n44\n71\n61\n44\n51\n45\n39\n64\n49\n60\n64\n59\n47\n65\n55\n58\n54\n76\n35\n63\n41\n45\n54\n48\n65\n66\n69\n6\n11\n10\n8\n8\n\n\n4\n218353\n505942\n2015-03-20 00:00:00\n61\n65\nright\nmedium\nmedium\n48\n43\n70\n60\n43\n50\n44\n38\n63\n48\n60\n64\n59\n46\n65\n54\n58\n54\n76\n34\n62\n40\n44\n53\n47\n62\n63\n66\n5\n10\n9\n7\n7\n\n\n5\n218353\n505942\n2007-02-22 00:00:00\n61\n65\nright\nmedium\nmedium\n48\n43\n70\n60\n43\n50\n44\n38\n63\n48\n60\n64\n59\n46\n65\n54\n58\n54\n76\n34\n62\n40\n44\n53\n47\n62\n63\n66\n5\n10\n9\n7\n7\n\n\n6\n189615\n155782\n2016-04-21 00:00:00\n74\n76\nleft\nhigh\nmedium\n80\n53\n58\n71\n40\n73\n70\n69\n68\n71\n79\n78\n78\n67\n90\n71\n85\n79\n56\n62\n68\n67\n60\n66\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n7\n189615\n155782\n2016-04-07 00:00:00\n74\n76\nleft\nhigh\nmedium\n80\n53\n58\n71\n32\n73\n70\n69\n68\n71\n79\n78\n78\n67\n90\n71\n85\n79\n56\n60\n68\n67\n60\n66\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n8\n189615\n155782\n2016-01-07 00:00:00\n73\n75\nleft\nhigh\nmedium\n79\n52\n57\n70\n29\n71\n68\n69\n68\n70\n79\n78\n78\n67\n90\n71\n84\n79\n56\n59\n67\n66\n58\n65\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n9\n189615\n155782\n2015-12-24 00:00:00\n73\n75\nleft\nhigh\nmedium\n79\n51\n57\n70\n29\n71\n68\n69\n68\n70\n79\n78\n78\n67\n90\n71\n84\n79\n56\n58\n67\n66\n58\n65\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n10\n189615\n155782\n2015-12-17 00:00:00\n73\n75\nleft\nhigh\nmedium\n79\n51\n57\n70\n29\n71\n68\n69\n68\n70\n79\n78\n78\n67\n90\n71\n84\n79\n56\n58\n67\n66\n58\n65\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n\n\n\n\n%%sql\nSELECT * FROM Team LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nteam_api_id\nteam_fifa_api_id\nteam_long_name\nteam_short_name\n\n\n1\n9987\n673\nKRC Genk\nGEN\n\n\n2\n9993\n675\nBeerschot AC\nBAC\n\n\n3\n10000\n15005\nSV Zulte-Waregem\nZUL\n\n\n4\n9994\n2007\nSporting Lokeren\nLOK\n\n\n5\n9984\n1750\nKSV Cercle Brugge\nCEB\n\n\n6\n8635\n229\nRSC Anderlecht\nAND\n\n\n7\n9991\n674\nKAA Gent\nGEN\n\n\n8\n9998\n1747\nRAEC Mons\nMON\n\n\n9\n7947\nNone\nFCV Dender EH\nDEN\n\n\n10\n9985\n232\nStandard de Liège\nSTL\n\n\n\n\n\n\n%%sql\nSELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nteam_fifa_api_id\nteam_api_id\ndate\nbuildUpPlaySpeed\nbuildUpPlaySpeedClass\nbuildUpPlayDribbling\nbuildUpPlayDribblingClass\nbuildUpPlayPassing\nbuildUpPlayPassingClass\nbuildUpPlayPositioningClass\nchanceCreationPassing\nchanceCreationPassingClass\nchanceCreationCrossing\nchanceCreationCrossingClass\nchanceCreationShooting\nchanceCreationShootingClass\nchanceCreationPositioningClass\ndefencePressure\ndefencePressureClass\ndefenceAggression\ndefenceAggressionClass\ndefenceTeamWidth\ndefenceTeamWidthClass\ndefenceDefenderLineClass\n\n\n1\n434\n9930\n2010-02-22 00:00:00\n60\nBalanced\nNone\nLittle\n50\nMixed\nOrganised\n60\nNormal\n65\nNormal\n55\nNormal\nOrganised\n50\nMedium\n55\nPress\n45\nNormal\nCover\n\n\n2\n434\n9930\n2014-09-19 00:00:00\n52\nBalanced\n48\nNormal\n56\nMixed\nOrganised\n54\nNormal\n63\nNormal\n64\nNormal\nOrganised\n47\nMedium\n44\nPress\n54\nNormal\nCover\n\n\n3\n434\n9930\n2015-09-10 00:00:00\n47\nBalanced\n41\nNormal\n54\nMixed\nOrganised\n54\nNormal\n63\nNormal\n64\nNormal\nOrganised\n47\nMedium\n44\nPress\n54\nNormal\nCover\n\n\n4\n77\n8485\n2010-02-22 00:00:00\n70\nFast\nNone\nLittle\n70\nLong\nOrganised\n70\nRisky\n70\nLots\n70\nLots\nOrganised\n60\nMedium\n70\nDouble\n70\nWide\nCover\n\n\n5\n77\n8485\n2011-02-22 00:00:00\n47\nBalanced\nNone\nLittle\n52\nMixed\nOrganised\n53\nNormal\n48\nNormal\n52\nNormal\nOrganised\n47\nMedium\n47\nPress\n52\nNormal\nCover\n\n\n6\n77\n8485\n2012-02-22 00:00:00\n58\nBalanced\nNone\nLittle\n62\nMixed\nOrganised\n45\nNormal\n70\nLots\n55\nNormal\nOrganised\n40\nMedium\n40\nPress\n60\nNormal\nCover\n\n\n7\n77\n8485\n2013-09-20 00:00:00\n62\nBalanced\nNone\nLittle\n45\nMixed\nOrganised\n40\nNormal\n50\nNormal\n55\nNormal\nOrganised\n42\nMedium\n42\nPress\n60\nNormal\nCover\n\n\n8\n77\n8485\n2014-09-19 00:00:00\n58\nBalanced\n64\nNormal\n62\nMixed\nOrganised\n56\nNormal\n68\nLots\n57\nNormal\nOrganised\n41\nMedium\n42\nPress\n60\nNormal\nCover\n\n\n9\n77\n8485\n2015-09-10 00:00:00\n59\nBalanced\n64\nNormal\n53\nMixed\nOrganised\n51\nNormal\n72\nLots\n63\nNormal\nFree Form\n49\nMedium\n45\nPress\n63\nNormal\nCover\n\n\n10\n614\n8576\n2010-02-22 00:00:00\n60\nBalanced\nNone\nLittle\n40\nMixed\nOrganised\n45\nNormal\n35\nNormal\n55\nNormal\nOrganised\n30\nDeep\n70\nDouble\n30\nNarrow\nOffside Trap\n\n\n\n\n\n\n%%sql\nCREATE TABLE Team_table AS\nSELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n\n\n\n%%sql\nDROP TABLE IF EXISTS Team_table\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n\n\n\nsql_query = %sql SELECT * FROM Team LIMIT 10\ndf = sql_query.DataFrame()\n\n * sqlite:///database.sqlite\nDone.\n\n\n\ndf\n\n\n\n\n\n\n\n\nid\nteam_api_id\nteam_fifa_api_id\nteam_long_name\nteam_short_name\n\n\n\n\n0\n1\n9987\n673.0\nKRC Genk\nGEN\n\n\n1\n2\n9993\n675.0\nBeerschot AC\nBAC\n\n\n2\n3\n10000\n15005.0\nSV Zulte-Waregem\nZUL\n\n\n3\n4\n9994\n2007.0\nSporting Lokeren\nLOK\n\n\n4\n5\n9984\n1750.0\nKSV Cercle Brugge\nCEB\n\n\n5\n6\n8635\n229.0\nRSC Anderlecht\nAND\n\n\n6\n7\n9991\n674.0\nKAA Gent\nGEN\n\n\n7\n8\n9998\n1747.0\nRAEC Mons\nMON\n\n\n8\n9\n7947\nNaN\nFCV Dender EH\nDEN\n\n\n9\n10\n9985\n232.0\nStandard de Liège\nSTL\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,7))\n\nplot = %sql SELECT team_short_name, count(*) FROM Team GROUP BY team_short_name ORDER BY team_short_name\nplot.bar();\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\n\nplot.pie();\n\n\n\n\n\ntype(plot)\n\nsql.run.ResultSet"
  },
  {
    "objectID": "posts/2020-10-16-dask-NLP-gutenberg-books.html",
    "href": "posts/2020-10-16-dask-NLP-gutenberg-books.html",
    "title": "Using Dask with dask.bag and regex to parse The Brothers Karamazov from project gutenberg",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\n\nimport dask.bag as db\nimport re\n\n\nbook_bag = db.from_url('https://www.gutenberg.org/files/28054/28054-0.txt')\n\n\nbook_bag.take(5)\n\n(b'\\xef\\xbb\\xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor\\r\\n',\n b'Dostoyevsky\\r\\n',\n b'\\r\\n',\n b'\\r\\n',\n b'\\r\\n')\n\n\n\nremove_spaces = book_bag.map(lambda x:x.strip())\n\n\nremove_spaces.take(10)\n\n(b'\\xef\\xbb\\xbfThe Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n b'Dostoyevsky',\n b'',\n b'',\n b'',\n b'This ebook is for the use of anyone anywhere in the United States and most',\n b'other parts of the world at no cost and with almost no restrictions',\n b'whatsoever. You may copy it, give it away or re\\xe2\\x80\\x90use it under the terms of',\n b'the Project Gutenberg License included with this eBook or online at',\n b'http://www.gutenberg.org/license. If you are not located in the United')\n\n\n\ndef decode_to_ascii(x):\n    return x.decode(\"ascii\",\"ignore\") \n\n\nascii_text = remove_spaces.map(decode_to_ascii)\n\n\nascii_text.take(10)\n\n('The Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n 'Dostoyevsky',\n '',\n '',\n '',\n 'This ebook is for the use of anyone anywhere in the United States and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever. You may copy it, give it away or reuse it under the terms of',\n 'the Project Gutenberg License included with this eBook or online at',\n 'http://www.gutenberg.org/license. If you are not located in the United')\n\n\n\ndef remove_punctuation(x):\n    return re.sub(r'[^\\w\\s]','',x)\n\n\nremove_punctuation = ascii_text.map(remove_punctuation)\n\n\nremove_punctuation.take(10)\n\n('The Project Gutenberg EBook of The Brothers Karamazov by Fyodor',\n 'Dostoyevsky',\n '',\n '',\n '',\n 'This ebook is for the use of anyone anywhere in the United States and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever You may copy it give it away or reuse it under the terms of',\n 'the Project Gutenberg License included with this eBook or online at',\n 'httpwwwgutenbergorglicense If you are not located in the United')\n\n\n\nlower_text = remove_punctuation.map(str.lower)\n\n\nlower_text.take(10)\n\n('the project gutenberg ebook of the brothers karamazov by fyodor',\n 'dostoyevsky',\n '',\n '',\n '',\n 'this ebook is for the use of anyone anywhere in the united states and most',\n 'other parts of the world at no cost and with almost no restrictions',\n 'whatsoever you may copy it give it away or reuse it under the terms of',\n 'the project gutenberg license included with this ebook or online at',\n 'httpwwwgutenbergorglicense if you are not located in the united')\n\n\n\nsplit_word_list = lower_text.map(lambda x: x.split(' '))\n\n\nsplit_word_list.take(10)\n\n(['the',\n  'project',\n  'gutenberg',\n  'ebook',\n  'of',\n  'the',\n  'brothers',\n  'karamazov',\n  'by',\n  'fyodor'],\n ['dostoyevsky'],\n [''],\n [''],\n [''],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'in',\n  'the',\n  'united',\n  'states',\n  'and',\n  'most'],\n ['other',\n  'parts',\n  'of',\n  'the',\n  'world',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with',\n  'almost',\n  'no',\n  'restrictions'],\n ['whatsoever',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or',\n  'reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of'],\n ['the',\n  'project',\n  'gutenberg',\n  'license',\n  'included',\n  'with',\n  'this',\n  'ebook',\n  'or',\n  'online',\n  'at'],\n ['httpwwwgutenbergorglicense',\n  'if',\n  'you',\n  'are',\n  'not',\n  'located',\n  'in',\n  'the',\n  'united'])\n\n\n\ndef remove_empty_words(word_list):\n    return list(filter(lambda a: a != '', word_list))\n\nnon_empty_words = split_word_list.filter(remove_empty_words)\n\n\nnon_empty_words.take(10)\n\n(['the',\n  'project',\n  'gutenberg',\n  'ebook',\n  'of',\n  'the',\n  'brothers',\n  'karamazov',\n  'by',\n  'fyodor'],\n ['dostoyevsky'],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'in',\n  'the',\n  'united',\n  'states',\n  'and',\n  'most'],\n ['other',\n  'parts',\n  'of',\n  'the',\n  'world',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with',\n  'almost',\n  'no',\n  'restrictions'],\n ['whatsoever',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or',\n  'reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of'],\n ['the',\n  'project',\n  'gutenberg',\n  'license',\n  'included',\n  'with',\n  'this',\n  'ebook',\n  'or',\n  'online',\n  'at'],\n ['httpwwwgutenbergorglicense',\n  'if',\n  'you',\n  'are',\n  'not',\n  'located',\n  'in',\n  'the',\n  'united'],\n ['states',\n  'youll',\n  'have',\n  'to',\n  'check',\n  'the',\n  'laws',\n  'of',\n  'the',\n  'country',\n  'where',\n  'you',\n  'are',\n  'located'],\n ['before', 'using', 'this', 'ebook'],\n ['title', 'the', 'brothers', 'karamazov'])\n\n\n\nall_words = non_empty_words.flatten()\n\n\ntype(all_words)\n\ndask.bag.core.Bag\n\n\n\nall_words.take(30)\n\n('the',\n 'project',\n 'gutenberg',\n 'ebook',\n 'of',\n 'the',\n 'brothers',\n 'karamazov',\n 'by',\n 'fyodor',\n 'dostoyevsky',\n 'this',\n 'ebook',\n 'is',\n 'for',\n 'the',\n 'use',\n 'of',\n 'anyone',\n 'anywhere',\n 'in',\n 'the',\n 'united',\n 'states',\n 'and',\n 'most',\n 'other',\n 'parts',\n 'of',\n 'the')\n\n\n\nchange_to_key_value = all_words.map(lambda x: (x, 1))\n\n\nchange_to_key_value.take(4)\n\n(('the', 1), ('project', 1), ('gutenberg', 1), ('ebook', 1))\n\n\n\ngrouped_words = all_words.groupby(lambda x:x)\n\n\ngrouped_words.take(1)\n\n(('the',\n  ['the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   'the',\n   ...]),)\n\n\n\nword_count = grouped_words.map(lambda x: (x[0], len(x[1])))\n\n\nword_count.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nchange_to_key_value.take(10)\n\n(('the', 1),\n ('project', 1),\n ('gutenberg', 1),\n ('ebook', 1),\n ('of', 1),\n ('the', 1),\n ('brothers', 1),\n ('karamazov', 1),\n ('by', 1),\n ('fyodor', 1))\n\n\n\n# Take a running count of a word\n# In this case, the default value of \n# count needs to be provided\ndef add_bin_op(count, x):\n    return count + x[1]\n\n# Take the output from multiple bin_op(s)\n# and add them to get the total count of\n# a word\ndef add_combine_op(x, y):\n    return x + y\n\nword_count = change_to_key_value.foldby(lambda x: x[0],\n                                       add_bin_op, 0,\n                                       add_combine_op)\n\n\nword_count.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nmuch_easier = all_words.frequencies()\n\n\nmuch_easier.take(10)\n\n(('the', 15379),\n ('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('of', 7410),\n ('brothers', 82),\n ('karamazov', 170),\n ('by', 1165),\n ('fyodor', 303),\n ('dostoyevsky', 3))\n\n\n\nRemoving stop words in top word frequency counts\n\nfrom spacy.lang.en import STOP_WORDS\n\n\nwithout_stopwords = all_words.filter(lambda x: x not in STOP_WORDS)\n\n\nnew_freq = without_stopwords.frequencies()\n\n\nnew_freq.take(20)\n\n(('project', 89),\n ('gutenberg', 88),\n ('ebook', 14),\n ('brothers', 82),\n ('karamazov', 170),\n ('fyodor', 303),\n ('dostoyevsky', 3),\n ('use', 77),\n ('united', 24),\n ('states', 21),\n ('parts', 19),\n ('world', 182),\n ('cost', 12),\n ('restrictions', 2),\n ('whatsoever', 5),\n ('copy', 16),\n ('away', 445),\n ('reuse', 2),\n ('terms', 33),\n ('license', 14))\n\n\n\nnew_freq.topk(10)\n\ndask.bag&lt;topk-aggregate, npartitions=1&gt;\n\n\n\nnew_freq.topk(10, key=lambda x: x[1]).compute()\n\n[('alyosha', 1176),\n ('said', 993),\n ('know', 843),\n ('man', 842),\n ('mitya', 814),\n ('dont', 784),\n ('come', 772),\n ('father', 721),\n ('ivan', 677),\n ('time', 669)]"
  },
  {
    "objectID": "posts/2020-12-31-TextSummarizer-Dickens.html",
    "href": "posts/2020-12-31-TextSummarizer-Dickens.html",
    "title": "Text summarizer in Python, A Tale of Two Cities",
    "section": "",
    "text": "# Natural Language Tool Kit (NLTK)\nimport nltk\nnltk.download('stopwords')\n\n# Regular Expression for text preprocessing\nimport re\n\n# Heap (priority) queue algorithm to get the top sentences\nimport heapq\n\n# NumPy for numerical computing\nimport numpy as np\n\n# pandas for creating DataFrames\nimport pandas as pd\n\n# matplotlib for plot\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip."
  },
  {
    "objectID": "posts/2020-12-31-TextSummarizer-Dickens.html#credit-code-from-httpsgithub.comlouisteo9personal-text-summarizer",
    "href": "posts/2020-12-31-TextSummarizer-Dickens.html#credit-code-from-httpsgithub.comlouisteo9personal-text-summarizer",
    "title": "Text summarizer in Python, A Tale of Two Cities",
    "section": "",
    "text": "# Natural Language Tool Kit (NLTK)\nimport nltk\nnltk.download('stopwords')\n\n# Regular Expression for text preprocessing\nimport re\n\n# Heap (priority) queue algorithm to get the top sentences\nimport heapq\n\n# NumPy for numerical computing\nimport numpy as np\n\n# pandas for creating DataFrames\nimport pandas as pd\n\n# matplotlib for plot\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n[nltk_data]   Unzipping corpora/stopwords.zip."
  },
  {
    "objectID": "posts/2020-12-31-TextSummarizer-Dickens.html#load-text-data",
    "href": "posts/2020-12-31-TextSummarizer-Dickens.html#load-text-data",
    "title": "Text summarizer in Python, A Tale of Two Cities",
    "section": "Load text data",
    "text": "Load text data\n\nimport requests\nimport re\n\nr = requests.get(\"https://www.gutenberg.org/files/98/98-0.txt\")\nraw_text = r.text\n\n\nprint(raw_text[0:1000])\n\nï»¿The Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens\n\nThis eBook is for the use of anyone anywhere in the United States and most\nother parts of the world at no cost and with almost no restrictions\nwhatsoever.  You may copy it, give it away or re-use it under the terms of\nthe Project Gutenberg License included with this eBook or online at\nwww.gutenberg.org.  If you are not located in the United States, you'll have\nto check the laws of the country where you are located before using this ebook.\n\nTitle: A Tale of Two Cities\n       A Story of the French Revolution\n       \nAuthor: Charles Dickens\n\nRelease Date: January, 1994 [EBook #98]\n[Most recently updated: December 20, 2020]\n\nLanguage: English\n\nCharacter set encoding: UTF-8\n\n*** START OF THIS PROJECT GUTENBERG EBOOK A TALE OF TWO CITIES ***\n\n\n\n\nProduced by Judith Boss, and David Widger\n\n\n\n\nA TALE OF TWO CITIES\n\nA STORY OF THE FRENCH REVOLUTION\n\nBy Charles Dickens\n\n\nCONTENTS\n\n\n\n\n\n# # load text file\n# with open('https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt', 'r') as f:\n#     file_data = f.read()\n\n\n# text_file = open(\"https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt\", \"r\")\n# lines = raw_text.readlines()\n\n\n# lines = raw_text.readlines()\n\n\n# text_file.close()\n\n\n# df = pd.read_txt('https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt')\n\n\n# # view text data\n# print(lines)"
  },
  {
    "objectID": "posts/2020-12-31-TextSummarizer-Dickens.html#preprocess-text",
    "href": "posts/2020-12-31-TextSummarizer-Dickens.html#preprocess-text",
    "title": "Text summarizer in Python, A Tale of Two Cities",
    "section": "Preprocess text",
    "text": "Preprocess text\n\ntext = raw_text\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # replace reference number i.e. [1], [10], [20] with empty space, if any..\ntext = re.sub(r'\\s+',' ',text) # replace one or more spaces with single space\n#print(text)\n\nNext, we form a clean text with lower case (without special characters, digits and extra spaces) and split it into individual word, for word score computation and formation of the word histogram.\nThe reason to form a clean text is so that the algorithm won’t treat, i.e. “understanding” and understanding, as two different words.\n\n# generate clean text\nclean_text = text.lower() # convert all uppercase characters into lowercase characters\nclean_text = re.sub(r'\\W',' ',clean_text) # replace character other than [a-zA-Z0-9] with empty space\nclean_text = re.sub(r'\\d',' ',clean_text) # replace digit with empty space\nclean_text = re.sub(r'\\s+',' ',clean_text) # replace one or more spaces with a single space\n\n#print(clean_text)"
  },
  {
    "objectID": "posts/2020-12-31-TextSummarizer-Dickens.html#split-text-into-sentences",
    "href": "posts/2020-12-31-TextSummarizer-Dickens.html#split-text-into-sentences",
    "title": "Text summarizer in Python, A Tale of Two Cities",
    "section": "Split text into sentences",
    "text": "Split text into sentences\nWe split (tokenize) the text into sentences using NLTK sent_tokenize() method. We will evaluate the importance of each of sentences, then decide if we should each include in our summary.\n\n# split (tokenize) the sentences\nsentences = nltk.sent_tokenize(text)\n#print(sentences)\n\n\nimport nltk\nnltk.download('punkt')\n\nfrom nltk import word_tokenize,sent_tokenize\n\n[nltk_data] Downloading package punkt to /home/david/nltk_data...\n[nltk_data]   Unzipping tokenizers/punkt.zip."
  },
  {
    "objectID": "posts/2020-12-31-TextSummarizer-Dickens.html#remove-stop-words",
    "href": "posts/2020-12-31-TextSummarizer-Dickens.html#remove-stop-words",
    "title": "Text summarizer in Python, A Tale of Two Cities",
    "section": "Remove stop words",
    "text": "Remove stop words\nStop words are English words which do not add much meaning to a sentence. They can be safely ignored without sacrificing the meaning of the sentence. We already downloaded a file with English stop words in the first section of the notebook.\nHere, we will get the list of stop words and store them in stop_word variable.\n\n# get stop words list\nstop_words = nltk.corpus.stopwords.words('english')\nprint(stop_words)\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
  },
  {
    "objectID": "posts/2020-12-31-TextSummarizer-Dickens.html#build-word-histogram",
    "href": "posts/2020-12-31-TextSummarizer-Dickens.html#build-word-histogram",
    "title": "Text summarizer in Python, A Tale of Two Cities",
    "section": "Build word histogram",
    "text": "Build word histogram\n\n# create an empty dictionary to house the word count\nword_count = {}\n\n# loop through tokenized words, remove stop words and save word count to dictionary\nfor word in nltk.word_tokenize(clean_text):\n    # remove stop words\n    if word not in stop_words:\n        # save word count to dictionary\n        if word not in word_count.keys():\n            word_count[word] = 1\n        else:\n            word_count[word] += 1\n\nLet’s plot the word histogram and see the results.\n\n# plt.figure(figsize=(16,10))\n# plt.xticks(rotation = 90)\n# plt.bar(word_count.keys(), word_count.values())\n# plt.show()\n\n\ndef plot_top_words(word_count_dict, show_top_n=20):\n    word_count_table = pd.DataFrame.from_dict(word_count_dict, orient = 'index').rename(columns={0: 'score'})\n    word_count_table.sort_values(by='score').tail(show_top_n).plot(kind='barh', figsize=(10,10))\n    plt.show()\n\n\nplot_top_words(word_count, 20)\n\n\n\n\n\nsentence_score = {}\n\n# loop through tokenized sentence, only take sentences that have less than 30 words, then add word score to form sentence score\nfor sentence in sentences:\n    # check if word in sentence is in word_count dictionary\n    for word in nltk.word_tokenize(sentence.lower()):\n        if word in word_count.keys():\n            # only take sentence that has less than 30 words\n            if len(sentence.split(' ')) &lt; 30:\n                # add word score to sentence score\n                if sentence not in sentence_score.keys():\n                    sentence_score[sentence] = word_count[word]\n                else:\n                    sentence_score[sentence] += word_count[word]\n\n\ndf_sentence_score = pd.DataFrame.from_dict(sentence_score, orient = 'index').rename(columns={0: 'score'})\ndf_sentence_score.sort_values(by='score', ascending = False)\n\n\n\n\n\n\n\n\nscore\n\n\n\n\nI know this is a confidence,â€? she modestly said, after a little hesitation, and in earnest tears, â€œI know you would say this to no one else.\n2445\n\n\nThus engaged, with her right elbow supported by her left hand, Madame Defarge said nothing when her lord came in, but coughed just one grain of cough.\n2373\n\n\nVengeance and retribution require a long time; it is the rule.â€? â€œIt does not take a long time to strike a man with Lightning,â€? said Defarge.\n2312\n\n\nâ€œWe have borne this a long time,â€? said Madame Defarge, turning her eyes again upon Lucie.\n2208\n\n\nâ€œYou had better, Lucie,â€? said Mr. Lorry, doing all he could to propitiate, by tone and manner, â€œhave the dear child here, and our good Pross.\n2197\n\n\n...\n...\n\n\nWhat are you hooroaring at?\n1\n\n\nâ€œHim and his hooroars!\n1\n\n\nâ€œCome on at a footpace!\n1\n\n\nBut he was not persuaded.\n1\n\n\nâ€œDonâ€™t be ungrateful.\n1\n\n\n\n\n3832 rows × 1 columns\n\n\n\n\nbest_sentences = heapq.nlargest(3, sentence_score, key=sentence_score.get)\n\n\nprint('SUMMARY')\nprint('------------------------')\n\nfor sentence in sentences:\n    if sentence in best_sentences:\n        print (sentence)\n\nSUMMARY\n------------------------\nThus engaged, with her right elbow supported by her left hand, Madame Defarge said nothing when her lord came in, but coughed just one grain of cough.\nI know this is a confidence,â she modestly said, after a little hesitation, and in earnest tears, âI know you would say this to no one else.\nVengeance and retribution require a long time; it is the rule.â âIt does not take a long time to strike a man with Lightning,â said Defarge."
  },
  {
    "objectID": "posts/2020-10-01-StockMarketPortfolioAnaylsis_snp+.html",
    "href": "posts/2020-10-01-StockMarketPortfolioAnaylsis_snp+.html",
    "title": "Stock Market and Portfolio Anaylsis of the S&P 500 various metrics with pandas and quandl",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_YIELD_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-01-31\n1.98\n\n\n2010-02-28\n2.03\n\n\n2010-03-31\n1.90\n\n\n2010-04-30\n1.83\n\n\n2010-05-31\n1.95\n\n\n...\n...\n\n\n2020-07-01\n1.91\n\n\n2020-07-31\n1.86\n\n\n2020-08-31\n1.76\n\n\n2020-09-01\n1.69\n\n\n2020-09-30\n1.69\n\n\n\n\n136 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_PE_RATIO_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-01-01\n20.70\n\n\n2010-02-01\n18.91\n\n\n2010-03-01\n18.91\n\n\n2010-04-01\n19.01\n\n\n2010-05-01\n17.30\n\n\n...\n...\n\n\n2020-07-01\n27.57\n\n\n2020-07-31\n28.12\n\n\n2020-08-01\n29.16\n\n\n2020-08-31\n30.09\n\n\n2020-09-01\n30.32\n\n\n\n\n138 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_PE_RATIO_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-01-01\n20.70\n\n\n2010-02-01\n18.91\n\n\n2010-03-01\n18.91\n\n\n2010-04-01\n19.01\n\n\n2010-05-01\n17.30\n\n\n...\n...\n\n\n2020-07-01\n27.57\n\n\n2020-07-31\n28.12\n\n\n2020-08-01\n29.16\n\n\n2020-08-31\n30.09\n\n\n2020-09-01\n30.32\n\n\n\n\n138 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_EARNINGS_YIELD_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-01-01\n4.83\n\n\n2010-02-01\n5.29\n\n\n2010-03-01\n5.29\n\n\n2010-04-01\n5.26\n\n\n2010-05-01\n5.78\n\n\n...\n...\n\n\n2020-07-01\n3.63\n\n\n2020-07-31\n3.56\n\n\n2020-08-01\n3.43\n\n\n2020-08-31\n3.32\n\n\n2020-09-01\n3.30\n\n\n\n\n138 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_INFLADJ_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-01-01\n1343.51\n\n\n2010-02-01\n1302.03\n\n\n2010-03-01\n1371.58\n\n\n2010-04-01\n1423.00\n\n\n2010-05-01\n1336.08\n\n\n...\n...\n\n\n2020-07-01\n3207.62\n\n\n2020-07-31\n3271.12\n\n\n2020-08-01\n3391.71\n\n\n2020-08-31\n3500.31\n\n\n2020-09-01\n3526.65\n\n\n\n\n138 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-01-31\n26.59\n\n\n2010-02-28\n26.38\n\n\n2010-03-31\n26.08\n\n\n2010-04-30\n26.09\n\n\n2010-05-31\n26.12\n\n\n...\n...\n\n\n2020-02-29\n59.23\n\n\n2020-03-31\n59.81\n\n\n2020-04-30\n60.25\n\n\n2020-05-31\n60.28\n\n\n2020-06-30\n59.99\n\n\n\n\n126 rows × 1 columns\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_YEAR\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-12-31\n26.87\n\n\n2011-12-31\n30.34\n\n\n2012-12-31\n35.26\n\n\n2013-12-31\n38.90\n\n\n2014-12-31\n43.52\n\n\n2015-12-31\n47.53\n\n\n2016-12-31\n49.05\n\n\n2017-12-31\n51.43\n\n\n2018-12-31\n55.43\n\n\n2019-03-31\n55.35\n\n\n2019-06-30\n56.17\n\n\n2019-09-30\n57.32\n\n\n2019-12-31\n58.72\n\n\n2020-03-31\n59.18\n\n\n2020-06-30\n59.99\n\n\n\n\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_GROWTH_YEAR\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-12-31\n1.45\n\n\n2011-12-31\n16.26\n\n\n2012-12-31\n18.25\n\n\n2013-12-31\n11.99\n\n\n2014-12-31\n12.72\n\n\n2015-12-31\n10.00\n\n\n2016-12-31\n5.33\n\n\n2017-12-31\n7.07\n\n\n2018-12-31\n9.84\n\n\n2019-03-31\n9.87\n\n\n2019-06-30\n9.98\n\n\n2019-09-30\n9.32\n\n\n2019-12-31\n8.36\n\n\n2020-03-31\n8.45\n\n\n2020-06-30\n6.43\n\n\n\n\n\n\n\n\nSP500 = quandl.get(\"MULTPL/SP500_DIV_GROWTH_QUARTER\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-03-31\n-19.63\n\n\n2010-06-30\n-13.90\n\n\n2010-09-30\n-6.48\n\n\n2010-12-31\n1.45\n\n\n2011-03-31\n6.97\n\n\n2011-06-30\n10.46\n\n\n2011-09-30\n12.65\n\n\n2011-12-31\n16.26\n\n\n2012-03-31\n16.74\n\n\n2012-06-30\n16.35\n\n\n2012-09-30\n17.51\n\n\n2012-12-31\n18.25\n\n\n2013-03-31\n17.40\n\n\n2013-06-30\n17.47\n\n\n2013-09-30\n16.27\n\n\n2013-12-31\n11.99\n\n\n2014-03-31\n12.82\n\n\n2014-06-30\n12.37\n\n\n2014-09-30\n11.89\n\n\n2014-12-31\n12.72\n\n\n2015-03-31\n12.64\n\n\n2015-06-30\n11.67\n\n\n2015-09-30\n10.43\n\n\n2015-12-31\n10.00\n\n\n2016-03-31\n7.52\n\n\n2016-06-30\n6.51\n\n\n2016-09-30\n5.92\n\n\n2016-12-31\n5.33\n\n\n2017-03-31\n5.71\n\n\n2017-06-30\n6.21\n\n\n2017-09-30\n6.99\n\n\n2017-12-31\n7.07\n\n\n2018-03-31\n7.81\n\n\n2018-06-30\n7.99\n\n\n2018-09-30\n8.65\n\n\n2018-12-31\n9.84\n\n\n2019-03-31\n9.87\n\n\n2019-06-30\n9.98\n\n\n2019-09-30\n9.32\n\n\n2019-12-31\n8.36\n\n\n2020-03-31\n8.45\n\n\n2020-06-30\n6.43\n\n\n\n\n\n\n\n\nSP500.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-01-01\n1123.58\n\n\n2010-02-01\n1089.16\n\n\n2010-03-01\n1152.05\n\n\n2010-04-01\n1197.32\n\n\n2010-05-01\n1125.06\n\n\n\n\n\n\n\n\nSP500.tail()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2020-07-01\n3207.62\n\n\n2020-07-31\n3271.12\n\n\n2020-08-01\n3391.71\n\n\n2020-08-31\n3500.31\n\n\n2020-09-01\n3526.65\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nSP500['Value'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 from 2010 to 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 from 2010 to 2020 Value')"
  },
  {
    "objectID": "posts/2020-09-03-NLP_Seaborn_Heatmaps.html",
    "href": "posts/2020-09-03-NLP_Seaborn_Heatmaps.html",
    "title": "NLP Heatmaps with Seaborn",
    "section": "",
    "text": "from jupyterthemes import jtplot\nimport warnings\nfrom imblearn.over_sampling import SMOTE\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nimport pandas_profiling\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import preprocessing\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# ignore warnings\nwarnings.filterwarnings('ignore')\n\njtplot.style(theme='oceans16', context='notebook',\n             ticks=True, grid=False, figsize=(10, 9))\n\n\ndf=pd.read_csv('../processed_data/nf_complete.csv')\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 126 entries, 0 to 125\nData columns (total 23 columns):\n #   Column                 Non-Null Count  Dtype \n---  ------                 --------------  ----- \n 0   Unnamed: 0             126 non-null    int64 \n 1   year                   126 non-null    int64 \n 2   title                  126 non-null    object\n 3   abstract               126 non-null    object\n 4   theme                  126 non-null    object\n 5   China                  126 non-null    int64 \n 6   Russia                 126 non-null    int64 \n 7   War                    126 non-null    int64 \n 8   President              126 non-null    int64 \n 9   US                     126 non-null    int64 \n 10  Vietnam                126 non-null    int64 \n 11  Cold War               126 non-null    int64 \n 12  World War              126 non-null    int64 \n 13  Vietnam War            126 non-null    int64 \n 14  Korean War             126 non-null    int64 \n 15  Survey                 126 non-null    int64 \n 16  Case Study             126 non-null    int64 \n 17  Trade                  126 non-null    int64 \n 18  Humanitarian           126 non-null    int64 \n 19  fixed_effects          126 non-null    int64 \n 20  instrumental_variable  126 non-null    int64 \n 21  regression             126 non-null    int64 \n 22  experimental           126 non-null    int64 \ndtypes: int64(20), object(3)\nmemory usage: 22.8+ KB\n\n\n\nimport plotly_express as ple\nple.histogram(df.sort_values('year').groupby(['year','theme'])['Cold War'].sum().reset_index(), x=\"year\", y=\"Cold War\", histfunc=\"sum\", color=\"theme\")\n\n\n\n        \n        \n            \n            \n        \n\n\n\n# ple.lidifferences(dfm_regional.sort_values('year').groupby(['year','theme'])['Cold War'].sum().reset_index(),\n#          x='year',\n#          y='Cold War',\n#          line_group='theme',\n#          color='theme'\n#         )\n\n\n# Create the crosstab DataFrame\npd_crosstab = pd.crosstab(df[\"theme\"], df[\"year\"])\n\n# Plot a heatmap of the table with no color bar and using the BuGn palette\nsns.heatmap(pd_crosstab, cbar=False, cmap=\"GnBu\", linewidths=0.3)\n\n# Rotate tick marks for visibility\nplt.yticks(rotation=0)\nplt.xticks(rotation=90)\n\nplt.tight_layout()\n#plt.savefig('./img/theme_heat_1.png', bbox_inches='tight', dpi=500)\n\n\n#Show the plot\nplt.show()\nplt.clf()\n\n\n\n\n&lt;Figure size 720x648 with 0 Axes&gt;\n\n\n\nsns.clustermap(pd_crosstab, cmap='Greens', robust=True)\n\n# plot using a color palette\n#sns.heatmap(df, cmap=\"YlGnBu\")\n#sns.heatmap(df, cmap=\"Blues\")\n#sns.heatmap(df, cmap=\"BuPu\")\n#sns.heatmap(df, cmap=\"Greens\")\n\n\n\n\n\n# Import seaborn library\nimport seaborn as sns\n\n# Get correlation matrix of the meat DataFrame\ncorr_meat = df.corr(method='pearson')\n\n# Customize the heatmap of the corr_meat correlation matrix and rotate the x-axis labels\nfig = sns.clustermap(pd_crosstab,\n                     row_cluster=True,\n                     col_cluster=True,\n                     figsize=(10, 10))\n\nplt.setp(fig.ax_heatmap.xaxis.get_majorticklabels(), rotation=90)\nplt.setp(fig.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n\n\n\n\n\ndata_normalized = pd_crosstab\n\n# Standardize the mean and variance within a stat, so different stats can be comparable\n# (This is the same as changing all the columns to Z-scores)\ndata_normalized = (data_normalized - data_normalized.mean())/data_normalized.var()\n\n# Normalize these values to range from -1 to 1\ndata_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min())\n\ndata_normalized = data_normalized.T\n\n# Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram.\nsns.clustermap(data_normalized, cmap='Blues');\n\n\n\n\n\ndata_normalized = pd_crosstab\n\n# Standardize the mean and variance within a stat, so different stats can be comparable\n# (This is the same as changing all the columns to Z-scores)\ndata_normalized = (data_normalized - data_normalized.mean())/data_normalized.var()\n\n# Normalize these values to range from -1 to 1\ndata_normalized = (data_normalized)/(data_normalized.max() - data_normalized.min())\n\n#data_normalized = data_normalized.T\n\n# Can use a semicolon after the command to suppress output of the row_dendrogram and col_dendrogram.\nsns.clustermap(data_normalized, cmap='BuPu');\n\n\n\n\n\nimport matplotlib.pyplot as plt\nsns.clustermap(data_normalized);\nfig = plt.gcf()\nfig.savefig('clusteredheatmap_bbox_tight.png', bbox_inches='tight')\n\n\n\n\n\ntidy_df = pd.melt(df.reset_index(), id_vars='index')\ndf.T.head()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n...\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n\n\n\n\nUnnamed: 0\n0\n1\n2\n3\n4\n5\n6\n7\n14\n13\n...\n128\n130\n123\n125\n131\n132\n133\n134\n135\n136\n\n\nyear\n2000\n2000\n2000\n2000\n2000\n2000\n2000\n2000\n2001\n2001\n...\n2017\n2017\n2017\n2017\n2018\n2018\n2018\n2018\n2018\n2018\n\n\ntitle\n\"Institutions at the Domestic/International Ne...\nBorn to Lose and Doomed to Survive: State Deat...\nThe significance of “allegiance” in internatio...\nThe significance of “allegiance” in internatio...\nTruth-Telling and Mythmaking in Post-Soviet Ru...\nBuilding a Cape Fear Metropolis: Fort Bragg, F...\nThe Glories and the Sadness: Shaping the natio...\nWhat leads longstanding adversaries to engage ...\nA School for the Nation: Military Institution...\nThe 'American Century' Army: The Origins of t...\n...\nFully Committed? Religiously Committed State P...\nStraddling the Threshold of Two Worlds: Soldie...\nU.S. Army’s Investigation and Adjudication of ...\nGrand Strategic Crucibles: The Lasting Effects...\nTrust in International Politics: The Role of L...\nPlanning for the Short Haul: Trade Among Belli...\nClinging to the Anti-Imperial Mantle: The Repu...\nThe New Navy's Pacific Wars: Peripheral Confl...\nStop or I'll Shoot, Comply and I Won't: The Di...\nUnexpected Humanitarians: Albania, the U.S. Mi...\n\n\nabstract\nCivil-military relations are frequently studie...\nUnder what conditions do states die, or exit t...\nMy dissertation employs original and secondary...\n\\nThis study revises prevailing interpretation...\nCan distorted and pernicious ideas r histo...\nMy dissertation examines the cultural and econ...\nIn my dissertation I compare the ways in whic...\nThis dissertation develops a socio-psychoanal...\nBeginning in Europe in the latter half of the ...\nThis dissertation covers the period 1949-1959 ...\n...\nThis dissertation argues that the higher the l...\nThis dissertation explores how American soldie...\nThis dissertation examines the U.S. Army’s res...\nWhen and how do military interventions shape g...\nIn my dissertation, I focus on how leader rela...\nIn times of war, why do belligerents continue ...\nMy dissertation project, Clinging to the Anti-...\nUsing a transnational methodology and sources ...\nThere is a dilemma at the heart of coercion. S...\nUsing archives and oral history, this disserta...\n\n\ntheme\nIR scholarship\nIR scholarship\nIR scholarship\nConflit Between States\nConflict Between States\nDomestic Military History\nCulture\nCulture / Peace Process\nMilitary History\nMilitary History\n...\nIR Scholarship\nMilitary History\nMilitary History\nIR Scholarship\nNuclear Weapons\nConflict between states\nCold War\nMilitary History\nIR Scholarship\nMilitary History\n\n\n\n\n5 rows × 126 columns"
  },
  {
    "objectID": "posts/2020-10-06-NLP-sqlitedb.html",
    "href": "posts/2020-10-06-NLP-sqlitedb.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for NLP",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here\n\nimport pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf=pd.read_csv('nf_complete.csv')\n\n\ndf.columns\n\nIndex(['Unnamed: 0', 'year', 'title', 'abstract', 'theme', 'China', 'Russia',\n       'War', 'President', 'US', 'Vietnam', 'Cold War', 'World War',\n       'Vietnam War', 'Korean War', 'Survey', 'Case Study', 'Trade',\n       'Humanitarian', 'fixed_effects', 'instrumental_variable', 'regression',\n       'experimental'],\n      dtype='object')\n\n\n\ndf[[\"year\",\"title\"]]\n\n\n\n\n\n\n\n\nyear\ntitle\n\n\n\n\n0\n2000\n\"Institutions at the Domestic/International Ne...\n\n\n1\n2000\nBorn to Lose and Doomed to Survive: State Deat...\n\n\n2\n2000\nThe significance of “allegiance” in internatio...\n\n\n3\n2000\nThe significance of “allegiance” in internatio...\n\n\n4\n2000\nTruth-Telling and Mythmaking in Post-Soviet Ru...\n\n\n...\n...\n...\n\n\n121\n2018\nPlanning for the Short Haul: Trade Among Belli...\n\n\n122\n2018\nClinging to the Anti-Imperial Mantle: The Repu...\n\n\n123\n2018\nThe New Navy's Pacific Wars: Peripheral Confl...\n\n\n124\n2018\nStop or I'll Shoot, Comply and I Won't: The Di...\n\n\n125\n2018\nUnexpected Humanitarians: Albania, the U.S. Mi...\n\n\n\n\n126 rows × 2 columns\n\n\n\n\ndf_subset = df[[\"year\", \"title\", \"abstract\", \"theme\", \"War\", 'Cold War',\"Trade\"]]\ndf_subset\n\n\n\n\n\n\n\n\nyear\ntitle\nabstract\ntheme\nWar\nCold War\nTrade\n\n\n\n\n0\n2000\n\"Institutions at the Domestic/International Ne...\nCivil-military relations are frequently studie...\nIR scholarship\n1\n0\n0\n\n\n1\n2000\nBorn to Lose and Doomed to Survive: State Deat...\nUnder what conditions do states die, or exit t...\nIR scholarship\n1\n1\n0\n\n\n2\n2000\nThe significance of “allegiance” in internatio...\nMy dissertation employs original and secondary...\nIR scholarship\n1\n0\n0\n\n\n3\n2000\nThe significance of “allegiance” in internatio...\n\\nThis study revises prevailing interpretation...\nConflit Between States\n0\n1\n0\n\n\n4\n2000\nTruth-Telling and Mythmaking in Post-Soviet Ru...\nCan distorted and pernicious ideas about histo...\nConflict Between States\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n121\n2018\nPlanning for the Short Haul: Trade Among Belli...\nIn times of war, why do belligerents continue ...\nConflict between states\n1\n0\n1\n\n\n122\n2018\nClinging to the Anti-Imperial Mantle: The Repu...\nMy dissertation project, Clinging to the Anti-...\nCold War\n0\n1\n0\n\n\n123\n2018\nThe New Navy's Pacific Wars: Peripheral Confl...\nUsing a transnational methodology and sources ...\nMilitary History\n1\n0\n0\n\n\n124\n2018\nStop or I'll Shoot, Comply and I Won't: The Di...\nThere is a dilemma at the heart of coercion. S...\nIR Scholarship\n0\n0\n1\n\n\n125\n2018\nUnexpected Humanitarians: Albania, the U.S. Mi...\nUsing archives and oral history, this disserta...\nMilitary History\n0\n0\n0\n\n\n\n\n126 rows × 7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"title\", \"abstract\", \"theme\", \"War\", 'Cold War',\"Trade\"]\n\n\ndf_subset\n\n\n\n\n\n\n\n\nyear\ntitle\nabstract\ntheme\nWar\nCold War\nTrade\n\n\n\n\n0\n2000\n\"Institutions at the Domestic/International Ne...\nCivil-military relations are frequently studie...\nIR scholarship\n1\n0\n0\n\n\n1\n2000\nBorn to Lose and Doomed to Survive: State Deat...\nUnder what conditions do states die, or exit t...\nIR scholarship\n1\n1\n0\n\n\n2\n2000\nThe significance of “allegiance” in internatio...\nMy dissertation employs original and secondary...\nIR scholarship\n1\n0\n0\n\n\n3\n2000\nThe significance of “allegiance” in internatio...\n\\nThis study revises prevailing interpretation...\nConflit Between States\n0\n1\n0\n\n\n4\n2000\nTruth-Telling and Mythmaking in Post-Soviet Ru...\nCan distorted and pernicious ideas about histo...\nConflict Between States\n1\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n121\n2018\nPlanning for the Short Haul: Trade Among Belli...\nIn times of war, why do belligerents continue ...\nConflict between states\n1\n0\n1\n\n\n122\n2018\nClinging to the Anti-Imperial Mantle: The Repu...\nMy dissertation project, Clinging to the Anti-...\nCold War\n0\n1\n0\n\n\n123\n2018\nThe New Navy's Pacific Wars: Peripheral Confl...\nUsing a transnational methodology and sources ...\nMilitary History\n1\n0\n0\n\n\n124\n2018\nStop or I'll Shoot, Comply and I Won't: The Di...\nThere is a dilemma at the heart of coercion. S...\nIR Scholarship\n0\n0\n1\n\n\n125\n2018\nUnexpected Humanitarians: Albania, the U.S. Mi...\nUsing archives and oral history, this disserta...\nMilitary History\n0\n0\n0\n\n\n\n\n126 rows × 7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///nf_nlp.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nnf_nlp_table = db.Table('nf_nlp_table', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('title',db.String, nullable=True),\n    db.Column('abstract',db.String, nullable=True),\n    db.Column('theme',db.String, nullable=True),\n    db.Column('War',db.Integer, nullable=True),\n    db.Column('Cold War',db.Integer, nullable=True),\n    db.Column('Trade', db.Integer, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nnf_nlp_table\n\nTable('nf_nlp_table', MetaData(bind=None), Column('year', Integer(), table=&lt;nf_nlp_table&gt;), Column('title', String(), table=&lt;nf_nlp_table&gt;), Column('abstract', String(), table=&lt;nf_nlp_table&gt;), Column('theme', String(), table=&lt;nf_nlp_table&gt;), Column('War', Integer(), table=&lt;nf_nlp_table&gt;), Column('Cold War', Integer(), table=&lt;nf_nlp_table&gt;), Column('Trade', Integer(), table=&lt;nf_nlp_table&gt;), schema=None)\n\n\n\ndf_subset.to_sql('nf_nlp_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT year, theme, title FROM nf_nlp_table LIMIT 10\").fetchall()\n\n[(2000, 'IR scholarship', '\"Institutions at the Domestic/International Nexus: the political-military  origins of military effectiveness, strategic integration and war'),\n (2000, 'IR scholarship', 'Born to Lose and Doomed to Survive: State Death and Survival in the International System'),\n (2000, 'IR scholarship', 'The significance of “allegiance” in international relations'),\n (2000, 'Conflit Between States', 'The significance of “allegiance” in international relations'),\n (2000, 'Conflict Between States', 'Truth-Telling and Mythmaking in Post-Soviet Russia: Historical Ideas, Mass Education, and Interstate Conflict'),\n (2000, 'Domestic Military History', 'Building a Cape Fear Metropolis: Fort Bragg, Fayetteville, and the  Sandhills of North Carolina'),\n (2000, 'Culture', 'The Glories and the Sadness: Shaping the national Memory of the First World War in Great Britain, Canada and Australia'),\n (2000, 'Culture / Peace Process', 'What leads longstanding adversaries to engage in conflict resolution'),\n (2001, 'Military History', 'A School for the Nation: Military  Institutions and the Boundaries of Nationality'),\n (2001, 'Military History', \"The 'American Century' Army:  The Origins of the U.S. Cold War Army, 1949-1959\")]\n\n\n\nsql = \"\"\"\nSELECT\n  year\n, theme\n, title\nFROM nf_nlp_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n\n\n\nyear\ntheme\ntitle\n\n\n\n\n96\n2014\nIR Scholarship\n“Multiparty Mediation: Identifying Characteris...\n\n\n97\n2014\nIR Scholarship\nThe Justice Dilemma: International Criminal Ac...\n\n\n98\n2014\nIR Scholarship\nBeyond Revolution and Repression: U.S. Foreign...\n\n\n99\n2014\nIR Scholarship\nProtection States Trust?: Major Power Patronag...\n\n\n100\n2014\nNuclear Weapons\nThe Constraining Power of the Nuclear Nonproli...\n\n\n101\n2015\nMilitary History\nSelling Her the Military: Recruiting Women int...\n\n\n102\n2015\nIR Scholarship\nAmerican Evangelicals, Israel, and Modern Chri...\n\n\n103\n2015\nNon-state\nWho Can Keep the Peace? Insurgent Organization...\n\n\n104\n2015\nIR Scholarship\nCredibility in Crisis: The Role of Leadership ...\n\n\n105\n2015\nIR Scholarship\nEvaluating the Changing of the Guards: Survey ...\n\n\n106\n2015\nSoviet Union\nExtracting the Eagle’s Talons: The Soviet Unio...\n\n\n107\n2015\nIR Scholarship\nThe Control War: Communist Revolutionary Warfa...\n\n\n108\n2015\nNuclear Weapons\nNuclear Weapons and Foreign Policy\n\n\n109\n2016\nCiv-Mil\nSecuring Control and Controlling Security: Civ...\n\n\n110\n2016\nMilitary History\nDigging for Victory: The Stalinist State’s Mob...\n\n\n111\n2016\nNon-state\nPersuading Power: Insurgent Diplomacy and the ...\n\n\n112\n2016\nConflict between states\nA Prelude to Violence? The Effect of Nationali...\n\n\n113\n2016\nConflict between states\nEngaging the ‘Evil Empire’: East – West Relati...\n\n\n114\n2017\nIR Scholarship\nMore Talk, Less Action: Why Costless Diplomacy...\n\n\n115\n2017\nCold War\nExperiments in Peace: Asian Neutralism, Human ...\n\n\n116\n2017\nIR Scholarship\nFully Committed? Religiously Committed State P...\n\n\n117\n2017\nMilitary History\nStraddling the Threshold of Two Worlds: Soldie...\n\n\n118\n2017\nMilitary History\nU.S. Army’s Investigation and Adjudication of ...\n\n\n119\n2017\nIR Scholarship\nGrand Strategic Crucibles: The Lasting Effects...\n\n\n120\n2018\nNuclear Weapons\nTrust in International Politics: The Role of L...\n\n\n121\n2018\nConflict between states\nPlanning for the Short Haul: Trade Among Belli...\n\n\n122\n2018\nCold War\nClinging to the Anti-Imperial Mantle: The Repu...\n\n\n123\n2018\nMilitary History\nThe New Navy's Pacific Wars: Peripheral Confl...\n\n\n124\n2018\nIR Scholarship\nStop or I'll Shoot, Comply and I Won't: The Di...\n\n\n125\n2018\nMilitary History\nUnexpected Humanitarians: Albania, the U.S. Mi..."
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html",
    "href": "posts/2021-06-05-causalml-tree-based-models.html",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "",
    "text": "# Code from https://github.com/uber/causalml/tree/master/examples\nimport numpy as np\nimport pandas as pd\n\nfrom causalml.dataset import make_uplift_classification\nfrom causalml.inference.tree import UpliftRandomForestClassifier\nfrom causalml.metrics import plot_gain\n\nfrom sklearn.model_selection import train_test_split"
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html#create-a-synthetic-population",
    "href": "posts/2021-06-05-causalml-tree-based-models.html#create-a-synthetic-population",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "Create a synthetic population",
    "text": "Create a synthetic population\nThe uplift curve is calculated on a synthetic population that consists of those that were in the control group and those who happened to be in the treatment group recommended by the model. We use the synthetic population to calculate the actual treatment effect within predicted treatment effect quantiles. Because the data is randomized, we have a roughly equal number of treatment and control observations in the predicted quantiles and there is no self selection to treatment groups.\n\n# If all deltas are negative, assing to control; otherwise assign to the treatment\n# with the highest delta\nbest_treatment = np.where((result &lt; 0).all(axis=1),\n                           'control',\n                           result.idxmax(axis=1))\n\n# Create indicator variables for whether a unit happened to have the\n# recommended treatment or was in the control group\nactual_is_best = np.where(df_test['treatment_group_key'] == best_treatment, 1, 0)\nactual_is_control = np.where(df_test['treatment_group_key'] == 'control', 1, 0)\n\n\nsynthetic = (actual_is_best == 1) | (actual_is_control == 1)\nsynth = result[synthetic]"
  },
  {
    "objectID": "posts/2021-06-05-causalml-tree-based-models.html#calculate-the-observed-treatment-effect-per-predicted-treatment-effect-quantile",
    "href": "posts/2021-06-05-causalml-tree-based-models.html#calculate-the-observed-treatment-effect-per-predicted-treatment-effect-quantile",
    "title": "CausalML uplift with tree-based algorithms",
    "section": "Calculate the observed treatment effect per predicted treatment effect quantile",
    "text": "Calculate the observed treatment effect per predicted treatment effect quantile\nWe use the observed treatment effect to calculate the uplift curve, which answers the question: how much of the total cumulative uplift could we have captured by targeting a subset of the population sorted according to the predicted uplift, from highest to lowest?\nCausalML has the plot_gain() function which calculates the uplift curve given a DataFrame containing the treatment assignment, observed outcome and the predicted treatment effect.\n\nauuc_metrics = (synth.assign(is_treated = 1 - actual_is_control[synthetic],\n                             conversion = df_test.loc[synthetic, 'conversion'].values,\n                             uplift_tree = synth.max(axis=1))\n                     .drop(columns=list(uplift_model.classes_)))\n\n\nplot_gain(auuc_metrics, outcome_col='conversion', treatment_col='is_treated')"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html",
    "href": "posts/2021-06-01-model-evaluation.html",
    "title": "Model Evaluation",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.size'] = 20\nplt.rcParams['figure.figsize'] = [12, 8]\nplt.rcParams['lines.linewidth'] = 2.5\nplt.rcParams['savefig.bbox'] = 'tight'\nplt.rcParams[\"savefig.dpi\"] = 300\n\nsklearn.set_config(display='diagram')\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\nX.head()\n\n\n\n\n\n\n\n\nage\nhypertension\nheart_disease\navg_glucose_level\nbmi\n\n\n\n\n1\n58.0\n1\n0\n87.96\n39.2\n\n\n3\n70.0\n0\n0\n69.04\n35.9\n\n\n6\n52.0\n0\n0\n77.59\n17.7\n\n\n7\n75.0\n0\n1\n243.53\n27.0\n\n\n8\n32.0\n0\n0\n77.67\n32.3"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#is-this-data-imbalanced",
    "href": "posts/2021-06-01-model-evaluation.html#is-this-data-imbalanced",
    "title": "Model Evaluation",
    "section": "Is this data imbalanced?",
    "text": "Is this data imbalanced?\n\ny.value_counts()\n\n0    28524\n1      548\nName: stroke, dtype: int64\n\n\n\ny = (y == 1).astype('int')\n\n\ny.value_counts()\n\n0    28524\n1      548\nName: stroke, dtype: int64"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#train-models-for-prediction",
    "href": "posts/2021-06-01-model-evaluation.html#train-models-for-prediction",
    "title": "Model Evaluation",
    "section": "Train models for prediction",
    "text": "Train models for prediction\n\nLinear model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=42)\n\n\nlog_reg = Pipeline([\n    ('scaler', StandardScaler()),\n    ('log_reg', LogisticRegression(random_state=42))])\nlog_reg.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('scaler', StandardScaler()),\n                ('log_reg', LogisticRegression(random_state=42))])StandardScalerStandardScaler()LogisticRegressionLogisticRegression(random_state=42)\n\n\n\ny_pred = log_reg.predict(X_test)\n\n\ny_pred\n\narray([0, 0, 0, ..., 0, 0, 0])\n\n\n\nlog_reg.score(X_test, y_test)\n\n0.9811502476609797\n\n\n\nfrom sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n# %load solutions/01-ex01-solutions.py\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#thresholds",
    "href": "posts/2021-06-01-model-evaluation.html#thresholds",
    "title": "Model Evaluation",
    "section": "Thresholds",
    "text": "Thresholds"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#default",
    "href": "posts/2021-06-01-model-evaluation.html#default",
    "title": "Model Evaluation",
    "section": "Default",
    "text": "Default\n\ny_pred = log_reg.predict(X_test)\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\nUsing probabilities\n\ny_proba = log_reg.predict_proba(X_test)\n\n\ny_proba[65:70]\n\narray([[9.99513156e-01, 4.86844076e-04],\n       [9.82052253e-01, 1.79477470e-02],\n       [9.94416208e-01, 5.58379215e-03],\n       [8.50081179e-01, 1.49918821e-01],\n       [9.95569630e-01, 4.43036992e-03]])\n\n\n\ny_pred[65:70]\n\narray([0, 0, 0, 0, 0])\n\n\n\nThreshold at 0.50\n\ny_pred_50 = y_proba[:, 1] &gt; 0.5\nprint(classification_report(y_test, y_pred_50))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\nThreshold at 0.25\n\ny_pred_25 = y_proba[:, 1] &gt; 0.25\nprint(classification_report(y_test, y_pred_25))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n\n\nThreshold at 0.75\n\ny_pred_75 = y_proba[:, 1] &gt; 0.75\nprint(classification_report(y_test, y_pred_75))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n\nfrom sklearn.metrics import plot_precision_recall_curve\nplot_precision_recall_curve(log_reg, X_test, y_test, name=\"LogisticRegression\")\n\n&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f6fc27e78b0&gt;\n\n\n\n\n\n\nfrom sklearn.metrics import plot_roc_curve\nplot_roc_curve(log_reg, X_test, y_test, name=\"LogisticRegression\")\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f6fc075e790&gt;\n\n\n\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 8))\nplot_precision_recall_curve(log_reg, X_test, y_test, name=\"LogisticRegression\", ax=ax1)\nplot_roc_curve(log_reg, X_test, y_test, name=\"LogisticRegression\", ax=ax2)\n\n&lt;sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f6fc06594f0&gt;\n\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nfig, ax = plt.subplots()\nplot_precision_recall_curve(log_reg, X_test, y_test, ax=ax, name=\"Logistic Regression\")\nplot_precision_recall_curve(rf, X_test, y_test, ax=ax, name=\"Random Forest\")\n\n&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f6fc05cd940&gt;"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#exercise-2",
    "href": "posts/2021-06-01-model-evaluation.html#exercise-2",
    "title": "Model Evaluation",
    "section": "Exercise 2",
    "text": "Exercise 2\n\nPlot the roc curve of the logistic regression model and the random forest model on the same axes.\nTrain a sklearn.dummy.DummyClassifier(strategy='prior') on the training dataset and plot the precision recall curve and the roc curve with the test dataset.\nWhat is the ROC AUC and the average precision for the dummy classifer?\nExtra: Compute the f1 score for three models we have trained so far. While model performs the best according to the f1 score? Hint: f1_score is in sklearn.metrics"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#decision-function",
    "href": "posts/2021-06-01-model-evaluation.html#decision-function",
    "title": "Model Evaluation",
    "section": "Decision function",
    "text": "Decision function\n\nComparing decision function vs predictions\n\nlog_reg_decision = log_reg.decision_function(X_test)\n\n\nnp.all((log_reg_decision &gt; 0) ==  log_reg.predict(X_test))\n\nTrue\n\n\n\nlog_reg_pred = log_reg.predict_proba(X_test)\n\n\nlog_reg_pred\n\narray([[0.99375825, 0.00624175],\n       [0.99785633, 0.00214367],\n       [0.98641586, 0.01358414],\n       ...,\n       [0.9772597 , 0.0227403 ],\n       [0.93760339, 0.06239661],\n       [0.99690984, 0.00309016]])\n\n\n\n\nComputing the predict_proba from the decision function\n\n1/(1 + np.exp(-log_reg_decision))\n\narray([0.00624175, 0.00214367, 0.01358414, ..., 0.0227403 , 0.06239661,\n       0.00309016])\n\n\n\nlog_reg_pred[:, 1]\n\narray([0.00624175, 0.00214367, 0.01358414, ..., 0.0227403 , 0.06239661,\n       0.00309016])\n\n\n\n\nRanking metrics\n\nfrom sklearn.metrics import average_precision_score\n\n\nUsing the decision function to compute the average precision\n\naverage_precision_score(y_test, log_reg_decision)\n\n0.09421224656746816\n\n\n\n\nUsing predict_proba to compute the average precision\n\naverage_precision_score(y_test, log_reg_pred[:, 1])\n\n0.09421224656746816"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#exercise-3",
    "href": "posts/2021-06-01-model-evaluation.html#exercise-3",
    "title": "Model Evaluation",
    "section": "Exercise 3",
    "text": "Exercise 3\n\nCompute the roc_auc_score for the random forest. Hint: Use predict_proba.\nTrain a sklearn.svm.SVC model on the training datast and compute the average precision. Hint: Use decision_function.\n\n\n# %load solutions/01-ex03-solutions.py\nfrom sklearn.metrics import roc_auc_score\n\nrf_proba = rf.predict_proba(X_test)\n\nroc_auc_score(y_test, rf_proba[:, 1])\n\nfrom sklearn.svm import SVC\n\nsvc = SVC(random_state=0)\nsvc.fit(X_train, y_train)\n\nsvc_decision = svc.decision_function(X_test)\n\nroc_auc_score(y_test, svc_decision)\n\n0.538190915167353"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#multiclass",
    "href": "posts/2021-06-01-model-evaluation.html#multiclass",
    "title": "Model Evaluation",
    "section": "Multiclass",
    "text": "Multiclass\n\n## Reading the dataset using pandas\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/CTG.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nFileName\nDate\nSegFile\nb\ne\nLBE\nLB\nAC\nFM\nUC\n...\nC\nD\nE\nAD\nDE\nLD\nFS\nSUSP\nCLASS\nNSP\n\n\n\n\n0\nVariab10.txt\n12/1/1996\nCTG0001.txt\n240.0\n357.0\n120.0\n120.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n9.0\n2.0\n\n\n1\nFmcs_1.txt\n5/3/1996\nCTG0002.txt\n5.0\n632.0\n132.0\n132.0\n4.0\n0.0\n4.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n2\nFmcs_1.txt\n5/3/1996\nCTG0003.txt\n177.0\n779.0\n133.0\n133.0\n2.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n3\nFmcs_1.txt\n5/3/1996\nCTG0004.txt\n411.0\n1192.0\n134.0\n134.0\n2.0\n0.0\n6.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n4\nFmcs_1.txt\n5/3/1996\nCTG0005.txt\n533.0\n1147.0\n132.0\n132.0\n4.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2124\nS8001045.dsp\n6/6/1998\nCTG2127.txt\n1576.0\n3049.0\n140.0\n140.0\n1.0\n0.0\n9.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n2.0\n\n\n2125\nS8001045.dsp\n6/6/1998\nCTG2128.txt\n2796.0\n3415.0\n142.0\n142.0\n1.0\n1.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n2126\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2127\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2128\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n564.0\n23.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2129 rows × 40 columns\n\n\n\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['NSP', 'FileName', 'Date', 'SegFile'], axis = 1), df['NSP']\n\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=42)\n\n\nrf = RandomForestClassifier(random_state=42).fit(X_train, y_train)\n\n\nfrom sklearn.metrics import plot_confusion_matrix\n\n\nplot_confusion_matrix(rf, X_test, y_test, cmap='gray_r')\n\n&lt;sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7f6fb9cb03a0&gt;\n\n\n\n\n\n\ny_pred = rf.predict(X_test)\n\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n         1.0       0.97      1.00      0.98       414\n         2.0       0.98      0.84      0.91        74\n         3.0       1.00      1.00      1.00        44\n\n    accuracy                           0.98       532\n   macro avg       0.99      0.95      0.96       532\nweighted avg       0.98      0.98      0.97       532\n\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n\nrf_y_pred_proba = rf.predict_proba(X_test)\n\n\nroc_auc_score(y_test, rf_y_pred_proba, multi_class='ovo')\n\n0.9980180279093323\n\n\n\nroc_auc_score(y_test, rf_y_pred_proba, multi_class='ovr')\n\n0.9985885196707779"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#regression",
    "href": "posts/2021-06-01-model-evaluation.html#regression",
    "title": "Model Evaluation",
    "section": "Regression",
    "text": "Regression\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import StandardScaler\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)\n\n\nfrom sklearn.pipeline import make_pipeline\n\nridge = make_pipeline(StandardScaler(), Ridge())\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()), ('ridge', Ridge())])StandardScalerStandardScaler()RidgeRidge()\n\n\n\nridge.score(X_test, y_test)\n\n0.7482100751181446\n\n\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_absolute_error\n\n\nridge_pred = ridge.predict(X_test)\n\n\nLook at predictions\n\nridge_pred[:10]\n\narray([2027961.35376795, 1530619.74443469, 1639626.98468177,\n        965135.26705736,  438579.62245477,  690017.03701487,\n        337043.03751396,  630817.75885244, 2231143.82393699,\n        -50934.54700348])\n\n\n\nridge_r2 = r2_score(y_test, ridge_pred)\nridge_r2\n\n0.7482100751181446\n\n\n\nridge_mse = mean_squared_error(y_test, ridge_pred)\nridge_mse\n\n165402066384.76834\n\n\n\nridge_mae = mean_absolute_error(y_test, ridge_pred)\nridge_mae\n\n275823.0351447304"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#prediction-plots",
    "href": "posts/2021-06-01-model-evaluation.html#prediction-plots",
    "title": "Model Evaluation",
    "section": "Prediction plots",
    "text": "Prediction plots\n\nfig, ax = plt.subplots()\ndelta = y_test - ridge_pred\nax.plot(ridge_pred, delta, 'o', alpha=0.5)\nax.axhline(y=0, c='k', ls='--')\nax.set(xlabel='predicted', ylabel='y_true - predicited', aspect='equal');"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#prediction-plots-histogram",
    "href": "posts/2021-06-01-model-evaluation.html#prediction-plots-histogram",
    "title": "Model Evaluation",
    "section": "Prediction plots histogram",
    "text": "Prediction plots histogram\n\nfig, ax = plt.subplots()\nax.hist(delta, bins=30)\nax.set(xlabel=\"y_true - predicted\", ylabel=\"Counts\");\n\n\n\n\n\n# %load solutions/01-ex04-solutions.py\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=42).fit(X_train, y_train)\nrf_pred = rf.predict(X_test)\n\n\nr2_score(y_test, rf_pred)\n\nmean_squared_error(y_test, rf_pred)\n\nmean_absolute_error(y_test, rf_pred)\n\n243725.82533333328"
  },
  {
    "objectID": "posts/2021-06-01-model-evaluation.html#prediction-plots-per-feature",
    "href": "posts/2021-06-01-model-evaluation.html#prediction-plots-per-feature",
    "title": "Model Evaluation",
    "section": "Prediction plots per feature",
    "text": "Prediction plots per feature\n\nfrom sklearn.datasets import load_boston\nimport pandas as pd\n\n\nX_df = pd.DataFrame(X, columns=boston.feature_names)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)\n\n\nridge = make_pipeline(StandardScaler(), Ridge(random_state=42))\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('ridge', Ridge(random_state=42))])StandardScalerStandardScaler()RidgeRidge(random_state=42)\n\n\n\nridge_pred = ridge.predict(X_test)\n\n\nX_test.head()\n\n\n\n\n\n\n\n\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nit\n\n\n\n\n166\n763953.0\n2006\n7688.67\n259335\n0.0\n0.4375\n0.000000\n6349262\n\n\n262\n112137.0\n2006\n21900.19\n1000069\n0.0\n0.0000\n0.000000\n5304833\n\n\n11\n178705.0\n2007\n7360.92\n299892\n0.0\n0.0000\n0.324324\n7040099\n\n\n139\n241282.0\n2003\n6867.70\n53903\n0.0\n0.0000\n0.516129\n3586373\n\n\n78\n100000.0\n2002\n2523.73\n41726\n0.0\n0.0000\n0.400000\n2138758\n\n\n\n\n\n\n\n\nX_analysis = X_test.assign(\n    delta=y_test - ridge_pred\n)\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\ncolumns = X_analysis.columns\nn_features = X.shape[1]\n\nfig, axes = plt.subplots(3, 5, figsize=(20, 10), constrained_layout=True)\nfor i, ax in enumerate(axes.ravel()):\n    if i &gt;= n_features:\n        ax.set_visible(False)\n        continue\n    sns.scatterplot(x=columns[i], y='delta', ax=ax, data=X_analysis)\n    ax.axhline(y=0, c='k', ls='--')"
  },
  {
    "objectID": "posts/2020-11-01-Housing_Prediction.html",
    "href": "posts/2020-11-01-Housing_Prediction.html",
    "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\nFind the quandl api documentation here -\nfrom sklearn.datasets import fetch_california_housing\n\ncalifornia = fetch_california_housing()\n\nX = california.data\ny = california.target * 100000\n\nprint(f'Data shape is {X.shape}')\nprint(f'Target shape is {y.shape}')\n\nData shape is (20640, 8)\nTarget shape is (20640,)\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport quandl\n\nquandl.ApiConfig.api_key = ''\n%matplotlib inline\nquandl_call = (\n    \"ZILLOW/{category}{code}_{indicator}\"\n)\n\n\ndef download_data(category, code, indicator):\n    \"\"\"\n    Reads in a single dataset from Zillow Quandl API\n    \n    Parameters\n    ----------\n    category : \"Chicago_Area\" or \"Evanston\"\n    \n    code : \"Evanston\" or \"Chicago\"\n    \n    indicator : \"Sales_Price\" or \"other\"\n\n    \n    Returns\n    -------\n    DataFrame\n    \"\"\"\n    AREA_CATEGORY_dict = {\"Evanston\": \"C\", \"Chicago_Area\": \"C\"}\n    AREA_CODE_dict = {\"Evanston\": \"64604\", \"Chicago\": \"36156\"}\n    INDICATOR_CODE_dict = {\"Sales_Price\": \"SP\"}\n    \n    \n    \n    category = AREA_CATEGORY_dict[category]\n    code = AREA_CODE_dict[code]\n    indicator = INDICATOR_CODE_dict[indicator]\n\n    \n    \n    return quandl.get(quandl_call.format(category=category, code=code, indicator=indicator))\n# data = quandl.get_table(\"ZILLOW/REGIONS\", paginate=True)\n# col = 'region'\n# mask = np.column_stack([data[col].str.contains(r\"Boston\", na=False) for col in data])\n# data.loc[mask.any(axis=1)]\n# col = 'region'\n# mask = np.column_stack([data[col].str.contains(r\"Evanston\", na=False) for col in data])\n# df=data.loc[mask.any(axis=1)]\n#df['region']"
  },
  {
    "objectID": "posts/2020-11-01-Housing_Prediction.html#chicago-and-evanston-home-sale-prices",
    "href": "posts/2020-11-01-Housing_Prediction.html#chicago-and-evanston-home-sale-prices",
    "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
    "section": "Chicago and Evanston Home Sale Prices",
    "text": "Chicago and Evanston Home Sale Prices\n\nEV_SP = download_data('Chicago_Area', 'Evanston', 'Sales_Price')\nCH_SP = download_data('Chicago_Area', 'Chicago', 'Sales_Price')\n\n\nCH_SP.query(\"Value &gt; 270000\")\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2008-03-31\n325100.0\n\n\n2008-04-30\n314800.0\n\n\n2008-05-31\n286900.0\n\n\n2008-06-30\n274600.0\n\n\n2019-03-31\n290800.0\n\n\n2019-04-30\n292000.0\n\n\n2019-05-31\n276000.0\n\n\n2019-06-30\n271500.0\n\n\n2020-01-31\n281400.0\n\n\n2020-02-29\n302900.0\n\n\n2020-03-31\n309200.0\n\n\n\n\n\n\n\n\nfrom pandasql import sqldf\npysqldf = lambda q: sqldf(q, globals())\n\nq = \"\"\"SELECT date\n        ,Value \n       FROM CH_SP\n       WHERE Value &gt; 270000\n       LIMIT 10;\"\"\"\n\nvalues = pysqldf(q)\nvalues\n\n\n\n\n\n\n\n\nDate\nValue\n\n\n\n\n0\n2008-03-31 00:00:00.000000\n325100.0\n\n\n1\n2008-04-30 00:00:00.000000\n314800.0\n\n\n2\n2008-05-31 00:00:00.000000\n286900.0\n\n\n3\n2008-06-30 00:00:00.000000\n274600.0\n\n\n4\n2019-03-31 00:00:00.000000\n290800.0\n\n\n5\n2019-04-30 00:00:00.000000\n292000.0\n\n\n6\n2019-05-31 00:00:00.000000\n276000.0\n\n\n7\n2019-06-30 00:00:00.000000\n271500.0\n\n\n8\n2020-01-31 00:00:00.000000\n281400.0\n\n\n9\n2020-02-29 00:00:00.000000\n302900.0\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Value')\n\nEV_SP['Value'].plot(label='Evanston')\nCH_SP['Value'].plot(label='Chicago')\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f3bf2e42c18&gt;\n\n\n\n\n\n\nimport seaborn as sns\nfrom scipy.stats import norm\n\nsns.distplot(CH_SP['Value'], fit=norm);\n\n\n\n\n\nsns.distplot(EV_SP['Value'], fit=norm);"
  },
  {
    "objectID": "posts/2022-10-02-Prophet-Time-Series.html",
    "href": "posts/2022-10-02-Prophet-Time-Series.html",
    "title": "Using Prophet for S&P 500 forcasting",
    "section": "",
    "text": "## Prophet docs can be found here: https://facebook.github.io/prophet/docs/quick_start.html\n\n\n## This notebooks is published here: https://deepnote.com/publish/e870766e-a2ca-4167-99b6-3cb070a87c2e\n\n\nimport pandas as pd\nimport numpy as np\nimport datetime\n\nimport quandl\n\nimport statsmodels.api as sm\nfrom prophet import Prophet\n\nimport matplotlib.pyplot as plt\n\nimport pandas_datareader.data as web\nimport datetime\n\n\nstart = datetime.datetime(1960, 1, 1)\nend = pd.to_datetime('today')\n\n#SP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\nSP500=web.DataReader('^GSPC', 'yahoo', start, end)\nSP500\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n1960-01-04\n59.910000\n59.910000\n59.910000\n59.910000\n3990000\n59.910000\n\n\n1960-01-05\n60.389999\n60.389999\n60.389999\n60.389999\n3710000\n60.389999\n\n\n1960-01-06\n60.130001\n60.130001\n60.130001\n60.130001\n3730000\n60.130001\n\n\n1960-01-07\n59.689999\n59.689999\n59.689999\n59.689999\n3310000\n59.689999\n\n\n1960-01-08\n59.500000\n59.500000\n59.500000\n59.500000\n3290000\n59.500000\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-09-26\n3715.669922\n3644.760010\n3682.719971\n3655.040039\n4886140000\n3655.040039\n\n\n2022-09-27\n3717.530029\n3623.290039\n3686.439941\n3647.290039\n4577740000\n3647.290039\n\n\n2022-09-28\n3736.739990\n3640.610107\n3651.939941\n3719.040039\n4684850000\n3719.040039\n\n\n2022-09-29\n3687.010010\n3610.399902\n3687.010010\n3640.469971\n4681810000\n3640.469971\n\n\n2022-09-30\n3671.439941\n3584.129883\n3633.479980\n3585.620117\n5645360000\n3585.620117\n\n\n\n\n15795 rows × 6 columns\n\n\n\n\ndf = SP500\ndf.head()\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n1960-01-04\n59.910000\n59.910000\n59.910000\n59.910000\n3990000\n59.910000\n\n\n1960-01-05\n60.389999\n60.389999\n60.389999\n60.389999\n3710000\n60.389999\n\n\n1960-01-06\n60.130001\n60.130001\n60.130001\n60.130001\n3730000\n60.130001\n\n\n1960-01-07\n59.689999\n59.689999\n59.689999\n59.689999\n3310000\n59.689999\n\n\n1960-01-08\n59.500000\n59.500000\n59.500000\n59.500000\n3290000\n59.500000\n\n\n\n\n\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nDatetimeIndex: 15795 entries, 1960-01-04 to 2022-09-30\nData columns (total 6 columns):\n #   Column     Non-Null Count  Dtype  \n---  ------     --------------  -----  \n 0   High       15795 non-null  float64\n 1   Low        15795 non-null  float64\n 2   Open       15795 non-null  float64\n 3   Close      15795 non-null  float64\n 4   Volume     15795 non-null  int64  \n 5   Adj Close  15795 non-null  float64\ndtypes: float64(5), int64(1)\nmemory usage: 863.8 KB\n\n\n\ndf['ds'] = df.index\ndf['y'] = df['Close']\n\n\ndf\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\nds\nDate\ny\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n1960-01-04\n59.910000\n59.910000\n59.910000\n59.910000\n3990000\n59.910000\n1960-01-04\n1960-01-04\n59.910000\n\n\n1960-01-05\n60.389999\n60.389999\n60.389999\n60.389999\n3710000\n60.389999\n1960-01-05\n1960-01-05\n60.389999\n\n\n1960-01-06\n60.130001\n60.130001\n60.130001\n60.130001\n3730000\n60.130001\n1960-01-06\n1960-01-06\n60.130001\n\n\n1960-01-07\n59.689999\n59.689999\n59.689999\n59.689999\n3310000\n59.689999\n1960-01-07\n1960-01-07\n59.689999\n\n\n1960-01-08\n59.500000\n59.500000\n59.500000\n59.500000\n3290000\n59.500000\n1960-01-08\n1960-01-08\n59.500000\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2022-09-26\n3715.669922\n3644.760010\n3682.719971\n3655.040039\n4886140000\n3655.040039\n2022-09-26\n2022-09-26\n3655.040039\n\n\n2022-09-27\n3717.530029\n3623.290039\n3686.439941\n3647.290039\n4577740000\n3647.290039\n2022-09-27\n2022-09-27\n3647.290039\n\n\n2022-09-28\n3736.739990\n3640.610107\n3651.939941\n3719.040039\n4684850000\n3719.040039\n2022-09-28\n2022-09-28\n3719.040039\n\n\n2022-09-29\n3687.010010\n3610.399902\n3687.010010\n3640.469971\n4681810000\n3640.469971\n2022-09-29\n2022-09-29\n3640.469971\n\n\n2022-09-30\n3671.439941\n3584.129883\n3633.479980\n3585.620117\n5645360000\n3585.620117\n2022-09-30\n2022-09-30\n3585.620117\n\n\n\n\n15795 rows × 9 columns\n\n\n\n\nm = Prophet()\nm.fit(df)\n\n09:52:26 - cmdstanpy - INFO - Chain [1] start processing\n09:52:51 - cmdstanpy - INFO - Chain [1] done processing\n\n\n&lt;prophet.forecaster.Prophet at 0x7f0c31996280&gt;\n\n\n\nfuture = m.make_future_dataframe(periods=1825)\nfuture.tail()\n\n\n\n\n\n\n\n\nds\n\n\n\n\n17615\n2027-09-25\n\n\n17616\n2027-09-26\n\n\n17617\n2027-09-27\n\n\n17618\n2027-09-28\n\n\n17619\n2027-09-29\n\n\n\n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n\n\n\nds\nyhat\nyhat_lower\nyhat_upper\n\n\n\n\n17615\n2027-09-25\n5248.521837\n4779.444781\n5825.074365\n\n\n17616\n2027-09-26\n5248.679328\n4775.622024\n5799.211934\n\n\n17617\n2027-09-27\n5246.342880\n4773.457729\n5827.190606\n\n\n17618\n2027-09-28\n5247.156866\n4800.381042\n5823.564385\n\n\n17619\n2027-09-29\n5247.542703\n4815.556065\n5824.921157\n\n\n\n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)"
  },
  {
    "objectID": "posts/2020-10-12-dask-xgboost-fiscal-data..html",
    "href": "posts/2020-10-12-dask-xgboost-fiscal-data..html",
    "title": "Visualizing Operations with Dask Dataframes on Fiscal Data",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\n\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine(\"sqlite:///fiscal_data.db\")\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\n#engine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.3\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nEast China\nZhejiang\n11648.7\n668128\n3162299\n656175.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f0272396c50&gt;\n\n\n\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/5995/1\nDashboard: http://192.168.1.71:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province     gdp    fdi     it specific\nnpartitions=5                                                       \n0              int64  object   object  object  int64  int64  float64\n72               ...     ...      ...     ...    ...    ...      ...\n...              ...     ...      ...     ...    ...    ...      ...\n288              ...     ...      ...     ...    ...    ...      ...\n359              ...     ...      ...     ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.describe().visualize(filename='describe.png')\n\n\n\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.3\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n\n\n\n\n\n\nmax_gdp_per_region = ddf.groupby('region')['gdp'].max()\n\n\nmax_gdp_per_region.visualize()\n\n\n\n\n\nmax_gdp_per_region.compute()\n\nregion\nEast China             9705.02\nNorth China            9846.81\nNorthwest China         956.32\nSouth Central China     9439.6\nSouthwest China          937.5\nNortheast China        9304.52\nName: gdp, dtype: object\n\n\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\nnpartitions=5\n\n\n\n\n\n\n\n\n\n\n\n0\nint64\nobject\nobject\nobject\nint64\nint64\nfloat64\n\n\n72\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n288\n...\n...\n...\n...\n...\n...\n...\n\n\n359\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.npartitions\n\n5\n\n\n\nddf.npartitions\n\n5\n\n\n\nlen(ddf)\n\n360\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2, n_workers=3, memory_limit=\"4GB\")\nclient\n\n/home/gao/anaconda3/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 39701 instead\n  http_address[\"port\"], self.http_server.port\n\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/5995/20\nDashboard: http://192.168.1.71:39701/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nclient.id\n\n'Client-9f9a71c2-0c90-11eb-976b-cff3b7a8059e'\n\n\n\nddf.describe().compute()\n\n\n\n\n\n\n\n\nyear\nfdi\nit\nspecific\n\n\n\n\ncount\n360.000000\n3.600000e+02\n3.600000e+02\n3.560000e+02\n\n\nmean\n2001.500000\n1.961394e+05\n2.165819e+06\n5.834707e+05\n\n\nstd\n3.456857\n3.030440e+05\n1.769294e+06\n6.540553e+05\n\n\nmin\n1996.000000\n2.000000e+00\n1.478970e+05\n8.964000e+03\n\n\n25%\n1998.750000\n3.309900e+04\n1.077466e+06\n2.237530e+05\n\n\n50%\n2001.500000\n1.411025e+05\n2.020634e+06\n4.243700e+05\n\n\n75%\n2004.250000\n4.065125e+05\n3.375492e+06\n1.011846e+06\n\n\nmax\n2007.000000\n1.743140e+06\n1.053331e+07\n3.937966e+06\n\n\n\n\n\n\n\n\nddf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\n# ddf[\"province\"] = ddf[\"province\"].astype(float)\n# ddf[\"region\"] = ddf[\"region\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\nnpartitions=5\n\n\n\n\n\n\n\n\n\n\n\n0\nint64\nobject\nobject\nfloat64\nfloat64\nfloat64\nfloat64\n\n\n72\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n288\n...\n...\n...\n...\n...\n...\n...\n\n\n359\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: assign, 65 tasks\n\n\n\nddf.nlargest(20, 'gdp').compute()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n71\n2007\nSouth Central China\nGuangdong\n31777.01\n1712603.0\n4947824.0\n859482.0\n\n\n70\n2006\nSouth Central China\nGuangdong\n26587.76\n1451065.0\n4559252.0\n1897575.0\n\n\n263\n2007\nEast China\nShandong\n25776.91\n1101159.0\n6357869.0\n2121243.0\n\n\n69\n2005\nSouth Central China\nGuangdong\n22557.37\n1236400.0\n4327217.0\n1491588.0\n\n\n262\n2006\nEast China\nShandong\n21900.19\n1000069.0\n5304833.0\n1204547.0\n\n\n179\n2007\nEast China\nJiangsu\n21742.05\n1743140.0\n3557071.0\n1188989.0\n\n\n68\n2004\nSouth Central China\nGuangdong\n18864.62\n1001158.0\n5193902.0\n1491588.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576.0\n2939778.0\n844647.0\n\n\n178\n2006\nEast China\nJiangsu\n18598.69\n1318339.0\n2926542.0\n1388043.0\n\n\n261\n2005\nEast China\nShandong\n18366.87\n897000.0\n4142859.0\n1011203.0\n\n\n67\n2003\nSouth Central China\nGuangdong\n15844.64\n782294.0\n4073606.0\n1550764.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935.0\n2553268.0\n1017303.0\n\n\n260\n2004\nEast China\nShandong\n15021.84\n870064.0\n3732990.0\n1011203.0\n\n\n143\n2007\nSouth Central China\nHenan\n15012.46\n306162.0\n10533312.0\n3860764.0\n\n\n177\n2005\nEast China\nJiangsu\n15003.60\n1213800.0\n3479548.0\n1483371.0\n\n\n119\n2007\nNorth China\nHebei\n13607.32\n241621.0\n7537692.0\n2981235.0\n\n\n66\n2002\nSouth Central China\nGuangdong\n13502.42\n1133400.0\n3545004.0\n1235386.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000.0\n2370200.0\n656175.0\n\n\n275\n2007\nEast China\nShanghai\n12494.01\n792000.0\n2386339.0\n272744.0\n\n\n176\n2004\nEast China\nJiangsu\n12442.87\n1056365.0\n2410257.0\n1483371.0\n\n\n\n\n\n\n\n\nwithout_ec = ddf[ddf.region !='East China']\n\n\nwithout_ec.nlargest(20, 'gdp').compute()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n71\n2007\nSouth Central China\nGuangdong\n31777.01\n1712603.0\n4947824.0\n859482.0\n\n\n70\n2006\nSouth Central China\nGuangdong\n26587.76\n1451065.0\n4559252.0\n1897575.0\n\n\n69\n2005\nSouth Central China\nGuangdong\n22557.37\n1236400.0\n4327217.0\n1491588.0\n\n\n68\n2004\nSouth Central China\nGuangdong\n18864.62\n1001158.0\n5193902.0\n1491588.0\n\n\n67\n2003\nSouth Central China\nGuangdong\n15844.64\n782294.0\n4073606.0\n1550764.0\n\n\n143\n2007\nSouth Central China\nHenan\n15012.46\n306162.0\n10533312.0\n3860764.0\n\n\n119\n2007\nNorth China\nHebei\n13607.32\n241621.0\n7537692.0\n2981235.0\n\n\n66\n2002\nSouth Central China\nGuangdong\n13502.42\n1133400.0\n3545004.0\n1235386.0\n\n\n142\n2006\nSouth Central China\nHenan\n12362.79\n184526.0\n7601825.0\n2018158.0\n\n\n65\n2001\nSouth Central China\nGuangdong\n12039.25\n1193203.0\n2152243.0\n1257232.0\n\n\n118\n2006\nNorth China\nHebei\n11467.60\n201434.0\n5831974.0\n1253141.0\n\n\n64\n2000\nSouth Central China\nGuangdong\n10741.25\n1128091.0\n1927102.0\n714572.0\n\n\n141\n2005\nSouth Central China\nHenan\n10587.42\n123000.0\n5676863.0\n1171796.0\n\n\n299\n2007\nSouthwest China\nSichuan\n10562.39\n149322.0\n10384846.0\n3937966.0\n\n\n117\n2005\nNorth China\nHebei\n10012.11\n191000.0\n4503640.0\n859056.0\n\n\n23\n2007\nNorth China\nBeijing\n9846.81\n506572.0\n1962192.0\n752279.0\n\n\n167\n2007\nSouth Central China\nHunan\n9439.60\n327051.0\n8340692.0\n3156087.0\n\n\n155\n2007\nSouth Central China\nHubei\n9333.40\n276622.0\n7666512.0\n2922784.0\n\n\n215\n2007\nNortheast China\nLiaoning\n9304.52\n598554.0\n5502192.0\n3396397.0\n\n\n63\n1999\nSouth Central China\nGuangdong\n9250.68\n1165750.0\n1789235.0\n988521.0\n\n\n\n\n\n\n\n\nddf['province'].compute()\n\n0         Anhui\n1         Anhui\n2         Anhui\n3         Anhui\n4         Anhui\n         ...   \n355    Zhejiang\n356    Zhejiang\n357    Zhejiang\n358    Zhejiang\n359    Zhejiang\nName: province, Length: 360, dtype: object\n\n\n\nddf.where(ddf['province']=='Zhejiang').compute()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003.0\nEast China\nZhejiang\n9705.02\n498055.0\n2261631.0\n391292.0\n\n\n356\n2004.0\nEast China\nZhejiang\n11648.70\n668128.0\n3162299.0\n656175.0\n\n\n357\n2005.0\nEast China\nZhejiang\n13417.68\n772000.0\n2370200.0\n656175.0\n\n\n358\n2006.0\nEast China\nZhejiang\n15718.47\n888935.0\n2553268.0\n1017303.0\n\n\n359\n2007.0\nEast China\nZhejiang\n18753.73\n1036576.0\n2939778.0\n844647.0\n\n\n\n\n360 rows × 7 columns\n\n\n\n\nmask_after_2010 = ddf.where(ddf['year']&gt;2000)\n\n\nmask_after_2010.compute()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003.0\nEast China\nZhejiang\n9705.02\n498055.0\n2261631.0\n391292.0\n\n\n356\n2004.0\nEast China\nZhejiang\n11648.70\n668128.0\n3162299.0\n656175.0\n\n\n357\n2005.0\nEast China\nZhejiang\n13417.68\n772000.0\n2370200.0\n656175.0\n\n\n358\n2006.0\nEast China\nZhejiang\n15718.47\n888935.0\n2553268.0\n1017303.0\n\n\n359\n2007.0\nEast China\nZhejiang\n18753.73\n1036576.0\n2939778.0\n844647.0\n\n\n\n\n360 rows × 7 columns\n\n\n\n\ndef add_some_text(cname, *args, **kwargs):\n    return \"Region name is \" + cname\n\ndummy_values = ddf['region'].apply(add_some_text, axis=1)\n\n/home/gao/anaconda3/lib/python3.7/site-packages/dask/dataframe/core.py:3208: UserWarning: \nYou did not provide metadata, so Dask is running your function on a small dataset to guess output types. It is possible that Dask will guess incorrectly.\nTo provide an explicit output types or to silence this message, please provide the `meta=` keyword, as described in the map or apply function that you are using.\n  Before: .apply(func)\n  After:  .apply(func, meta=('region', 'object'))\n\n  warnings.warn(meta_warning(meta))\n\n\n\ndummy_values\n\nDask Series Structure:\nnpartitions=5\n0      object\n72        ...\n        ...  \n288       ...\n359       ...\nName: region, dtype: object\nDask Name: apply, 75 tasks\n\n\n\ndummy_values.visualize()\n\n\n\n\n\ndummy_values.compute()\n\n0      Region name is East China\n1      Region name is East China\n2      Region name is East China\n3      Region name is East China\n4      Region name is East China\n                 ...            \n355    Region name is East China\n356    Region name is East China\n357    Region name is East China\n358    Region name is East China\n359    Region name is East China\nName: region, Length: 360, dtype: object\n\n\n\nmax_per_region_yr = ddf.groupby('region').apply(lambda x: x.loc[x['gdp'].idxmax(), 'year'])\n\n/home/gao/.local/lib/python3.7/site-packages/ipykernel_launcher.py:1: UserWarning: `meta` is not specified, inferred from partial data. Please provide `meta` if the result is unexpected.\n  Before: .apply(func)\n  After:  .apply(func, meta={'x': 'f8', 'y': 'f8'}) for dataframe result\n  or:     .apply(func, meta=('x', 'f8'))            for series result\n  \"\"\"Entry point for launching an IPython kernel.\n\n\n\nmax_per_region_yr.visualize()\n\n\n\n\n\nmax_per_region_yr.compute()\n\nregion\nNorth China            2007\nNortheast China        2007\nNorthwest China        2007\nSouth Central China    2007\nEast China             2007\nSouthwest China        2007\ndtype: int64"
  },
  {
    "objectID": "posts/2020-10-09-Dask_Fiscal-Data-Sqlite-db.html",
    "href": "posts/2020-10-09-Dask_Fiscal-Data-Sqlite-db.html",
    "title": "Moving fiscal data from a pandas dataframe to a sqlite local database",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\n\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv('df_panel_fix.csv')\n\n\ndf.columns\n\nIndex(['Unnamed: 0', 'province', 'specific', 'general', 'year', 'gdp', 'fdi',\n       'rnr', 'rr', 'i', 'fr', 'reg', 'it'],\n      dtype='object')\n\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset\n\n\n\n\n\n\n\n\nyear\nreg\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\n\n\ndf_subset\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal_data.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nfiscal_data = db.Table('fiscal_data', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('region',db.String, nullable=True),\n    db.Column('province',db.String, nullable=True),\n    db.Column('gdp',db.String, nullable=True),\n    db.Column('fdi',db.Integer, nullable=True),\n    db.Column('it',db.Integer, nullable=True),\n    db.Column('specific', db.Integer, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nfiscal_data\n\nTable('fiscal_data', MetaData(bind=None), Column('year', Integer(), table=&lt;fiscal_data&gt;), Column('region', String(), table=&lt;fiscal_data&gt;), Column('province', String(), table=&lt;fiscal_data&gt;), Column('gdp', String(), table=&lt;fiscal_data&gt;), Column('fdi', Integer(), table=&lt;fiscal_data&gt;), Column('it', Integer(), table=&lt;fiscal_data&gt;), Column('specific', Integer(), table=&lt;fiscal_data&gt;), schema=None)\n\n\n\ndf_subset.to_sql('fiscal_data', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT year, region, province, gdp FROM fiscal_data LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3'),\n (1997, 'East China', 'Anhui', '2347.32'),\n (1998, 'East China', 'Anhui', '2542.96'),\n (1999, 'East China', 'Anhui', '2712.34'),\n (2000, 'East China', 'Anhui', '2902.09'),\n (2001, 'East China', 'Anhui', '3246.71'),\n (2002, 'East China', 'Anhui', '3519.72'),\n (2003, 'East China', 'Anhui', '3923.11'),\n (2004, 'East China', 'Anhui', '4759.3'),\n (2005, 'East China', 'Anhui', '5350.17')]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp \nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\n\n\n\n\n330\n2002\nNorthwest China\nXinjiang\n1612.65\n\n\n331\n2003\nNorthwest China\nXinjiang\n1886.35\n\n\n332\n2004\nNorthwest China\nXinjiang\n2209.09\n\n\n333\n2005\nNorthwest China\nXinjiang\n2604.19\n\n\n334\n2006\nNorthwest China\nXinjiang\n3045.26\n\n\n335\n2007\nNorthwest China\nXinjiang\n3523.16\n\n\n336\n1996\nSouthwest China\nYunnan\n1517.69\n\n\n337\n1997\nSouthwest China\nYunnan\n1676.17\n\n\n338\n1998\nSouthwest China\nYunnan\n1831.33\n\n\n339\n1999\nSouthwest China\nYunnan\n1899.82\n\n\n340\n2000\nSouthwest China\nYunnan\n2011.19\n\n\n341\n2001\nSouthwest China\nYunnan\n2138.31\n\n\n342\n2002\nSouthwest China\nYunnan\n2312.82\n\n\n343\n2003\nSouthwest China\nYunnan\n2556.02\n\n\n344\n2004\nSouthwest China\nYunnan\n3081.91\n\n\n345\n2005\nSouthwest China\nYunnan\n3462.73\n\n\n346\n2006\nSouthwest China\nYunnan\n3988.14\n\n\n347\n2007\nSouthwest China\nYunnan\n4772.52\n\n\n348\n1996\nEast China\nZhejiang\n4188.53\n\n\n349\n1997\nEast China\nZhejiang\n4686.11\n\n\n350\n1998\nEast China\nZhejiang\n5052.62\n\n\n351\n1999\nEast China\nZhejiang\n5443.92\n\n\n352\n2000\nEast China\nZhejiang\n6141.03\n\n\n353\n2001\nEast China\nZhejiang\n6898.34\n\n\n354\n2002\nEast China\nZhejiang\n8003.67\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n\n\n356\n2004\nEast China\nZhejiang\n11648.7\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n\n\n\n\n\n\n\n\n#http://manpages.ubuntu.com/manpages/precise/man1/sqlite3.1.html\n\n\n# sqlite3 fiscal_data.db\n\n# create table memos(text, priority INTEGER);\n# insert into memos values('example 1', 10);\n# insert into memos values('example 2', 100);\n# select * from memos;\n\n# sqlite3 -line fiscal_data.db 'select * from memos where priority &gt; 20;'"
  },
  {
    "objectID": "posts/2020-10-13-dask-xgboost-fiscal-data.html",
    "href": "posts/2020-10-13-dask-xgboost-fiscal-data.html",
    "title": "Moving Dask XGboost with Fiscal Data, saving and loading Dask XGboost models",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\n\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_table LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', 631930, 147002, 2093.3, 50661),\n (1997, 'East China', 'Anhui', 657860, 151981, 2347.32, 43443),\n (1998, 'East China', 'Anhui', 889463, 174930, 2542.96, 27673),\n (1999, 'East China', 'Anhui', 1227364, 285324, 2712.34, 26131),\n (2000, 'East China', 'Anhui', 1499110, 195580, 2902.09, 31847),\n (2001, 'East China', 'Anhui', 2165189, 250898, 3246.71, 33672),\n (2002, 'East China', 'Anhui', 2404936, 434149, 3519.72, 38375),\n (2003, 'East China', 'Anhui', 2815820, 619201, 3923.11, 36720),\n (2004, 'East China', 'Anhui', 3422176, 898441, 4759.3, 54669),\n (2005, 'East China', 'Anhui', 3874846, 898441, 5350.17, 69000)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2677958128&gt;\n\n\n\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/13442/1\nDashboard: http://192.168.1.71:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nclient.restart()\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/13442/1\nDashboard: http://192.168.1.71:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province      gdp    fdi     it specific\nnpartitions=5                                                        \n0              int64  object   object  float64  int64  int64  float64\n72               ...     ...      ...      ...    ...    ...      ...\n...              ...     ...      ...      ...    ...    ...      ...\n288              ...     ...      ...      ...    ...    ...      ...\n359              ...     ...      ...      ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n\n\n\n\n\n\nclient.id\n\n'Client-e79fe0fe-0d59-11eb-b482-f9dc9eaa58ee'\n\n\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n# ddf[\"province\"] = ddf[\"province\"].astype(float)\n# ddf[\"region\"] = ddf[\"region\"].astype(float)\n\n\nx=ddf[feat_list].persist()\ny=ddf[target].persist()\n\n\nx\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nyear\nfdi\n\n\nnpartitions=5\n\n\n\n\n\n\n0\nint64\nfloat64\n\n\n72\n...\n...\n\n\n...\n...\n...\n\n\n288\n...\n...\n\n\n359\n...\n...\n\n\n\n\n\nDask Name: getitem, 5 tasks\n\n\n\ny.compute()\n\n\n\n\n\n\n\n\ngdp\n\n\n\n\n0\n2093.30\n\n\n1\n2347.32\n\n\n2\n2542.96\n\n\n3\n2712.34\n\n\n4\n2902.09\n\n\n...\n...\n\n\n355\n9705.02\n\n\n356\n11648.70\n\n\n357\n13417.68\n\n\n358\n15718.47\n\n\n359\n18753.73\n\n\n\n\n360 rows × 1 columns\n\n\n\n\nprint(x.shape,y.shape)\n\n(Delayed('int-97d0cf00-db85-425b-a0d2-08297142db86'), 2) (Delayed('int-01cdae78-a995-48c1-9b93-277a008ad57a'), 1)\n\n\n\nx.count().compute()\n\nyear    360\nfdi     360\ndtype: int64\n\n\n\nfrom dask_ml.xgboost import XGBRegressor\n\n\nXGBR = XGBRegressor()\n\n\n%%time\nXGBR_model = XGBR.fit(x,y)\n\nCPU times: user 54.2 s, sys: 1.02 s, total: 55.2 s\nWall time: 18.4 s\n\n\n\nXGBR_model\n\nXGBRegressor()\n\n\n\nXGBR_model.save_model('fiscal_model')\n\n\nXGBR_model.load_model('fiscal_model')\n\n[08:43:57] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror."
  },
  {
    "objectID": "posts/2020-11-06-example_kaggle_project_submission.html",
    "href": "posts/2020-11-06-example_kaggle_project_submission.html",
    "title": "Kaggle Submission Example",
    "section": "",
    "text": "import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ndf_train = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/datasets/\"+\\\n    \"kaggle_iris_train.csv\", na_values=['NA','?'])\n\n# Encode feature vector\ndf_train.drop('id', axis=1, inplace=True)\n\nnum_classes = len(df_train.groupby('species').species.nunique())\n\nprint(\"Number of classes: {}\".format(num_classes))\n\n# Convert to numpy - Classification\nx = df_train[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ndummies = pd.get_dummies(df_train['species']) # Classification\nspecies = dummies.columns\ny = dummies.values\n    \n# Split into train/test\nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.25, random_state=45)\n\n# Train, with early stopping\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=x.shape[1], activation='relu'))\nmodel.add(Dense(25))\nmodel.add(Dense(y.shape[1],activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n                        patience=5, verbose=1, mode='auto',\n                       restore_best_weights=True)\n\nmodel.fit(x_train,y_train,validation_data=(x_test,y_test),\n          callbacks=[monitor],verbose=0,epochs=1000)\n\nNumber of classes: 3\nRestoring model weights from the end of the best epoch.\nEpoch 00055: early stopping\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x178e5493fc8&gt;\n\n\nNow that we’ve trained the neural network, we can check its log loss.\n\nfrom sklearn import metrics\n\n# Calculate multi log loss error\npred = model.predict(x_test)\nscore = metrics.log_loss(y_test, pred)\nprint(\"Log loss score: {}\".format(score))\n\nLog loss score: 0.3136451941728592\n\n\nNow we are ready to generate the Kaggle submission file. We will use the iris test data that does not contain a \\(y\\) target value. It is our job to predict this value and submit to Kaggle.\n\n# Generate Kaggle submit file\n\n# Encode feature vector\ndf_test = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/datasets/\"+\\\n    \"kaggle_iris_test.csv\", na_values=['NA','?'])\n\n# Convert to numpy - Classification\nids = df_test['id']\ndf_test.drop('id', axis=1, inplace=True)\nx = df_test[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ny = dummies.values\n\n# Generate predictions\npred = model.predict(x)\n#pred\n\n# Create submission data set\n\ndf_submit = pd.DataFrame(pred)\ndf_submit.insert(0,'id',ids)\ndf_submit.columns = ['id','species-0','species-1','species-2']\n\n# Write submit file locally\ndf_submit.to_csv(\"iris_submit.csv\", index=False) \n\nprint(df_submit)\n\n     id  species-0  species-1  species-2\n0   100   0.022236   0.533230   0.444534\n1   101   0.003699   0.394908   0.601393\n2   102   0.004600   0.420394   0.575007\n3   103   0.956168   0.040161   0.003672\n4   104   0.975333   0.022761   0.001906\n5   105   0.966681   0.030938   0.002381\n6   106   0.992637   0.007049   0.000314\n7   107   0.002810   0.358485   0.638705\n8   108   0.026152   0.557480   0.416368\n9   109   0.001194   0.350682   0.648124\n10  110   0.000649   0.268023   0.731328\n11  111   0.994907   0.004923   0.000170\n12  112   0.072954   0.587299   0.339747\n13  113   0.000571   0.258208   0.741221\n14  114   0.977138   0.021400   0.001463\n15  115   0.004665   0.449740   0.545596\n16  116   0.073553   0.567955   0.358493\n17  117   0.968778   0.029240   0.001982\n18  118   0.983742   0.015341   0.000918\n19  119   0.986016   0.013193   0.000792\n20  120   0.023752   0.583601   0.392647\n21  121   0.032858   0.584882   0.382260\n22  122   0.004007   0.395656   0.600338\n23  123   0.000885   0.240763   0.758352\n24  124   0.000531   0.271212   0.728256\n25  125   0.985742   0.013471   0.000787\n26  126   0.001298   0.320333   0.678369\n27  127   0.001753   0.342856   0.655391\n28  128   0.001147   0.317827   0.681026\n29  129   0.981223   0.017589   0.001188\n30  130   0.036438   0.578421   0.385140\n31  131   0.976528   0.021834   0.001638\n32  132   0.003681   0.405441   0.590878\n33  133   0.024478   0.539376   0.436146\n34  134   0.012039   0.466313   0.521649\n35  135   0.963704   0.033453   0.002844\n36  136   0.000614   0.244336   0.755050\n37  137   0.008160   0.490362   0.501478\n38  138   0.976859   0.021646   0.001495\n39  139   0.003789   0.317224   0.678987\n40  140   0.962254   0.034885   0.002861\n41  141   0.000792   0.289380   0.709828\n42  142   0.000253   0.239028   0.760719\n43  143   0.001390   0.298506   0.700104\n44  144   0.968422   0.029224   0.002354\n45  145   0.029218   0.524128   0.446654\n46  146   0.130497   0.579122   0.290381\n47  147   0.023003   0.499443   0.477553\n48  148   0.022195   0.527769   0.450036\n49  149   0.983695   0.015325   0.000980\n50  150   0.942703   0.052154   0.005144\n\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport pandas as pd\nimport io\nimport os\nimport requests\nimport numpy as np\nfrom sklearn import metrics\n\nsave_path = \".\"\n\ndf = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/datasets/\"+\\\n    \"kaggle_auto_train.csv\", \n    na_values=['NA', '?'])\n\ncars = df['name']\n\n# Handle missing value\ndf['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n\n# Pandas to Numpy\nx = df[['cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin']].values\ny = df['mpg'].values # regression\n\n# Split into train/test\nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.25, random_state=42)\n\n# Build the neural network\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\nmodel.add(Dense(10, activation='relu')) # Hidden 2\nmodel.add(Dense(1)) # Output\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n                        verbose=1, mode='auto', restore_best_weights=True)\nmodel.fit(x_train,y_train,validation_data=(x_test,y_test),\n          verbose=2,callbacks=[monitor],epochs=1000)\n\n# Predict\npred = model.predict(x_test)\n\nTrain on 261 samples, validate on 88 samples\nEpoch 1/1000\n261/261 - 0s - loss: 382597.1196 - val_loss: 246687.4858\nEpoch 2/1000\n261/261 - 0s - loss: 192257.0072 - val_loss: 98804.3558\nEpoch 3/1000\n261/261 - 0s - loss: 67605.7908 - val_loss: 28617.0703\nEpoch 4/1000\n261/261 - 0s - loss: 15922.8367 - val_loss: 3325.1682\nEpoch 5/1000\n261/261 - 0s - loss: 1270.3832 - val_loss: 512.5387\nEpoch 6/1000\n261/261 - 0s - loss: 1118.9636 - val_loss: 1651.5679\nEpoch 7/1000\n261/261 - 0s - loss: 1703.0441 - val_loss: 1161.2368\nEpoch 8/1000\n261/261 - 0s - loss: 900.1420 - val_loss: 452.0660\nEpoch 9/1000\n261/261 - 0s - loss: 355.7248 - val_loss: 304.3305\nEpoch 10/1000\n261/261 - 0s - loss: 336.1776 - val_loss: 353.2767\nEpoch 11/1000\n261/261 - 0s - loss: 364.7770 - val_loss: 337.0882\nEpoch 12/1000\n261/261 - 0s - loss: 334.1086 - val_loss: 301.5655\nEpoch 13/1000\n261/261 - 0s - loss: 318.2330 - val_loss: 295.2506\nEpoch 14/1000\n261/261 - 0s - loss: 315.3628 - val_loss: 294.1454\nEpoch 15/1000\n261/261 - 0s - loss: 313.4151 - val_loss: 292.0427\nEpoch 16/1000\n261/261 - 0s - loss: 310.5834 - val_loss: 290.4511\nEpoch 17/1000\n261/261 - 0s - loss: 308.1132 - val_loss: 289.9176\nEpoch 18/1000\n261/261 - 0s - loss: 307.3153 - val_loss: 287.1054\nEpoch 19/1000\n261/261 - 0s - loss: 305.2746 - val_loss: 285.1501\nEpoch 20/1000\n261/261 - 0s - loss: 303.8164 - val_loss: 283.2582\nEpoch 21/1000\n261/261 - 0s - loss: 302.2492 - val_loss: 281.4607\nEpoch 22/1000\n261/261 - 0s - loss: 300.0016 - val_loss: 279.4577\nEpoch 23/1000\n261/261 - 0s - loss: 296.3905 - val_loss: 279.2795\nEpoch 24/1000\n261/261 - 0s - loss: 296.2508 - val_loss: 278.0922\nEpoch 25/1000\n261/261 - 0s - loss: 295.3600 - val_loss: 275.6349\nEpoch 26/1000\n261/261 - 0s - loss: 291.1920 - val_loss: 271.5592\nEpoch 27/1000\n261/261 - 0s - loss: 293.0040 - val_loss: 270.6060\nEpoch 28/1000\n261/261 - 0s - loss: 288.8120 - val_loss: 267.5230\nEpoch 29/1000\n261/261 - 0s - loss: 285.0153 - val_loss: 267.6846\nEpoch 30/1000\n261/261 - 0s - loss: 284.5063 - val_loss: 267.5903\nEpoch 31/1000\n261/261 - 0s - loss: 283.2598 - val_loss: 263.2579\nEpoch 32/1000\n261/261 - 0s - loss: 279.1897 - val_loss: 259.1413\nEpoch 33/1000\n261/261 - 0s - loss: 278.0727 - val_loss: 257.1468\nEpoch 34/1000\n261/261 - 0s - loss: 275.0580 - val_loss: 255.3159\nEpoch 35/1000\n261/261 - 0s - loss: 275.2246 - val_loss: 257.6078\nEpoch 36/1000\n261/261 - 0s - loss: 273.1009 - val_loss: 253.1600\nEpoch 37/1000\n261/261 - 0s - loss: 268.6169 - val_loss: 248.6043\nEpoch 38/1000\n261/261 - 0s - loss: 266.2035 - val_loss: 246.5989\nEpoch 39/1000\n261/261 - 0s - loss: 263.9700 - val_loss: 245.5532\nEpoch 40/1000\n261/261 - 0s - loss: 262.1468 - val_loss: 242.2550\nEpoch 41/1000\n261/261 - 0s - loss: 259.1994 - val_loss: 239.2889\nEpoch 42/1000\n261/261 - 0s - loss: 258.9926 - val_loss: 237.0006\nEpoch 43/1000\n261/261 - 0s - loss: 253.8787 - val_loss: 239.7331\nEpoch 44/1000\n261/261 - 0s - loss: 255.4787 - val_loss: 234.9061\nEpoch 45/1000\n261/261 - 0s - loss: 251.2081 - val_loss: 231.0518\nEpoch 46/1000\n261/261 - 0s - loss: 248.3354 - val_loss: 228.7012\nEpoch 47/1000\n261/261 - 0s - loss: 246.8801 - val_loss: 225.7509\nEpoch 48/1000\n261/261 - 0s - loss: 243.6159 - val_loss: 224.8320\nEpoch 49/1000\n261/261 - 0s - loss: 242.0351 - val_loss: 222.3293\nEpoch 50/1000\n261/261 - 0s - loss: 240.8072 - val_loss: 218.9842\nEpoch 51/1000\n261/261 - 0s - loss: 237.3082 - val_loss: 216.6910\nEpoch 52/1000\n261/261 - 0s - loss: 236.4236 - val_loss: 219.1308\nEpoch 53/1000\n261/261 - 0s - loss: 233.8834 - val_loss: 213.7722\nEpoch 54/1000\n261/261 - 0s - loss: 229.9621 - val_loss: 209.7647\nEpoch 55/1000\n261/261 - 0s - loss: 227.2555 - val_loss: 207.4864\nEpoch 56/1000\n261/261 - 0s - loss: 226.4306 - val_loss: 204.9454\nEpoch 57/1000\n261/261 - 0s - loss: 223.0296 - val_loss: 204.7334\nEpoch 58/1000\n261/261 - 0s - loss: 220.8694 - val_loss: 201.1248\nEpoch 59/1000\n261/261 - 0s - loss: 217.6376 - val_loss: 197.8849\nEpoch 60/1000\n261/261 - 0s - loss: 216.9886 - val_loss: 196.0564\nEpoch 61/1000\n261/261 - 0s - loss: 214.6863 - val_loss: 193.1452\nEpoch 62/1000\n261/261 - 0s - loss: 210.8178 - val_loss: 190.9064\nEpoch 63/1000\n261/261 - 0s - loss: 208.5358 - val_loss: 189.0982\nEpoch 64/1000\n261/261 - 0s - loss: 206.8594 - val_loss: 188.4019\nEpoch 65/1000\n261/261 - 0s - loss: 204.5793 - val_loss: 184.1434\nEpoch 66/1000\n261/261 - 0s - loss: 202.2459 - val_loss: 182.0629\nEpoch 67/1000\n261/261 - 0s - loss: 200.4653 - val_loss: 179.7517\nEpoch 68/1000\n261/261 - 0s - loss: 199.4847 - val_loss: 181.0924\nEpoch 69/1000\n261/261 - 0s - loss: 196.1007 - val_loss: 176.6571\nEpoch 70/1000\n261/261 - 0s - loss: 192.8669 - val_loss: 173.5703\nEpoch 71/1000\n261/261 - 0s - loss: 192.0731 - val_loss: 171.1448\nEpoch 72/1000\n261/261 - 0s - loss: 188.9124 - val_loss: 169.1036\nEpoch 73/1000\n261/261 - 0s - loss: 187.2660 - val_loss: 168.4244\nEpoch 74/1000\n261/261 - 0s - loss: 184.3366 - val_loss: 164.9515\nEpoch 75/1000\n261/261 - 0s - loss: 182.0560 - val_loss: 162.9232\nEpoch 76/1000\n261/261 - 0s - loss: 180.9339 - val_loss: 160.5111\nEpoch 77/1000\n261/261 - 0s - loss: 177.7289 - val_loss: 160.0768\nEpoch 78/1000\n261/261 - 0s - loss: 177.0166 - val_loss: 157.8780\nEpoch 79/1000\n261/261 - 0s - loss: 174.2729 - val_loss: 155.0140\nEpoch 80/1000\n261/261 - 0s - loss: 174.1473 - val_loss: 152.7101\nEpoch 81/1000\n261/261 - 0s - loss: 170.2462 - val_loss: 150.7686\nEpoch 82/1000\n261/261 - 0s - loss: 168.1250 - val_loss: 148.6464\nEpoch 83/1000\n261/261 - 0s - loss: 165.2611 - val_loss: 147.3025\nEpoch 84/1000\n261/261 - 0s - loss: 163.6456 - val_loss: 144.6445\nEpoch 85/1000\n261/261 - 0s - loss: 162.0391 - val_loss: 142.6984\nEpoch 86/1000\n261/261 - 0s - loss: 159.2869 - val_loss: 142.8578\nEpoch 87/1000\n261/261 - 0s - loss: 158.4979 - val_loss: 140.0451\nEpoch 88/1000\n261/261 - 0s - loss: 155.8697 - val_loss: 137.2706\nEpoch 89/1000\n261/261 - 0s - loss: 153.9711 - val_loss: 135.4351\nEpoch 90/1000\n261/261 - 0s - loss: 154.6780 - val_loss: 135.3691\nEpoch 91/1000\n261/261 - 0s - loss: 151.5339 - val_loss: 132.4053\nEpoch 92/1000\n261/261 - 0s - loss: 149.8378 - val_loss: 129.7334\nEpoch 93/1000\n261/261 - 0s - loss: 146.4563 - val_loss: 128.3390\nEpoch 94/1000\n261/261 - 0s - loss: 144.4933 - val_loss: 127.0931\nEpoch 95/1000\n261/261 - 0s - loss: 142.9235 - val_loss: 124.5410\nEpoch 96/1000\n261/261 - 0s - loss: 141.2332 - val_loss: 122.6840\nEpoch 97/1000\n261/261 - 0s - loss: 139.6225 - val_loss: 121.8140\nEpoch 98/1000\n261/261 - 0s - loss: 137.8158 - val_loss: 119.7630\nEpoch 99/1000\n261/261 - 0s - loss: 136.0081 - val_loss: 118.2237\nEpoch 100/1000\n261/261 - 0s - loss: 134.2485 - val_loss: 117.2276\nEpoch 101/1000\n261/261 - 0s - loss: 132.6553 - val_loss: 114.9724\nEpoch 102/1000\n261/261 - 0s - loss: 130.9867 - val_loss: 113.3426\nEpoch 103/1000\n261/261 - 0s - loss: 129.7633 - val_loss: 112.5253\nEpoch 104/1000\n261/261 - 0s - loss: 127.4988 - val_loss: 109.9802\nEpoch 105/1000\n261/261 - 0s - loss: 126.5202 - val_loss: 108.6993\nEpoch 106/1000\n261/261 - 0s - loss: 127.0090 - val_loss: 109.9802\nEpoch 107/1000\n261/261 - 0s - loss: 123.9040 - val_loss: 105.5228\nEpoch 108/1000\n261/261 - 0s - loss: 122.4337 - val_loss: 106.0400\nEpoch 109/1000\n261/261 - 0s - loss: 120.6300 - val_loss: 103.0620\nEpoch 110/1000\n261/261 - 0s - loss: 118.5036 - val_loss: 101.1414\nEpoch 111/1000\n261/261 - 0s - loss: 119.0572 - val_loss: 100.2416\nEpoch 112/1000\n261/261 - 0s - loss: 115.5790 - val_loss: 99.5907\nEpoch 113/1000\n261/261 - 0s - loss: 114.3071 - val_loss: 96.6901\nEpoch 114/1000\n261/261 - 0s - loss: 112.3629 - val_loss: 95.6015\nEpoch 115/1000\n261/261 - 0s - loss: 111.1829 - val_loss: 94.8623\nEpoch 116/1000\n261/261 - 0s - loss: 110.1737 - val_loss: 92.5723\nEpoch 117/1000\n261/261 - 0s - loss: 108.3667 - val_loss: 92.2069\nEpoch 118/1000\n261/261 - 0s - loss: 106.8793 - val_loss: 90.0196\nEpoch 119/1000\n261/261 - 0s - loss: 111.7453 - val_loss: 89.3325\nEpoch 120/1000\n261/261 - 0s - loss: 108.2630 - val_loss: 93.4876\nEpoch 121/1000\n261/261 - 0s - loss: 106.3677 - val_loss: 86.3017\nEpoch 122/1000\n261/261 - 0s - loss: 101.7241 - val_loss: 85.6503\nEpoch 123/1000\n261/261 - 0s - loss: 100.5858 - val_loss: 83.9417\nEpoch 124/1000\n261/261 - 0s - loss: 98.9622 - val_loss: 83.3914\nEpoch 125/1000\n261/261 - 0s - loss: 97.9784 - val_loss: 81.5708\nEpoch 126/1000\n261/261 - 0s - loss: 96.6995 - val_loss: 80.4465\nEpoch 127/1000\n261/261 - 0s - loss: 95.5034 - val_loss: 79.5468\nEpoch 128/1000\n261/261 - 0s - loss: 93.9933 - val_loss: 78.7416\nEpoch 129/1000\n261/261 - 0s - loss: 93.2547 - val_loss: 77.2559\nEpoch 130/1000\n261/261 - 0s - loss: 92.0739 - val_loss: 76.4692\nEpoch 131/1000\n261/261 - 0s - loss: 91.3897 - val_loss: 75.0902\nEpoch 132/1000\n261/261 - 0s - loss: 89.5802 - val_loss: 74.2796\nEpoch 133/1000\n261/261 - 0s - loss: 89.2358 - val_loss: 73.7019\nEpoch 134/1000\n261/261 - 0s - loss: 89.2894 - val_loss: 71.7912\nEpoch 135/1000\n261/261 - 0s - loss: 86.9927 - val_loss: 70.9630\nEpoch 136/1000\n261/261 - 0s - loss: 84.9979 - val_loss: 71.5301\nEpoch 137/1000\n261/261 - 0s - loss: 85.4751 - val_loss: 69.3716\nEpoch 138/1000\n261/261 - 0s - loss: 84.5646 - val_loss: 69.2690\nEpoch 139/1000\n261/261 - 0s - loss: 83.6890 - val_loss: 67.7983\nEpoch 140/1000\n261/261 - 0s - loss: 80.8676 - val_loss: 66.0073\nEpoch 141/1000\n261/261 - 0s - loss: 79.7220 - val_loss: 65.3198\nEpoch 142/1000\n261/261 - 0s - loss: 79.1109 - val_loss: 65.2558\nEpoch 143/1000\n261/261 - 0s - loss: 78.7909 - val_loss: 63.5800\nEpoch 144/1000\n261/261 - 0s - loss: 77.2276 - val_loss: 62.5765\nEpoch 145/1000\n261/261 - 0s - loss: 75.8473 - val_loss: 61.7780\nEpoch 146/1000\n261/261 - 0s - loss: 74.8493 - val_loss: 60.8583\nEpoch 147/1000\n261/261 - 0s - loss: 74.0530 - val_loss: 59.8856\nEpoch 148/1000\n261/261 - 0s - loss: 73.0771 - val_loss: 59.4027\nEpoch 149/1000\n261/261 - 0s - loss: 72.2401 - val_loss: 58.3119\nEpoch 150/1000\n261/261 - 0s - loss: 72.1309 - val_loss: 57.5037\nEpoch 151/1000\n261/261 - 0s - loss: 70.7773 - val_loss: 57.7769\nEpoch 152/1000\n261/261 - 0s - loss: 70.5883 - val_loss: 56.1087\nEpoch 153/1000\n261/261 - 0s - loss: 68.6020 - val_loss: 55.3935\nEpoch 154/1000\n261/261 - 0s - loss: 68.0137 - val_loss: 55.3946\nEpoch 155/1000\n261/261 - 0s - loss: 68.3630 - val_loss: 54.5067\nEpoch 156/1000\n261/261 - 0s - loss: 68.1104 - val_loss: 53.5928\nEpoch 157/1000\n261/261 - 0s - loss: 66.8734 - val_loss: 53.7972\nEpoch 158/1000\n261/261 - 0s - loss: 64.6184 - val_loss: 51.8031\nEpoch 159/1000\n261/261 - 0s - loss: 64.5744 - val_loss: 51.4003\nEpoch 160/1000\n261/261 - 0s - loss: 63.6910 - val_loss: 50.6856\nEpoch 161/1000\n261/261 - 0s - loss: 64.0145 - val_loss: 49.9536\nEpoch 162/1000\n261/261 - 0s - loss: 61.8386 - val_loss: 49.8901\nEpoch 163/1000\n261/261 - 0s - loss: 61.9306 - val_loss: 49.2521\nEpoch 164/1000\n261/261 - 0s - loss: 60.7556 - val_loss: 47.9911\nEpoch 165/1000\n261/261 - 0s - loss: 60.2802 - val_loss: 47.2594\nEpoch 166/1000\n261/261 - 0s - loss: 59.2542 - val_loss: 46.9898\nEpoch 167/1000\n261/261 - 0s - loss: 58.3004 - val_loss: 46.5502\nEpoch 168/1000\n261/261 - 0s - loss: 57.8545 - val_loss: 45.7245\nEpoch 169/1000\n261/261 - 0s - loss: 56.9617 - val_loss: 45.0827\nEpoch 170/1000\n261/261 - 0s - loss: 56.9749 - val_loss: 45.1476\nEpoch 171/1000\n261/261 - 0s - loss: 55.8050 - val_loss: 44.0151\nEpoch 172/1000\n261/261 - 0s - loss: 56.0478 - val_loss: 43.5957\nEpoch 173/1000\n261/261 - 0s - loss: 55.2461 - val_loss: 43.9503\nEpoch 174/1000\n261/261 - 0s - loss: 54.0493 - val_loss: 42.5281\nEpoch 175/1000\n261/261 - 0s - loss: 54.2585 - val_loss: 42.0300\nEpoch 176/1000\n261/261 - 0s - loss: 52.9849 - val_loss: 42.1091\nEpoch 177/1000\n261/261 - 0s - loss: 52.6699 - val_loss: 41.1280\nEpoch 178/1000\n261/261 - 0s - loss: 52.5766 - val_loss: 40.6279\nEpoch 179/1000\n261/261 - 0s - loss: 51.2797 - val_loss: 41.5560\nEpoch 180/1000\n261/261 - 0s - loss: 51.3167 - val_loss: 39.8998\nEpoch 181/1000\n261/261 - 0s - loss: 50.6548 - val_loss: 40.3602\nEpoch 182/1000\n261/261 - 0s - loss: 49.9360 - val_loss: 38.9575\nEpoch 183/1000\n261/261 - 0s - loss: 49.3195 - val_loss: 38.5161\nEpoch 184/1000\n261/261 - 0s - loss: 48.8159 - val_loss: 38.2727\nEpoch 185/1000\n261/261 - 0s - loss: 48.5230 - val_loss: 38.3134\nEpoch 186/1000\n261/261 - 0s - loss: 48.1472 - val_loss: 37.5338\nEpoch 187/1000\n261/261 - 0s - loss: 49.0451 - val_loss: 37.0337\nEpoch 188/1000\n261/261 - 0s - loss: 46.9509 - val_loss: 37.4614\nEpoch 189/1000\n261/261 - 0s - loss: 46.8951 - val_loss: 36.4360\nEpoch 190/1000\n261/261 - 0s - loss: 46.1027 - val_loss: 36.2615\nEpoch 191/1000\n261/261 - 0s - loss: 45.6384 - val_loss: 35.6610\nEpoch 192/1000\n261/261 - 0s - loss: 46.9916 - val_loss: 35.5148\nEpoch 193/1000\n261/261 - 0s - loss: 49.5148 - val_loss: 37.5214\nEpoch 194/1000\n261/261 - 0s - loss: 46.2516 - val_loss: 35.9050\nEpoch 195/1000\n261/261 - 0s - loss: 45.1961 - val_loss: 35.2473\nEpoch 196/1000\n261/261 - 0s - loss: 43.8845 - val_loss: 34.1322\nEpoch 197/1000\n261/261 - 0s - loss: 43.4610 - val_loss: 33.6880\nEpoch 198/1000\n261/261 - 0s - loss: 42.6286 - val_loss: 33.7127\nEpoch 199/1000\n261/261 - 0s - loss: 42.4154 - val_loss: 33.2152\nEpoch 200/1000\n261/261 - 0s - loss: 42.0020 - val_loss: 32.9451\nEpoch 201/1000\n261/261 - 0s - loss: 41.6191 - val_loss: 32.5093\nEpoch 202/1000\n261/261 - 0s - loss: 43.7235 - val_loss: 32.3695\nEpoch 203/1000\n261/261 - 0s - loss: 43.0863 - val_loss: 34.2041\nEpoch 204/1000\n261/261 - 0s - loss: 41.0544 - val_loss: 32.0973\nEpoch 205/1000\n261/261 - 0s - loss: 40.7787 - val_loss: 32.5461\nEpoch 206/1000\n261/261 - 0s - loss: 41.6360 - val_loss: 31.2820\nEpoch 207/1000\n261/261 - 0s - loss: 40.7417 - val_loss: 32.2974\nEpoch 208/1000\n261/261 - 0s - loss: 39.9822 - val_loss: 30.7600\nEpoch 209/1000\n261/261 - 0s - loss: 39.3857 - val_loss: 32.5769\nEpoch 210/1000\n261/261 - 0s - loss: 39.1410 - val_loss: 30.4246\nEpoch 211/1000\n261/261 - 0s - loss: 38.7447 - val_loss: 30.0492\nEpoch 212/1000\n261/261 - 0s - loss: 37.9753 - val_loss: 29.8627\nEpoch 213/1000\n261/261 - 0s - loss: 38.3355 - val_loss: 29.6306\nEpoch 214/1000\n261/261 - 0s - loss: 37.3530 - val_loss: 29.6433\nEpoch 215/1000\n261/261 - 0s - loss: 37.1885 - val_loss: 29.3205\nEpoch 216/1000\n261/261 - 0s - loss: 36.7803 - val_loss: 29.0165\nEpoch 217/1000\n261/261 - 0s - loss: 37.1867 - val_loss: 28.8259\nEpoch 218/1000\n261/261 - 0s - loss: 36.1244 - val_loss: 29.7593\nEpoch 219/1000\n261/261 - 0s - loss: 37.7266 - val_loss: 28.7380\nEpoch 220/1000\n261/261 - 0s - loss: 35.7875 - val_loss: 28.4919\nEpoch 221/1000\n261/261 - 0s - loss: 35.6227 - val_loss: 28.1351\nEpoch 222/1000\n261/261 - 0s - loss: 35.3527 - val_loss: 27.9021\nEpoch 223/1000\n261/261 - 0s - loss: 34.9739 - val_loss: 27.9576\nEpoch 224/1000\n261/261 - 0s - loss: 34.7204 - val_loss: 27.6015\nEpoch 225/1000\n261/261 - 0s - loss: 34.7849 - val_loss: 27.3595\nEpoch 226/1000\n261/261 - 0s - loss: 34.6823 - val_loss: 27.3375\nEpoch 227/1000\n261/261 - 0s - loss: 34.6561 - val_loss: 27.0810\nEpoch 228/1000\n261/261 - 0s - loss: 34.7526 - val_loss: 26.9909\nEpoch 229/1000\n261/261 - 0s - loss: 33.2568 - val_loss: 28.1786\nEpoch 230/1000\n261/261 - 0s - loss: 33.8126 - val_loss: 26.6188\nEpoch 231/1000\n261/261 - 0s - loss: 33.7432 - val_loss: 26.5590\nEpoch 232/1000\n261/261 - 0s - loss: 32.8556 - val_loss: 26.4733\nEpoch 233/1000\n261/261 - 0s - loss: 32.6516 - val_loss: 26.1711\nEpoch 234/1000\n261/261 - 0s - loss: 32.9949 - val_loss: 25.9887\nEpoch 235/1000\n261/261 - 0s - loss: 33.1170 - val_loss: 26.3875\nEpoch 236/1000\n261/261 - 0s - loss: 32.5374 - val_loss: 25.7300\nEpoch 237/1000\n261/261 - 0s - loss: 32.6500 - val_loss: 25.6121\nEpoch 238/1000\n261/261 - 0s - loss: 32.4430 - val_loss: 25.6679\nEpoch 239/1000\n261/261 - 0s - loss: 32.0512 - val_loss: 25.5607\nEpoch 240/1000\n261/261 - 0s - loss: 31.8485 - val_loss: 25.3327\nEpoch 241/1000\n261/261 - 0s - loss: 31.3820 - val_loss: 25.6274\nEpoch 242/1000\n261/261 - 0s - loss: 32.3983 - val_loss: 24.9405\nEpoch 243/1000\n261/261 - 0s - loss: 30.7282 - val_loss: 24.9071\nEpoch 244/1000\n261/261 - 0s - loss: 30.4659 - val_loss: 24.7043\nEpoch 245/1000\n261/261 - 0s - loss: 30.7127 - val_loss: 24.6449\nEpoch 246/1000\n261/261 - 0s - loss: 29.9609 - val_loss: 24.4984\nEpoch 247/1000\n261/261 - 0s - loss: 30.2372 - val_loss: 24.3524\nEpoch 248/1000\n261/261 - 0s - loss: 30.2689 - val_loss: 24.3986\nEpoch 249/1000\n261/261 - 0s - loss: 30.7721 - val_loss: 24.2271\nEpoch 250/1000\n261/261 - 0s - loss: 30.6043 - val_loss: 24.0360\nEpoch 251/1000\n261/261 - 0s - loss: 30.3024 - val_loss: 24.0987\nEpoch 252/1000\n261/261 - 0s - loss: 28.9162 - val_loss: 23.7909\nEpoch 253/1000\n261/261 - 0s - loss: 29.2801 - val_loss: 23.8153\nEpoch 254/1000\n261/261 - 0s - loss: 29.3222 - val_loss: 23.5515\nEpoch 255/1000\n261/261 - 0s - loss: 28.5132 - val_loss: 23.8399\nEpoch 256/1000\n261/261 - 0s - loss: 28.9835 - val_loss: 23.3674\nEpoch 257/1000\n261/261 - 0s - loss: 28.2271 - val_loss: 23.4548\nEpoch 258/1000\n261/261 - 0s - loss: 27.8565 - val_loss: 23.1535\nEpoch 259/1000\n261/261 - 0s - loss: 27.8770 - val_loss: 23.1761\nEpoch 260/1000\n261/261 - 0s - loss: 27.5445 - val_loss: 22.9507\nEpoch 261/1000\n261/261 - 0s - loss: 27.6223 - val_loss: 22.8882\nEpoch 262/1000\n261/261 - 0s - loss: 27.3854 - val_loss: 22.9048\nEpoch 263/1000\n261/261 - 0s - loss: 27.3946 - val_loss: 22.6476\nEpoch 264/1000\n261/261 - 0s - loss: 27.0089 - val_loss: 22.5546\nEpoch 265/1000\n261/261 - 0s - loss: 26.9027 - val_loss: 22.4856\nEpoch 266/1000\n261/261 - 0s - loss: 26.7630 - val_loss: 22.4675\nEpoch 267/1000\n261/261 - 0s - loss: 27.0150 - val_loss: 22.3077\nEpoch 268/1000\n261/261 - 0s - loss: 26.3339 - val_loss: 22.1958\nEpoch 269/1000\n261/261 - 0s - loss: 26.5861 - val_loss: 22.3650\nEpoch 270/1000\n261/261 - 0s - loss: 26.3245 - val_loss: 22.0337\nEpoch 271/1000\n261/261 - 0s - loss: 26.0610 - val_loss: 21.9219\nEpoch 272/1000\n261/261 - 0s - loss: 25.9908 - val_loss: 22.0404\nEpoch 273/1000\n261/261 - 0s - loss: 25.7291 - val_loss: 22.0628\nEpoch 274/1000\n261/261 - 0s - loss: 28.5037 - val_loss: 22.0770\nEpoch 275/1000\n261/261 - 0s - loss: 26.8031 - val_loss: 21.6196\nEpoch 276/1000\n261/261 - 0s - loss: 26.1467 - val_loss: 21.5624\nEpoch 277/1000\n261/261 - 0s - loss: 25.3375 - val_loss: 22.5604\nEpoch 278/1000\n261/261 - 0s - loss: 25.9187 - val_loss: 21.3272\nEpoch 279/1000\n261/261 - 0s - loss: 25.1155 - val_loss: 21.4415\nEpoch 280/1000\n261/261 - 0s - loss: 24.9094 - val_loss: 21.2730\nEpoch 281/1000\n261/261 - 0s - loss: 24.6625 - val_loss: 21.0808\nEpoch 282/1000\n261/261 - 0s - loss: 24.9405 - val_loss: 21.0107\nEpoch 283/1000\n261/261 - 0s - loss: 24.4015 - val_loss: 21.2510\nEpoch 284/1000\n261/261 - 0s - loss: 24.8920 - val_loss: 20.8377\nEpoch 285/1000\n261/261 - 0s - loss: 24.1691 - val_loss: 20.7580\nEpoch 286/1000\n261/261 - 0s - loss: 24.4140 - val_loss: 20.6990\nEpoch 287/1000\n261/261 - 0s - loss: 24.0014 - val_loss: 20.6096\nEpoch 288/1000\n261/261 - 0s - loss: 23.7787 - val_loss: 20.5134\nEpoch 289/1000\n261/261 - 0s - loss: 23.7206 - val_loss: 20.6378\nEpoch 290/1000\n261/261 - 0s - loss: 24.6226 - val_loss: 20.3711\nEpoch 291/1000\n261/261 - 0s - loss: 23.3452 - val_loss: 20.4992\nEpoch 292/1000\n261/261 - 0s - loss: 23.6446 - val_loss: 20.4952\nEpoch 293/1000\n261/261 - 0s - loss: 24.2056 - val_loss: 20.6786\nEpoch 294/1000\n261/261 - 0s - loss: 25.1895 - val_loss: 20.4868\nEpoch 295/1000\nRestoring model weights from the end of the best epoch.\n261/261 - 0s - loss: 23.6036 - val_loss: 20.3795\nEpoch 00295: early stopping\n\n\n\nimport numpy as np\n\n# Measure RMSE error.  RMSE is common for regression.\nscore = np.sqrt(metrics.mean_squared_error(pred,y_test))\nprint(\"Final score (RMSE): {}\".format(score))\n\nFinal score (RMSE): 4.5134384517538795\n\n\n\nimport pandas as pd\n\n# Generate Kaggle submit file\n\n# Encode feature vector\ndf_test = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/datasets/\"+\\\n    \"kaggle_auto_test.csv\", na_values=['NA','?'])\n\n# Convert to numpy - regression\nids = df_test['id']\ndf_test.drop('id', axis=1, inplace=True)\n\n# Handle missing value\ndf_test['horsepower'] = df_test['horsepower'].\\\n    fillna(df['horsepower'].median())\n\nx = df_test[['cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin']].values\n\n\n\n\n# Generate predictions\npred = model.predict(x)\n#pred\n\n# Create submission data set\n\ndf_submit = pd.DataFrame(pred)\ndf_submit.insert(0,'id',ids)\ndf_submit.columns = ['id','mpg']\n\n# Write submit file locally\ndf_submit.to_csv(\"auto_submit.csv\", index=False) \n\nprint(df_submit)\n\n     id        mpg\n0   350  29.112602\n1   351  27.803200\n2   352  27.981804\n3   353  30.487831\n4   354  27.227440\n5   355  26.438324\n6   356  27.886986\n7   357  29.103935\n8   358  26.447609\n9   359  30.027260\n10  360  30.312553\n11  361  30.712151\n12  362  23.952263\n13  363  24.858467\n14  364  23.459129\n15  365  22.638985\n16  366  26.032127\n17  367  26.197884\n18  368  28.448906\n19  369  28.138954\n20  370  27.352821\n21  371  27.313377\n22  372  26.464119\n23  373  26.689583\n24  374  26.546562\n25  375  27.829781\n26  376  27.466354\n27  377  30.343369\n28  378  29.985909\n29  379  27.807251\n30  380  28.450882\n31  381  26.574844\n32  382  28.199501\n33  383  29.615051\n34  384  29.048317\n35  385  29.320534\n36  386  29.582710\n37  387  24.533165\n38  388  24.426888\n39  389  24.658607\n40  390  21.805504\n41  391  26.026482\n42  392  24.947670\n43  393  26.902489\n44  394  26.575218\n45  395  33.546684\n46  396  24.233910\n47  397  28.609993\n48  398  28.913261"
  },
  {
    "objectID": "posts/2020-11-06-example_kaggle_project_submission.html#notebook-and-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "href": "posts/2020-11-06-example_kaggle_project_submission.html#notebook-and-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "title": "Kaggle Submission Example",
    "section": "",
    "text": "import os\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom tensorflow.keras.callbacks import EarlyStopping\n\ndf_train = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/datasets/\"+\\\n    \"kaggle_iris_train.csv\", na_values=['NA','?'])\n\n# Encode feature vector\ndf_train.drop('id', axis=1, inplace=True)\n\nnum_classes = len(df_train.groupby('species').species.nunique())\n\nprint(\"Number of classes: {}\".format(num_classes))\n\n# Convert to numpy - Classification\nx = df_train[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ndummies = pd.get_dummies(df_train['species']) # Classification\nspecies = dummies.columns\ny = dummies.values\n    \n# Split into train/test\nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.25, random_state=45)\n\n# Train, with early stopping\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=x.shape[1], activation='relu'))\nmodel.add(Dense(25))\nmodel.add(Dense(y.shape[1],activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n                        patience=5, verbose=1, mode='auto',\n                       restore_best_weights=True)\n\nmodel.fit(x_train,y_train,validation_data=(x_test,y_test),\n          callbacks=[monitor],verbose=0,epochs=1000)\n\nNumber of classes: 3\nRestoring model weights from the end of the best epoch.\nEpoch 00055: early stopping\n\n\n&lt;tensorflow.python.keras.callbacks.History at 0x178e5493fc8&gt;\n\n\nNow that we’ve trained the neural network, we can check its log loss.\n\nfrom sklearn import metrics\n\n# Calculate multi log loss error\npred = model.predict(x_test)\nscore = metrics.log_loss(y_test, pred)\nprint(\"Log loss score: {}\".format(score))\n\nLog loss score: 0.3136451941728592\n\n\nNow we are ready to generate the Kaggle submission file. We will use the iris test data that does not contain a \\(y\\) target value. It is our job to predict this value and submit to Kaggle.\n\n# Generate Kaggle submit file\n\n# Encode feature vector\ndf_test = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/datasets/\"+\\\n    \"kaggle_iris_test.csv\", na_values=['NA','?'])\n\n# Convert to numpy - Classification\nids = df_test['id']\ndf_test.drop('id', axis=1, inplace=True)\nx = df_test[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ny = dummies.values\n\n# Generate predictions\npred = model.predict(x)\n#pred\n\n# Create submission data set\n\ndf_submit = pd.DataFrame(pred)\ndf_submit.insert(0,'id',ids)\ndf_submit.columns = ['id','species-0','species-1','species-2']\n\n# Write submit file locally\ndf_submit.to_csv(\"iris_submit.csv\", index=False) \n\nprint(df_submit)\n\n     id  species-0  species-1  species-2\n0   100   0.022236   0.533230   0.444534\n1   101   0.003699   0.394908   0.601393\n2   102   0.004600   0.420394   0.575007\n3   103   0.956168   0.040161   0.003672\n4   104   0.975333   0.022761   0.001906\n5   105   0.966681   0.030938   0.002381\n6   106   0.992637   0.007049   0.000314\n7   107   0.002810   0.358485   0.638705\n8   108   0.026152   0.557480   0.416368\n9   109   0.001194   0.350682   0.648124\n10  110   0.000649   0.268023   0.731328\n11  111   0.994907   0.004923   0.000170\n12  112   0.072954   0.587299   0.339747\n13  113   0.000571   0.258208   0.741221\n14  114   0.977138   0.021400   0.001463\n15  115   0.004665   0.449740   0.545596\n16  116   0.073553   0.567955   0.358493\n17  117   0.968778   0.029240   0.001982\n18  118   0.983742   0.015341   0.000918\n19  119   0.986016   0.013193   0.000792\n20  120   0.023752   0.583601   0.392647\n21  121   0.032858   0.584882   0.382260\n22  122   0.004007   0.395656   0.600338\n23  123   0.000885   0.240763   0.758352\n24  124   0.000531   0.271212   0.728256\n25  125   0.985742   0.013471   0.000787\n26  126   0.001298   0.320333   0.678369\n27  127   0.001753   0.342856   0.655391\n28  128   0.001147   0.317827   0.681026\n29  129   0.981223   0.017589   0.001188\n30  130   0.036438   0.578421   0.385140\n31  131   0.976528   0.021834   0.001638\n32  132   0.003681   0.405441   0.590878\n33  133   0.024478   0.539376   0.436146\n34  134   0.012039   0.466313   0.521649\n35  135   0.963704   0.033453   0.002844\n36  136   0.000614   0.244336   0.755050\n37  137   0.008160   0.490362   0.501478\n38  138   0.976859   0.021646   0.001495\n39  139   0.003789   0.317224   0.678987\n40  140   0.962254   0.034885   0.002861\n41  141   0.000792   0.289380   0.709828\n42  142   0.000253   0.239028   0.760719\n43  143   0.001390   0.298506   0.700104\n44  144   0.968422   0.029224   0.002354\n45  145   0.029218   0.524128   0.446654\n46  146   0.130497   0.579122   0.290381\n47  147   0.023003   0.499443   0.477553\n48  148   0.022195   0.527769   0.450036\n49  149   0.983695   0.015325   0.000980\n50  150   0.942703   0.052154   0.005144\n\n\n\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport pandas as pd\nimport io\nimport os\nimport requests\nimport numpy as np\nfrom sklearn import metrics\n\nsave_path = \".\"\n\ndf = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/datasets/\"+\\\n    \"kaggle_auto_train.csv\", \n    na_values=['NA', '?'])\n\ncars = df['name']\n\n# Handle missing value\ndf['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n\n# Pandas to Numpy\nx = df[['cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin']].values\ny = df['mpg'].values # regression\n\n# Split into train/test\nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.25, random_state=42)\n\n# Build the neural network\nmodel = Sequential()\nmodel.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\nmodel.add(Dense(10, activation='relu')) # Hidden 2\nmodel.add(Dense(1)) # Output\nmodel.compile(loss='mean_squared_error', optimizer='adam')\nmonitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, \n                        verbose=1, mode='auto', restore_best_weights=True)\nmodel.fit(x_train,y_train,validation_data=(x_test,y_test),\n          verbose=2,callbacks=[monitor],epochs=1000)\n\n# Predict\npred = model.predict(x_test)\n\nTrain on 261 samples, validate on 88 samples\nEpoch 1/1000\n261/261 - 0s - loss: 382597.1196 - val_loss: 246687.4858\nEpoch 2/1000\n261/261 - 0s - loss: 192257.0072 - val_loss: 98804.3558\nEpoch 3/1000\n261/261 - 0s - loss: 67605.7908 - val_loss: 28617.0703\nEpoch 4/1000\n261/261 - 0s - loss: 15922.8367 - val_loss: 3325.1682\nEpoch 5/1000\n261/261 - 0s - loss: 1270.3832 - val_loss: 512.5387\nEpoch 6/1000\n261/261 - 0s - loss: 1118.9636 - val_loss: 1651.5679\nEpoch 7/1000\n261/261 - 0s - loss: 1703.0441 - val_loss: 1161.2368\nEpoch 8/1000\n261/261 - 0s - loss: 900.1420 - val_loss: 452.0660\nEpoch 9/1000\n261/261 - 0s - loss: 355.7248 - val_loss: 304.3305\nEpoch 10/1000\n261/261 - 0s - loss: 336.1776 - val_loss: 353.2767\nEpoch 11/1000\n261/261 - 0s - loss: 364.7770 - val_loss: 337.0882\nEpoch 12/1000\n261/261 - 0s - loss: 334.1086 - val_loss: 301.5655\nEpoch 13/1000\n261/261 - 0s - loss: 318.2330 - val_loss: 295.2506\nEpoch 14/1000\n261/261 - 0s - loss: 315.3628 - val_loss: 294.1454\nEpoch 15/1000\n261/261 - 0s - loss: 313.4151 - val_loss: 292.0427\nEpoch 16/1000\n261/261 - 0s - loss: 310.5834 - val_loss: 290.4511\nEpoch 17/1000\n261/261 - 0s - loss: 308.1132 - val_loss: 289.9176\nEpoch 18/1000\n261/261 - 0s - loss: 307.3153 - val_loss: 287.1054\nEpoch 19/1000\n261/261 - 0s - loss: 305.2746 - val_loss: 285.1501\nEpoch 20/1000\n261/261 - 0s - loss: 303.8164 - val_loss: 283.2582\nEpoch 21/1000\n261/261 - 0s - loss: 302.2492 - val_loss: 281.4607\nEpoch 22/1000\n261/261 - 0s - loss: 300.0016 - val_loss: 279.4577\nEpoch 23/1000\n261/261 - 0s - loss: 296.3905 - val_loss: 279.2795\nEpoch 24/1000\n261/261 - 0s - loss: 296.2508 - val_loss: 278.0922\nEpoch 25/1000\n261/261 - 0s - loss: 295.3600 - val_loss: 275.6349\nEpoch 26/1000\n261/261 - 0s - loss: 291.1920 - val_loss: 271.5592\nEpoch 27/1000\n261/261 - 0s - loss: 293.0040 - val_loss: 270.6060\nEpoch 28/1000\n261/261 - 0s - loss: 288.8120 - val_loss: 267.5230\nEpoch 29/1000\n261/261 - 0s - loss: 285.0153 - val_loss: 267.6846\nEpoch 30/1000\n261/261 - 0s - loss: 284.5063 - val_loss: 267.5903\nEpoch 31/1000\n261/261 - 0s - loss: 283.2598 - val_loss: 263.2579\nEpoch 32/1000\n261/261 - 0s - loss: 279.1897 - val_loss: 259.1413\nEpoch 33/1000\n261/261 - 0s - loss: 278.0727 - val_loss: 257.1468\nEpoch 34/1000\n261/261 - 0s - loss: 275.0580 - val_loss: 255.3159\nEpoch 35/1000\n261/261 - 0s - loss: 275.2246 - val_loss: 257.6078\nEpoch 36/1000\n261/261 - 0s - loss: 273.1009 - val_loss: 253.1600\nEpoch 37/1000\n261/261 - 0s - loss: 268.6169 - val_loss: 248.6043\nEpoch 38/1000\n261/261 - 0s - loss: 266.2035 - val_loss: 246.5989\nEpoch 39/1000\n261/261 - 0s - loss: 263.9700 - val_loss: 245.5532\nEpoch 40/1000\n261/261 - 0s - loss: 262.1468 - val_loss: 242.2550\nEpoch 41/1000\n261/261 - 0s - loss: 259.1994 - val_loss: 239.2889\nEpoch 42/1000\n261/261 - 0s - loss: 258.9926 - val_loss: 237.0006\nEpoch 43/1000\n261/261 - 0s - loss: 253.8787 - val_loss: 239.7331\nEpoch 44/1000\n261/261 - 0s - loss: 255.4787 - val_loss: 234.9061\nEpoch 45/1000\n261/261 - 0s - loss: 251.2081 - val_loss: 231.0518\nEpoch 46/1000\n261/261 - 0s - loss: 248.3354 - val_loss: 228.7012\nEpoch 47/1000\n261/261 - 0s - loss: 246.8801 - val_loss: 225.7509\nEpoch 48/1000\n261/261 - 0s - loss: 243.6159 - val_loss: 224.8320\nEpoch 49/1000\n261/261 - 0s - loss: 242.0351 - val_loss: 222.3293\nEpoch 50/1000\n261/261 - 0s - loss: 240.8072 - val_loss: 218.9842\nEpoch 51/1000\n261/261 - 0s - loss: 237.3082 - val_loss: 216.6910\nEpoch 52/1000\n261/261 - 0s - loss: 236.4236 - val_loss: 219.1308\nEpoch 53/1000\n261/261 - 0s - loss: 233.8834 - val_loss: 213.7722\nEpoch 54/1000\n261/261 - 0s - loss: 229.9621 - val_loss: 209.7647\nEpoch 55/1000\n261/261 - 0s - loss: 227.2555 - val_loss: 207.4864\nEpoch 56/1000\n261/261 - 0s - loss: 226.4306 - val_loss: 204.9454\nEpoch 57/1000\n261/261 - 0s - loss: 223.0296 - val_loss: 204.7334\nEpoch 58/1000\n261/261 - 0s - loss: 220.8694 - val_loss: 201.1248\nEpoch 59/1000\n261/261 - 0s - loss: 217.6376 - val_loss: 197.8849\nEpoch 60/1000\n261/261 - 0s - loss: 216.9886 - val_loss: 196.0564\nEpoch 61/1000\n261/261 - 0s - loss: 214.6863 - val_loss: 193.1452\nEpoch 62/1000\n261/261 - 0s - loss: 210.8178 - val_loss: 190.9064\nEpoch 63/1000\n261/261 - 0s - loss: 208.5358 - val_loss: 189.0982\nEpoch 64/1000\n261/261 - 0s - loss: 206.8594 - val_loss: 188.4019\nEpoch 65/1000\n261/261 - 0s - loss: 204.5793 - val_loss: 184.1434\nEpoch 66/1000\n261/261 - 0s - loss: 202.2459 - val_loss: 182.0629\nEpoch 67/1000\n261/261 - 0s - loss: 200.4653 - val_loss: 179.7517\nEpoch 68/1000\n261/261 - 0s - loss: 199.4847 - val_loss: 181.0924\nEpoch 69/1000\n261/261 - 0s - loss: 196.1007 - val_loss: 176.6571\nEpoch 70/1000\n261/261 - 0s - loss: 192.8669 - val_loss: 173.5703\nEpoch 71/1000\n261/261 - 0s - loss: 192.0731 - val_loss: 171.1448\nEpoch 72/1000\n261/261 - 0s - loss: 188.9124 - val_loss: 169.1036\nEpoch 73/1000\n261/261 - 0s - loss: 187.2660 - val_loss: 168.4244\nEpoch 74/1000\n261/261 - 0s - loss: 184.3366 - val_loss: 164.9515\nEpoch 75/1000\n261/261 - 0s - loss: 182.0560 - val_loss: 162.9232\nEpoch 76/1000\n261/261 - 0s - loss: 180.9339 - val_loss: 160.5111\nEpoch 77/1000\n261/261 - 0s - loss: 177.7289 - val_loss: 160.0768\nEpoch 78/1000\n261/261 - 0s - loss: 177.0166 - val_loss: 157.8780\nEpoch 79/1000\n261/261 - 0s - loss: 174.2729 - val_loss: 155.0140\nEpoch 80/1000\n261/261 - 0s - loss: 174.1473 - val_loss: 152.7101\nEpoch 81/1000\n261/261 - 0s - loss: 170.2462 - val_loss: 150.7686\nEpoch 82/1000\n261/261 - 0s - loss: 168.1250 - val_loss: 148.6464\nEpoch 83/1000\n261/261 - 0s - loss: 165.2611 - val_loss: 147.3025\nEpoch 84/1000\n261/261 - 0s - loss: 163.6456 - val_loss: 144.6445\nEpoch 85/1000\n261/261 - 0s - loss: 162.0391 - val_loss: 142.6984\nEpoch 86/1000\n261/261 - 0s - loss: 159.2869 - val_loss: 142.8578\nEpoch 87/1000\n261/261 - 0s - loss: 158.4979 - val_loss: 140.0451\nEpoch 88/1000\n261/261 - 0s - loss: 155.8697 - val_loss: 137.2706\nEpoch 89/1000\n261/261 - 0s - loss: 153.9711 - val_loss: 135.4351\nEpoch 90/1000\n261/261 - 0s - loss: 154.6780 - val_loss: 135.3691\nEpoch 91/1000\n261/261 - 0s - loss: 151.5339 - val_loss: 132.4053\nEpoch 92/1000\n261/261 - 0s - loss: 149.8378 - val_loss: 129.7334\nEpoch 93/1000\n261/261 - 0s - loss: 146.4563 - val_loss: 128.3390\nEpoch 94/1000\n261/261 - 0s - loss: 144.4933 - val_loss: 127.0931\nEpoch 95/1000\n261/261 - 0s - loss: 142.9235 - val_loss: 124.5410\nEpoch 96/1000\n261/261 - 0s - loss: 141.2332 - val_loss: 122.6840\nEpoch 97/1000\n261/261 - 0s - loss: 139.6225 - val_loss: 121.8140\nEpoch 98/1000\n261/261 - 0s - loss: 137.8158 - val_loss: 119.7630\nEpoch 99/1000\n261/261 - 0s - loss: 136.0081 - val_loss: 118.2237\nEpoch 100/1000\n261/261 - 0s - loss: 134.2485 - val_loss: 117.2276\nEpoch 101/1000\n261/261 - 0s - loss: 132.6553 - val_loss: 114.9724\nEpoch 102/1000\n261/261 - 0s - loss: 130.9867 - val_loss: 113.3426\nEpoch 103/1000\n261/261 - 0s - loss: 129.7633 - val_loss: 112.5253\nEpoch 104/1000\n261/261 - 0s - loss: 127.4988 - val_loss: 109.9802\nEpoch 105/1000\n261/261 - 0s - loss: 126.5202 - val_loss: 108.6993\nEpoch 106/1000\n261/261 - 0s - loss: 127.0090 - val_loss: 109.9802\nEpoch 107/1000\n261/261 - 0s - loss: 123.9040 - val_loss: 105.5228\nEpoch 108/1000\n261/261 - 0s - loss: 122.4337 - val_loss: 106.0400\nEpoch 109/1000\n261/261 - 0s - loss: 120.6300 - val_loss: 103.0620\nEpoch 110/1000\n261/261 - 0s - loss: 118.5036 - val_loss: 101.1414\nEpoch 111/1000\n261/261 - 0s - loss: 119.0572 - val_loss: 100.2416\nEpoch 112/1000\n261/261 - 0s - loss: 115.5790 - val_loss: 99.5907\nEpoch 113/1000\n261/261 - 0s - loss: 114.3071 - val_loss: 96.6901\nEpoch 114/1000\n261/261 - 0s - loss: 112.3629 - val_loss: 95.6015\nEpoch 115/1000\n261/261 - 0s - loss: 111.1829 - val_loss: 94.8623\nEpoch 116/1000\n261/261 - 0s - loss: 110.1737 - val_loss: 92.5723\nEpoch 117/1000\n261/261 - 0s - loss: 108.3667 - val_loss: 92.2069\nEpoch 118/1000\n261/261 - 0s - loss: 106.8793 - val_loss: 90.0196\nEpoch 119/1000\n261/261 - 0s - loss: 111.7453 - val_loss: 89.3325\nEpoch 120/1000\n261/261 - 0s - loss: 108.2630 - val_loss: 93.4876\nEpoch 121/1000\n261/261 - 0s - loss: 106.3677 - val_loss: 86.3017\nEpoch 122/1000\n261/261 - 0s - loss: 101.7241 - val_loss: 85.6503\nEpoch 123/1000\n261/261 - 0s - loss: 100.5858 - val_loss: 83.9417\nEpoch 124/1000\n261/261 - 0s - loss: 98.9622 - val_loss: 83.3914\nEpoch 125/1000\n261/261 - 0s - loss: 97.9784 - val_loss: 81.5708\nEpoch 126/1000\n261/261 - 0s - loss: 96.6995 - val_loss: 80.4465\nEpoch 127/1000\n261/261 - 0s - loss: 95.5034 - val_loss: 79.5468\nEpoch 128/1000\n261/261 - 0s - loss: 93.9933 - val_loss: 78.7416\nEpoch 129/1000\n261/261 - 0s - loss: 93.2547 - val_loss: 77.2559\nEpoch 130/1000\n261/261 - 0s - loss: 92.0739 - val_loss: 76.4692\nEpoch 131/1000\n261/261 - 0s - loss: 91.3897 - val_loss: 75.0902\nEpoch 132/1000\n261/261 - 0s - loss: 89.5802 - val_loss: 74.2796\nEpoch 133/1000\n261/261 - 0s - loss: 89.2358 - val_loss: 73.7019\nEpoch 134/1000\n261/261 - 0s - loss: 89.2894 - val_loss: 71.7912\nEpoch 135/1000\n261/261 - 0s - loss: 86.9927 - val_loss: 70.9630\nEpoch 136/1000\n261/261 - 0s - loss: 84.9979 - val_loss: 71.5301\nEpoch 137/1000\n261/261 - 0s - loss: 85.4751 - val_loss: 69.3716\nEpoch 138/1000\n261/261 - 0s - loss: 84.5646 - val_loss: 69.2690\nEpoch 139/1000\n261/261 - 0s - loss: 83.6890 - val_loss: 67.7983\nEpoch 140/1000\n261/261 - 0s - loss: 80.8676 - val_loss: 66.0073\nEpoch 141/1000\n261/261 - 0s - loss: 79.7220 - val_loss: 65.3198\nEpoch 142/1000\n261/261 - 0s - loss: 79.1109 - val_loss: 65.2558\nEpoch 143/1000\n261/261 - 0s - loss: 78.7909 - val_loss: 63.5800\nEpoch 144/1000\n261/261 - 0s - loss: 77.2276 - val_loss: 62.5765\nEpoch 145/1000\n261/261 - 0s - loss: 75.8473 - val_loss: 61.7780\nEpoch 146/1000\n261/261 - 0s - loss: 74.8493 - val_loss: 60.8583\nEpoch 147/1000\n261/261 - 0s - loss: 74.0530 - val_loss: 59.8856\nEpoch 148/1000\n261/261 - 0s - loss: 73.0771 - val_loss: 59.4027\nEpoch 149/1000\n261/261 - 0s - loss: 72.2401 - val_loss: 58.3119\nEpoch 150/1000\n261/261 - 0s - loss: 72.1309 - val_loss: 57.5037\nEpoch 151/1000\n261/261 - 0s - loss: 70.7773 - val_loss: 57.7769\nEpoch 152/1000\n261/261 - 0s - loss: 70.5883 - val_loss: 56.1087\nEpoch 153/1000\n261/261 - 0s - loss: 68.6020 - val_loss: 55.3935\nEpoch 154/1000\n261/261 - 0s - loss: 68.0137 - val_loss: 55.3946\nEpoch 155/1000\n261/261 - 0s - loss: 68.3630 - val_loss: 54.5067\nEpoch 156/1000\n261/261 - 0s - loss: 68.1104 - val_loss: 53.5928\nEpoch 157/1000\n261/261 - 0s - loss: 66.8734 - val_loss: 53.7972\nEpoch 158/1000\n261/261 - 0s - loss: 64.6184 - val_loss: 51.8031\nEpoch 159/1000\n261/261 - 0s - loss: 64.5744 - val_loss: 51.4003\nEpoch 160/1000\n261/261 - 0s - loss: 63.6910 - val_loss: 50.6856\nEpoch 161/1000\n261/261 - 0s - loss: 64.0145 - val_loss: 49.9536\nEpoch 162/1000\n261/261 - 0s - loss: 61.8386 - val_loss: 49.8901\nEpoch 163/1000\n261/261 - 0s - loss: 61.9306 - val_loss: 49.2521\nEpoch 164/1000\n261/261 - 0s - loss: 60.7556 - val_loss: 47.9911\nEpoch 165/1000\n261/261 - 0s - loss: 60.2802 - val_loss: 47.2594\nEpoch 166/1000\n261/261 - 0s - loss: 59.2542 - val_loss: 46.9898\nEpoch 167/1000\n261/261 - 0s - loss: 58.3004 - val_loss: 46.5502\nEpoch 168/1000\n261/261 - 0s - loss: 57.8545 - val_loss: 45.7245\nEpoch 169/1000\n261/261 - 0s - loss: 56.9617 - val_loss: 45.0827\nEpoch 170/1000\n261/261 - 0s - loss: 56.9749 - val_loss: 45.1476\nEpoch 171/1000\n261/261 - 0s - loss: 55.8050 - val_loss: 44.0151\nEpoch 172/1000\n261/261 - 0s - loss: 56.0478 - val_loss: 43.5957\nEpoch 173/1000\n261/261 - 0s - loss: 55.2461 - val_loss: 43.9503\nEpoch 174/1000\n261/261 - 0s - loss: 54.0493 - val_loss: 42.5281\nEpoch 175/1000\n261/261 - 0s - loss: 54.2585 - val_loss: 42.0300\nEpoch 176/1000\n261/261 - 0s - loss: 52.9849 - val_loss: 42.1091\nEpoch 177/1000\n261/261 - 0s - loss: 52.6699 - val_loss: 41.1280\nEpoch 178/1000\n261/261 - 0s - loss: 52.5766 - val_loss: 40.6279\nEpoch 179/1000\n261/261 - 0s - loss: 51.2797 - val_loss: 41.5560\nEpoch 180/1000\n261/261 - 0s - loss: 51.3167 - val_loss: 39.8998\nEpoch 181/1000\n261/261 - 0s - loss: 50.6548 - val_loss: 40.3602\nEpoch 182/1000\n261/261 - 0s - loss: 49.9360 - val_loss: 38.9575\nEpoch 183/1000\n261/261 - 0s - loss: 49.3195 - val_loss: 38.5161\nEpoch 184/1000\n261/261 - 0s - loss: 48.8159 - val_loss: 38.2727\nEpoch 185/1000\n261/261 - 0s - loss: 48.5230 - val_loss: 38.3134\nEpoch 186/1000\n261/261 - 0s - loss: 48.1472 - val_loss: 37.5338\nEpoch 187/1000\n261/261 - 0s - loss: 49.0451 - val_loss: 37.0337\nEpoch 188/1000\n261/261 - 0s - loss: 46.9509 - val_loss: 37.4614\nEpoch 189/1000\n261/261 - 0s - loss: 46.8951 - val_loss: 36.4360\nEpoch 190/1000\n261/261 - 0s - loss: 46.1027 - val_loss: 36.2615\nEpoch 191/1000\n261/261 - 0s - loss: 45.6384 - val_loss: 35.6610\nEpoch 192/1000\n261/261 - 0s - loss: 46.9916 - val_loss: 35.5148\nEpoch 193/1000\n261/261 - 0s - loss: 49.5148 - val_loss: 37.5214\nEpoch 194/1000\n261/261 - 0s - loss: 46.2516 - val_loss: 35.9050\nEpoch 195/1000\n261/261 - 0s - loss: 45.1961 - val_loss: 35.2473\nEpoch 196/1000\n261/261 - 0s - loss: 43.8845 - val_loss: 34.1322\nEpoch 197/1000\n261/261 - 0s - loss: 43.4610 - val_loss: 33.6880\nEpoch 198/1000\n261/261 - 0s - loss: 42.6286 - val_loss: 33.7127\nEpoch 199/1000\n261/261 - 0s - loss: 42.4154 - val_loss: 33.2152\nEpoch 200/1000\n261/261 - 0s - loss: 42.0020 - val_loss: 32.9451\nEpoch 201/1000\n261/261 - 0s - loss: 41.6191 - val_loss: 32.5093\nEpoch 202/1000\n261/261 - 0s - loss: 43.7235 - val_loss: 32.3695\nEpoch 203/1000\n261/261 - 0s - loss: 43.0863 - val_loss: 34.2041\nEpoch 204/1000\n261/261 - 0s - loss: 41.0544 - val_loss: 32.0973\nEpoch 205/1000\n261/261 - 0s - loss: 40.7787 - val_loss: 32.5461\nEpoch 206/1000\n261/261 - 0s - loss: 41.6360 - val_loss: 31.2820\nEpoch 207/1000\n261/261 - 0s - loss: 40.7417 - val_loss: 32.2974\nEpoch 208/1000\n261/261 - 0s - loss: 39.9822 - val_loss: 30.7600\nEpoch 209/1000\n261/261 - 0s - loss: 39.3857 - val_loss: 32.5769\nEpoch 210/1000\n261/261 - 0s - loss: 39.1410 - val_loss: 30.4246\nEpoch 211/1000\n261/261 - 0s - loss: 38.7447 - val_loss: 30.0492\nEpoch 212/1000\n261/261 - 0s - loss: 37.9753 - val_loss: 29.8627\nEpoch 213/1000\n261/261 - 0s - loss: 38.3355 - val_loss: 29.6306\nEpoch 214/1000\n261/261 - 0s - loss: 37.3530 - val_loss: 29.6433\nEpoch 215/1000\n261/261 - 0s - loss: 37.1885 - val_loss: 29.3205\nEpoch 216/1000\n261/261 - 0s - loss: 36.7803 - val_loss: 29.0165\nEpoch 217/1000\n261/261 - 0s - loss: 37.1867 - val_loss: 28.8259\nEpoch 218/1000\n261/261 - 0s - loss: 36.1244 - val_loss: 29.7593\nEpoch 219/1000\n261/261 - 0s - loss: 37.7266 - val_loss: 28.7380\nEpoch 220/1000\n261/261 - 0s - loss: 35.7875 - val_loss: 28.4919\nEpoch 221/1000\n261/261 - 0s - loss: 35.6227 - val_loss: 28.1351\nEpoch 222/1000\n261/261 - 0s - loss: 35.3527 - val_loss: 27.9021\nEpoch 223/1000\n261/261 - 0s - loss: 34.9739 - val_loss: 27.9576\nEpoch 224/1000\n261/261 - 0s - loss: 34.7204 - val_loss: 27.6015\nEpoch 225/1000\n261/261 - 0s - loss: 34.7849 - val_loss: 27.3595\nEpoch 226/1000\n261/261 - 0s - loss: 34.6823 - val_loss: 27.3375\nEpoch 227/1000\n261/261 - 0s - loss: 34.6561 - val_loss: 27.0810\nEpoch 228/1000\n261/261 - 0s - loss: 34.7526 - val_loss: 26.9909\nEpoch 229/1000\n261/261 - 0s - loss: 33.2568 - val_loss: 28.1786\nEpoch 230/1000\n261/261 - 0s - loss: 33.8126 - val_loss: 26.6188\nEpoch 231/1000\n261/261 - 0s - loss: 33.7432 - val_loss: 26.5590\nEpoch 232/1000\n261/261 - 0s - loss: 32.8556 - val_loss: 26.4733\nEpoch 233/1000\n261/261 - 0s - loss: 32.6516 - val_loss: 26.1711\nEpoch 234/1000\n261/261 - 0s - loss: 32.9949 - val_loss: 25.9887\nEpoch 235/1000\n261/261 - 0s - loss: 33.1170 - val_loss: 26.3875\nEpoch 236/1000\n261/261 - 0s - loss: 32.5374 - val_loss: 25.7300\nEpoch 237/1000\n261/261 - 0s - loss: 32.6500 - val_loss: 25.6121\nEpoch 238/1000\n261/261 - 0s - loss: 32.4430 - val_loss: 25.6679\nEpoch 239/1000\n261/261 - 0s - loss: 32.0512 - val_loss: 25.5607\nEpoch 240/1000\n261/261 - 0s - loss: 31.8485 - val_loss: 25.3327\nEpoch 241/1000\n261/261 - 0s - loss: 31.3820 - val_loss: 25.6274\nEpoch 242/1000\n261/261 - 0s - loss: 32.3983 - val_loss: 24.9405\nEpoch 243/1000\n261/261 - 0s - loss: 30.7282 - val_loss: 24.9071\nEpoch 244/1000\n261/261 - 0s - loss: 30.4659 - val_loss: 24.7043\nEpoch 245/1000\n261/261 - 0s - loss: 30.7127 - val_loss: 24.6449\nEpoch 246/1000\n261/261 - 0s - loss: 29.9609 - val_loss: 24.4984\nEpoch 247/1000\n261/261 - 0s - loss: 30.2372 - val_loss: 24.3524\nEpoch 248/1000\n261/261 - 0s - loss: 30.2689 - val_loss: 24.3986\nEpoch 249/1000\n261/261 - 0s - loss: 30.7721 - val_loss: 24.2271\nEpoch 250/1000\n261/261 - 0s - loss: 30.6043 - val_loss: 24.0360\nEpoch 251/1000\n261/261 - 0s - loss: 30.3024 - val_loss: 24.0987\nEpoch 252/1000\n261/261 - 0s - loss: 28.9162 - val_loss: 23.7909\nEpoch 253/1000\n261/261 - 0s - loss: 29.2801 - val_loss: 23.8153\nEpoch 254/1000\n261/261 - 0s - loss: 29.3222 - val_loss: 23.5515\nEpoch 255/1000\n261/261 - 0s - loss: 28.5132 - val_loss: 23.8399\nEpoch 256/1000\n261/261 - 0s - loss: 28.9835 - val_loss: 23.3674\nEpoch 257/1000\n261/261 - 0s - loss: 28.2271 - val_loss: 23.4548\nEpoch 258/1000\n261/261 - 0s - loss: 27.8565 - val_loss: 23.1535\nEpoch 259/1000\n261/261 - 0s - loss: 27.8770 - val_loss: 23.1761\nEpoch 260/1000\n261/261 - 0s - loss: 27.5445 - val_loss: 22.9507\nEpoch 261/1000\n261/261 - 0s - loss: 27.6223 - val_loss: 22.8882\nEpoch 262/1000\n261/261 - 0s - loss: 27.3854 - val_loss: 22.9048\nEpoch 263/1000\n261/261 - 0s - loss: 27.3946 - val_loss: 22.6476\nEpoch 264/1000\n261/261 - 0s - loss: 27.0089 - val_loss: 22.5546\nEpoch 265/1000\n261/261 - 0s - loss: 26.9027 - val_loss: 22.4856\nEpoch 266/1000\n261/261 - 0s - loss: 26.7630 - val_loss: 22.4675\nEpoch 267/1000\n261/261 - 0s - loss: 27.0150 - val_loss: 22.3077\nEpoch 268/1000\n261/261 - 0s - loss: 26.3339 - val_loss: 22.1958\nEpoch 269/1000\n261/261 - 0s - loss: 26.5861 - val_loss: 22.3650\nEpoch 270/1000\n261/261 - 0s - loss: 26.3245 - val_loss: 22.0337\nEpoch 271/1000\n261/261 - 0s - loss: 26.0610 - val_loss: 21.9219\nEpoch 272/1000\n261/261 - 0s - loss: 25.9908 - val_loss: 22.0404\nEpoch 273/1000\n261/261 - 0s - loss: 25.7291 - val_loss: 22.0628\nEpoch 274/1000\n261/261 - 0s - loss: 28.5037 - val_loss: 22.0770\nEpoch 275/1000\n261/261 - 0s - loss: 26.8031 - val_loss: 21.6196\nEpoch 276/1000\n261/261 - 0s - loss: 26.1467 - val_loss: 21.5624\nEpoch 277/1000\n261/261 - 0s - loss: 25.3375 - val_loss: 22.5604\nEpoch 278/1000\n261/261 - 0s - loss: 25.9187 - val_loss: 21.3272\nEpoch 279/1000\n261/261 - 0s - loss: 25.1155 - val_loss: 21.4415\nEpoch 280/1000\n261/261 - 0s - loss: 24.9094 - val_loss: 21.2730\nEpoch 281/1000\n261/261 - 0s - loss: 24.6625 - val_loss: 21.0808\nEpoch 282/1000\n261/261 - 0s - loss: 24.9405 - val_loss: 21.0107\nEpoch 283/1000\n261/261 - 0s - loss: 24.4015 - val_loss: 21.2510\nEpoch 284/1000\n261/261 - 0s - loss: 24.8920 - val_loss: 20.8377\nEpoch 285/1000\n261/261 - 0s - loss: 24.1691 - val_loss: 20.7580\nEpoch 286/1000\n261/261 - 0s - loss: 24.4140 - val_loss: 20.6990\nEpoch 287/1000\n261/261 - 0s - loss: 24.0014 - val_loss: 20.6096\nEpoch 288/1000\n261/261 - 0s - loss: 23.7787 - val_loss: 20.5134\nEpoch 289/1000\n261/261 - 0s - loss: 23.7206 - val_loss: 20.6378\nEpoch 290/1000\n261/261 - 0s - loss: 24.6226 - val_loss: 20.3711\nEpoch 291/1000\n261/261 - 0s - loss: 23.3452 - val_loss: 20.4992\nEpoch 292/1000\n261/261 - 0s - loss: 23.6446 - val_loss: 20.4952\nEpoch 293/1000\n261/261 - 0s - loss: 24.2056 - val_loss: 20.6786\nEpoch 294/1000\n261/261 - 0s - loss: 25.1895 - val_loss: 20.4868\nEpoch 295/1000\nRestoring model weights from the end of the best epoch.\n261/261 - 0s - loss: 23.6036 - val_loss: 20.3795\nEpoch 00295: early stopping\n\n\n\nimport numpy as np\n\n# Measure RMSE error.  RMSE is common for regression.\nscore = np.sqrt(metrics.mean_squared_error(pred,y_test))\nprint(\"Final score (RMSE): {}\".format(score))\n\nFinal score (RMSE): 4.5134384517538795\n\n\n\nimport pandas as pd\n\n# Generate Kaggle submit file\n\n# Encode feature vector\ndf_test = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/datasets/\"+\\\n    \"kaggle_auto_test.csv\", na_values=['NA','?'])\n\n# Convert to numpy - regression\nids = df_test['id']\ndf_test.drop('id', axis=1, inplace=True)\n\n# Handle missing value\ndf_test['horsepower'] = df_test['horsepower'].\\\n    fillna(df['horsepower'].median())\n\nx = df_test[['cylinders', 'displacement', 'horsepower', 'weight',\n       'acceleration', 'year', 'origin']].values\n\n\n\n\n# Generate predictions\npred = model.predict(x)\n#pred\n\n# Create submission data set\n\ndf_submit = pd.DataFrame(pred)\ndf_submit.insert(0,'id',ids)\ndf_submit.columns = ['id','mpg']\n\n# Write submit file locally\ndf_submit.to_csv(\"auto_submit.csv\", index=False) \n\nprint(df_submit)\n\n     id        mpg\n0   350  29.112602\n1   351  27.803200\n2   352  27.981804\n3   353  30.487831\n4   354  27.227440\n5   355  26.438324\n6   356  27.886986\n7   357  29.103935\n8   358  26.447609\n9   359  30.027260\n10  360  30.312553\n11  361  30.712151\n12  362  23.952263\n13  363  24.858467\n14  364  23.459129\n15  365  22.638985\n16  366  26.032127\n17  367  26.197884\n18  368  28.448906\n19  369  28.138954\n20  370  27.352821\n21  371  27.313377\n22  372  26.464119\n23  373  26.689583\n24  374  26.546562\n25  375  27.829781\n26  376  27.466354\n27  377  30.343369\n28  378  29.985909\n29  379  27.807251\n30  380  28.450882\n31  381  26.574844\n32  382  28.199501\n33  383  29.615051\n34  384  29.048317\n35  385  29.320534\n36  386  29.582710\n37  387  24.533165\n38  388  24.426888\n39  389  24.658607\n40  390  21.805504\n41  391  26.026482\n42  392  24.947670\n43  393  26.902489\n44  394  26.575218\n45  395  33.546684\n46  396  24.233910\n47  397  28.609993\n48  398  28.913261"
  },
  {
    "objectID": "posts/2020-10-10-Dask_Fiscal-db-to-dask-dataframe.html",
    "href": "posts/2020-10-10-Dask_Fiscal-db-to-dask-dataframe.html",
    "title": "Working with dask data frames. Reading Fiscal Data from a sqlite db to a dask dataframe. Computing, visualizing and groupby with dask dataframes. Using dask.distributed locally.",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\n\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal_data.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nengine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n[(1996, 'East China', 'Anhui', '2093.3', 50661, 631930, 147002)]\n\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_data\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.3\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nEast China\nZhejiang\n11648.7\n668128\n3162299\n656175.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 7 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province     gdp    fdi     it specific\nnpartitions=5                                                       \n0              int64  object   object  object  int64  int64  float64\n72               ...     ...      ...     ...    ...    ...      ...\n...              ...     ...      ...     ...    ...    ...      ...\n288              ...     ...      ...     ...    ...    ...      ...\n359              ...     ...      ...     ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.npartitions\n\n5\n\n\n\nddf.npartitions\n\n5\n\n\n\nlen(ddf)\n\n360\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/12451/1\nDashboard: http://localhost:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nddf.describe().compute()\n\n\n\n\n\n\n\n\nyear\nfdi\nit\nspecific\n\n\n\n\ncount\n360.000000\n3.600000e+02\n3.600000e+02\n3.560000e+02\n\n\nmean\n2001.500000\n1.961394e+05\n2.165819e+06\n5.834707e+05\n\n\nstd\n3.456857\n3.030440e+05\n1.769294e+06\n6.540553e+05\n\n\nmin\n1996.000000\n2.000000e+00\n1.478970e+05\n8.964000e+03\n\n\n25%\n1998.750000\n3.309900e+04\n1.077466e+06\n2.237530e+05\n\n\n50%\n2001.500000\n1.411025e+05\n2.020634e+06\n4.243700e+05\n\n\n75%\n2004.250000\n4.065125e+05\n3.375492e+06\n1.011846e+06\n\n\nmax\n2007.000000\n1.743140e+06\n1.053331e+07\n3.937966e+06\n\n\n\n\n\n\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.3\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n\n\n\n\n\n\ngroupby_yr = ddf.groupby('year').count()\n\n\ngroupby_yr.compute()\n\n\n\n\n\n\n\n\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\nyear\n\n\n\n\n\n\n\n\n\n\n1996\n30\n30\n30\n30\n30\n29\n\n\n1997\n30\n30\n30\n30\n30\n28\n\n\n1998\n30\n30\n30\n30\n30\n30\n\n\n1999\n30\n30\n30\n30\n30\n30\n\n\n2000\n30\n30\n30\n30\n30\n29\n\n\n2001\n30\n30\n30\n30\n30\n30\n\n\n2002\n30\n30\n30\n30\n30\n30\n\n\n2003\n30\n30\n30\n30\n30\n30\n\n\n2004\n30\n30\n30\n30\n30\n30\n\n\n2005\n30\n30\n30\n30\n30\n30\n\n\n2006\n30\n30\n30\n30\n30\n30\n\n\n2007\n30\n30\n30\n30\n30\n30\n\n\n\n\n\n\n\n\ngroup_region = ddf.groupby('region')['gdp'].sum()\n\n\ngroup_region.compute()\n\nregion\nEast China             2093.32347.322542.962712.342902.093246.713519....\nNorth China            1789.22077.092377.182678.823161.663707.964315....\nNorthwest China        722.52793.57887.67956.321052.881125.371232.031...\nSouth Central China    6834.977774.538530.889250.6810741.2512039.2513...\nSouthwest China        1315.121509.751602.381663.21791.01976.862232.8...\nNortheast China        2370.52667.52774.42866.33151.43390.13637.24057...\nName: gdp, dtype: object\n\n\n\nddf.nlargest(5, 'fdi').compute()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n179\n2007\nEast China\nJiangsu\n21742.05\n1743140\n3557071\n1188989.0\n\n\n71\n2007\nSouth Central China\nGuangdong\n31777.01\n1712603\n4947824\n859482.0\n\n\n70\n2006\nSouth Central China\nGuangdong\n26587.76\n1451065\n4559252\n1897575.0\n\n\n178\n2006\nEast China\nJiangsu\n18598.69\n1318339\n2926542\n1388043.0\n\n\n69\n2005\nSouth Central China\nGuangdong\n22557.37\n1236400\n4327217\n1491588.0\n\n\n\n\n\n\n\n\nddf.sum().visualize()\n\n\n\n\n\nddf.sum().visualize(rankdir=\"LR\")\n\n\n\n\n\n(ddf).visualize(rankdir=\"LR\")\n\n\n\n\n\nddf.visualize(rankdir=\"LR\")\n\n\n\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-12-11_Xgboost_Health_data.html",
    "href": "posts/2020-12-11_Xgboost_Health_data.html",
    "title": "Xgboost for Health Data (Pipeline Step 3)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sqlalchemy as db\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n30669\nMale\n3.0\n0\n0\nNo\nchildren\nRural\n95.12\n18.0\nNaN\n0\n\n\n1\n30468\nMale\n58.0\n1\n0\nYes\nPrivate\nUrban\n87.96\n39.2\nnever smoked\n0\n\n\n2\n16523\nFemale\n8.0\n0\n0\nNo\nPrivate\nUrban\n110.89\n17.6\nNaN\n0\n\n\n3\n56543\nFemale\n70.0\n0\n0\nYes\nPrivate\nRural\n69.04\n35.9\nformerly smoked\n0\n\n\n4\n46136\nMale\n14.0\n0\n0\nNo\nNever_worked\nRural\n161.28\n19.1\nNaN\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n43395\n56196\nFemale\n10.0\n0\n0\nNo\nchildren\nUrban\n58.64\n20.4\nnever smoked\n0\n\n\n43396\n5450\nFemale\n56.0\n0\n0\nYes\nGovt_job\nUrban\n213.61\n55.4\nformerly smoked\n0\n\n\n43397\n28375\nFemale\n82.0\n1\n0\nYes\nPrivate\nUrban\n91.94\n28.9\nformerly smoked\n0\n\n\n43398\n27973\nMale\n40.0\n0\n0\nYes\nPrivate\nUrban\n99.16\n33.2\nnever smoked\n0\n\n\n43399\n36271\nFemale\n82.0\n0\n0\nYes\nPrivate\nUrban\n79.48\n20.6\nnever smoked\n0\n\n\n\n\n43400 rows × 12 columns\n\n\n\n\ndf = df.drop(columns = ['id'])\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\n\ntarget = df.stroke.values\nfeatures = df.drop(columns =[\"stroke\"])\n\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=0)\n\n\ndf.dtypes\n\ngender                object\nage                  float64\nhypertension           int64\nheart_disease          int64\never_married          object\nwork_type             object\nResidence_type        object\navg_glucose_level    float64\nbmi                  float64\nsmoking_status        object\nstroke                 int64\ndtype: object\n\n\n\n# Label Encoding\nfor f in df.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   \n\n\nfor f in df.columns:\n    print(f)\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n\n\ndf\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n1\n3.0\n0\n0\n0\n4\n0\n95.12\n18.0\n1\n0\n\n\n1\n1\n58.0\n1\n0\n1\n2\n1\n87.96\n39.2\n2\n0\n\n\n2\n0\n8.0\n0\n0\n0\n2\n1\n110.89\n17.6\n1\n0\n\n\n3\n0\n70.0\n0\n0\n1\n2\n0\n69.04\n35.9\n0\n0\n\n\n4\n1\n14.0\n0\n0\n0\n1\n0\n161.28\n19.1\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n43395\n0\n10.0\n0\n0\n0\n4\n1\n58.64\n20.4\n2\n0\n\n\n43396\n0\n56.0\n0\n0\n1\n0\n1\n213.61\n55.4\n0\n0\n\n\n43397\n0\n82.0\n1\n0\n1\n2\n1\n91.94\n28.9\n0\n0\n\n\n43398\n1\n40.0\n0\n0\n1\n2\n1\n99.16\n33.2\n2\n0\n\n\n43399\n0\n82.0\n0\n0\n1\n2\n1\n79.48\n20.6\n2\n0\n\n\n\n\n43400 rows × 11 columns\n\n\n\n\n# Label Encoding\nfor f in X_train.columns:\n    if X_train[f].dtype=='object' or X_test[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(X_train[f].values) + list(X_test[f].values))\n        X_train[f] = lbl.transform(list(X_train[f].values))\n        X_test[f] = lbl.transform(list(X_test[f].values))   \n\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate\n\n\nimport xgboost as xgb\n\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    random_state=2021,\n    tree_method='auto'\n#    tree_method='hist'\n#    tree_method='gpu_hist'\n)\n\n\n## K Fold Cross Validation (5 Folds)\n\n\nkfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n\n\n# Define Parameter grid for hyperparameter search process\n\n\nparam_grid = { \n    'colsample_bytree':[.75,1],\n    'learning_rate':[0.01,0.05,0.1,0.3,0.5],\n    'max_depth':[1,2,3,5],\n    'subsample':[.75,1],\n    'n_estimators': list(range(50, 400, 50))\n}\n\n\n## Run the GridSearch,optimizing on ROC\n\n\ngrid_search = GridSearchCV(estimator=clf, scoring='roc_auc', param_grid=param_grid, n_jobs=-1, cv=kfold)\n\n\n%%time\ngrid_result = grid_search.fit(X_train, y_train)\n\n/home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\n\n[09:05:39] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nCPU times: user 27 s, sys: 996 ms, total: 28 s\nWall time: 20min 15s\n\n\n\nprint(f'Best: {grid_result.best_score_} using {grid_result.best_params_}','\\n')\n\nBest: 0.8617766137590029 using {'colsample_bytree': 0.75, 'learning_rate': 0.01, 'max_depth': 5, 'n_estimators': 300, 'subsample': 0.75} \n\n\n\n\n#Set our final hyperparameters to the tuned values\nxgbcl = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n         gamma=0.0, max_delta_step=0.0, min_child_weight=1.0,\n         missing=None, n_jobs=-1, objective='binary:logistic', random_state=42, reg_alpha=0.0,\n         reg_lambda=1.0, scale_pos_weight=1.0, tree_method='auto',\n         colsample_bytree = grid_result.best_params_['colsample_bytree'], \n         learning_rate = grid_result.best_params_['learning_rate'], \n         max_depth = grid_result.best_params_['max_depth'], \n         subsample = grid_result.best_params_['subsample'], \n         n_estimators = grid_result.best_params_['n_estimators'])\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#refit the model on k-folds to get stable avg error metrics\nscores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, \n                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n\nprint('Training 5-fold Cross Validation Results:\\n')\nprint('AUC: ', scores['test_roc_auc'].mean())\nprint('Accuracy: ', scores['test_accuracy'].mean())\nprint('Precision: ', scores['test_precision'].mean())\nprint('Recall: ', scores['test_recall'].mean())\nprint('F1: ', scores['test_f1'].mean(), '\\n')\n\nTraining 5-fold Cross Validation Results:\n\nAUC:  0.8632093427558646\nAccuracy:  0.981278801843318\nPrecision:  0.0\nRecall:  0.0\nF1:  0.0 \n\n\n\n\nimport sklearn.metrics as metrics\n\n\n#Fit the final model\nxgbcl.fit(X_train, y_train)\n\n#Generate predictions against our training and test data\npred_train = xgbcl.predict(X_train)\nproba_train = xgbcl.predict_proba(X_train)\npred_test = xgbcl.predict(X_test)\nproba_test = xgbcl.predict_proba(X_test)\n\n# Print model report\nprint(\"Classification report (Test): \\n\")\nprint(metrics.classification_report(y_test, pred_test))\nprint(\"Confusion matrix (Test): \\n\")\nprint(metrics.confusion_matrix(y_test, pred_test)/len(y_test))\n\nprint ('\\nTrain Accuracy:', metrics.accuracy_score(y_train, pred_train))\nprint ('Test Accuracy:', metrics.accuracy_score(y_test, pred_test))\n\nprint ('\\nTrain AUC:', metrics.roc_auc_score(y_train, proba_train[:,1]))\nprint ('Test AUC:', metrics.roc_auc_score(y_test, proba_test[:,1]))\n\n# calculate the fpr and tpr for all thresholds of the classification\ntrain_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1])\ntest_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1])\n\ntrain_roc_auc = metrics.auc(train_fpr, train_tpr)\ntest_roc_auc = metrics.auc(test_fpr, test_tpr)\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=[7,5])\nplt.title('Receiver Operating Characteristic')\nplt.plot(train_fpr, train_tpr, 'b', label = 'Train AUC = %0.2f' % train_roc_auc)\nplt.plot(test_fpr, test_tpr, 'g', label = 'Test AUC = %0.2f' % test_roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# plot feature importance\nxgb.plot_importance(xgbcl, importance_type='gain');\n\n[20:30:35] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nClassification report (Test): \n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      8547\n           1       0.00      0.00      0.00       133\n\n    accuracy                           0.98      8680\n   macro avg       0.49      0.50      0.50      8680\nweighted avg       0.97      0.98      0.98      8680\n\nConfusion matrix (Test): \n\n[[0.98467742 0.        ]\n [0.01532258 0.        ]]\n\nTrain Accuracy: 0.981278801843318\nTest Accuracy: 0.9846774193548387\n\nTrain AUC: 0.8907315481700571\nTest AUC: 0.8661175578468812\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n\n\n\n#take a random row of data\nX_rand = features.sample(1, random_state = 5)\ndisplay(df.iloc[X_rand.index])\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n33658\n0\n60.0\n0\n0\n1\n2\n0\n108.13\n28.6\n2\n0\n\n\n\n\n\n\n\n\nX = features\n\n\n#Set our final hyperparameters to the tuned values\nxgbcl = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n         gamma=0.0, max_delta_step=0.0, min_child_weight=1.0,\n         missing=None, n_jobs=-1, objective='binary:logistic', random_state=42, reg_alpha=0.0,\n         reg_lambda=1.0, scale_pos_weight=1.0, tree_method='auto',\n         colsample_bytree = grid_result.best_params_['colsample_bytree'], \n         learning_rate = grid_result.best_params_['learning_rate'], \n         max_depth = grid_result.best_params_['max_depth'], \n         subsample = grid_result.best_params_['subsample'], \n         n_estimators = grid_result.best_params_['n_estimators'])\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#refit the model on k-folds to get stable avg error metrics\nscores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, \n                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n\nprint('Training 5-fold Cross Validation Results:\\n')\nprint('AUC: ', scores['test_roc_auc'].mean())\nprint('Accuracy: ', scores['test_accuracy'].mean())\nprint('Precision: ', scores['test_precision'].mean())\nprint('Recall: ', scores['test_recall'].mean())\nprint('F1: ', scores['test_f1'].mean(), '\\n')\n\nNameError: name 'grid_result' is not defined\n\n\n\n#Generate predictions against our training and test data\npred_train = clf.predict(X_train)\nproba_train = clf.predict_proba(X_train)\npred_test = clf.predict(X_test)\nproba_test = clf.predict_proba(X_test)\n\n\nimport sklearn.metrics as metrics\n# Print model report\nprint(\"Classification report (Test): \\n\")\nprint(metrics.classification_report(y_test, pred_test))\nprint(\"Confusion matrix (Test): \\n\")\nprint(metrics.confusion_matrix(y_test, pred_test)/len(y_test))\n\nprint ('\\nTrain Accuracy:', metrics.accuracy_score(y_train, pred_train))\nprint ('Test Accuracy:', metrics.accuracy_score(y_test, pred_test))\n\nprint ('\\nTrain AUC:', metrics.roc_auc_score(y_train, proba_train[:,1]))\nprint ('Test AUC:', metrics.roc_auc_score(y_test, proba_test[:,1]))\n\nClassification report (Test): \n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      8547\n           1       0.20      0.02      0.03       133\n\n    accuracy                           0.98      8680\n   macro avg       0.59      0.51      0.51      8680\nweighted avg       0.97      0.98      0.98      8680\n\nConfusion matrix (Test): \n\n[[9.83755760e-01 9.21658986e-04]\n [1.50921659e-02 2.30414747e-04]]\n\nTrain Accuracy: 0.9959389400921659\nTest Accuracy: 0.9839861751152074\n\nTrain AUC: 0.9999601273396401\nTest AUC: 0.835284948066903\n\n\n\n# calculate the fpr and tpr for all thresholds of the classification\ntrain_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1])\ntest_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1])\n\ntrain_roc_auc = metrics.auc(train_fpr, train_tpr)\ntest_roc_auc = metrics.auc(test_fpr, test_tpr)\n\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=[7,5])\nplt.title('Receiver Operating Characteristic')\nplt.plot(train_fpr, train_tpr, 'b', label = 'Train AUC = %0.2f' % train_roc_auc)\nplt.plot(test_fpr, test_tpr, 'g', label = 'Test AUC = %0.2f' % test_roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# plot feature importance\nxgb.plot_importance(clf, importance_type='gain');\n\n\n\n\n\n\n\n\n#take a random row of data\nX_rand = features.sample(1, random_state = 5)\ndisplay(df.iloc[X_rand.index])\n\n\n\n\n\n\n\n\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n33658\nFemale\n60.0\n0\n0\nYes\nPrivate\nRural\n108.13\n28.6\nnever smoked\n0\n\n\n\n\n\n\n\n\nimport shap\n\n\n## kernel shap sends data as numpy array which has no column names, so we fix it\ndef xgb_predict(data_asarray):\n    data_asframe =  pd.DataFrame(data_asarray, columns=feature_names)\n    return estimator.predict(data_asframe)\n\n\n#### Kernel SHAP\nX_summary = shap.kmeans(X_train, 10)\n\n\n#shap_kernel_explainer = shap.KernelExplainer(xgbcl, X_summary)\n\n\ndef xgb_predict(df):\n    data_asframe =  pd.DataFrame(df, columns=feature_names)\n    return estimator.predict(data_asframe)\n\n\n#### Tree SHAP\nshap_tree_explainer = shap.TreeExplainer(xgbcl, feature_perturbation = \"interventional\")\n\n\nshap.initjs()\n## shapely values with kernel SHAP\nshap.force_plot(shap_tree_explainer.expected_value, shap_values_single, X_test.iloc[[5]])\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap.initjs()\n## shapely values with Tree SHAP\nshap_values_single = shap_tree_explainer.shap_values(X_test.iloc[[10]])\nshap.force_plot(shap_tree_explainer.expected_value, shap_values_single, X_test.iloc[[5]])\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap_values = shap_tree_explainer.shap_values(X_train)\nshap.summary_plot(shap_values, features)\n\nAssertionError: Feature and SHAP matrices must have the same number of rows!\n\n\n\n\n\n\n# #Display all features and SHAP values\n# display(pd.DataFrame(data=shap_values, columns=X_train.columns, index=[126]).transpose().sort_values(by=126, ascending=True))\n\n\nX_train.columns\n\nIndex(['gender', 'age', 'hypertension', 'heart_disease', 'ever_married',\n       'work_type', 'Residence_type', 'avg_glucose_level', 'bmi',\n       'smoking_status'],\n      dtype='object')\n\n\n\nshap.dependence_plot('age', shap_values, X_train, interaction_index='bmi')\nshap.dependence_plot('bmi', shap_values, X_train) #when we don't specify an interaction_index, the strongest one is automatically chosen for us\nshap.dependence_plot('heart_disease', shap_values, X_train, interaction_index='age')"
  },
  {
    "objectID": "posts/2020-11-15-Adam_optimizer.html",
    "href": "posts/2020-11-15-Adam_optimizer.html",
    "title": "Adam optimizer",
    "section": "",
    "text": "credit: code from https://github.com/enochkan/building-from-scratch/blob/main/adam-optimizer-from-scratch.ipynb https://arxiv.org/abs/1412.6980\n\nimport numpy as np\n\n\nclass AdamOptim():\n    def __init__(self, eta=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n        self.m_dw, self.v_dw = 0, 0\n        self.m_db, self.v_db = 0, 0\n        self.beta1 = beta1\n        self.beta2 = beta2\n        self.epsilon = epsilon\n        self.eta = eta\n    def update(self, t, w, b, dw, db):\n        ## dw, db are from current minibatch\n        ## momentum beta 1\n        # *** weights *** #\n        self.m_dw = self.beta1*self.m_dw + (1-self.beta1)*dw\n        # *** biases *** #\n        self.m_db = self.beta1*self.m_db + (1-self.beta1)*db\n\n        ## rms beta 2\n        # *** weights *** #\n        self.v_dw = self.beta2*self.v_dw + (1-self.beta2)*(dw**2)\n        # *** biases *** #\n        self.v_db = self.beta2*self.v_db + (1-self.beta2)*(db)\n\n        ## bias correction\n        m_dw_corr = self.m_dw/(1-self.beta1**t)\n        m_db_corr = self.m_db/(1-self.beta1**t)\n        v_dw_corr = self.v_dw/(1-self.beta2**t)\n        v_db_corr = self.v_db/(1-self.beta2**t)\n\n        ## update weights and biases\n        w = w - self.eta*(m_dw_corr/(np.sqrt(v_dw_corr)+self.epsilon))\n        b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))\n        return w, b\n\n\n## define loss functions and gradient descent. We don't really use the loss function here.\ndef loss_function(m):\n    return m**2-2*m+1\n## take derivative\ndef grad_function(m):\n    return 2*m-2\ndef check_convergence(w0, w1):\n    return (w0 == w1)\n## initialize weights and biases, and our optimizer\nw_0 = 0\nb_0 = 0\nadam = AdamOptim()\nt = 1 \nconverged = False\n\nwhile not converged:\n    dw = grad_function(w_0)\n    db = grad_function(b_0)\n    w_0_old = w_0\n    w_0, b_0 = adam.update(t,w=w_0, b=b_0, dw=dw, db=db)\n    if check_convergence(w_0, w_0_old):\n        print('converged after '+str(t)+' iterations')\n        break\n    else:\n        print('iteration '+str(t)+': weight='+str(w_0))\n        t+=1\n\niteration 1: weight=0.009999999950000001\niteration 2: weight=0.01999725400385255\niteration 3: weight=0.029989900621600046\niteration 4: weight=0.039976060276935343\niteration 5: weight=0.049953839711732076\niteration 6: weight=0.05992133621693422\niteration 7: weight=0.06987664190678831\niteration 8: weight=0.07981784795404925\niteration 9: weight=0.08974304875491491\niteration 10: weight=0.0996503459940126\niteration 11: weight=0.10953785258172263\niteration 12: weight=0.11940369643843479\niteration 13: weight=0.12924602410293135\niteration 14: weight=0.13906300414491304\niteration 15: weight=0.14885283036466956\niteration 16: weight=0.15861372476597732\niteration 17: weight=0.1683439402914239\niteration 18: weight=0.17804176331244895\niteration 19: weight=0.1877055158694015\niteration 20: weight=0.19733355765979776\niteration 21: weight=0.2069242877756729\niteration 22: weight=0.21647614619342795\niteration 23: weight=0.22598761502184558\niteration 24: weight=0.23545721951596985\niteration 25: weight=0.24488352886630008\niteration 26: weight=0.25426515677423506\niteration 27: weight=0.26360076182591813\niteration 28: weight=0.2728890476775851\niteration 29: weight=0.2821287630662142\niteration 30: weight=0.2913187016597368\niteration 31: weight=0.3004577017613055\niteration 32: weight=0.30954464588215314\niteration 33: weight=0.3185784601974346\niteration 34: weight=0.32755811389914286\niteration 35: weight=0.3364826184597571\niteration 36: weight=0.3453510268197323\niteration 37: weight=0.3541624325113025\niteration 38: weight=0.36291596873035775\niteration 39: weight=0.3716108073673929\niteration 40: weight=0.38024615800772815\niteration 41: weight=0.3888212669103811\niteration 42: weight=0.3973354159741451\niteration 43: weight=0.405787921698609\niteration 44: weight=0.4141781341470465\niteration 45: weight=0.42250543591732403\niteration 46: weight=0.4307692411262221\niteration 47: weight=0.4389689944118521\niteration 48: weight=0.44710416995817326\niteration 49: weight=0.455174270544982\niteration 50: weight=0.4631788266261575\niteration 51: weight=0.47111739543840325\niteration 52: weight=0.47898956014222716\niteration 53: weight=0.4867949289964492\niteration 54: weight=0.4945331345671171\niteration 55: weight=0.502203832971342\niteration 56: weight=0.5098067031562412\niteration 57: weight=0.5173414462128868\niteration 58: weight=0.5248077847249037\niteration 59: weight=0.5322054621511477\niteration 60: weight=0.5395342422417011\niteration 61: weight=0.5467939084862693\niteration 62: weight=0.5539842635939275\niteration 63: weight=0.5611051290030595\niteration 64: weight=0.568156344420244\niteration 65: weight=0.5751377673867769\niteration 66: weight=0.5820492728714711\niteration 67: weight=0.5888907528883383\niteration 68: weight=0.5956621161377404\niteration 69: weight=0.6023632876695909\niteration 70: weight=0.6089942085671857\niteration 71: weight=0.615554835650261\niteration 72: weight=0.6220451411958915\niteration 73: weight=0.6284651126758695\niteration 74: weight=0.6348147525092407\niteration 75: weight=0.641094077828706\niteration 76: weight=0.6473031202596423\niteration 77: weight=0.653441925710538\niteration 78: weight=0.6595105541736849\niteration 79: weight=0.6655090795350186\niteration 80: weight=0.6714375893920466\niteration 81: weight=0.6772961848788547\niteration 82: weight=0.6830849804972335\niteration 83: weight=0.6888041039530174\niteration 84: weight=0.6944536959967778\niteration 85: weight=0.7000339102680659\niteration 86: weight=0.7055449131424467\niteration 87: weight=0.7109868835806149\niteration 88: weight=0.7163600129789347\niteration 89: weight=0.7216645050207873\niteration 90: weight=0.7269005755281592\niteration 91: weight=0.7320684523129467\niteration 92: weight=0.7371683750274948\niteration 93: weight=0.7422005950139324\niteration 94: weight=0.7471653751519031\niteration 95: weight=0.7520629897043302\niteration 96: weight=0.7568937241608946\niteration 97: weight=0.7616578750789351\niteration 98: weight=0.7663557499215196\niteration 99: weight=0.7709876668924653\niteration 100: weight=0.7755539547681211\niteration 101: weight=0.7800549527257501\niteration 102: weight=0.7844910101683854\niteration 103: weight=0.7888624865460541\niteration 104: weight=0.7931697511732935\niteration 105: weight=0.7974131830429079\niteration 106: weight=0.8015931706359366\niteration 107: weight=0.8057101117278274\niteration 108: weight=0.8097644131908287\niteration 109: weight=0.8137564907926365\niteration 110: weight=0.8176867689913462\niteration 111: weight=0.8215556807267823\niteration 112: weight=0.8253636672082894\niteration 113: weight=0.8291111776990878\niteration 114: weight=0.8327986692973092\niteration 115: weight=0.8364266067138391\niteration 116: weight=0.8399954620471094\niteration 117: weight=0.8435057145549907\niteration 118: weight=0.846957850423946\niteration 119: weight=0.8503523625356184\niteration 120: weight=0.8536897502310298\niteration 121: weight=0.8569705190725782\niteration 122: weight=0.860195180604026\niteration 123: weight=0.8633642521086788\niteration 124: weight=0.8664782563659569\niteration 125: weight=0.8695377214065675\niteration 126: weight=0.8725431802664898\niteration 127: weight=0.8754951707399848\niteration 128: weight=0.8783942351318468\niteration 129: weight=0.8812409200091125\niteration 130: weight=0.8840357759524461\niteration 131: weight=0.8867793573074179\niteration 132: weight=0.8894722219358927\niteration 133: weight=0.8921149309677457\niteration 134: weight=0.8947080485531188\niteration 135: weight=0.8972521416154303\niteration 136: weight=0.8997477796053487\niteration 137: weight=0.9021955342559357\niteration 138: weight=0.9045959793391636\niteration 139: weight=0.9069496904240053\niteration 140: weight=0.9092572446362935\niteration 141: weight=0.9115192204205393\niteration 142: weight=0.9137361973038962\niteration 143: weight=0.9159087556624508\niteration 144: weight=0.918037476490016\niteration 145: weight=0.9201229411695963\niteration 146: weight=0.9221657312476885\niteration 147: weight=0.9241664282115778\niteration 148: weight=0.9261256132697787\niteration 149: weight=0.9280438671357686\niteration 150: weight=0.9299217698151505\niteration 151: weight=0.9317599003963796\niteration 152: weight=0.933558836845177\niteration 153: weight=0.9353191558027519\niteration 154: weight=0.9370414323879417\niteration 155: weight=0.938726240003377\niteration 156: weight=0.9403741501457696\niteration 157: weight=0.9419857322204138\niteration 158: weight=0.9435615533599866\niteration 159: weight=0.9451021782477242\niteration 160: weight=0.946608168945046\niteration 161: weight=0.9480800847236891\niteration 162: weight=0.9495184819024132\niteration 163: weight=0.950923913688324\niteration 164: weight=0.9522969300228624\niteration 165: weight=0.9536380774324956\niteration 166: weight=0.9549478988841421\niteration 167: weight=0.9562269336453583\niteration 168: weight=0.9574757171493036\niteration 169: weight=0.9586947808645\niteration 170: weight=0.9598846521693933\niteration 171: weight=0.9610458542317184\niteration 172: weight=0.9621789058926664\niteration 173: weight=0.9632843215558446\niteration 174: weight=0.9643626110810171\niteration 175: weight=0.9654142796826064\niteration 176: weight=0.9664398278329352\niteration 177: weight=0.9674397511701782\niteration 178: weight=0.9684145404109944\niteration 179: weight=0.9693646812678026\niteration 180: weight=0.9702906543706602\niteration 181: weight=0.9711929351937009\niteration 182: weight=0.9720719939860842\niteration 183: weight=0.9729282957074055\niteration 184: weight=0.9737622999675106\niteration 185: weight=0.9745744609706605\niteration 186: weight=0.9753652274639815\niteration 187: weight=0.9761350426901402\niteration 188: weight=0.9768843443441767\niteration 189: weight=0.9776135645344266\niteration 190: weight=0.9783231297474617\niteration 191: weight=0.9790134608169765\niteration 192: weight=0.9796849728965454\niteration 193: weight=0.9803380754361745\niteration 194: weight=0.9809731721625682\niteration 195: weight=0.9815906610630333\niteration 196: weight=0.9821909343729368\niteration 197: weight=0.9827743785666372\niteration 198: weight=0.9833413743518047\niteration 199: weight=0.9838922966670466\niteration 200: weight=0.984427514682753\niteration 201: weight=0.9849473918050764\niteration 202: weight=0.98545228568296\niteration 203: weight=0.985942548218127\niteration 204: weight=0.9864185255779455\niteration 205: weight=0.9868805582110803\niteration 206: weight=0.9873289808658458\niteration 207: weight=0.9877641226111719\niteration 208: weight=0.9881863068600975\niteration 209: weight=0.988595851395703\niteration 210: weight=0.9889930683993977\niteration 211: weight=0.9893782644814731\niteration 212: weight=0.9897517407138409\niteration 213: weight=0.9901137926648667\niteration 214: weight=0.9904647104362185\niteration 215: weight=0.9908047787016447\niteration 216: weight=0.9911342767475994\niteration 217: weight=0.9914534785156336\niteration 218: weight=0.9917626526464719\niteration 219: weight=0.9920620625256936\niteration 220: weight=0.9923519663309407\niteration 221: weight=0.9926326170805743\niteration 222: weight=0.9929042626837034\niteration 223: weight=0.9931671459915103\niteration 224: weight=0.9934215048497993\niteration 225: weight=0.993667572152694\niteration 226: weight=0.993905575897414\niteration 227: weight=0.9941357392400594\niteration 228: weight=0.9943582805523336\niteration 229: weight=0.9945734134791384\niteration 230: weight=0.9947813469969744\niteration 231: weight=0.9949822854730818\niteration 232: weight=0.9951764287252586\niteration 233: weight=0.995363972082295\niteration 234: weight=0.9955451064449625\niteration 235: weight=0.9957200183474998\niteration 236: weight=0.9958888900195372\niteration 237: weight=0.9960518994484044\niteration 238: weight=0.9962092204417675\niteration 239: weight=0.9963610226905409\niteration 240: weight=0.9965074718320245\niteration 241: weight=0.9966487295132145\niteration 242: weight=0.9967849534542412\niteration 243: weight=0.9969162975118857\niteration 244: weight=0.99704291174313\niteration 245: weight=0.9971649424686971\niteration 246: weight=0.9972825323365392\niteration 247: weight=0.9973958203852304\niteration 248: weight=0.9975049421072285\niteration 249: weight=0.9976100295119631\niteration 250: weight=0.9977112111887169\niteration 251: weight=0.997808612369263\niteration 252: weight=0.9979023549902253\niteration 253: weight=0.997992557755128\niteration 254: weight=0.9980793361961048\niteration 255: weight=0.9981628027352366\niteration 256: weight=0.9982430667454899\niteration 257: weight=0.9983202346112277\niteration 258: weight=0.9983944097882684\niteration 259: weight=0.9984656928634669\niteration 260: weight=0.998534181613794\niteration 261: weight=0.9985999710648938\niteration 262: weight=0.9986631535490955\niteration 263: weight=0.9987238187628614\niteration 264: weight=0.9987820538236523\niteration 265: weight=0.9988379433261909\niteration 266: weight=0.9988915693981099\niteration 267: weight=0.9989430117549652\niteration 268: weight=0.9989923477546039\niteration 269: weight=0.9990396524508698\niteration 270: weight=0.9990849986466379\niteration 271: weight=0.9991284569461627\niteration 272: weight=0.9991700958067327\niteration 273: weight=0.9992099815896199\niteration 274: weight=0.9992481786103167\niteration 275: weight=0.9992847491880505\niteration 276: weight=0.9993197536945726\niteration 277: weight=0.9993532506022106\niteration 278: weight=0.9993852965311832\niteration 279: weight=0.999415946296171\niteration 280: weight=0.9994452529521384\niteration 281: weight=0.9994732678394059\niteration 282: weight=0.999500040627969\niteration 283: weight=0.9995256193610621\niteration 284: weight=0.9995500504979664\niteration 285: weight=0.9995733789560621\niteration 286: weight=0.9995956481521241\niteration 287: weight=0.9996169000428623\niteration 288: weight=0.9996371751647086\niteration 289: weight=0.9996565126728514\niteration 290: weight=0.9996749503795203\niteration 291: weight=0.9996925247915246\niteration 292: weight=0.9997092711470486\niteration 293: weight=0.9997252234517061\niteration 294: weight=0.9997404145138615\niteration 295: weight=0.9997548759792192\niteration 296: weight=0.9997686383646873\niteration 297: weight=0.9997817310915225\niteration 298: weight=0.9997941825177595\niteration 299: weight=0.9998060199699335\niteration 300: weight=0.9998172697740993\niteration 301: weight=0.9998279572861581\niteration 302: weight=0.9998381069214939\niteration 303: weight=0.9998477421839309\niteration 304: weight=0.9998568856940179\niteration 305: weight=0.9998655592166464\niteration 306: weight=0.9998737836880133\niteration 307: weight=0.9998815792419323\niteration 308: weight=0.9998889652355071\niteration 309: weight=0.9998959602741706\niteration 310: weight=0.9999025822361022\niteration 311: weight=0.9999088482960293\niteration 312: weight=0.9999147749484246\niteration 313: weight=0.999920378030106\niteration 314: weight=0.9999256727422497\niteration 315: weight=0.9999306736718255\niteration 316: weight=0.9999353948124632\niteration 317: weight=0.9999398495847598\niteration 318: weight=0.9999440508560381\niteration 319: weight=0.9999480109595638\niteration 320: weight=0.9999517417132329\niteration 321: weight=0.9999552544377384\niteration 322: weight=0.9999585599742249\niteration 323: weight=0.9999616687014424\niteration 324: weight=0.9999645905524075\niteration 325: weight=0.999967335030582\niteration 326: weight=0.9999699112255794\niteration 327: weight=0.9999723278284075\niteration 328: weight=0.9999745931462574\niteration 329: weight=0.9999767151168482\niteration 330: weight=0.9999787013223368\niteration 331: weight=0.9999805590028027\niteration 332: weight=0.9999822950693158\niteration 333: weight=0.9999839161165983\niteration 334: weight=0.9999854284352881\niteration 335: weight=0.9999868380238144\niteration 336: weight=0.9999881505998927\niteration 337: weight=0.9999893716116505\niteration 338: weight=0.9999905062483903\niteration 339: weight=0.9999915594509997\niteration 340: weight=0.9999925359220176\niteration 341: weight=0.9999934401353642\niteration 342: weight=0.9999942763457434\niteration 343: weight=0.9999950485977274\niteration 344: weight=0.9999957607345287\niteration 345: weight=0.999996416406471\niteration 346: weight=0.9999970190791647\niteration 347: weight=0.9999975720413955\niteration 348: weight=0.9999980784127344\niteration 349: weight=0.9999985411508757\niteration 350: weight=0.9999989630587119\niteration 351: weight=0.9999993467911512\niteration 352: weight=0.9999996948616862\niteration 353: weight=1.0000000096487203\niteration 354: weight=1.0000002934016596\niteration 355: weight=1.0000005482467753\niteration 356: weight=1.0000007761928456\niteration 357: weight=1.0000009791365825\niteration 358: weight=1.0000011588678495\niteration 359: weight=1.0000013170746784\niteration 360: weight=1.0000014553480892\niteration 361: weight=1.0000015751867204\niteration 362: weight=1.0000016780012766\niteration 363: weight=1.0000017651187962\niteration 364: weight=1.0000018377867486\niteration 365: weight=1.0000018971769635\niteration 366: weight=1.0000019443894\niteration 367: weight=1.0000019804557585\niteration 368: weight=1.0000020063429436\niteration 369: weight=1.0000020229563793\niteration 370: weight=1.0000020311431854\niteration 371: weight=1.0000020316952167\niteration 372: weight=1.0000020253519715\niteration 373: weight=1.0000020128033733\niteration 374: weight=1.0000019946924315\niteration 375: weight=1.0000019716177821\niteration 376: weight=1.0000019441361176\niteration 377: weight=1.0000019127645048\niteration 378: weight=1.000001877982599\niteration 379: weight=1.0000018402347561\niteration 380: weight=1.0000017999320474\niteration 381: weight=1.000001757454179\niteration 382: weight=1.0000017131513226\niteration 383: weight=1.0000016673458578\niteration 384: weight=1.000001620334032\niteration 385: weight=1.0000015723875386\niteration 386: weight=1.0000015237550193\niteration 387: weight=1.0000014746634915\niteration 388: weight=1.0000014253197047\niteration 389: weight=1.0000013759114286\niteration 390: weight=1.0000013266086762\niteration 391: weight=1.0000012775648637\niteration 392: weight=1.0000012289179103\niteration 393: weight=1.0000011807912805\niteration 394: weight=1.0000011332949712\niteration 395: weight=1.000001086526446\niteration 396: weight=1.000001040571519\niteration 397: weight=1.0000009955051905\niteration 398: weight=1.0000009513924366\niteration 399: weight=1.0000009082889536\niteration 400: weight=1.0000008662418622\niteration 401: weight=1.0000008252903696\niteration 402: weight=1.0000007854663946\niteration 403: weight=1.0000007467951555\niteration 404: weight=1.000000709295723\niteration 405: weight=1.0000006729815407\niteration 406: weight=1.0000006378609128\niteration 407: weight=1.0000006039374625\niteration 408: weight=1.0000005712105615\niteration 409: weight=1.0000005396757328\niteration 410: weight=1.0000005093250262\niteration 411: weight=1.0000004801473712\niteration 412: weight=1.0000004521289048\niteration 413: weight=1.0000004252532784\niteration 414: weight=1.0000003995019433\niteration 415: weight=1.000000374854416\niteration 416: weight=1.0000003512885252\niteration 417: weight=1.000000328780641\niteration 418: weight=1.0000003073058867\niteration 419: weight=1.0000002868383355\niteration 420: weight=1.0000002673511916\niteration 421: weight=1.000000248816957\niteration 422: weight=1.000000231207586\niteration 423: weight=1.0000002144946256\niteration 424: weight=1.0000001986493454\niteration 425: weight=1.0000001836428556\niteration 426: weight=1.0000001694462146\niteration 427: weight=1.0000001560305274\niteration 428: weight=1.0000001433670334\niteration 429: weight=1.0000001314271876\niteration 430: weight=1.0000001201827318\niteration 431: weight=1.0000001096057591\niteration 432: weight=1.000000099668772\niteration 433: weight=1.000000090344732\niteration 434: weight=1.000000081607105\niteration 435: weight=1.0000000734299008\niteration 436: weight=1.0000000657877053\niteration 437: weight=1.0000000586557103\niteration 438: weight=1.0000000520097374\niteration 439: weight=1.0000000458262583\niteration 440: weight=1.0000000400824103\niteration 441: weight=1.000000034756009\niteration 442: weight=1.0000000298255576\niteration 443: weight=1.0000000252702532\niteration 444: weight=1.0000000210699902\niteration 445: weight=1.0000000172053607\niteration 446: weight=1.0000000136576537\niteration 447: weight=1.0000000104088511\niteration 448: weight=1.000000007441623\niteration 449: weight=1.0000000047393205\niteration 450: weight=1.0000000022859659\niteration 451: weight=1.0000000000662446\niteration 452: weight=0.9999999980654928\niteration 453: weight=0.9999999962696857\niteration 454: weight=0.9999999946654243\niteration 455: weight=0.9999999932399216\niteration 456: weight=0.9999999919809877\niteration 457: weight=0.9999999908770149\niteration 458: weight=0.999999989916962\niteration 459: weight=0.9999999890903385\niteration 460: weight=0.9999999883871876\niteration 461: weight=0.9999999877980703\niteration 462: weight=0.9999999873140484\niteration 463: weight=0.9999999869266677\niteration 464: weight=0.9999999866279413\niteration 465: weight=0.9999999864103329\niteration 466: weight=0.9999999862667397\niteration 467: weight=0.9999999861904763\niteration 468: weight=0.9999999861752578\niteration 469: weight=0.9999999862151839\niteration 470: weight=0.9999999863047233\niteration 471: weight=0.9999999864386969\niteration 472: weight=0.9999999866122634\niteration 473: weight=0.9999999868209039\niteration 474: weight=0.9999999870604066\niteration 475: weight=0.9999999873268534\niteration 476: weight=0.9999999876166047\niteration 477: weight=0.9999999879262865\niteration 478: weight=0.9999999882527769\niteration 479: weight=0.999999988593193\niteration 480: weight=0.9999999889448784\niteration 481: weight=0.9999999893053915\niteration 482: weight=0.9999999896724932\niteration 483: weight=0.9999999900441359\niteration 484: weight=0.9999999904184526\niteration 485: weight=0.9999999907937464\niteration 486: weight=0.9999999911684802\niteration 487: weight=0.9999999915412672\niteration 488: weight=0.9999999919108616\niteration 489: weight=0.9999999922761498\niteration 490: weight=0.9999999926361411\niteration 491: weight=0.9999999929899606\niteration 492: weight=0.9999999933368405\niteration 493: weight=0.9999999936761135\niteration 494: weight=0.9999999940072047\niteration 495: weight=0.9999999943296259\niteration 496: weight=0.9999999946429682\niteration 497: weight=0.9999999949468965\niteration 498: weight=0.9999999952411437\niteration 499: weight=0.999999995525505\niteration 500: weight=0.9999999957998327\niteration 501: weight=0.9999999960640314\niteration 502: weight=0.9999999963180537\niteration 503: weight=0.999999996561895\niteration 504: weight=0.9999999967955902\niteration 505: weight=0.9999999970192094\niteration 506: weight=0.9999999972328546\niteration 507: weight=0.9999999974366559\niteration 508: weight=0.9999999976307687\niteration 509: weight=0.9999999978153705\niteration 510: weight=0.9999999979906583\niteration 511: weight=0.9999999981568457\niteration 512: weight=0.999999998314161\niteration 513: weight=0.9999999984628445\niteration 514: weight=0.9999999986031467\niteration 515: weight=0.9999999987353264\niteration 516: weight=0.9999999988596489\niteration 517: weight=0.9999999989763841\niteration 518: weight=0.9999999990858058\niteration 519: weight=0.9999999991881894\niteration 520: weight=0.9999999992838112\niteration 521: weight=0.9999999993729474\niteration 522: weight=0.9999999994558726\niteration 523: weight=0.9999999995328595\niteration 524: weight=0.9999999996041773\niteration 525: weight=0.9999999996700918\niteration 526: weight=0.9999999997308642\niteration 527: weight=0.9999999997867507\niteration 528: weight=0.999999999838002\niteration 529: weight=0.9999999998848629\niteration 530: weight=0.9999999999275718\niteration 531: weight=0.9999999999663607\niteration 532: weight=1.0000000000014544\niteration 533: weight=1.0000000000330709\niteration 534: weight=1.0000000000614209\niteration 535: weight=1.0000000000867075\niteration 536: weight=1.0000000001091267\niteration 537: weight=1.000000000128867\niteration 538: weight=1.0000000001461087\niteration 539: weight=1.0000000001610256\niteration 540: weight=1.0000000001737834\niteration 541: weight=1.0000000001845408\niteration 542: weight=1.0000000001934488\niteration 543: weight=1.0000000002006517\niteration 544: weight=1.0000000002062863\niteration 545: weight=1.0000000002104827\niteration 546: weight=1.0000000002133642\niteration 547: weight=1.0000000002150473\niteration 548: weight=1.0000000002156426\niteration 549: weight=1.0000000002152538\niteration 550: weight=1.0000000002139788\niteration 551: weight=1.0000000002119098\niteration 552: weight=1.0000000002091332\niteration 553: weight=1.0000000002057299\niteration 554: weight=1.0000000002017755\niteration 555: weight=1.0000000001973406\niteration 556: weight=1.000000000192491\niteration 557: weight=1.0000000001872875\niteration 558: weight=1.0000000001817868\niteration 559: weight=1.0000000001760414\niteration 560: weight=1.0000000001700993\niteration 561: weight=1.000000000164005\niteration 562: weight=1.0000000001577996\niteration 563: weight=1.00000000015152\niteration 564: weight=1.0000000001452\niteration 565: weight=1.000000000138871\niteration 566: weight=1.0000000001325604\niteration 567: weight=1.0000000001262936\niteration 568: weight=1.000000000120093\niteration 569: weight=1.0000000001139786\niteration 570: weight=1.000000000107968\niteration 571: weight=1.0000000001020772\niteration 572: weight=1.0000000000963196\niteration 573: weight=1.0000000000907068\niteration 574: weight=1.000000000085249\niteration 575: weight=1.0000000000799543\niteration 576: weight=1.0000000000748297\niteration 577: weight=1.0000000000698805\niteration 578: weight=1.0000000000651112\niteration 579: weight=1.0000000000605247\niteration 580: weight=1.0000000000561229\niteration 581: weight=1.000000000051907\niteration 582: weight=1.000000000047877\niteration 583: weight=1.0000000000440323\niteration 584: weight=1.0000000000403715\niteration 585: weight=1.0000000000368925\niteration 586: weight=1.0000000000335927\niteration 587: weight=1.000000000030469\niteration 588: weight=1.0000000000275175\niteration 589: weight=1.0000000000247347\niteration 590: weight=1.000000000022116\niteration 591: weight=1.0000000000196572\niteration 592: weight=1.000000000017353\niteration 593: weight=1.0000000000151987\niteration 594: weight=1.000000000013189\niteration 595: weight=1.0000000000113185\niteration 596: weight=1.000000000009582\niteration 597: weight=1.0000000000079738\niteration 598: weight=1.0000000000064888\niteration 599: weight=1.0000000000051212\niteration 600: weight=1.0000000000038658\niteration 601: weight=1.0000000000027172\niteration 602: weight=1.0000000000016698\niteration 603: weight=1.0000000000007185\niteration 604: weight=0.9999999999998581\niteration 605: weight=0.9999999999990835\niteration 606: weight=0.9999999999983898\niteration 607: weight=0.9999999999977722\niteration 608: weight=0.999999999997226\niteration 609: weight=0.9999999999967466\niteration 610: weight=0.9999999999963296\niteration 611: weight=0.9999999999959708\niteration 612: weight=0.999999999995666\niteration 613: weight=0.9999999999954114\niteration 614: weight=0.9999999999952033\niteration 615: weight=0.999999999995038\niteration 616: weight=0.999999999994912\niteration 617: weight=0.999999999994822\niteration 618: weight=0.999999999994765\niteration 619: weight=0.9999999999947379\niteration 620: weight=0.999999999994738\niteration 621: weight=0.9999999999947625\niteration 622: weight=0.999999999994809\niteration 623: weight=0.9999999999948752\niteration 624: weight=0.9999999999949587\niteration 625: weight=0.9999999999950575\niteration 626: weight=0.9999999999951696\niteration 627: weight=0.9999999999952933\niteration 628: weight=0.9999999999954268\niteration 629: weight=0.9999999999955685\niteration 630: weight=0.9999999999957171\niteration 631: weight=0.9999999999958711\niteration 632: weight=0.9999999999960292\niteration 633: weight=0.9999999999961904\niteration 634: weight=0.9999999999963536\niteration 635: weight=0.9999999999965179\niteration 636: weight=0.9999999999966823\niteration 637: weight=0.9999999999968462\niteration 638: weight=0.9999999999970088\niteration 639: weight=0.9999999999971695\niteration 640: weight=0.9999999999973277\niteration 641: weight=0.9999999999974829\niteration 642: weight=0.9999999999976348\niteration 643: weight=0.9999999999977829\niteration 644: weight=0.9999999999979269\niteration 645: weight=0.9999999999980665\niteration 646: weight=0.9999999999982015\niteration 647: weight=0.9999999999983318\niteration 648: weight=0.9999999999984571\niteration 649: weight=0.9999999999985775\niteration 650: weight=0.9999999999986927\niteration 651: weight=0.9999999999988028\niteration 652: weight=0.9999999999989078\niteration 653: weight=0.9999999999990076\niteration 654: weight=0.9999999999991023\niteration 655: weight=0.999999999999192\niteration 656: weight=0.9999999999992767\niteration 657: weight=0.9999999999993565\niteration 658: weight=0.9999999999994316\niteration 659: weight=0.9999999999995018\niteration 660: weight=0.9999999999995676\niteration 661: weight=0.999999999999629\niteration 662: weight=0.999999999999686\niteration 663: weight=0.999999999999739\niteration 664: weight=0.999999999999788\niteration 665: weight=0.9999999999998331\niteration 666: weight=0.9999999999998747\niteration 667: weight=0.9999999999999126\niteration 668: weight=0.9999999999999473\niteration 669: weight=0.9999999999999788\niteration 670: weight=1.0000000000000073\niteration 671: weight=1.0000000000000329\niteration 672: weight=1.0000000000000557\niteration 673: weight=1.0000000000000762\niteration 674: weight=1.0000000000000941\niteration 675: weight=1.00000000000011\niteration 676: weight=1.0000000000001235\niteration 677: weight=1.0000000000001352\niteration 678: weight=1.000000000000145\niteration 679: weight=1.0000000000001532\niteration 680: weight=1.0000000000001599\niteration 681: weight=1.000000000000165\niteration 682: weight=1.0000000000001688\niteration 683: weight=1.0000000000001714\niteration 684: weight=1.000000000000173\niteration 685: weight=1.0000000000001734\niteration 686: weight=1.000000000000173\niteration 687: weight=1.0000000000001716\niteration 688: weight=1.0000000000001696\niteration 689: weight=1.000000000000167\niteration 690: weight=1.0000000000001639\niteration 691: weight=1.00000000000016\niteration 692: weight=1.0000000000001559\niteration 693: weight=1.0000000000001514\niteration 694: weight=1.0000000000001465\niteration 695: weight=1.0000000000001414\niteration 696: weight=1.0000000000001361\niteration 697: weight=1.0000000000001306\niteration 698: weight=1.000000000000125\niteration 699: weight=1.0000000000001195\niteration 700: weight=1.0000000000001137\niteration 701: weight=1.000000000000108\niteration 702: weight=1.0000000000001021\niteration 703: weight=1.0000000000000966\niteration 704: weight=1.000000000000091\niteration 705: weight=1.0000000000000855\niteration 706: weight=1.0000000000000802\niteration 707: weight=1.0000000000000748\niteration 708: weight=1.0000000000000697\niteration 709: weight=1.0000000000000648\niteration 710: weight=1.00000000000006\niteration 711: weight=1.0000000000000553\niteration 712: weight=1.0000000000000508\niteration 713: weight=1.0000000000000466\niteration 714: weight=1.0000000000000426\niteration 715: weight=1.0000000000000386\niteration 716: weight=1.0000000000000349\niteration 717: weight=1.0000000000000313\niteration 718: weight=1.000000000000028\niteration 719: weight=1.0000000000000249\niteration 720: weight=1.000000000000022\niteration 721: weight=1.000000000000019\niteration 722: weight=1.0000000000000164\niteration 723: weight=1.000000000000014\niteration 724: weight=1.0000000000000118\niteration 725: weight=1.0000000000000098\niteration 726: weight=1.0000000000000078\niteration 727: weight=1.000000000000006\niteration 728: weight=1.0000000000000044\niteration 729: weight=1.0000000000000029\niteration 730: weight=1.0000000000000016\niteration 731: weight=1.0000000000000002\niteration 732: weight=0.9999999999999991\niteration 733: weight=0.9999999999999981\niteration 734: weight=0.9999999999999972\niteration 735: weight=0.9999999999999964\niteration 736: weight=0.9999999999999958\niteration 737: weight=0.9999999999999952\niteration 738: weight=0.9999999999999947\niteration 739: weight=0.9999999999999942\niteration 740: weight=0.9999999999999939\niteration 741: weight=0.9999999999999936\niteration 742: weight=0.9999999999999933\niteration 743: weight=0.9999999999999931\niteration 744: weight=0.999999999999993\niteration 745: weight=0.9999999999999929\nconverged after 746 iterations\n\n\n/tmp/ipykernel_196800/2243492909.py:31: RuntimeWarning: invalid value encountered in sqrt\n  b = b - self.eta*(m_db_corr/(np.sqrt(v_db_corr)+self.epsilon))"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "",
    "text": "import numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\ndf = pd.read_csv('df_panel_fix.csv')\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\ndf=df_subset\ndf\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 7 columns"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#distributions-of-dependant-variables",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#distributions-of-dependant-variables",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\nsns.distplot(df['gdp'])\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1043c20588&gt;\n\n\n\n\n\n\nsns.distplot(df['fdi'])\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f10437f74a8&gt;\n\n\n\n\n\n\nsns.distplot(df['it'])\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f1043a09ef0&gt;\n\n\n\n\n\n\nsns.distplot(df['specific'].dropna())\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f10439b7a20&gt;\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f10439a19e8&gt;]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are &gt; 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then &gt; 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])&gt;3].hist(column = ['gdp'])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f104364e0f0&gt;]],\n      dtype=object)\n\n\n\n\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])&lt;3]\n\n\ndf_no_gdp_outliers\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n-0.521466\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n-0.464746\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n-0.421061\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n-0.383239\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n-0.340870\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n354\n2002\nEast China\nZhejiang\n8003.67\n307610\n1962633\n365437.0\n0.798274\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n1.178172\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n1.612181\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n2.007180\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n2.520929\n\n\n\n\n350 rows × 8 columns\n\n\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7f10429f5ba8&gt;]],\n      dtype=object)\n\n\n\n\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\n\n\n\n\n\n\nyear\nprovince\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\nregion\n\n\n\n\n\n\n\n\n\n\n\nEast China\n84\n84\n84\n84\n84\n84\n84\n\n\nNorth China\n48\n48\n48\n48\n48\n47\n48\n\n\nNortheast China\n36\n36\n36\n36\n36\n36\n36\n\n\nNorthwest China\n60\n60\n60\n60\n60\n60\n60\n\n\nSouth Central China\n72\n72\n72\n72\n72\n72\n72\n\n\nSouthwest China\n60\n60\n60\n60\n60\n57\n60\n\n\n\n\n\n\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n\n\n\n\n\n\nyear\nregion\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\nprovince\n\n\n\n\n\n\n\n\n\n\n\nAnhui\n12\n12\n12\n12\n12\n12\n12\n\n\nBeijing\n12\n12\n12\n12\n12\n12\n12\n\n\nChongqing\n12\n12\n12\n12\n12\n9\n12\n\n\nFujian\n12\n12\n12\n12\n12\n12\n12\n\n\nGansu\n12\n12\n12\n12\n12\n12\n12\n\n\nGuangdong\n12\n12\n12\n12\n12\n12\n12\n\n\nGuangxi\n12\n12\n12\n12\n12\n12\n12\n\n\nGuizhou\n12\n12\n12\n12\n12\n12\n12\n\n\nHainan\n12\n12\n12\n12\n12\n12\n12\n\n\nHebei\n12\n12\n12\n12\n12\n11\n12\n\n\nHeilongjiang\n12\n12\n12\n12\n12\n12\n12\n\n\nHenan\n12\n12\n12\n12\n12\n12\n12\n\n\nHubei\n12\n12\n12\n12\n12\n12\n12\n\n\nHunan\n12\n12\n12\n12\n12\n12\n12\n\n\nJiangsu\n12\n12\n12\n12\n12\n12\n12\n\n\nJiangxi\n12\n12\n12\n12\n12\n12\n12\n\n\nJilin\n12\n12\n12\n12\n12\n12\n12\n\n\nLiaoning\n12\n12\n12\n12\n12\n12\n12\n\n\nNingxia\n12\n12\n12\n12\n12\n12\n12\n\n\nQinghai\n12\n12\n12\n12\n12\n12\n12\n\n\nShaanxi\n12\n12\n12\n12\n12\n12\n12\n\n\nShandong\n12\n12\n12\n12\n12\n12\n12\n\n\nShanghai\n12\n12\n12\n12\n12\n12\n12\n\n\nShanxi\n12\n12\n12\n12\n12\n12\n12\n\n\nSichuan\n12\n12\n12\n12\n12\n12\n12\n\n\nTianjin\n12\n12\n12\n12\n12\n12\n12\n\n\nTibet\n12\n12\n12\n12\n12\n12\n12\n\n\nXinjiang\n12\n12\n12\n12\n12\n12\n12\n\n\nYunnan\n12\n12\n12\n12\n12\n12\n12\n\n\nZhejiang\n12\n12\n12\n12\n12\n12\n12"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#subset-by-needed-columns",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#subset-by-needed-columns",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Subset by needed columns",
    "text": "Subset by needed columns\n\ndf_no_gdp_outliers.columns\n\nIndex(['year', 'region', 'province', 'gdp', 'fdi', 'it', 'specific',\n       'gdp_zscore'],\n      dtype='object')\n\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\n\n\n\n\n\n\nregion\ngdp\nfdi\nit\n\n\n\n\n0\nEast China\n2093.30\n50661\n631930\n\n\n1\nEast China\n2347.32\n43443\n657860\n\n\n2\nEast China\n2542.96\n27673\n889463\n\n\n3\nEast China\n2712.34\n26131\n1227364\n\n\n4\nEast China\n2902.09\n31847\n1499110\n\n\n...\n...\n...\n...\n...\n\n\n354\nEast China\n8003.67\n307610\n1962633\n\n\n355\nEast China\n9705.02\n498055\n2261631\n\n\n356\nEast China\n11648.70\n668128\n3162299\n\n\n357\nEast China\n13417.68\n772000\n2370200\n\n\n358\nEast China\n15718.47\n888935\n2553268\n\n\n\n\n350 rows × 4 columns"
  },
  {
    "objectID": "posts/2020-10-18-ttests-distributions-crosstabs.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "href": "posts/2020-10-18-ttests-distributions-crosstabs.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "title": "Evaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments",
    "section": "Genearate an experimental_crosstab to be used in statistical tests",
    "text": "Genearate an experimental_crosstab to be used in statistical tests\n\nexperimental_crosstab = df_no_gdp_outliers_subset.groupby('region').agg(['size', 'mean', 'std'])\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\nexperimental_crosstab = experimental_crosstab.reset_index()\n\n\nexperimental_crosstab\n\n\n\n\n\n\n\n\nregion\ngdp\nfdi\nit\n\n\n\n\nsize\nmean\nstd\nsize\nmean\nstd\nsize\nmean\nstd\n\n\n\n\n0\nEast China\n78\n6070.604231\n3500.372702\n78\n355577.897436\n275635.866746\n78\n1.775615e+06\n1.153030e+06\n\n\n1\nNorth China\n48\n4239.038542\n2866.705149\n48\n169600.583333\n127011.475909\n48\n1.733719e+06\n1.548794e+06\n\n\n2\nNortheast China\n36\n3849.076944\n1948.531835\n36\n136623.750000\n142734.495232\n36\n2.665148e+06\n1.768442e+06\n\n\n3\nNorthwest China\n60\n1340.026167\n1174.399739\n60\n15111.133333\n22954.193559\n60\n1.703538e+06\n1.446408e+06\n\n\n4\nSouth Central China\n68\n4835.540882\n3697.129915\n68\n218931.426471\n339981.399823\n68\n2.500962e+06\n2.196436e+06\n\n\n5\nSouthwest China\n60\n2410.398833\n2144.589994\n60\n25405.083333\n31171.373876\n60\n2.424971e+06\n2.002198e+06\n\n\n\n\n\n\n\n\nexperimental_crosstab.to_csv('fiscal_experimental_crosstab.csv')"
  },
  {
    "objectID": "posts/2021-02-06-Portfolio-Allocation-Sharpe-Ratio.html",
    "href": "posts/2021-02-06-Portfolio-Allocation-Sharpe-Ratio.html",
    "title": "Sharpe Ratio and Portfolio Values",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\nimport pandas as pd\nimport pandas_datareader.data as web\nimport datetime\nimport quandl"
  },
  {
    "objectID": "posts/2021-02-06-Portfolio-Allocation-Sharpe-Ratio.html#create-a-portfolio",
    "href": "posts/2021-02-06-Portfolio-Allocation-Sharpe-Ratio.html#create-a-portfolio",
    "title": "Sharpe Ratio and Portfolio Values",
    "section": "Create a Portfolio",
    "text": "Create a Portfolio\n\ndf = pd.read_csv('https://stocks-snp-500.herokuapp.com/stocks/index_stocks_table.csv?_size=max')\ndf.head()\n\ndf['Date'] = pd.to_datetime(df.Date)\ndf.index = pd.to_datetime(df.Date)\ndf2 = df.drop('Date', axis=1)\ndf3 = df2.drop('rowid', axis=1)\ndf3 = df3.drop('Russell_2000_stock', axis=1)\ndf3 = df3.drop('SP500_stock', axis=1)\ndf3\n\nstocks = df3\n\n\nstocks\n\n\n\n\n\n\n\n\nFXAIX_stock\nVRTTX_stock\nFNCMX_stock\nFSMAX_stock\n\n\nDate\n\n\n\n\n\n\n\n\n2018-01-02\n94.230003\n238.889999\n91.989998\n62.529999\n\n\n2018-01-03\n94.830002\n240.289993\n92.760002\n62.740002\n\n\n2018-01-04\n95.230003\n241.199997\n92.930000\n62.849998\n\n\n2018-01-05\n95.900002\n242.750000\n93.699997\n63.090000\n\n\n2018-01-08\n96.059998\n243.199997\n93.970001\n63.270000\n\n\n...\n...\n...\n...\n...\n\n\n2021-02-08\n135.880005\n355.709991\n175.610001\n93.940002\n\n\n2021-02-09\n135.750000\n355.730011\n175.839996\n94.400002\n\n\n2021-02-10\n135.699997\n355.579987\n175.380005\n94.309998\n\n\n2021-02-11\n135.960007\n356.440002\n176.100006\n94.790001\n\n\n2021-02-12\n136.600006\n358.170013\n176.979996\n95.250000\n\n\n\n\n785 rows × 4 columns\n\n\n\n\n# stocks\n\n# stocks['FXAIX_stock Daily Return'] = stocks['FXAIX_stock'].pct_change(1)\n\n# cum_ret = 100 * (stocks['FXAIX_stock'][-1] / stocks['FXAIX_stock'][0] - 1 )\n# print('Our return {} was percent!'.format(cum_ret))\n\n# stocks['FXAIX_stock Daily Return'].mean()\n\n# stocks['FXAIX_stock Daily Return'].std()\n\n# stocks['FXAIX_stock Daily Return'].plot(kind = 'kde')\n\n# portfolio_val=stocks\n\n# SR = portfolio_val['FXAIX_stock Daily Return'].mean() / portfolio_val['FXAIX_stock Daily Return'].std()\n\n# SR\n\n# ASR = (252 ** 0.5) * SR\n\n# ASR\n\n# portfolio_val['FXAIX_stock Daily Return'].std()\n\n# import matplotlib.pyplot as plt\n# %matplotlib inline\n\n\n# fig = plt.figure(figsize = (12, 8))\n# portfolio_val['FXAIX_stock Daily Return'].plot(kind = 'kde')\n\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nVRTTX_stock = web.DataReader('VRTTX', 'yahoo', start, end)\nVRTTX_stock.head()\n\n\nFNCMX_stock = web.DataReader('FNCMX', 'yahoo', start, end)\nFNCMX_stock.head()\n\nFSMAX_stock = web.DataReader('FSMAX', 'yahoo', start, end)\nFSMAX_stock.head()\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n65.269997\n65.269997\n65.269997\n65.269997\n0.0\n65.192108\n\n\n2020-01-03\n65.129997\n65.129997\n65.129997\n65.129997\n0.0\n65.052277\n\n\n2020-01-06\n65.279999\n65.279999\n65.279999\n65.279999\n0.0\n65.202103\n\n\n2020-01-07\n65.180000\n65.180000\n65.180000\n65.180000\n0.0\n65.102219\n\n\n2020-01-08\n65.410004\n65.410004\n65.410004\n65.410004\n0.0\n65.331947\n\n\n\n\n\n\n\n\nVRTTX_stock\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n285.730011\n285.730011\n285.730011\n285.730011\n0.0\n280.905823\n\n\n2020-01-03\n283.959991\n283.959991\n283.959991\n283.959991\n0.0\n279.165680\n\n\n2020-01-06\n284.910004\n284.910004\n284.910004\n284.910004\n0.0\n280.099640\n\n\n2020-01-07\n284.200012\n284.200012\n284.200012\n284.200012\n0.0\n279.401642\n\n\n2020-01-08\n285.559998\n285.559998\n285.559998\n285.559998\n0.0\n280.738678\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-02-08\n355.709991\n355.709991\n355.709991\n355.709991\n0.0\n355.709991\n\n\n2021-02-09\n355.730011\n355.730011\n355.730011\n355.730011\n0.0\n355.730011\n\n\n2021-02-10\n355.579987\n355.579987\n355.579987\n355.579987\n0.0\n355.579987\n\n\n2021-02-11\n356.440002\n356.440002\n356.440002\n356.440002\n0.0\n356.440002\n\n\n2021-02-12\n358.170013\n358.170013\n358.170013\n358.170013\n0.0\n358.170013\n\n\n\n\n282 rows × 6 columns\n\n\n\n\nFNCMX_stock\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n114.180000\n114.180000\n114.180000\n114.180000\n0.0\n113.419273\n\n\n2020-01-03\n113.269997\n113.269997\n113.269997\n113.269997\n0.0\n112.515327\n\n\n2020-01-06\n113.849998\n113.849998\n113.849998\n113.849998\n0.0\n113.091469\n\n\n2020-01-07\n113.839996\n113.839996\n113.839996\n113.839996\n0.0\n113.081535\n\n\n2020-01-08\n114.650002\n114.650002\n114.650002\n114.650002\n0.0\n113.886139\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-02-08\n175.610001\n175.610001\n175.610001\n175.610001\n0.0\n175.610001\n\n\n2021-02-09\n175.839996\n175.839996\n175.839996\n175.839996\n0.0\n175.839996\n\n\n2021-02-10\n175.380005\n175.380005\n175.380005\n175.380005\n0.0\n175.380005\n\n\n2021-02-11\n176.100006\n176.100006\n176.100006\n176.100006\n0.0\n176.100006\n\n\n2021-02-12\n176.979996\n176.979996\n176.979996\n176.979996\n0.0\n176.979996\n\n\n\n\n282 rows × 6 columns\n\n\n\n\nfor stock_df in (FXAIX_stock, VRTTX_stock, FNCMX_stock, FSMAX_stock):\n    stock_df['Normed Return'] = stock_df['Adj Close'] / stock_df.iloc[0]['Adj Close']\n\n\nstock_df\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\nNormed Return\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n65.269997\n65.269997\n65.269997\n65.269997\n0.0\n65.192108\n1.000000\n\n\n2020-01-03\n65.129997\n65.129997\n65.129997\n65.129997\n0.0\n65.052277\n0.997855\n\n\n2020-01-06\n65.279999\n65.279999\n65.279999\n65.279999\n0.0\n65.202103\n1.000153\n\n\n2020-01-07\n65.180000\n65.180000\n65.180000\n65.180000\n0.0\n65.102219\n0.998621\n\n\n2020-01-08\n65.410004\n65.410004\n65.410004\n65.410004\n0.0\n65.331947\n1.002145\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-02-08\n93.940002\n93.940002\n93.940002\n93.940002\n0.0\n93.940002\n1.440972\n\n\n2021-02-09\n94.400002\n94.400002\n94.400002\n94.400002\n0.0\n94.400002\n1.448028\n\n\n2021-02-10\n94.309998\n94.309998\n94.309998\n94.309998\n0.0\n94.309998\n1.446647\n\n\n2021-02-11\n94.790001\n94.790001\n94.790001\n94.790001\n0.0\n94.790001\n1.454010\n\n\n2021-02-12\n95.250000\n95.250000\n95.250000\n95.250000\n0.0\n95.250000\n1.461066\n\n\n\n\n282 rows × 7 columns\n\n\n\n\n## Allocations\n\n\nfor stock_df,allo in zip([FXAIX_stock, VRTTX_stock, FNCMX_stock, FSMAX_stock],[.7, .1, .1, .1]):\n    stock_df['Allocation'] = stock_df['Normed Return'] * allo\n\n\n\nstock_df\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\nNormed Return\nAllocation\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n65.269997\n65.269997\n65.269997\n65.269997\n0.0\n65.192108\n1.000000\n0.100000\n\n\n2020-01-03\n65.129997\n65.129997\n65.129997\n65.129997\n0.0\n65.052277\n0.997855\n0.099786\n\n\n2020-01-06\n65.279999\n65.279999\n65.279999\n65.279999\n0.0\n65.202103\n1.000153\n0.100015\n\n\n2020-01-07\n65.180000\n65.180000\n65.180000\n65.180000\n0.0\n65.102219\n0.998621\n0.099862\n\n\n2020-01-08\n65.410004\n65.410004\n65.410004\n65.410004\n0.0\n65.331947\n1.002145\n0.100215\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-02-08\n93.940002\n93.940002\n93.940002\n93.940002\n0.0\n93.940002\n1.440972\n0.144097\n\n\n2021-02-09\n94.400002\n94.400002\n94.400002\n94.400002\n0.0\n94.400002\n1.448028\n0.144803\n\n\n2021-02-10\n94.309998\n94.309998\n94.309998\n94.309998\n0.0\n94.309998\n1.446647\n0.144665\n\n\n2021-02-11\n94.790001\n94.790001\n94.790001\n94.790001\n0.0\n94.790001\n1.454010\n0.145401\n\n\n2021-02-12\n95.250000\n95.250000\n95.250000\n95.250000\n0.0\n95.250000\n1.461066\n0.146107\n\n\n\n\n282 rows × 8 columns\n\n\n\n\n## Investment\n\n\nfor stock_df in [FXAIX_stock, VRTTX_stock, FNCMX_stock, FSMAX_stock]:\n    stock_df['Position Values'] = stock_df['Allocation'] * 100000\n\n\nstock_df\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\nNormed Return\nAllocation\nPosition Values\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n65.269997\n65.269997\n65.269997\n65.269997\n0.0\n65.192108\n1.000000\n0.100000\n10000.000000\n\n\n2020-01-03\n65.129997\n65.129997\n65.129997\n65.129997\n0.0\n65.052277\n0.997855\n0.099786\n9978.550848\n\n\n2020-01-06\n65.279999\n65.279999\n65.279999\n65.279999\n0.0\n65.202103\n1.000153\n0.100015\n10001.533085\n\n\n2020-01-07\n65.180000\n65.180000\n65.180000\n65.180000\n0.0\n65.102219\n0.998621\n0.099862\n9986.211594\n\n\n2020-01-08\n65.410004\n65.410004\n65.410004\n65.410004\n0.0\n65.331947\n1.002145\n0.100215\n10021.450322\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2021-02-08\n93.940002\n93.940002\n93.940002\n93.940002\n0.0\n93.940002\n1.440972\n0.144097\n14409.719995\n\n\n2021-02-09\n94.400002\n94.400002\n94.400002\n94.400002\n0.0\n94.400002\n1.448028\n0.144803\n14480.280543\n\n\n2021-02-10\n94.309998\n94.309998\n94.309998\n94.309998\n0.0\n94.309998\n1.446647\n0.144665\n14466.474582\n\n\n2021-02-11\n94.790001\n94.790001\n94.790001\n94.790001\n0.0\n94.790001\n1.454010\n0.145401\n14540.103641\n\n\n2021-02-12\n95.250000\n95.250000\n95.250000\n95.250000\n0.0\n95.250000\n1.461066\n0.146107\n14610.664189\n\n\n\n\n282 rows × 9 columns\n\n\n\n\n\n\n## Total Portfolio Value\n\nportfolio_val = pd.concat([FXAIX_stock['Position Values'],\n                           VRTTX_stock['Position Values'],\n                           FNCMX_stock['Position Values'],\n                           FSMAX_stock['Position Values']],\n                          axis = 1)\n\nportfolio_val.head()\n\nportfolio_val.columns = ['FXAIX_stock Pos', 'VRTTX_stock Pos', 'FNCMX_stock Pos', 'FSMAX_stock Pos']\n\nportfolio_val.head()\n\nportfolio_val['Total Pos'] = portfolio_val.sum(axis = 1)\n\nportfolio_val.head()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nportfolio_val['Total Pos'].plot(figsize = (12, 8))\nplt.title('Total Portfolio Value')\n\nportfolio_val.drop('Total Pos',\n                   axis = 1).plot(kind = 'line', figsize = (12, 8))\n\nportfolio_val.tail()\n\n\n\n\n\n\n\n\nFXAIX_stock Pos\nVRTTX_stock Pos\nFNCMX_stock Pos\nFSMAX_stock Pos\nTotal Pos\n\n\nDate\n\n\n\n\n\n\n\n\n\n2021-02-08\n85012.841550\n12662.962553\n15483.259184\n14409.719995\n127568.783283\n\n\n2021-02-09\n84931.504458\n12663.675231\n15503.537547\n14480.280543\n127578.997779\n\n\n2021-02-10\n84900.220227\n12658.334494\n15462.980820\n14466.474582\n127488.010123\n\n\n2021-02-11\n85062.894411\n12688.950302\n15526.462202\n14540.103641\n127818.410556\n\n\n2021-02-12\n85463.307752\n12750.537170\n15604.049511\n14610.664189\n128428.558622\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nportfolio_val['Daily Return'] = portfolio_val['Total Pos'].pct_change(1)\n\n\ncum_ret = 100 * (portfolio_val['Total Pos'][-1] / portfolio_val['Total Pos'][0] - 1 )\nprint('Our return {} was percent!'.format(cum_ret))\n\nOur return 28.428558621835933 was percent!\n\n\n\nportfolio_val['Daily Return'].mean()\n\n0.001112432148531155\n\n\n\nportfolio_val['Daily Return'].std()\n\n0.020973776003181118\n\n\n\nportfolio_val['Daily Return'].plot(kind = 'kde')\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\nSR = portfolio_val['Daily Return'].mean() / portfolio_val['Daily Return'].std()\n\nSR\n\nASR = (252 ** 0.5) * SR\n\nASR\n\nportfolio_val['Daily Return'].std()\n\nportfolio_val['Daily Return'].mean()\n\n0.001112432148531155\n\n\n\nfig = plt.figure(figsize = (12, 8))\nportfolio_val['Daily Return'].plot(kind = 'kde')\n\n&lt;AxesSubplot:ylabel='Density'&gt;\n\n\n\n\n\n\nfig = plt.figure(figsize = (12, 8))\nFXAIX_stock['Adj Close'].pct_change(1).plot(kind ='kde', label = 'FXAIX_stock')\nVRTTX_stock['Adj Close'].pct_change(1).plot(kind ='kde', label = 'VRTTX_stock')\nFNCMX_stock['Adj Close'].pct_change(1).plot(kind ='kde', label = 'FNCMX_stock')\nFSMAX_stock['Adj Close'].pct_change(1).plot(kind ='kde', label = 'FSMAX_stock')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f617d8dcf40&gt;"
  },
  {
    "objectID": "posts/2020-10-31-sql_calls_in_jupyter_Soccer_explore.html",
    "href": "posts/2020-10-31-sql_calls_in_jupyter_Soccer_explore.html",
    "title": "Working with sqlite databases in Jupyter for Visualizing European Soccer Match Data",
    "section": "",
    "text": "This post includes code and notes from data-analysis-using-sql.\nimport sqlalchemy as db\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n%load_ext sql\nengine = db.create_engine('sqlite:///database.sqlite')\nconnection = engine.connect()\nmetadata = db.MetaData()\nconnection\n\n&lt;sqlalchemy.engine.base.Connection at 0x7f70c27cc0b8&gt;\ntables = pd.read_sql(\"\"\"SELECT *\n                        FROM sqlite_master\n                        WHERE type='table';\"\"\", connection)\ntables\n\n\n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nsqlite_sequence\nsqlite_sequence\n4\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n1\ntable\nPlayer_Attributes\nPlayer_Attributes\n11\nCREATE TABLE \"Player_Attributes\" (\\n\\t`id`\\tIN...\n\n\n2\ntable\nPlayer\nPlayer\n14\nCREATE TABLE `Player` (\\n\\t`id`\\tINTEGER PRIMA...\n\n\n3\ntable\nMatch\nMatch\n18\nCREATE TABLE `Match` (\\n\\t`id`\\tINTEGER PRIMAR...\n\n\n4\ntable\nLeague\nLeague\n24\nCREATE TABLE `League` (\\n\\t`id`\\tINTEGER PRIMA...\n\n\n5\ntable\nCountry\nCountry\n26\nCREATE TABLE `Country` (\\n\\t`id`\\tINTEGER PRIM...\n\n\n6\ntable\nTeam\nTeam\n29\nCREATE TABLE \"Team\" (\\n\\t`id`\\tINTEGER PRIMARY...\n\n\n7\ntable\nTeam_Attributes\nTeam_Attributes\n2\nCREATE TABLE `Team_Attributes` (\\n\\t`id`\\tINTE...\n# %%sql\n# SELECT *\n# FROM sqlite_master\n# WHERE type='table'\n# ;\nengine.execute(\"SELECT * FROM Country LIMIT 10\").fetchall()\n\n[(1, 'Belgium'),\n (1729, 'England'),\n (4769, 'France'),\n (7809, 'Germany'),\n (10257, 'Italy'),\n (13274, 'Netherlands'),\n (15722, 'Poland'),\n (17642, 'Portugal'),\n (19694, 'Scotland'),\n (21518, 'Spain')]\n%%sql\nSELECT * \nFROM Match \nLIMIT 3;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\ncountry_id\nleague_id\nseason\nstage\ndate\nmatch_api_id\nhome_team_api_id\naway_team_api_id\nhome_team_goal\naway_team_goal\nhome_player_X1\nhome_player_X2\nhome_player_X3\nhome_player_X4\nhome_player_X5\nhome_player_X6\nhome_player_X7\nhome_player_X8\nhome_player_X9\nhome_player_X10\nhome_player_X11\naway_player_X1\naway_player_X2\naway_player_X3\naway_player_X4\naway_player_X5\naway_player_X6\naway_player_X7\naway_player_X8\naway_player_X9\naway_player_X10\naway_player_X11\nhome_player_Y1\nhome_player_Y2\nhome_player_Y3\nhome_player_Y4\nhome_player_Y5\nhome_player_Y6\nhome_player_Y7\nhome_player_Y8\nhome_player_Y9\nhome_player_Y10\nhome_player_Y11\naway_player_Y1\naway_player_Y2\naway_player_Y3\naway_player_Y4\naway_player_Y5\naway_player_Y6\naway_player_Y7\naway_player_Y8\naway_player_Y9\naway_player_Y10\naway_player_Y11\nhome_player_1\nhome_player_2\nhome_player_3\nhome_player_4\nhome_player_5\nhome_player_6\nhome_player_7\nhome_player_8\nhome_player_9\nhome_player_10\nhome_player_11\naway_player_1\naway_player_2\naway_player_3\naway_player_4\naway_player_5\naway_player_6\naway_player_7\naway_player_8\naway_player_9\naway_player_10\naway_player_11\ngoal\nshoton\nshotoff\nfoulcommit\ncard\ncross\ncorner\npossession\nB365H\nB365D\nB365A\nBWH\nBWD\nBWA\nIWH\nIWD\nIWA\nLBH\nLBD\nLBA\nPSH\nPSD\nPSA\nWHH\nWHD\nWHA\nSJH\nSJD\nSJA\nVCH\nVCD\nVCA\nGBH\nGBD\nGBA\nBSH\nBSD\nBSA\n\n\n1\n1\n1\n2008/2009\n1\n2008-08-17 00:00:00\n492473\n9987\n9993\n1\n1\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.73\n3.4\n5\n1.75\n3.35\n4.2\n1.85\n3.2\n3.5\n1.8\n3.3\n3.75\nNone\nNone\nNone\n1.7\n3.3\n4.33\n1.9\n3.3\n4\n1.65\n3.4\n4.5\n1.78\n3.25\n4\n1.73\n3.4\n4.2\n\n\n2\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492474\n10000\n9994\n0\n0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.95\n3.2\n3.6\n1.8\n3.3\n3.95\n1.9\n3.2\n3.5\n1.9\n3.2\n3.5\nNone\nNone\nNone\n1.83\n3.3\n3.6\n1.95\n3.3\n3.8\n2\n3.25\n3.25\n1.85\n3.25\n3.75\n1.91\n3.25\n3.6\n\n\n3\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492475\n9984\n8635\n0\n3\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2.38\n3.3\n2.75\n2.4\n3.3\n2.55\n2.6\n3.1\n2.3\n2.5\n3.2\n2.5\nNone\nNone\nNone\n2.5\n3.25\n2.4\n2.63\n3.3\n2.5\n2.35\n3.25\n2.65\n2.5\n3.2\n2.5\n2.3\n3.2\n2.75\n# %%sql\n# DROP TABLE IF EXISTS Team_table\n# CREATE TABLE Team_table AS\n# SELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\nsql_query = %sql SELECT * FROM Team_table LIMIT 10\ndf = sql_query.DataFrame()\ndf\n\n * sqlite:///database.sqlite\nDone.\ncountries = pd.read_sql(\"\"\"SELECT *\n                        FROM Country;\"\"\", connection)\ncountries.style.highlight_max()\n\n\n\n\n\n\nid\nname\n\n\n\n\n0\n1\nBelgium\n\n\n1\n1729\nEngland\n\n\n2\n4769\nFrance\n\n\n3\n7809\nGermany\n\n\n4\n10257\nItaly\n\n\n5\n13274\nNetherlands\n\n\n6\n15722\nPoland\n\n\n7\n17642\nPortugal\n\n\n8\n19694\nScotland\n\n\n9\n21518\nSpain\n\n\n10\n24558\nSwitzerland\n# leagues = pd.read_sql(\"\"\"SELECT *\n#                         FROM League\n#                         JOIN Country ON Country.id = League.country_id;\"\"\", connection)\n# leagues\n%%sql\nDROP TABLE IF EXISTS Match_Table;\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n%sql SELECT * FROM Match LIMIT 1;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\ncountry_id\nleague_id\nseason\nstage\ndate\nmatch_api_id\nhome_team_api_id\naway_team_api_id\nhome_team_goal\naway_team_goal\nhome_player_X1\nhome_player_X2\nhome_player_X3\nhome_player_X4\nhome_player_X5\nhome_player_X6\nhome_player_X7\nhome_player_X8\nhome_player_X9\nhome_player_X10\nhome_player_X11\naway_player_X1\naway_player_X2\naway_player_X3\naway_player_X4\naway_player_X5\naway_player_X6\naway_player_X7\naway_player_X8\naway_player_X9\naway_player_X10\naway_player_X11\nhome_player_Y1\nhome_player_Y2\nhome_player_Y3\nhome_player_Y4\nhome_player_Y5\nhome_player_Y6\nhome_player_Y7\nhome_player_Y8\nhome_player_Y9\nhome_player_Y10\nhome_player_Y11\naway_player_Y1\naway_player_Y2\naway_player_Y3\naway_player_Y4\naway_player_Y5\naway_player_Y6\naway_player_Y7\naway_player_Y8\naway_player_Y9\naway_player_Y10\naway_player_Y11\nhome_player_1\nhome_player_2\nhome_player_3\nhome_player_4\nhome_player_5\nhome_player_6\nhome_player_7\nhome_player_8\nhome_player_9\nhome_player_10\nhome_player_11\naway_player_1\naway_player_2\naway_player_3\naway_player_4\naway_player_5\naway_player_6\naway_player_7\naway_player_8\naway_player_9\naway_player_10\naway_player_11\ngoal\nshoton\nshotoff\nfoulcommit\ncard\ncross\ncorner\npossession\nB365H\nB365D\nB365A\nBWH\nBWD\nBWA\nIWH\nIWD\nIWA\nLBH\nLBD\nLBA\nPSH\nPSD\nPSA\nWHH\nWHD\nWHA\nSJH\nSJD\nSJA\nVCH\nVCD\nVCA\nGBH\nGBD\nGBA\nBSH\nBSD\nBSA\n\n\n1\n1\n1\n2008/2009\n1\n2008-08-17 00:00:00\n492473\n9987\n9993\n1\n1\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.73\n3.4\n5\n1.75\n3.35\n4.2\n1.85\n3.2\n3.5\n1.8\n3.3\n3.75\nNone\nNone\nNone\n1.7\n3.3\n4.33\n1.9\n3.3\n4\n1.65\n3.4\n4.5\n1.78\n3.25\n4\n1.73\n3.4\n4.2\n%sql SELECT * FROM Country LIMIT 1;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nname\n\n\n1\nBelgium\n%sql SELECT * FROM League LIMIT 1;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\ncountry_id\nname\n\n\n1\n1\nBelgium Jupiler League\n%%sql\nDROP TABLE IF EXISTS Match_df;\nCREATE TABLE Match_df AS\n\n\nSELECT Match.id, \n                                        Country.name AS country_name, \n                                        League.name AS league_name, \n                                        season, \n                                        stage, \n                                        date,\n                                        HT.team_long_name AS  home_team,\n                                        AT.team_long_name AS away_team,\n                                        home_team_goal, \n                                        away_team_goal                                        \n                                FROM Match\n                                JOIN Country on Country.id = Match.country_id\n                                JOIN League on League.id = Match.league_id\n                                LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id\n                                LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id\n                                ORDER by date\n;\n\n * sqlite:///database.sqlite\nDone.\nDone.\n\n\n[]\n%%sql\nSELECT COUNT(*) FROM Match_df;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nCOUNT(*)\n\n\n25979\nsql_query = %sql SELECT * FROM Match_df LIMIT 10\ndf = sql_query.DataFrame()\ndf\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\n\n\n\n\n0\n24559\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-18 00:00:00\nBSC Young Boys\nFC Basel\n1\n2\n\n\n1\n24560\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-19 00:00:00\nFC Aarau\nFC Sion\n3\n1\n\n\n2\n24561\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nFC Luzern\nFC Vaduz\n1\n2\n\n\n3\n24562\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nNeuchâtel Xamax\nFC Zürich\n1\n2\n\n\n4\n24613\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Basel\nGrasshopper Club Zürich\n1\n0\n\n\n5\n24614\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nAC Bellinzona\nNeuchâtel Xamax\n1\n2\n\n\n6\n24615\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Zürich\nFC Luzern\n1\n0\n\n\n7\n24616\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-24 00:00:00\nFC Sion\nBSC Young Boys\n2\n1\n\n\n8\n24617\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-24 00:00:00\nFC Vaduz\nFC Aarau\n0\n2\n\n\n9\n24668\nSwitzerland\nSwitzerland Super League\n2008/2009\n3\n2008-07-26 00:00:00\nFC Basel\nAC Bellinzona\n2\n0"
  },
  {
    "objectID": "posts/2020-10-31-sql_calls_in_jupyter_Soccer_explore.html#build-home-team-win-label-for-classification",
    "href": "posts/2020-10-31-sql_calls_in_jupyter_Soccer_explore.html#build-home-team-win-label-for-classification",
    "title": "Working with sqlite databases in Jupyter for Visualizing European Soccer Match Data",
    "section": "Build home team win label for classification",
    "text": "Build home team win label for classification\n\n%%sql\nDROP TABLE IF EXISTS Match_Wins;\nCREATE TABLE Match_Wins AS\n\n\nSELECT *\n, CASE WHEN home_team_goal &gt; away_team_goal\nTHEN 1\nELSE 0\nEND AS home_team_win\nFROM Match_df\n;\n\n * sqlite:///database.sqlite\nDone.\nDone.\n\n\n[]\n\n\n\nsql_query = %sql SELECT * FROM Match_Wins\ndf = sql_query.DataFrame()\ndf\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\nhome_team_win\n\n\n\n\n0\n24559\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-18 00:00:00\nBSC Young Boys\nFC Basel\n1\n2\n0\n\n\n1\n24560\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-19 00:00:00\nFC Aarau\nFC Sion\n3\n1\n1\n\n\n2\n24561\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nFC Luzern\nFC Vaduz\n1\n2\n0\n\n\n3\n24562\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nNeuchâtel Xamax\nFC Zürich\n1\n2\n0\n\n\n4\n24613\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Basel\nGrasshopper Club Zürich\n1\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n25974\n25945\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Basel\nGrasshopper Club Zürich\n0\n1\n0\n\n\n25975\n25946\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nLugano\nFC St. Gallen\n3\n0\n1\n\n\n25976\n25947\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Luzern\nFC Sion\n2\n2\n0\n\n\n25977\n25948\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Thun\nBSC Young Boys\n0\n3\n0\n\n\n25978\n25949\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Zürich\nFC Vaduz\n3\n1\n1\n\n\n\n\n25979 rows × 11 columns\n\n\n\n\nfrom dask import dataframe as dd \nddf = dd.from_pandas(df, npartitions=5)\n\n\nddf.head()\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\nhome_team_win\n\n\n\n\n0\n24559\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-18 00:00:00\nBSC Young Boys\nFC Basel\n1\n2\n0\n\n\n1\n24560\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-19 00:00:00\nFC Aarau\nFC Sion\n3\n1\n1\n\n\n2\n24561\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nFC Luzern\nFC Vaduz\n1\n2\n0\n\n\n3\n24562\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nNeuchâtel Xamax\nFC Zürich\n1\n2\n0\n\n\n4\n24613\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Basel\nGrasshopper Club Zürich\n1\n0\n1\n\n\n\n\n\n\n\n\ndf[\"home_team_goal\"] = df[\"home_team_goal\"].astype(float)\ndf[\"away_team_goal\"] = df[\"away_team_goal\"].astype(float)\ndf[\"stage\"] = df[\"stage\"].astype(float)\n\n\nfeat_list = [\n    \"home_team_goal\"\n    ,\"away_team_goal\"\n]\n\n\ntarget = ['home_team_win']\n\n\nX_train = ddf[feat_list].persist()\ny_train = ddf[target].persist()\n\n\nX_train.count().compute()\n\nhome_team_goal    25979\naway_team_goal    25979\ndtype: int64\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/12521/1\nDashboard: http://192.168.1.71:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask_ml.xgboost import XGBRegressor\n\nXGBR = XGBRegressor()\n\nXGBR_model = XGBR.fit(X_train,y_train)\n\n\n\n# from dask_ml.xgboost import XGBClassifier\n\n# XGBC = XGBClassifier()\n\n# XGBC_model = XGBC.fit(X_train,y_train)\n\n\nclient.close()\n\n\nX, y = df.iloc[:, 1:10], df[\"home_team_win\"]\nX\n\n\n\n\n\n\n\n\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\n\n\n\n\n0\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-18 00:00:00\nBSC Young Boys\nFC Basel\n1\n2\n\n\n1\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-19 00:00:00\nFC Aarau\nFC Sion\n3\n1\n\n\n2\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nFC Luzern\nFC Vaduz\n1\n2\n\n\n3\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nNeuchâtel Xamax\nFC Zürich\n1\n2\n\n\n4\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Basel\nGrasshopper Club Zürich\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n25974\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Basel\nGrasshopper Club Zürich\n0\n1\n\n\n25975\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nLugano\nFC St. Gallen\n3\n0\n\n\n25976\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Luzern\nFC Sion\n2\n2\n\n\n25977\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Thun\nBSC Young Boys\n0\n3\n\n\n25978\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Zürich\nFC Vaduz\n3\n1\n\n\n\n\n25979 rows × 9 columns\n\n\n\n\nX, y = ddf.iloc[:, 1:10], df[\"home_team_win\"]\nX\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\n\n\nnpartitions=5\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nobject\nobject\nobject\nint64\nobject\nobject\nobject\nint64\nint64\n\n\n5196\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20784\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n25978\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: getitem, 10 tasks\n\n\n\ny\n\n0    0\n1    1\n2    0\n3    0\n4    1\n5    0\n6    1\n7    1\n8    0\n9    1\nName: home_team_win, dtype: int64\n\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n\n# import xgboost\n\n# dtrain = xgboost.DMatrix(X_train, y_train)\n# dtest = xgboost.DMatrix(X_test, y_test)"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html",
    "href": "posts/2021-06-03-model-inspection.html",
    "title": "Model Inspection",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\n\nimport seaborn as sns\nsns.set_theme(font_scale=1.2)\nplt.rcParams['figure.figsize'] = [12, 8]\nplt.rcParams['savefig.bbox'] = 'tight'\nplt.rcParams[\"savefig.dpi\"] = 300\n\nsklearn.set_config(display='diagram')"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#load-the-dataset",
    "href": "posts/2021-06-03-model-inspection.html#load-the-dataset",
    "title": "Model Inspection",
    "section": "Load the dataset",
    "text": "Load the dataset\n\n# %load solutions/regression_example.py\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nX.head()\n\n\n\n\n\n\n\n\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nit\n\n\n\n\n4\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.000000\n1499110\n\n\n6\n66529.0\n2002\n3519.72\n38375\n0.0\n0.0\n0.000000\n2404936\n\n\n7\n52108.0\n2003\n3923.11\n36720\n0.0\n0.0\n0.000000\n2815820\n\n\n10\n279052.0\n2006\n6112.50\n139354\n0.0\n0.0\n0.324324\n5167300\n\n\n11\n178705.0\n2007\n7360.92\n299892\n0.0\n0.0\n0.324324\n7040099\n\n\n\n\n\n\n\n\ny.head()\n\n4      195580.0\n6      434149.0\n7      619201.0\n10    1457872.0\n11    2213991.0\nName: specific, dtype: float64\n\n\n\nInsert random data for demonstration\n\nimport numpy as np\n\nX = X.assign(ran_num=np.arange(0, X.shape[0]))\n\n\n\nSplit dataset\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#train-linear-model",
    "href": "posts/2021-06-03-model-inspection.html#train-linear-model",
    "title": "Model Inspection",
    "section": "Train linear model",
    "text": "Train linear model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import Ridge\nfrom sklearn.pipeline import Pipeline\n\nridge = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Ridge())\n])\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())])StandardScalerStandardScaler()RidgeRidge()\n\n\n\nridge.score(X_train, y_train)\n\n0.8843443502191103\n\n\n\nridge.score(X_test, y_test)\n\n0.7491370703502245"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#plot-coefficients",
    "href": "posts/2021-06-03-model-inspection.html#plot-coefficients",
    "title": "Model Inspection",
    "section": "Plot coefficients",
    "text": "Plot coefficients\nCoefficients represent the relationship between a feature and the target assuming that all other features remain constant.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndef plot_linear_coef(coefs, names, ax=None, sorted=False):\n    if ax is None:\n        fig, ax = plt.subplots()\n    coefs = pd.DataFrame(\n       coefs, columns=['Coefficients'],\n       index=names\n    )\n    \n    if sorted:\n        coefs = coefs.sort_values(by='Coefficients')\n\n    coefs.plot(kind='barh', ax=ax)\n    ax.axvline(x=0, color='.5')\n    return ax\n\nplot_linear_coef(ridge['reg'].coef_, names=X_train.columns, sorted=True);"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#coefficient-variability",
    "href": "posts/2021-06-03-model-inspection.html#coefficient-variability",
    "title": "Model Inspection",
    "section": "Coefficient variability",
    "text": "Coefficient variability\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import RepeatedKFold\n\n\nridges_cv = cross_validate(\n    ridge, X_train, y_train, cv=RepeatedKFold(n_splits=5, n_repeats=5),\n    return_estimator=True)\n\n\nridges_cv\n\n{'fit_time': array([0.00852752, 0.00879049, 0.00563025, 0.00589609, 0.00541282,\n        0.00482273, 0.00472617, 0.00440693, 0.00431228, 0.00409317,\n        0.00431299, 0.003896  , 0.00612736, 0.09311175, 0.00695705,\n        0.00576901, 0.00550413, 0.00539637, 0.00509334, 0.00491738,\n        0.00479674, 0.00459194, 0.00439835, 0.00426984, 0.00396895]),\n 'score_time': array([0.00384283, 0.00219274, 0.00248122, 0.00215101, 0.00219202,\n        0.00196052, 0.00188565, 0.00182557, 0.00175428, 0.00166225,\n        0.0016768 , 0.00160766, 0.00199437, 0.00305367, 0.00251174,\n        0.00237942, 0.00228405, 0.00211263, 0.00204182, 0.00194716,\n        0.00198007, 0.00189042, 0.00182319, 0.00166941, 0.00160313]),\n 'estimator': (Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())]),\n  Pipeline(steps=[('scale', StandardScaler()), ('reg', Ridge())])),\n 'test_score': array([0.78102813, 0.6260654 , 0.78600362, 0.74219093, 0.97805921,\n        0.63885999, 0.90069639, 0.86957882, 0.89608878, 0.97272764,\n        0.83761009, 0.93328631, 0.85460586, 0.47428742, 0.78822307,\n        0.8933257 , 0.80865875, 0.78604436, 0.66129305, 0.93062503,\n        0.81909785, 0.86437887, 0.65233286, 0.79389227, 0.96456357])}\n\n\n\nridge_coefs = pd.DataFrame(\n   [model['reg'].coef_ for model in ridges_cv['estimator']],\n   columns=X.columns\n)\n\n\nridge_coefs.head()\n\n\n\n\n\n\n\n\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nit\nran_num\n\n\n\n\n0\n366170.413056\n-53199.923796\n29027.500401\n103838.720536\n-38871.206032\n21826.179010\n18730.354007\n432388.097936\n15395.947631\n\n\n1\n278344.791911\n31229.815166\n105656.219289\n12157.657371\n-42445.111433\n33836.845100\n10283.384018\n490405.339232\n138.790062\n\n\n2\n370195.639332\n4058.337195\n180433.806468\n-20521.007552\n-46413.645489\n51399.811179\n-20177.524103\n315977.584466\n-43552.653962\n\n\n3\n344530.343098\n-33517.706292\n154739.154793\n-33164.713671\n-22366.886793\n-8720.784251\n37077.197240\n464446.608528\n-6371.954113\n\n\n4\n296527.214315\n-41401.308214\n78119.685147\n21683.387951\n-41251.611341\n41388.300696\n26437.090097\n418431.007311\n11901.614954\n\n\n\n\n\n\n\n\nPlotting the variability of the cofficients\n\nfig, ax = plt.subplots()\n_ = ax.boxplot(ridge_coefs, vert=False, labels=ridge_coefs.columns)\n\n\n\n\n\nfrom sklearn.linear_model import Lasso\nlasso = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Lasso(alpha=0.06))\n])\n\n\nlasso.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('scale', StandardScaler()), ('reg', Lasso(alpha=0.06))])StandardScalerStandardScaler()LassoLasso(alpha=0.06)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(18, 9))\nplot_linear_coef(lasso['reg'].coef_, names=X_train.columns, sorted=True, ax=ax1);\nplot_linear_coef(ridge['reg'].coef_, names=X_train.columns, sorted=True, ax=ax2);\n\n\n\n\n\nlasso_cvs = cross_validate(\n    lasso, X_train, y_train, return_estimator=True, cv=RepeatedKFold(n_splits=5, n_repeats=5)\n)\n\n\nlasso_coefs = pd.DataFrame(\n   [model['reg'].coef_ for model in lasso_cvs['estimator']],\n   columns=X.columns\n)\n\n\nfig, ax = plt.subplots()\n_ = ax.boxplot(lasso_coefs, vert=False, labels=ridge_coefs.columns)\n\n\n\n\n\n# %load solutions/03-ex01-solutions.py\nfrom sklearn.linear_model import Lasso\n\nlasso = Pipeline([\n    ('scale', StandardScaler()),\n    ('reg', Lasso(random_state=42, alpha=0.04))\n])\nlasso.fit(X_train, y_train)\n\nlasso.score(X_test, y_test)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\nplot_linear_coef(ridge['reg'].coef_, X_train.columns, ax=ax1)\nplot_linear_coef(lasso['reg'].coef_, X_train.columns, ax=ax2)\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#random-forest",
    "href": "posts/2021-06-03-model-inspection.html#random-forest",
    "title": "Model Inspection",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nrf.score(X_train, y_train)\n\n0.9711219647906122\n\n\n\nrf.score(X_test, y_test)\n\n0.7873539531176165\n\n\n\ndef plot_importances(importances, names, ax=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    indices = np.argsort(importances)\n    ax.barh(range(len(importances)), importances[indices])\n    ax.set(yticks=range(len(importances)),\n           yticklabels=np.array(names)[indices]);\n\n\nimportances = rf.feature_importances_\nplot_importances(importances, X_train.columns);\n\n\n\n\nPay attention to ran_num!"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#permutation-feature-importance",
    "href": "posts/2021-06-03-model-inspection.html#permutation-feature-importance",
    "title": "Model Inspection",
    "section": "Permutation Feature Importance",
    "text": "Permutation Feature Importance\n\nCan be used on the test data!\n\nfrom sklearn.inspection import permutation_importance\n\nrf_perm_results = permutation_importance(rf, X_test, y_test,\n                                        n_repeats=10, n_jobs=-1)\n\n\ndef plot_permutation_importance(perm_results, names, ax=None):\n    perm_sorted_idx = perm_results.importances_mean.argsort()\n    if ax is None:\n        fig, ax = plt.subplots()\n    _ = ax.boxplot(perm_results.importances[perm_sorted_idx].T, vert=False,\n                   labels=np.array(names)[perm_sorted_idx])\n    return ax\n\n\n_ = plot_permutation_importance(rf_perm_results, X_test.columns)\n\n\n\n\n\n\nLoad cancer dataset\n\n# %load solutions/classifier_example.py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7131\n           1       0.00      0.00      0.00       137\n\n    accuracy                           0.98      7268\n   macro avg       0.49      0.50      0.50      7268\nweighted avg       0.96      0.98      0.97      7268\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nrf.score(X_test, y_test)\n\n0.9808750687947165\n\n\n\n\nPermutation importance with random forest\n\nfrom sklearn.inspection import permutation_importance\n\nrf_result = permutation_importance(rf, X_train, y_train,\n                                   n_repeats=10, n_jobs=-1)\n\n\n\nTraining data\n\n_ = plot_permutation_importance(rf_result, X)\n\n/home/david/anaconda3/lib/python3.8/site-packages/matplotlib/text.py:1163: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n  if s != self._text:\n\n\n\n\n\n\nfrom scipy.stats import spearmanr\nfrom scipy.cluster import hierarchy\n\ncorr = spearmanr(X_train).correlation\ncorr_linkage = hierarchy.ward(corr)\ncorr_linkage\n\narray([[0.        , 1.        , 1.05218646, 2.        ],\n       [2.        , 5.        , 1.20582869, 3.        ],\n       [3.        , 4.        , 1.24728653, 2.        ],\n       [6.        , 7.        , 1.38884045, 5.        ]])\n\n\n\nfrom collections import defaultdict\n\ncluster_ids = hierarchy.fcluster(corr_linkage, 1, criterion='distance')\ncluster_id_to_feature_ids = defaultdict(list)\nfor idx, cluster_id in enumerate(cluster_ids):\n    cluster_id_to_feature_ids[cluster_id].append(idx)\nselected_features = [v[0] for v in cluster_id_to_feature_ids.values()]\n\nselected_features\n\n[0, 1, 2, 3, 4]\n\n\n\n\nFeature importance with selected features\n\nfrom sklearn.inspection import permutation_importance\n\nrf_sel_result = permutation_importance(\n    rf_sel, X_test, y_test, n_repeats=10, n_jobs=-1)\n\n\nfeatures_sel = data.feature_names[selected_features]\n_ = plot_permutation_importance(rf_sel_result, features_sel)"
  },
  {
    "objectID": "posts/2021-06-03-model-inspection.html#partial-dependence",
    "href": "posts/2021-06-03-model-inspection.html#partial-dependence",
    "title": "Model Inspection",
    "section": "Partial Dependence",
    "text": "Partial Dependence\n\nTrain a HistGradientBostingClassifer\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier \n\n\nhist = HistGradientBoostingClassifier(random_state=0)\nhist.fit(X_train, y_train)\n\nHistGradientBoostingClassifierHistGradientBoostingClassifier(random_state=0)\n\n\n\n# %load solutions/03-ex03-solutions.py\nfrom sklearn.datasets import load_boston\n\nboston = load_boston()\n\nX, y = boston.data, boston.target\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=0)\n\nfrom sklearn.ensemble import GradientBoostingRegressor\ngb = GradientBoostingRegressor(random_state=0)\n\ngb.fit(X_train, y_train)\n\ngb.score(X_train, y_train)\n\nplot_importances(gb.feature_importances_, boston.feature_names)\n\ngb_perm_results = permutation_importance(gb, X_test, y_test, n_repeats=10, n_jobs=-1)\n\nplot_permutation_importance(gb_perm_results, boston.feature_names)\n\nplot_partial_dependence(gb, X_test, features=[\"LSTAT\", \"RM\", \"DIS\", \"CRIM\"],\n                        feature_names=boston.feature_names, n_cols=2)\n\nplot_partial_dependence(gb, X_test, features=[('LSTAT', 'RM')],\n                        feature_names=boston.feature_names)\n\n&lt;sklearn.inspection._plot.partial_dependence.PartialDependenceDisplay at 0x7f0e9d226dc0&gt;"
  },
  {
    "objectID": "posts/2021-02-07-Portfolio-Optimization.html",
    "href": "posts/2021-02-07-Portfolio-Optimization.html",
    "title": "Portfolio Optimization",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\ndf = pd.read_csv('https://stocks-snp-500.herokuapp.com/stocks/index_stocks_table.csv?_size=max')\ndf.head()\n\ndf['Date'] = pd.to_datetime(df.Date)\ndf.index = pd.to_datetime(df.Date)\ndf2 = df.drop('Date', axis=1)\ndf3 = df2.drop('rowid', axis=1)\ndf3 = df3.drop('Russell_2000_stock', axis=1)\ndf3 = df3.drop('SP500_stock', axis=1)\ndf3\n\nstocks = df3\n# start = pd.to_datetime('2018-01-01')\n# end = pd.to_datetime('today')\n# import pandas as pd\n# import pandas_datareader.data as web\n# import datetime\n\n# MSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\n# MSFT_stock.head()\n\n# ZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\n# ZOOM_stock.head()\n\n# aapl_stock = web.DataReader('aapl', 'yahoo', start, end)\n# aapl_stock.head()\n\n# stock_df = pd.concat([df3, aapl_stock['Close'],ZOOM_stock['Close'],MSFT_stock['Close']],axis=1)\n# stock_df.columns = ['snp','aapl_stock','ZOOM_stock','MSFT_stock']\n\n# stock_df\n# stocks = pd.concat([df3, aapl,cisco,amzn],axis=1)\n# stocks.columns = ['snp','aapl','cisco','amzn']\n# stock_df\n# stocks = stock_df\nmean_daily_ret = stocks.pct_change(1).mean()\nmean_daily_ret\n\nFXAIX_stock    0.000580\nVRTTX_stock    0.000626\nFNCMX_stock    0.000963\nFSMAX_stock    0.000677\ndtype: float64\nstocks.pct_change(1).corr()\n\n\n\n\n\n\n\n\nFXAIX_stock\nVRTTX_stock\nFNCMX_stock\nFSMAX_stock\n\n\n\n\nFXAIX_stock\n1.000000\n0.996178\n0.950284\n0.926342\n\n\nVRTTX_stock\n0.996178\n1.000000\n0.950949\n0.944306\n\n\nFNCMX_stock\n0.950284\n0.950949\n1.000000\n0.891515\n\n\nFSMAX_stock\n0.926342\n0.944306\n0.891515\n1.000000"
  },
  {
    "objectID": "posts/2021-02-07-Portfolio-Optimization.html#log-returns-vs-arithmetic-returns",
    "href": "posts/2021-02-07-Portfolio-Optimization.html#log-returns-vs-arithmetic-returns",
    "title": "Portfolio Optimization",
    "section": "Log Returns vs Arithmetic Returns",
    "text": "Log Returns vs Arithmetic Returns\nWe will now switch over to using log returns instead of arithmetic returns, for many of our use cases they are almost the same,but most technical analyses require detrending/normalizing the time series and using log returns is a nice way to do that. Log returns are convenient to work with in many of the algorithms we will encounter.\nFor a full analysis of why we use log returns, check this great article.\n\nlog_ret = np.log(stocks/stocks.shift(1))\nlog_ret.head()\n\n\n\n\n\n\n\n\nFXAIX_stock\nVRTTX_stock\nFNCMX_stock\nFSMAX_stock\n\n\nDate\n\n\n\n\n\n\n\n\n2018-01-02\nNaN\nNaN\nNaN\nNaN\n\n\n2018-01-03\n0.006347\n0.005843\n0.008336\n0.003353\n\n\n2018-01-04\n0.004209\n0.003780\n0.001831\n0.001752\n\n\n2018-01-05\n0.007011\n0.006406\n0.008252\n0.003811\n\n\n2018-01-08\n0.001667\n0.001852\n0.002877\n0.002849\n\n\n\n\n\n\n\n\nlog_ret.hist(bins=100,figsize=(12,6));\nplt.tight_layout()\n\n\n\n\n\nlog_ret.describe().transpose()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nFXAIX_stock\n784.0\n0.000474\n0.014648\n-0.127150\n-0.003955\n0.001085\n0.006955\n0.089894\n\n\nVRTTX_stock\n784.0\n0.000517\n0.014807\n-0.131282\n-0.004037\n0.001219\n0.007279\n0.090501\n\n\nFNCMX_stock\n784.0\n0.000835\n0.016046\n-0.131396\n-0.004628\n0.001768\n0.008337\n0.089514\n\n\nFSMAX_stock\n784.0\n0.000537\n0.016836\n-0.154994\n-0.004773\n0.001653\n0.008149\n0.095016\n\n\n\n\n\n\n\n\nlog_ret.mean() * 252\n\nFXAIX_stock    0.119352\nVRTTX_stock    0.130180\nFNCMX_stock    0.210329\nFSMAX_stock    0.135276\ndtype: float64\n\n\n\n# Compute pairwise covariance of columns\nlog_ret.cov()\n\n\n\n\n\n\n\n\nFXAIX_stock\nVRTTX_stock\nFNCMX_stock\nFSMAX_stock\n\n\n\n\nFXAIX_stock\n0.000215\n0.000216\n0.000223\n0.000229\n\n\nVRTTX_stock\n0.000216\n0.000219\n0.000226\n0.000236\n\n\nFNCMX_stock\n0.000223\n0.000226\n0.000257\n0.000241\n\n\nFSMAX_stock\n0.000229\n0.000236\n0.000241\n0.000283\n\n\n\n\n\n\n\n\nlog_ret.cov()*252 # multiply by days\n\n\n\n\n\n\n\n\nFXAIX_stock\nVRTTX_stock\nFNCMX_stock\nFSMAX_stock\n\n\n\n\nFXAIX_stock\n0.054072\n0.054451\n0.056310\n0.057654\n\n\nVRTTX_stock\n0.054451\n0.055251\n0.056959\n0.059382\n\n\nFNCMX_stock\n0.056310\n0.056959\n0.064882\n0.060762\n\n\nFSMAX_stock\n0.057654\n0.059382\n0.060762\n0.071430"
  },
  {
    "objectID": "posts/2021-02-07-Portfolio-Optimization.html#single-run-for-some-random-allocation",
    "href": "posts/2021-02-07-Portfolio-Optimization.html#single-run-for-some-random-allocation",
    "title": "Portfolio Optimization",
    "section": "Single Run for Some Random Allocation",
    "text": "Single Run for Some Random Allocation\n\n# Set seed (optional)\nnp.random.seed(101)\n\n# Stock Columns\nprint('Stocks')\nprint(stocks.columns)\nprint('\\n')\n\n# Create Random Weights\nprint('Creating Random Weights')\nweights = np.array(np.random.random(4))\nprint(weights)\nprint('\\n')\n\n# Rebalance Weights\nprint('Rebalance to sum to 1.0')\nweights = weights / np.sum(weights)\nprint(weights)\nprint('\\n')\n\n# Expected Return\nprint('Expected Portfolio Return')\nexp_ret = np.sum(log_ret.mean() * weights) *252\nprint(exp_ret)\nprint('\\n')\n\n# Expected Variance\nprint('Expected Volatility')\nexp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\nprint(exp_vol)\nprint('\\n')\n\n# Sharpe Ratio\nSR = exp_ret/exp_vol\nprint('Sharpe Ratio')\nprint(SR)\n\nStocks\nIndex(['FXAIX_stock', 'VRTTX_stock', 'FNCMX_stock', 'FSMAX_stock'], dtype='object')\n\n\nCreating Random Weights\n[0.51639863 0.57066759 0.02847423 0.17152166]\n\n\nRebalance to sum to 1.0\n[0.40122278 0.44338777 0.02212343 0.13326603]\n\n\nExpected Portfolio Return\n0.12828800153609307\n\n\nExpected Volatility\n0.23642865366590426\n\n\nSharpe Ratio\n0.5426076727458592\n\n\n\nnum_ports = 15000\n\nall_weights = np.zeros((num_ports,len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    # Create Random Weights\n    weights = np.array(np.random.random(4))\n\n    # Rebalance Weights\n    weights = weights / np.sum(weights)\n    \n    # Save Weights\n    all_weights[ind,:] = weights\n\n    # Expected Return\n    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n\n    # Expected Variance\n    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n\n    # Sharpe Ratio\n    sharpe_arr[ind] = ret_arr[ind]/vol_arr[ind]\n\n\nsharpe_arr.max()\n\n0.8049153800614341\n\n\n\nsharpe_arr.argmax()\n\n9077\n\n\n\nall_weights[1419,:]\n\narray([0.26188068, 0.20759516, 0.00110226, 0.5294219 ])\n\n\n\nmax_sr_ret = ret_arr[1419]\nmax_sr_vol = vol_arr[1419]"
  },
  {
    "objectID": "posts/2021-02-07-Portfolio-Optimization.html#plotting-the-data",
    "href": "posts/2021-02-07-Portfolio-Optimization.html#plotting-the-data",
    "title": "Portfolio Optimization",
    "section": "Plotting the data",
    "text": "Plotting the data\n\nplt.figure(figsize=(12,8))\nplt.scatter(vol_arr,ret_arr,c=sharpe_arr,cmap='plasma')\nplt.colorbar(label='Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n# Add red dot for max SR\nplt.scatter(max_sr_vol,max_sr_ret,c='red',s=50,edgecolors='black')\n\n&lt;matplotlib.collections.PathCollection at 0x7f3f7467de50&gt;\n\n\n\n\n\n\ndef get_ret_vol_sr(weights):\n    \"\"\"\n    Takes in weights, returns array or return,volatility, sharpe ratio\n    \"\"\"\n    weights = np.array(weights)\n    ret = np.sum(log_ret.mean() * weights) * 252\n    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n    sr = ret/vol\n    return np.array([ret,vol,sr])\n\n\nfrom scipy.optimize import minimize\n\nTo fully understand all the parameters, check out: https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.minimize.html\n\n#help(minimize)\n\nOptimization works as a minimization function, since we actually want to maximize the Sharpe Ratio, we will need to turn it negative so we can minimize the negative sharpe (same as maximizing the postive sharpe)\n\ndef neg_sharpe(weights):\n    return  get_ret_vol_sr(weights)[2] * -1\n\n\n# Contraints\ndef check_sum(weights):\n    '''\n    Returns 0 if sum of weights is 1.0\n    '''\n    return np.sum(weights) - 1\n\n\n# By convention of minimize function it should be a function that returns zero for conditions\ncons = ({'type':'eq','fun': check_sum})\n\n\n# 0-1 bounds for each weight\nbounds = ((0, 1), (0, 1), (0, 1), (0, 1))\n\n\n# Initial Guess (equal distribution)\ninit_guess = [0.25,0.25,0.25,0.25]\n\n\n# Sequential Least SQuares Programming (SLSQP).\nopt_results = minimize(neg_sharpe,init_guess,method='SLSQP',bounds=bounds,constraints=cons)\n\n\nopt_results\n\n     fun: -0.8257252387825378\n     jac: array([ 2.48069711e-01,  2.13815376e-01, -7.45058060e-09,  2.42212258e-01])\n message: 'Optimization terminated successfully'\n    nfev: 15\n     nit: 3\n    njev: 3\n  status: 0\n success: True\n       x: array([0.00000000e+00, 2.77555756e-16, 1.00000000e+00, 0.00000000e+00])\n\n\n\nopt_results.x\n\narray([0.00000000e+00, 2.77555756e-16, 1.00000000e+00, 0.00000000e+00])\n\n\n\nget_ret_vol_sr(opt_results.x)\n\narray([0.21032899, 0.25472031, 0.82572524])"
  },
  {
    "objectID": "posts/2020-11-11-Pytorch.html#credit-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "href": "posts/2020-11-11-Pytorch.html#credit-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "title": "Pytorch",
    "section": "Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning",
    "text": "Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nfrom torch.autograd import Variable\nfrom sklearn import preprocessing\n\n\nclass Net(nn.Module):\n    def __init__(self, in_count, out_count):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(in_count, 50)\n        self.fc2 = nn.Linear(50, 25)\n        self.fc3 = nn.Linear(25, out_count)\n        self.softmax = nn.Softmax(dim=1)\n\n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        x = self.fc3(x)\n        return self.softmax(x)\n\n    \ndf = pd.read_csv(\n    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", \n    na_values=['NA', '?'])\n\nle = preprocessing.LabelEncoder()\n\nx = df[['sepal_l', 'sepal_w', 'petal_l', 'petal_w']].values\ny = le.fit_transform(df['species'])\nclasses = le.classes_\n\nx_train, x_test, y_train, y_test = train_test_split(    \n    x, y, test_size=0.25, random_state=42)\n\nx_train = Variable(torch.Tensor(x_train).float())\nx_test = Variable(torch.Tensor(x_test).float())\ny_train = Variable(torch.Tensor(y_train).long())\ny_test = Variable(torch.Tensor(y_test).long())\n\nnet = Net(x.shape[1],len(classes))\n\ncriterion = nn.CrossEntropyLoss()# cross entropy loss\n\n#optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\noptimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n\nfor epoch in range(1000):\n    optimizer.zero_grad()\n    out = net(x_train)\n    loss = criterion(out, y_train)\n    loss.backward()\n    optimizer.step()\n    \n    if epoch % 100 == 0:\n        print(f\"Epoch {epoch}, loss: {loss.item()}\")\n\npred_prob = net(x_test)\n_, pred = torch.max(pred_prob, 1)\n\nEpoch 0, loss: 1.0931435823440552\nEpoch 100, loss: 0.5726262331008911\nEpoch 200, loss: 0.5706601142883301\nEpoch 300, loss: 0.5699276328086853\nEpoch 400, loss: 0.5696418881416321\nEpoch 500, loss: 0.5695095062255859\nEpoch 600, loss: 0.5694397687911987\nEpoch 700, loss: 0.5693992376327515\nEpoch 800, loss: 0.5693740844726562\nEpoch 900, loss: 0.5693574547767639\n\n\n\ncorrect = accuracy_score(y_test,pred)\nprint(f\"Accuracy: {correct}\")\n\nAccuracy: 0.9736842105263158"
  },
  {
    "objectID": "posts/2020-11-04-Webscraping_Example.html",
    "href": "posts/2020-11-04-Webscraping_Example.html",
    "title": "Webscraping Text and Images with BeautifulSoup example",
    "section": "",
    "text": "This notebook code is from the app found here: https://github.com/kenichinakanishi/houseplant_classifier/\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom urllib.request import Request, urlopen\nfrom bs4 import BeautifulSoup\n\ndef getHTMLContent(link):\n    html = urlopen(link)\n    soup = BeautifulSoup(html, 'html.parser')\n    return soup\n\n\nreq = Request('https://www.aspca.org/pet-care/animal-poison-control/cats-plant-list', headers={'User-Agent': 'Mozilla/5.0'})\nwebpage = urlopen(req).read()\n# Soupify the webpage\nsoup = BeautifulSoup(webpage, 'lxml')       \n# Search through the parse tree to get all the content from the table         \ncontent_list = soup.find_all('span')[7:-4]       \n# Put it in a dataframe for further processing\ndf_cats = pd.DataFrame(content_list)           \n\n/home/gao/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py:305: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  values = np.array([convert(v) for v in values])\n\n\n\n# Clean up the strings\ndf_cats[0] = df_cats[0].apply(lambda x: str(x).split('&gt;')[1][:-3])\ndf_cats[4] = df_cats[4].apply(lambda x: str(x).split('&gt;')[1][:-3])\ndf_cats[1] = df_cats[1].apply(lambda x: str(x).split('(')[1][0:-4])\n# Get rid of useless columns and rename the columns\ndf_cats = df_cats.drop(columns=[2,3,5,6]).rename(columns = {0:'Name',1:'Alternative Names',4:'Scientific Name',7:'Family'})\n# Separate toxic and non-toxic plants\ndf_cats['Toxic to Cats'] = True\nfirst_nontoxic_cats = [index for index in df_cats[df_cats['Name'].str.startswith('A')].index if index&gt;100][0]\ndf_cats.loc[first_nontoxic_cats:,'Toxic to Cats'] = False\n\n\ndf_cats\n\n\n\n\n\n\n\n\nName\nAlternative Names\nScientific Name\nFamily\nToxic to Cats\n\n\n\n\n0\nAdam-and-Eve\nArum, Lord-and-Ladies, Wake Robin, Starch Root...\nArum maculatum\nAraceae\nTrue\n\n\n1\nAfrican Wonder Tree\n\nRicinus communis\n\nTrue\n\n\n2\nAlocasia\nElephant's Ear\nAlocasia spp.\nAraceae\nTrue\n\n\n3\nAloe\n\nAloe vera\nLiliaceae\nTrue\n\n\n4\nAmaryllis\nMany, including: Belladonna lily, Saint Joseph...\nAmaryllis spp.\nAmaryllidaceae\nTrue\n\n\n...\n...\n...\n...\n...\n...\n\n\n980\nYellowrocket\n\nBarbarea vulgaris\nBrassicaceae\nFalse\n\n\n981\nYorba Linda\n\nPeperomia rotundifolia\nPiperaceae\nFalse\n\n\n982\nZebra Haworthia\n\nHaworthia fasciata\nLiliaceae\nFalse\n\n\n983\nZinnia\n\nZinnia species\nAsteraceae\nFalse\n\n\n984\nZucchini Squash\n\nCucurbia pepo cv zucchini\nCucurbitaceae\nFalse\n\n\n\n\n985 rows × 5 columns\n\n\n\n\nreq = Request('https://www.aspca.org/pet-care/animal-poison-control/dogs-plant-list', headers={'User-Agent': 'Mozilla/5.0'})\nwebpage = urlopen(req).read()\nsoup = BeautifulSoup(webpage, 'lxml')                 # soupify the webpage\ncontent_list = soup.find_all('span')[7:-4]            # Get all the content from the table\ndf_dogs = pd.DataFrame(content_list)                  # Put it in a dataframe for processing\n\n/home/gao/anaconda3/lib/python3.7/site-packages/pandas/core/internals/construction.py:305: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n  values = np.array([convert(v) for v in values])\n\n\n\n# Clean up the strings\ndf_dogs[0] = df_dogs[0].apply(lambda x: str(x).split('&gt;')[1][:-3])\ndf_dogs[4] = df_dogs[4].apply(lambda x: str(x).split('&gt;')[1][:-3])\ndf_dogs[1] = df_dogs[1].apply(lambda x: str(x).split('(')[1][0:-4])\n# Get rid of useless columns and rename the columns\ndf_dogs = df_dogs.drop(columns=[2,3,5,6]).rename(columns = {0:'Name',1:'Alternative Names',4:'Scientific Name',7:'Family'})\n# Separate toxic and non-toxic plants\ndf_dogs['Toxic to Dogs'] = True\nfirst_nontoxic_dogs = [index for index in df_dogs[df_dogs['Name'].str.startswith('A')].index if index&gt;100][0]\ndf_dogs.loc[first_nontoxic_dogs:,'Toxic to Dogs'] = False\n\n\n# Merge dataframes into one, outer merge used to retain values that only exist on one side\ndf_catsdogs = df_dogs.merge(df_cats, how='outer', on=['Name','Alternative Names','Scientific Name','Family'])\ndf_catsdogs = df_catsdogs.fillna('Unknown')\naspca_df = df_catsdogs.copy()\n# Assume same toxicity for dogs and cats if unknown\naspca_df['Toxic to Cats'] = aspca_df.apply(lambda x: x['Toxic to Dogs'] if (x['Toxic to Cats'] == 'Unknown') else x['Toxic to Cats'], axis=1)\naspca_df['Toxic to Dogs'] = aspca_df.apply(lambda x: x['Toxic to Cats'] if (x['Toxic to Dogs'] == 'Unknown') else x['Toxic to Dogs'], axis=1)\n\n\n# Merge dataframes into one, outer merge used to retain values that only exist on one side\ndf_catsdogs = df_dogs.merge(df_cats, how='outer', on=['Name','Alternative Names','Scientific Name','Family'])\ndf_catsdogs = df_catsdogs.fillna('Unknown')\naspca_df = df_catsdogs.copy()\n# Assume same toxicity for dogs and cats if unknown\naspca_df['Toxic to Cats'] = aspca_df.apply(lambda x: x['Toxic to Dogs'] if (x['Toxic to Cats'] == 'Unknown') else x['Toxic to Cats'], axis=1)\naspca_df['Toxic to Dogs'] = aspca_df.apply(lambda x: x['Toxic to Cats'] if (x['Toxic to Dogs'] == 'Unknown') else x['Toxic to Dogs'], axis=1)\n\n\naspca_df.sample(10)\n\n\n\n\n\n\n\n\nName\nAlternative Names\nScientific Name\nFamily\nToxic to Dogs\nToxic to Cats\n\n\n\n\n810\nPink Splash\nFlamingo Plant, Polka Dot Plant, Measles Plant...\nHypoestes phyllostachya\nAcanthaceae\nFalse\nFalse\n\n\n120\nEnglish Ivy\nBranching Ivy, Glacier Ivy, Needlepoint Ivy, S...\nHedera helix\nAraliaceae\nTrue\nTrue\n\n\n564\nCrape Myrtle\nCrepe Myrtle\nLagerstroemia indica\nLythraceae\nFalse\nFalse\n\n\n201\nJapanese Yew\nEnglish Yew, Western Yew, Pacific Yew, Anglo-J...\nTaxus sp.\nTaxaceae\nTrue\nTrue\n\n\n635\nGiant Touch-Me-Not\nBuzzy Lizzie, Impatience Plant, Patient Lucy, ...\nImpatiens spp.\nBalsaminaceae\nFalse\nFalse\n\n\n92\nCowbane\nWater Hemlock, Poison Parsnip\nCicuta species\nApiaceae\nTrue\nTrue\n\n\n277\nOrnamental Pepper\nNatal Cherry, Winter Cherry, Jerusalem Cherry\nSolanum pseudocapsicum\nSolanaceae\nTrue\nTrue\n\n\n513\nCarrot Fern\n\nOnychium japonica\nPolypodiaceae\nFalse\nFalse\n\n\n712\nLeather Peperomia\n\nPeperomia crassifolia\nPiperaceae\nFalse\nFalse\n\n\n493\nCalifornia Pitcher Plant\nCobra Orchid, Cobra Plant, Cobra Lily, Chrysam...\nDarlingtonia californica\nSarraceniaceae\nFalse\nFalse\n\n\n\n\n\n\n\n\naspca_df = aspca_df.drop_duplicates('Scientific Name') # Get rid of duplicates\naspca_df = aspca_df.reset_index(drop=True).sort_index()   # Reset and sort index\n\n\naspca_df = aspca_df.drop(aspca_df[aspca_df['Scientific Name'].isin(['','NONE LISTED'])].index,axis=0).reset_index(drop=True).sort_index()    # Fix mistakes in database\n\n\n# Ensure proper punctuation for each scientific name.\ndef normalize_capitalization(x):\n  first_word, rest = x.split()[0], x.split()[1:]\n  first_word = [first_word.capitalize()]\n  rest = [word.lower() for word in rest]\n  return ' '.join(first_word+rest)\n\n# Clean up repeated species that have different names\ndef species_normalizer(word):\n  if word.split()[-1] in ['sp','species','spp','sp.','spp.']:\n    word = ''.join(word.split()[:-1])\n  return word\n\n# Remove cv from names, as it is an outdated way of referring to cultivars\ndef cv_remover(word):\n  if 'cv' in word:\n    word = word.replace(' cv ',' ')\n  return word\n\n# Remove var. from names\ndef var_remover(word):\n  if 'var' in word:\n    word = word.replace(' var. ',' ')\n  return word\n\n# Apply each of the functions\naspca_df['Scientific Name'] = aspca_df['Scientific Name'].apply(normalize_capitalization)\naspca_df['Scientific Name'] = aspca_df['Scientific Name'].apply(species_normalizer)\naspca_df['Scientific Name'] = aspca_df['Scientific Name'].apply(cv_remover)\naspca_df['Scientific Name'] = aspca_df['Scientific Name'].apply(var_remover)\n\n# Remove special characters\naspca_df['Scientific Name'] = aspca_df['Scientific Name'].apply(lambda x: ''.join([character for character in x if character.isalnum() or character.isspace()]))\n\n# Reset dataframe for further processing\naspca_df = aspca_df.sort_values('Scientific Name').drop_duplicates('Scientific Name')\naspca_df = aspca_df.reset_index(drop=True).sort_index()\n\n\naspca_df.sample(10)\n\n\n\n\n\n\n\n\nName\nAlternative Names\nScientific Name\nFamily\nToxic to Dogs\nToxic to Cats\n\n\n\n\n108\nAmerican Bittersweet\nBittersweet, Waxwork, Shrubby Bittersweet, Fal...\nCelastrus scandens\nCelastraceae\nTrue\nTrue\n\n\n530\nPacific Yew\nEnglish Yew, Western Yew, Japanese Yew, Anglo-...\nTaxus brevifolia\nTaxaceae\nTrue\nTrue\n\n\n467\nPie Plant\nRhubarb\nRheum rhabarbarium\nPolygonaceae\nTrue\nTrue\n\n\n164\nPheasant Plant\nZebra Plant\nCryptanthus zonatus\nBromeliaceae\nFalse\nFalse\n\n\n452\nPrimrose\n\nPrimula vulgaris\nPrimulaceae\nTrue\nTrue\n\n\n506\nJackson Brier\n\nSmilax lanceolata\nLiliaceae\nFalse\nFalse\n\n\n407\nIvy Peperomia\nPlantinum Peperomia, Silver leaf Peperomia, Iv...\nPeperomia griseoargentea\nPiperaceae\nFalse\nFalse\n\n\n147\nPoison Hemlock\nPoison Parsley, Spotted Hemlock, Winter Fern, ...\nConium maculatum\nUmbelliferae\nTrue\nTrue\n\n\n351\nCardinal Flower\nLobelia, Indian Pink\nLobelia cardinalis\nCampanulaceae\nTrue\nTrue\n\n\n236\nPink Brocade\n\nEpiscia cultivar\nGesneriaceae\nFalse\nFalse\n\n\n\n\n\n\n\n\nuse_cols = ['scientificName','taxonRank','family','genus','taxonomicStatus','taxonID', 'acceptedNameUsageID']\nwfo_df = pd.read_csv('../classification.txt', sep='\\t', lineterminator='\\n', usecols=use_cols)\nwfo_df = wfo_df.sort_values('taxonomicStatus')\n\n/home/gao/.local/lib/python3.7/site-packages/IPython/core/interactiveshell.py:3146: DtypeWarning: Columns (10) have mixed types.Specify dtype option on import or set low_memory=False.\n  interactivity=interactivity, compiler=compiler, result=result)\n\n\n\nwfo_df.sample(10)\n\n\n\n\n\n\n\n\ntaxonID\nscientificName\ntaxonRank\nfamily\ngenus\ntaxonomicStatus\nacceptedNameUsageID\n\n\n\n\n796160\nwfo-0000798814\nPeridium oblongifolium\nSPECIES\nPeraceae\nPeridium\nSynonym\nwfo-0000267144\n\n\n180708\nwfo-0000180970\nCracca smallii\nSPECIES\nFabaceae\nCracca\nSynonym\nwfo-0000178756\n\n\n911945\nwfo-0000914633\nThinopyrum turcicum\nSPECIES\nPoaceae\nThinopyrum\nSynonym\nwfo-0000866236\n\n\n167159\nwfo-0000167369\nIndigofera cinerea\nSPECIES\nFabaceae\nIndigofera\nSynonym\nwfo-0000173646\n\n\n642316\nwfo-0000644639\nDiaphanoptera khorasanica\nSPECIES\nCaryophyllaceae\nDiaphanoptera\nAccepted\nNaN\n\n\n464965\nwfo-0000466716\nPhyllocyclus minutiflorus\nSPECIES\nGentianaceae\nPhyllocyclus\nDoubtful\nNaN\n\n\n740337\nwfo-0000742945\nDaphne pseudomezereum var. koreana\nVARIETY\nThymelaeaceae\nDaphne\nSynonym\nwfo-0000637684\n\n\n868404\nwfo-0000871073\nFestuca montis-aurei\nSPECIES\nPoaceae\nFestuca\nSynonym\nwfo-0000869683\n\n\n186218\nwfo-0000186502\nLotononis curvicarpa\nSPECIES\nFabaceae\nLotononis\nAccepted\nNaN\n\n\n552490\nwfo-0000554468\nSpecklinia casualis\nSPECIES\nOrchidaceae\nSpecklinia\nSynonym\nwfo-0000339564\n\n\n\n\n\n\n\n\n# Don't need this column, we trust the WFO database more\naspca_df.drop('Family', axis=1, inplace=True)\n# Merge dataframes together to get trusted info\naspca_df = aspca_df.merge(wfo_df, how = 'left', left_on = ['Scientific Name'], right_on = ['scientificName'])\n# Sort by taxonomicStatus and drop duplicates keeping the first - keeping accepted names as priority\naspca_df = aspca_df.sort_values('taxonomicStatus').drop_duplicates('Scientific Name', keep='first').reset_index(drop=True)\n# Fill NaN's with Unknown\naspca_df = aspca_df.fillna('Unknown')\n\n\n# Clean up and deal with scientific names that are unknown, due to misspellings or otherwise.\naspca_df = aspca_df.sort_values('taxonomicStatus').drop_duplicates('Scientific Name', keep='first').reset_index(drop=True)\nunknown_idx = aspca_df[aspca_df.taxonomicStatus == 'Unknown'].index\nprint(len(unknown_idx))\n\n101\n\n\n\ndef get_closest_name(unknown_name, name_df = wfo_df, name_col = 'scientificName', threshold=0.9, verbose=False):\n  \"\"\" Matches an 'unknown_name' against accepted names in a 'name_df'. Will return names that are above a 'threshold' of closeness. \n\n  Parameters\n  ----------\n  unknown_name: str\n    Name we want to match against accepted names. \n  name_df: DataFrame\n    DataFrame containing accepted names.\n  name_col: str, name of name_df column \n    DataFrame column containing accepted names.\n  threshold: int\n    How closely does the unknown_name need to match with the accepted name.\n    If above this threshold, the name is added to a dictionary of possible names.\n  verbose: bool\n    Should the function print the entire list of possible names. \n\n  Returns:\n  ----------\n  str\n    Closest name to 'unknown_name' that was above the given 'threshold'.\n  \"\"\"\n  import operator\n  from difflib import SequenceMatcher\n  def similar(a, b):\n      return SequenceMatcher(None, a, b).ratio()\n  poss_names = {}\n  # Only look through entries with the same first letter to save time\n  for true_sciname in name_df[name_df[name_col].str.startswith(unknown_name[0])][name_col].values:\n    similar_score = similar(unknown_name, true_sciname)\n    if similar_score&gt;threshold:\n      poss_names[true_sciname]=similar_score\n  # If the dict is empty\n  if verbose == True:\n    print(poss_names)\n  if not bool(poss_names):\n    print(f'No names close enough to {unknown_name}.')\n    return ''\n  else:\n    print(f'{unknown_name} is closest to {max(poss_names.items(), key=operator.itemgetter(1))[0]}, with a score of {max(poss_names.items(), key=operator.itemgetter(1))[1]:.2f}')\n    return max(poss_names.items(), key=operator.itemgetter(1))[0]\n\n\ndef fix_name(unknown_name, true_name):\n  \"\"\" Fixes the aspca_df entries according to the accepted wfo_df entry.\n\n  Parameters\n  ----------\n  unknown_name: str\n    Name we want to fix. \n  true_name: DataFrame\n    Accepted name to use.\n  \"\"\"\n  # Get the series we're looking to change\n  unknown_data = aspca_df[aspca_df['Scientific Name'] == unknown_name]\n  # Grab accepted data from wfo database based on ID lookup\n  true_data = wfo_df[wfo_df['scientificName'] == true_name]\n  true_sciname = true_data.loc[:,'scientificName'].values[0]\n  true_family = true_data.loc[:,'family'].values[0]\n  true_genus = true_data.loc[:,'genus'].values[0]\n  true_taxonomicStatus = true_data.loc[:,'taxonomicStatus'].values[0]\n  # Change scientific name, family, genus and taxonomic status to accepted versions\n  aspca_df.iloc[unknown_data.index,2] = true_sciname\n  aspca_df.iloc[unknown_data.index,8] = true_family\n  aspca_df.iloc[unknown_data.index,9] = true_genus\n  aspca_df.iloc[unknown_data.index,10] = true_taxonomicStatus\n\n\nunknown_idx = aspca_df[aspca_df.taxonomicStatus == 'Unknown'].index\nprint(f'{len(unknown_idx)} plants currently cannot be matched.')\nfrom tqdm.notebook import tqdm\nfor i in tqdm(unknown_idx):\n  unknown_name = aspca_df.iloc[i,2]\n  closest_name = get_closest_name(unknown_name)\n  if closest_name == '':\n    continue\n  fix_name(unknown_name,closest_name)\n\n101 plants currently cannot be matched.\nMalus sylvestrus is closest to Malus sylvestris, with a score of 0.94\nNo names close enough to Maranta insignis.\nNo names close enough to Miltonia roezlii alba.\nNo names close enough to Neoregalia.\nNo names close enough to Nephrolepis exalta bostoniensis.\nNephrolepsis exalta is closest to Nephrolepis exaltata, with a score of 0.92\nNo names close enough to Nephrolepsis cordifolia duffii.\nNo names close enough to Lilium orientalis.\nNo names close enough to Nephrolepsis cordifolia plumosa.\nNephrolepis exalta is closest to Nephrolepis exaltata, with a score of 0.95\nNo names close enough to Lilium asiatica.\nHosta plataginea is closest to Hosta plantaginea, with a score of 0.97\nNo names close enough to Lampranthus piquet.\nKalmia poliifolia is closest to Kalmia polifolia, with a score of 0.97\nKalmia augustifolia is closest to Kalmia angustifolia, with a score of 0.95\nJasminium is closest to Jasminum, with a score of 0.94\nHoya publcalyx is closest to Hoya pubicalyx, with a score of 0.93\nNo names close enough to Hoya carnosa krinkle kurl.\nNo names close enough to Hemigraphis exotica.\nGynura aurantica is closest to Gynura aurantiaca, with a score of 0.97\nNo names close enough to Nolina tuberculata.\nGuzmania lingulata minor is closest to Guzmania lingulata var. minor, with a score of 0.91\nLavendula angustifolia is closest to Lavandula angustifolia, with a score of 0.95\nOnychium japonica is closest to Onychium japonicum, with a score of 0.91\nNo names close enough to Schefflera or brassia actinoplylla.\nPaeonis officinalis is closest to Paeonia officinalis, with a score of 0.95\nNo names close enough to Giant dracaena.\nTaxus canadensus is closest to Taxus canadensis, with a score of 0.94\nStapelia hirsata is closest to Stapelia hirsuta, with a score of 0.94\nSorghum vulgare var sudanesis is closest to Sorghum vulgare var. sudanense, with a score of 0.92\nSmilax walteria is closest to Smilax walteri, with a score of 0.97\nSecum weinbergii is closest to Sedum weinbergii, with a score of 0.94\nNo names close enough to Scindapsusphilodendron.\nSantpaulia confusa is closest to Saintpaulia confusa, with a score of 0.97\nRhipsalis cassutha is closest to Rhipsalis cassytha, with a score of 0.94\nRheum rhabarbarium is closest to Rheum rhabarbarum, with a score of 0.97\nOriganum vulgare hirtum is closest to Origanum vulgare var. hirtum, with a score of 0.90\nTolmeia menziesii is closest to Tolmiea menziesii, with a score of 0.94\nPodocarpus macrophylla is closest to Podocarpus macrophyllus, with a score of 0.93\nPloystichum munitum is closest to Polystichum munitum, with a score of 0.95\nPlectranthus oetendahlii is closest to Plectranthus oertendahlii, with a score of 0.98\nPlantanus occidentalis is closest to Platanus occidentalis, with a score of 0.98\nPilea cadieri is closest to Pilea cadierei, with a score of 0.96\nNo names close enough to Phoenix robellinii.\nNo names close enough to Peperomia serpens variegata.\nPeperomia prostata is closest to Peperomia prostrata, with a score of 0.97\nPeperomia griseoargentea is closest to Peperomia griseoargentia, with a score of 0.96\nPellonia pulchra is closest to Pellionia pulchra, with a score of 0.97\nRhapis flabelliformus is closest to Rhapis flabelliformis, with a score of 0.95\nFuschsia is closest to Fuchsia, with a score of 0.93\nNo names close enough to Begonia rex peace.\nEriogonium umbellatum is closest to Eriogonum umbellatum, with a score of 0.98\nCitrus aurantifolia is closest to Citrus aurantiifolia, with a score of 0.97\nCissus dicolor is closest to Cissus discolor, with a score of 0.97\nChlorophytum bichetti is closest to Chlorophytum bichetii, with a score of 0.95\nNo names close enough to Ceratostigma larpentiae.\nCattleya trianaei is closest to Cattleya trianae, with a score of 0.97\nCamellia japonica thea japonica is closest to Camellia japonica var. japonica, with a score of 0.90\nCaesalpinia gilliessi is closest to Caesalpinia gilliesii, with a score of 0.95\nBorage officinalis is closest to Borago officinalis, with a score of 0.94\nNo names close enough to Bertolonia mosaica.\nNo names close enough to Begonia semperflorens cultivar.\nBegonia scharfii is closest to Begonia scharffii, with a score of 0.97\nBegonia cleopatra is closest to Begonia cleopatrae, with a score of 0.97\nNo names close enough to Asparagus densiflorus sprengeri.\nArum palestinum is closest to Arum palaestinum, with a score of 0.97\nAnthurium scherzeranum is closest to Anthurium scherzerianum, with a score of 0.98\nAnthirrhinum multiflorum is closest to Antirrhinum multiflorum, with a score of 0.98\nAnoectuchilus setaceus is closest to Anoectochilus setaceus, with a score of 0.95\nAnethum graveolena is closest to Anethum graveolens, with a score of 0.94\nNo names close enough to Albiflora.\nNo names close enough to Acantha.\nTradescantia flumeninsis is closest to Tradescantia fluminensis, with a score of 0.92\nCitrus aurantium is closest to Citrus ×aurantium, with a score of 0.97\nEuonymus atropurpurea is closest to Euonymus atropurpureus, with a score of 0.93\nCitrus limonia is closest to Citrus ×limonia, with a score of 0.97\nCleome hasserlana is closest to Cleome hassleriana, with a score of 0.91\nEriogonium inflatum is closest to Eriogonum inflatum, with a score of 0.97\nNo names close enough to Episcia cultivar.\nEpidendrum atropurpeum is closest to Epidendrum atropurpureum, with a score of 0.96\nEleagnus is closest to Elaeagnus, with a score of 0.94\nNo names close enough to Echeveria puloliver.\nEcheveria pulinata is closest to Echeveria pulvinata, with a score of 0.97\nNo names close enough to Echevaria.\nNo names close enough to Dypsis lutescens chrysalidocarpus lutescens alternate scientific name.\nNo names close enough to Draceana.\nNo names close enough to Daucus carota sativa.\nCitrus paradisii is closest to Citrus paradisi, with a score of 0.97\nNo names close enough to Cycasrevolutazamia.\nNo names close enough to Cucurbita maxima turbaniformis.\nNo names close enough to Cucurbita maxima hubbard.\nNo names close enough to Cucurbita maxima butternut.\nNo names close enough to Cucurbita maxima buttercup.\nNo names close enough to Cucurbita maxima banana.\nNo names close enough to Cucurbia pepo zucchini.\nNo names close enough to Cryptanthus bivattus minor.\nColeus ampoinicus is closest to Coleus amboinicus, with a score of 0.94\nClivia minata is closest to Clivia miniata, with a score of 0.96\nClintonia umbelluata is closest to Clintonia umbellulata, with a score of 0.98\nNo names close enough to Cycasandzamia.\nVeitchia merillii is closest to Veitchia merrillii, with a score of 0.97\n\n\n\n\n\n\n\n# Scientific names that don't match anything on record automatically\nunknown_df = aspca_df[aspca_df.taxonomicStatus == 'Unknown']\n# Synonyms that don't have a database link to the accepted name\naspca_df = aspca_df.sort_values('taxonomicStatus').drop_duplicates('Scientific Name', keep='first').reset_index(drop=True)\nunknown_ids = aspca_df[(aspca_df.acceptedNameUsageID == 'Unknown') & (aspca_df.taxonomicStatus == 'Synonym')]\nlen(unknown_ids) + len(unknown_df)\n\n52\n\n\n\n# Manually fix some scientific names that don't match anything on record automatically\nfix_name('Nephrolepsis cordifolia plumosa', 'Nephrolepis cordifolia')\nfix_name('Nephrolepsis cordifolia duffii', 'Nephrolepis cordifolia')\nfix_name('Nephrolepis exalta bostoniensis', 'Nephrolepis exaltata')\nfix_name('Neoregalia', 'Neoregelia')\nfix_name('Miltonia roezlii alba', 'Miltonia roezlii')\nfix_name('Maranta insignis', 'Calathea insignis')\nfix_name('Lilium orientalis', 'Lilium japonicum')\nfix_name('Lampranthus piquet', 'Lampranthus piquetbergensis')\nfix_name('Hoya carnosa krinkle kurl', 'Hoya carnosa')\nfix_name('Hemigraphis exotica', 'Hemigraphis alternata')\nfix_name('Lilium asiatica', 'Lilium japonicum')\nfix_name('Nolina tuberculata', 'Beaucarnea recurvata')\nfix_name('Giant dracaena', 'Cordyline australis')\nfix_name('Scindapsusphilodendron', 'Philodendron scandens')\nfix_name('Schefflera or brassia actinoplylla', 'Schefflera actinophylla')\nfix_name('Phoenix robellinii', 'Phoenix roebelenii')\nfix_name('Peperomia serpens variegata', 'Peperomia serpens')\nfix_name('Bertolonia mosaica', 'Fittonia albivenis')\nfix_name('Begonia semperflorens cultivar', 'Begonia semperflorens')\nfix_name('Begonia rex peace', 'Begonia rex')\nfix_name('Asparagus densiflorus sprengeri', 'Asparagus densiflorus')\nfix_name('Albiflora', 'Tradescantia zebrina')\nfix_name('Acantha', 'Acanthus')\nfix_name('Episcia cultivar', 'Episcia')\nfix_name('Echevaria', 'Echeveria')\nfix_name('Echeveria puloliver', 'Echeveria harmsii')\nfix_name('Dypsis lutescens chrysalidocarpus lutescens alternate scientific name', 'Dypsis lutescens')\nfix_name('Draceana', 'Dracaena')\nfix_name('Daucus carota sativa', 'Daucus carota')\nfix_name('Ceratostigma larpentiae', 'Ceratostigma plumbaginoides')\nfix_name('Cycasrevolutazamia', 'Cycas revoluta')\nfix_name('Cucurbita maxima turbaniformis', 'Cucurbita maxima')\nfix_name('Cucurbita maxima hubbard', 'Cucurbita maxima')\nfix_name('Cucurbita maxima butternut', 'Cucurbita maxima')\nfix_name('Cucurbita maxima banana', 'Cucurbita maxima')\nfix_name('Cucurbita maxima buttercup', 'Cucurbita maxima')\nfix_name('Cucurbia pepo zucchini', 'Cucurbita pepo')\nfix_name('Cryptanthus bivattus minor', 'Cryptanthus bivittatus')\nfix_name('Cycasandzamia', 'Cycas')\n\n\n# Manually match up synonyms that don't have a database link to the accepted name\nfix_name('Chlorophytum bichetii', 'Chlorophytum laxum')\nfix_name('Rhapis flabelliformis', 'Rhapis excelsa')\nfix_name('Cleome hassleriana', 'Cleome spinosa')\nfix_name('Pellionia pulchra', 'Pellionia repens')\nfix_name('Cissus discolor', 'Cissus javana')\nfix_name('Miltonia roezlii', 'Miltoniopsis roezlii')\nfix_name('Sorghum vulgare var. sudanense', 'Sorghum bicolor')\nfix_name('Camellia japonica var. japonica', 'Camellia japonica')\nfix_name('Onychium japonicum', 'Onychium japonicum')\nfix_name('Epidendrum atropurpureum', 'Psychilis atropurpurea')\nfix_name('Philodendron scandens', 'Philodendron hederaceum')\nfix_name('Origanum vulgare var. hirtum', 'Origanum vulgare subsp. hirtum')\nfix_name('Guzmania lingulata var. minor', 'Guzmania lingulata var. concolor')\nfix_name('Lavandula angustifolia', 'Lavandula angustifolia')\nfix_name('Begonia semperflorens', 'Begonia cucullata')\nfix_name('Calathea insignis', 'Calathea crotalifera')\nfix_name('Citrus ×limonia', 'Citrus limon')\nfix_name('Coleus amboinicus', 'Plectranthus amboinicus')\nfix_name('Rhipsalis cassytha', 'Rhipsalis dichotoma')\nfix_name('Lycopersicon', 'Solanum lycopersicum')\nfix_name('Lachenalia lilacina', 'Iris domestica')\nfix_name('Cymopterus watsonii', 'Cymopterus terebinthinus')\n\n\n# Scientific names that don't match anything on record automatically\nunknown_df = aspca_df[aspca_df.taxonomicStatus == 'Unknown']\n# Synonyms that don't have a database link to the accepted name\naspca_df = aspca_df.sort_values('taxonomicStatus').drop_duplicates('Scientific Name', keep='first').reset_index(drop=True)\nunknown_ids = aspca_df[(aspca_df.acceptedNameUsageID == 'Unknown') & (aspca_df.taxonomicStatus == 'Synonym')]\nlen(unknown_ids) + len(unknown_df)\n\n1\n\n\n\nsynonym_idx = aspca_df[aspca_df['taxonomicStatus'].values == 'Synonym'].index\nprint(f'{len(synonym_idx)} entries have a more acceptable synonym')\n\n71 entries have a more acceptable synonym\n\n\n\n# Work to update the remaining scientific names that are synonyms for their accepted scientific names\naspca_df = aspca_df.sort_values('taxonomicStatus').drop_duplicates('Scientific Name', keep='first').reset_index(drop=True)\nsynonym_idx = aspca_df[aspca_df['taxonomicStatus'].values == 'Synonym'].index\n\n\nfor i in synonym_idx:\n  # Get the series we're looking to change\n  synonym_data = aspca_df.iloc[i,:]\n  synonym_name = synonym_data.loc['Scientific Name']\n  # Grab accepted data from wfo database based on ID lookup\n  true_data = wfo_df[wfo_df['taxonID'] == synonym_data.loc['acceptedNameUsageID']]\n  true_sciname = true_data.iloc[:,1].values[0]\n  fix_name(synonym_name,true_sciname)\n\nIndexError: index 0 is out of bounds for axis 0 with size 0\n\n\n\nsynonym_idx = aspca_df[aspca_df['taxonomicStatus'].values == 'Synonym'].index\nprint(f'{len(synonym_idx)} entries have a more acceptable synonym')\n\n31 entries have a more acceptable synonym\n\n\n\n# Sort and drop again\naspca_df = aspca_df.sort_values('taxonomicStatus').drop_duplicates('Scientific Name', keep='first')\naspca_df = aspca_df.sort_values('Scientific Name').reset_index(drop=True).sort_index()\n# Set genus of one-word names to be the name, rather than NaN\naspca_df.loc[aspca_df.fillna('Unknown')['genus']=='Unknown', 'genus'] = aspca_df.loc[aspca_df.fillna('Unknown')['genus']=='Unknown', 'Scientific Name']\n# Drop columns we no longer need\naspca_df = aspca_df.drop(['taxonID', 'scientificName', 'taxonomicStatus', 'acceptedNameUsageID', 'taxonRank'], axis=1)\n# Standardize column names\naspca_df.rename(columns = {'genus':'Genus', 'family':'Family'}, inplace=True)\n# Reorder columns\ncols = ['Name', 'Scientific Name', 'Genus', 'Family', 'Alternative Names', 'Toxic to Dogs', 'Toxic to Cats']\naspca_df = aspca_df[cols]\n\n\naspca_df.to_csv('Plant Toxicity - v6.csv')\naspca_df.sample(10)\n\n\n\n\n\n\n\n\nName\nScientific Name\nGenus\nFamily\nAlternative Names\nToxic to Dogs\nToxic to Cats\n\n\n\n\n102\nCelosia Globosa\nCelosia globosa\nCelosia\nAmaranthaceae\nGlobe Amarantha, Perpetua\nFalse\nFalse\n\n\n18\nAlocasia\nAlocasia\nAlocasia\nAraceae\nElephant's Ear\nTrue\nTrue\n\n\n386\nVariegated Philodendron\nPhilodendron hederaceum\nPhilodendron\nAraceae\n\nTrue\nTrue\n\n\n411\nAmerican Mandrake\nPodophyllum peltatum\nPodophyllum\nBerberidaceae\nMayapple, Indian Apple Root, Umbrella Leaf, Wi...\nTrue\nTrue\n\n\n94\nChestnut\nCastanea dentata\nCastanea\nFagaceae\nAmerican Chestnut\nFalse\nFalse\n\n\n291\nButterfly Iris\nIris spuria\nIris\nIridaceae\nSpuria Iris\nTrue\nTrue\n\n\n243\nClimbing Lily\nGloriosa superba\nGloriosa\nColchicaceae\nGloriosa Lily, Glory Lily, Superb Lily\nTrue\nTrue\n\n\n4\nMeasles Plant\nAcanthus\nAcanthus\nAcanthaceae\nPolka Dot Plant, Flamingo Plant, Baby’s Tears,...\nFalse\nFalse\n\n\n246\nOrange Star\nGuzmania lingulata var. concolor\nGuzmania\nBromeliaceae\n\nFalse\nFalse\n\n\n420\nAlgaroba\nProsopis limensis\nProsopis\nFabaceae\nKiawe, Mesquite\nFalse\nFalse\n\n\n\n\n\n\n\n\naspca_df.head()\n\n\n\n\n\n\n\n\nName\nScientific Name\nGenus\nFamily\nAlternative Names\nToxic to Dogs\nToxic to Cats\n\n\n\n\n0\nSand Verbena\nAbronia fragrans\nAbronia\nNyctaginaceae\nPrairie Snowball, Wild Lantana\nFalse\nFalse\n\n\n1\nPrayer Bean\nAbrus precatorius\nAbrus\nFabaceae\nRosary Pea, Buddhist Rosary Bead, Indian Bead,...\nTrue\nTrue\n\n\n2\nCopperleaf\nAcalypha godseffiana\nAcalypha\nEuphorbiaceae\nLance Copperleaf\nFalse\nFalse\n\n\n3\nChenille Plant\nAcalypha hispida\nAcalypha\nEuphorbiaceae\nPhilippine Medusa, Foxtail, Red-hot Cat Tail\nFalse\nFalse\n\n\n4\nMeasles Plant\nAcanthus\nAcanthus\nAcanthaceae\nPolka Dot Plant, Flamingo Plant, Baby’s Tears,...\nFalse\nFalse\n\n\n\n\n\n\n\n\naspca_df[aspca_df['Toxic to Dogs'] != aspca_df['Toxic to Cats']]\n\n\n\n\n\n\n\n\nName\nScientific Name\nGenus\nFamily\nAlternative Names\nToxic to Dogs\nToxic to Cats\n\n\n\n\n262\nDay Lilies (many varieties)\nHemerocallis\nHemerocallis\nXanthorrhoeaceae\n\nFalse\nTrue\n\n\n263\nOrange Day Lily\nHemerocallis graminea\nHemerocallis\nXanthorrhoeaceae\n\nFalse\nTrue\n\n\n296\nBlack Walnut\nJuglans nigra\nJuglans\nJuglandaceae\n\nTrue\nFalse\n\n\n317\nLily\nLilium\nLilium\nLiliaceae\n\nFalse\nTrue\n\n\n319\nTiger Lily\nLilium lancifolium\nLilium\nLiliaceae\n\nFalse\nTrue\n\n\n320\nEaster Lily\nLilium longiflorum\nLilium\nLiliaceae\n\nFalse\nTrue\n\n\n321\nRed Lily\nLilium philadelphicum\nLilium\nLiliaceae\n\nFalse\nTrue\n\n\n322\nJapanese Show Lily\nLilium speciosum\nLilium\nLiliaceae\n\nFalse\nTrue\n\n\n\n\n\n\n\n\naspca_df[['Family','Toxic to Dogs','Toxic to Cats']].pivot_table(index = 'Family').sort_values(by='Toxic to Dogs')[70:80]\n\n\n\n\n\n\n\n\nToxic to Cats\nToxic to Dogs\n\n\nFamily\n\n\n\n\n\n\nLauraceae\n0.500000\n0.500000\n\n\nProteaceae\n0.500000\n0.500000\n\n\nConvolvulaceae\n0.500000\n0.500000\n\n\nCommelinaceae\n0.500000\n0.500000\n\n\nEuphorbiaceae\n0.600000\n0.600000\n\n\nFabaceae\n0.600000\n0.600000\n\n\nBerberidaceae\n0.666667\n0.666667\n\n\nPolygonaceae\n0.666667\n0.666667\n\n\nApiaceae\n0.666667\n0.666667\n\n\nMoraceae\n0.666667\n0.666667\n\n\n\n\n\n\n\n\n# How many Families have mixed toxicity\nlen(aspca_df[['Family','Toxic to Dogs','Toxic to Cats']].pivot_table(index = 'Family').sort_values(by='Toxic to Dogs')[aspca_df[['Family','Toxic to Dogs','Toxic to Cats']].pivot_table(index = 'Family').sort_values(by='Toxic to Dogs')['Toxic to Dogs'].apply(lambda x: 0&lt;x&lt;1)])\n\n33\n\n\n\n# How many Families\nlen(aspca_df['Family'].unique())\n\n111\n\n\n\naspca_df[['Genus','Toxic to Dogs','Toxic to Cats']].pivot_table(index = 'Genus').sort_values(by='Toxic to Dogs')[208:218]\n\n\n\n\n\n\n\n\nToxic to Cats\nToxic to Dogs\n\n\nGenus\n\n\n\n\n\n\nSchefflera\n0.666667\n0.666667\n\n\nCordyline\n0.666667\n0.666667\n\n\nIris\n0.666667\n0.666667\n\n\nAloe\n0.666667\n0.666667\n\n\nDracaena\n0.800000\n0.800000\n\n\nAralia\n1.000000\n1.000000\n\n\nFicus\n1.000000\n1.000000\n\n\nApocynum\n1.000000\n1.000000\n\n\nSansevieria\n1.000000\n1.000000\n\n\nRumex\n1.000000\n1.000000\n\n\n\n\n\n\n\n\n# How many Genuses have mixed toxicity\nlen(aspca_df[['Genus','Toxic to Dogs','Toxic to Cats']].pivot_table(index = 'Genus').sort_values(by='Toxic to Dogs')[aspca_df[['Genus','Toxic to Dogs','Toxic to Cats']].pivot_table(index = 'Genus').sort_values(by='Toxic to Dogs')['Toxic to Dogs'].apply(lambda x: 0&lt;x&lt;1)])\n\n9\n\n\n\n# How many Genuses\nlen(aspca_df[['Genus','Toxic to Dogs','Toxic to Cats']].pivot_table(index = 'Genus').sort_values(by='Toxic to Dogs'))\n\n346\n\n\n\n# If running in Colabs\n!pip install selenium -q\n!apt-get update # to update ubuntu to correctly run apt install\n!apt install chromium-chromedriver -q\n!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n\nWARNING: You are using pip version 20.2.3; however, version 20.2.4 is available.\nYou should consider upgrading via the '/home/gao/anaconda3/bin/python -m pip install --upgrade pip' command.\nReading package lists... Done\nE: Could not open lock file /var/lib/apt/lists/lock - open (13: Permission denied)\nE: Unable to lock directory /var/lib/apt/lists/\nW: Problem unlinking the file /var/cache/apt/pkgcache.bin - RemoveCaches (13: Permission denied)\nW: Problem unlinking the file /var/cache/apt/srcpkgcache.bin - RemoveCaches (13: Permission denied)\nE: Could not open lock file /var/lib/dpkg/lock-frontend - open (13: Permission denied)\nE: Unable to acquire the dpkg frontend lock (/var/lib/dpkg/lock-frontend), are you root?\ncp: cannot stat '/usr/lib/chromium-browser/chromedriver': No such file or directory\n\n\n\nimport sys\nsys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n\n# Import and setup the Selenium webdriver\nfrom selenium import webdriver\nchrome_options = webdriver.ChromeOptions()\nchrome_options.add_argument('--headless')\nchrome_options.add_argument('--no-sandbox')\nchrome_options.add_argument('--disable-dev-shm-usage')\nwd = webdriver.Chrome('chromedriver',chrome_options=chrome_options)"
  },
  {
    "objectID": "posts/2020-10-17-dask-NLP-gutenberg-books.html",
    "href": "posts/2020-10-17-dask-NLP-gutenberg-books.html",
    "title": "Using Dask with dask.bag and regex to parse Notes from the Underground from project gutenberg",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\n\nimport dask.bag as db\nimport re\n\n\nbook_bag = db.from_url('https://www.gutenberg.org/cache/epub/600/pg600.txt')\n\n\nbook_bag.take(5)\n\n(b\"\\xef\\xbb\\xbfProject Gutenberg's Notes from the Underground, by Feodor Dostoevsky\\r\\n\",\n b'\\r\\n',\n b'This eBook is for the use of anyone anywhere at no cost and with\\r\\n',\n b'almost no restrictions whatsoever.  You may copy it, give it away or\\r\\n',\n b're-use it under the terms of the Project Gutenberg License included\\r\\n')\n\n\n\nremove_spaces = book_bag.map(lambda x:x.strip())\n\n\nremove_spaces.take(10)\n\n(b\"\\xef\\xbb\\xbfProject Gutenberg's Notes from the Underground, by Feodor Dostoevsky\",\n b'',\n b'This eBook is for the use of anyone anywhere at no cost and with',\n b'almost no restrictions whatsoever.  You may copy it, give it away or',\n b're-use it under the terms of the Project Gutenberg License included',\n b'with this eBook or online at www.gutenberg.net',\n b'',\n b'',\n b'Title: Notes from the Underground',\n b'')\n\n\n\ndef decode_to_ascii(x):\n    return x.decode(\"ascii\",\"ignore\") \n\n\nascii_text = remove_spaces.map(decode_to_ascii)\n\n\nascii_text.take(10)\n\n(\"Project Gutenberg's Notes from the Underground, by Feodor Dostoevsky\",\n '',\n 'This eBook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever.  You may copy it, give it away or',\n 're-use it under the terms of the Project Gutenberg License included',\n 'with this eBook or online at www.gutenberg.net',\n '',\n '',\n 'Title: Notes from the Underground',\n '')\n\n\n\ndef remove_punctuation(x):\n    return re.sub(r'[^\\w\\s]','',x)\n\n\nremove_punctuation = ascii_text.map(remove_punctuation)\n\n\nremove_punctuation.take(10)\n\n('Project Gutenbergs Notes from the Underground by Feodor Dostoevsky',\n '',\n 'This eBook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever  You may copy it give it away or',\n 'reuse it under the terms of the Project Gutenberg License included',\n 'with this eBook or online at wwwgutenbergnet',\n '',\n '',\n 'Title Notes from the Underground',\n '')\n\n\n\nlower_text = remove_punctuation.map(str.lower)\n\n\nlower_text.take(10)\n\n('project gutenbergs notes from the underground by feodor dostoevsky',\n '',\n 'this ebook is for the use of anyone anywhere at no cost and with',\n 'almost no restrictions whatsoever  you may copy it give it away or',\n 'reuse it under the terms of the project gutenberg license included',\n 'with this ebook or online at wwwgutenbergnet',\n '',\n '',\n 'title notes from the underground',\n '')\n\n\n\nsplit_word_list = lower_text.map(lambda x: x.split(' '))\n\n\nsplit_word_list.take(10)\n\n(['project',\n  'gutenbergs',\n  'notes',\n  'from',\n  'the',\n  'underground',\n  'by',\n  'feodor',\n  'dostoevsky'],\n [''],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with'],\n ['almost',\n  'no',\n  'restrictions',\n  'whatsoever',\n  '',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or'],\n ['reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of',\n  'the',\n  'project',\n  'gutenberg',\n  'license',\n  'included'],\n ['with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergnet'],\n [''],\n [''],\n ['title', 'notes', 'from', 'the', 'underground'],\n [''])\n\n\n\ndef remove_empty_words(word_list):\n    return list(filter(lambda a: a != '', word_list))\n\nnon_empty_words = split_word_list.filter(remove_empty_words)\n\n\nnon_empty_words.take(10)\n\n(['project',\n  'gutenbergs',\n  'notes',\n  'from',\n  'the',\n  'underground',\n  'by',\n  'feodor',\n  'dostoevsky'],\n ['this',\n  'ebook',\n  'is',\n  'for',\n  'the',\n  'use',\n  'of',\n  'anyone',\n  'anywhere',\n  'at',\n  'no',\n  'cost',\n  'and',\n  'with'],\n ['almost',\n  'no',\n  'restrictions',\n  'whatsoever',\n  '',\n  'you',\n  'may',\n  'copy',\n  'it',\n  'give',\n  'it',\n  'away',\n  'or'],\n ['reuse',\n  'it',\n  'under',\n  'the',\n  'terms',\n  'of',\n  'the',\n  'project',\n  'gutenberg',\n  'license',\n  'included'],\n ['with', 'this', 'ebook', 'or', 'online', 'at', 'wwwgutenbergnet'],\n ['title', 'notes', 'from', 'the', 'underground'],\n ['author', 'feodor', 'dostoevsky'],\n ['posting', 'date', 'september', '13', '2008', 'ebook', '600'],\n ['release', 'date', 'july', '1996'],\n ['language', 'english'])\n\n\n\nall_words = non_empty_words.flatten()\n\n\ntype(all_words)\n\ndask.bag.core.Bag\n\n\n\nall_words.take(30)\n\n('project',\n 'gutenbergs',\n 'notes',\n 'from',\n 'the',\n 'underground',\n 'by',\n 'feodor',\n 'dostoevsky',\n 'this',\n 'ebook',\n 'is',\n 'for',\n 'the',\n 'use',\n 'of',\n 'anyone',\n 'anywhere',\n 'at',\n 'no',\n 'cost',\n 'and',\n 'with',\n 'almost',\n 'no',\n 'restrictions',\n 'whatsoever',\n '',\n 'you',\n 'may')\n\n\n\nchange_to_key_value = all_words.map(lambda x: (x, 1))\n\n\nchange_to_key_value.take(4)\n\n(('project', 1), ('gutenbergs', 1), ('notes', 1), ('from', 1))\n\n\n\ngrouped_words = all_words.groupby(lambda x:x)\n\n\ngrouped_words.take(1)\n\n(('project',\n  ['project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project',\n   'project']),)\n\n\n\nword_count = grouped_words.map(lambda x: (x[0], len(x[1])))\n\n\nword_count.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nchange_to_key_value.take(10)\n\n(('project', 1),\n ('gutenbergs', 1),\n ('notes', 1),\n ('from', 1),\n ('the', 1),\n ('underground', 1),\n ('by', 1),\n ('feodor', 1),\n ('dostoevsky', 1),\n ('this', 1))\n\n\n\n# Take a running count of a word\n# In this case, the default value of \n# count needs to be provided\ndef add_bin_op(count, x):\n    return count + x[1]\n\n# Take the output from multiple bin_op(s)\n# and add them to get the total count of\n# a word\ndef add_combine_op(x, y):\n    return x + y\n\nword_count = change_to_key_value.foldby(lambda x: x[0],\n                                       add_bin_op, 0,\n                                       add_combine_op)\n\n\nword_count.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nmuch_easier = all_words.frequencies()\n\n\nmuch_easier.take(10)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('from', 186),\n ('the', 1555),\n ('underground', 26),\n ('by', 153),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('this', 237))\n\n\n\nRemoving stop words in top word frequency counts\n\nfrom spacy.lang.en import STOP_WORDS\n\n\nwithout_stopwords = all_words.filter(lambda x: x not in STOP_WORDS)\n\n\nnew_freq = without_stopwords.frequencies()\n\n\nnew_freq.take(20)\n\n(('project', 87),\n ('gutenbergs', 2),\n ('notes', 11),\n ('underground', 26),\n ('feodor', 3),\n ('dostoevsky', 3),\n ('ebook', 9),\n ('use', 18),\n ('cost', 5),\n ('restrictions', 3),\n ('whatsoever', 2),\n ('', 1896),\n ('copy', 12),\n ('away', 59),\n ('reuse', 2),\n ('terms', 24),\n ('gutenberg', 28),\n ('license', 15),\n ('included', 6),\n ('online', 4))\n\n\n\nnew_freq.topk(10)\n\ndask.bag&lt;topk-aggregate, npartitions=1&gt;\n\n\n\nnew_freq.topk(10, key=lambda x: x[1]).compute()\n\n[('', 1896),\n ('man', 122),\n ('know', 90),\n ('project', 87),\n ('time', 83),\n ('like', 82),\n ('come', 74),\n ('course', 73),\n ('love', 72),\n ('life', 69)]"
  },
  {
    "objectID": "posts/2020-10-30-Function-for-zillow-data-quandl-api.html",
    "href": "posts/2020-10-30-Function-for-zillow-data-quandl-api.html",
    "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\nFind the quandl api documentation here -\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport quandl\n\n\n%matplotlib inline\nquandl_call = (\n    \"ZILLOW/{category}{code}_{indicator}\"\n)\n\n\ndef download_data(category, code, indicator):\n    \"\"\"\n    Reads in a single dataset from the John Hopkins GitHub repo\n    as a DataFrame\n    \n    Parameters\n    ----------\n    category : \"Chicago_Area\" or \"Evanston\"\n    \n    code : \"Evanston\" or \"Chicago\"\n    \n    indicator : \"Sales_Price\" or \"other\"\n\n    \n    Returns\n    -------\n    DataFrame\n    \"\"\"\n    AREA_CATEGORY_dict = {\"Evanston\": \"C\", \"Chicago_Area\": \"C\"}\n    AREA_CODE_dict = {\"Evanston\": \"64604\", \"Chicago\": \"36156\"}\n    INDICATOR_CODE_dict = {\"Sales_Price\": \"SP\"}\n    \n    \n    \n    category = AREA_CATEGORY_dict[category]\n    code = AREA_CODE_dict[code]\n    indicator = INDICATOR_CODE_dict[indicator]\n\n    \n    \n    return quandl.get(quandl_call.format(category=category, code=code, indicator=indicator))\ndf = download_data('Chicago_Area', 'Evanston', 'Sales_Price')\ndf.plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bb6db9ba8&gt;\ndf['Value'].plot(label='Evanston House Prices')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bb4c88cf8&gt;\ntimeseries = df['Value']\ntimeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')\ntimeseries.rolling(12).std().plot(label='12 Month Rolling Std')\ntimeseries.plot()\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f4bb4a00eb8&gt;\ntimeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')\ntimeseries.plot()\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f4bb4b0aa90&gt;\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(df['Value'], freq=12)  \nfig = plt.figure()  \nfig = decomposition.plot()  \nfig.set_size_inches(15, 8)\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\nfrom statsmodels.tsa.arima_model import ARIMA\nimport statsmodels.api as sm\nmodel = sm.tsa.statespace.SARIMAX(df['Value'],order=(0,1,0), seasonal_order=(1,1,1,12))\nresults = model.fit()\nprint(results.summary())\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:165: ValueWarning: No frequency information was provided, so inferred frequency M will be used.\n  % freq, ValueWarning)\n\n\n                                 Statespace Model Results                                 \n==========================================================================================\nDep. Variable:                              Value   No. Observations:                  138\nModel:             SARIMAX(0, 1, 0)x(1, 1, 1, 12)   Log Likelihood               -1441.360\nDate:                            Mon, 26 Oct 2020   AIC                           2888.719\nTime:                                    06:57:44   BIC                           2897.204\nSample:                                03-31-2008   HQIC                          2892.166\n                                     - 08-31-2019                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.S.L12       0.3461      0.072      4.832      0.000       0.206       0.487\nma.S.L12      -0.8710      0.113     -7.736      0.000      -1.092      -0.650\nsigma2      6.479e+08   1.03e-11    6.3e+19      0.000    6.48e+08    6.48e+08\n===================================================================================\nLjung-Box (Q):                      120.48   Jarque-Bera (JB):                 9.48\nProb(Q):                              0.00   Prob(JB):                         0.01\nHeteroskedasticity (H):               0.66   Skew:                            -0.60\nProb(H) (two-sided):                  0.19   Kurtosis:                         3.62\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n[2] Covariance matrix is singular or near-singular, with condition number 5.31e+35. Standard errors may be unstable.\nresults.resid.plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd2947be0&gt;\nresults.resid.plot(kind='kde')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd28ffac8&gt;\ndf['forecast'] = results.predict(start = 1, end= 200, dynamic= True)  \ndf[['Value','forecast']].plot(figsize=(12,8))\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4bd2689f60&gt;\nfrom pandas.tseries.offsets import DateOffset\nfuture_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,24) ]\nfuture_dates\n\n[Timestamp('2019-08-31 00:00:00'),\n Timestamp('2019-09-30 00:00:00'),\n Timestamp('2019-10-31 00:00:00'),\n Timestamp('2019-11-30 00:00:00'),\n Timestamp('2019-12-31 00:00:00'),\n Timestamp('2020-01-31 00:00:00'),\n Timestamp('2020-02-29 00:00:00'),\n Timestamp('2020-03-31 00:00:00'),\n Timestamp('2020-04-30 00:00:00'),\n Timestamp('2020-05-31 00:00:00'),\n Timestamp('2020-06-30 00:00:00'),\n Timestamp('2020-07-31 00:00:00'),\n Timestamp('2020-08-31 00:00:00'),\n Timestamp('2020-09-30 00:00:00'),\n Timestamp('2020-10-31 00:00:00'),\n Timestamp('2020-11-30 00:00:00'),\n Timestamp('2020-12-31 00:00:00'),\n Timestamp('2021-01-31 00:00:00'),\n Timestamp('2021-02-28 00:00:00'),\n Timestamp('2021-03-31 00:00:00'),\n Timestamp('2021-04-30 00:00:00'),\n Timestamp('2021-05-31 00:00:00'),\n Timestamp('2021-06-30 00:00:00'),\n Timestamp('2021-07-31 00:00:00')]\nfuture_dates_df = pd.DataFrame(index=future_dates[1:],columns=df.columns)\nfuture_df = pd.concat([df,future_dates_df])\nfuture_df.head()\n\n\n\n\n\n\n\n\nValue\nforecast\n\n\n\n\n2008-03-31\n370900.0\nNaN\n\n\n2008-04-30\n389600.0\n370900.0\n\n\n2008-05-31\n367100.0\n370900.0\n\n\n2008-06-30\n365600.0\n370900.0\n\n\n2008-07-31\n339000.0\n370900.0\nfuture_df.tail()\n\n\n\n\n\n\n\n\nValue\nforecast\n\n\n\n\n2021-03-31\nNaN\nNaN\n\n\n2021-04-30\nNaN\nNaN\n\n\n2021-05-31\nNaN\nNaN\n\n\n2021-06-30\nNaN\nNaN\n\n\n2021-07-31\nNaN\nNaN\nfuture_df['forecast'] = results.predict(start = 1, end = 720, dynamic= True)  \nfuture_df[['Value', 'forecast']].plot(figsize=(12, 8)) \n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e3573c8&gt;"
  },
  {
    "objectID": "posts/2020-10-30-Function-for-zillow-data-quandl-api.html#exponentially-weighted-moving-averages",
    "href": "posts/2020-10-30-Function-for-zillow-data-quandl-api.html#exponentially-weighted-moving-averages",
    "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
    "section": "Exponentially-weighted moving averages",
    "text": "Exponentially-weighted moving averages\n\ndf['6-month-SMA']=df['Value'].rolling(window=6).mean()\ndf['12-month-SMA']=df['Value'].rolling(window=12).mean()\n\n\ndf['EWMA12'] = df['Value'].ewm(span=12).mean()\n\n\ndf[['Value','EWMA12']].plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e350860&gt;"
  },
  {
    "objectID": "posts/2020-10-30-Function-for-zillow-data-quandl-api.html#getting-at-the-trend-by-removing-the-cyclical-elements-of-housing-prices",
    "href": "posts/2020-10-30-Function-for-zillow-data-quandl-api.html#getting-at-the-trend-by-removing-the-cyclical-elements-of-housing-prices",
    "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
    "section": "Getting at the trend by removing the cyclical elements of Housing Prices",
    "text": "Getting at the trend by removing the cyclical elements of Housing Prices\n\n# Tuple unpacking\ndf_cycle, df_trend = sm.tsa.filters.hpfilter(df.Value)\n\n\ndf_cycle\n\nDate\n2008-03-31     1942.830509\n2008-04-30    24797.247533\n2008-05-31     6450.450288\n2008-06-30     9085.726225\n2008-07-31   -13417.668736\n                  ...     \n2019-04-30    25876.617108\n2019-05-31     6082.075050\n2019-06-30      789.779248\n2019-07-31    -4001.523626\n2019-08-31     1206.419488\nName: Value, Length: 138, dtype: float64\n\n\n\ndf[\"trend\"] = df_trend\n\n\ndf[['trend','Value']].plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e275358&gt;\n\n\n\n\n\n\ndf[['trend','Value']][\"2010-01-31\":].plot(figsize=(12,8))\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e20be10&gt;"
  },
  {
    "objectID": "posts/2020-10-30-Function-for-zillow-data-quandl-api.html#chicago-and-evanston-home-sale-prices",
    "href": "posts/2020-10-30-Function-for-zillow-data-quandl-api.html#chicago-and-evanston-home-sale-prices",
    "title": "Using quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs",
    "section": "Chicago and Evanston Home Sale Prices",
    "text": "Chicago and Evanston Home Sale Prices\n\nEV_SP = download_data('Chicago_Area', 'Evanston', 'Sales_Price')\nCH_SP = download_data('Chicago_Area', 'Chicago', 'Sales_Price')\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Value')\n\nEV_SP['Value'].plot(label='Evanston')\nCH_SP['Value'].plot(label='Chicago')\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f4b9e0d8828&gt;\n\n\n\n\n\n\nCH_SP.plot(figsize=(12,6))\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f4b9e056470&gt;"
  },
  {
    "objectID": "posts/2020-11-10-Dask-api-eda.html",
    "href": "posts/2020-11-10-Dask-api-eda.html",
    "title": "Dask API for analytics",
    "section": "",
    "text": "from dask.distributed import Client\n\nclient = Client(n_workers=4)\n\nclient\n\n/opt/venv/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 45123 instead\n  http_address[\"port\"], self.http_server.port\n\n\n\n\n\n\n\n\n\n\n\nScheduler: tcp://127.0.0.1:43829\nDashboard: http://127.0.0.1:45123/status\n\n\n\nWorkers: 4\nCores: 4\nMemory: 5.00 GB\n\n\n\n\n\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport dask.dataframe as dd\nimport aiohttp\n\nddf = dd.read_csv(\n    url,\n    blocksize=\"10 MiB\",\n).persist()\n\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nint64\nobject\nfloat64\nfloat64\nint64\nfloat64\nint64\nfloat64\nfloat64\nfloat64\nobject\nobject\nint64\n\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: read-csv, 1 tasks\n\n\n\n# See that we actually have a collection of Pandas DataFrames\nddf.map_partitions(type).compute()\n\n0    &lt;class 'pandas.core.frame.DataFrame'&gt;\ndtype: object\n\n\n\n# View head of Dask DataFrame\nddf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.30\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.34\n26131\nNaN\nNaN\nNaN\n1646891\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\n\n\n\n\n\n\n\n\ngdp = ddf.groupby('province').gdp.mean()\ngdp.compute()\n\nprovince\nAnhui            3905.870000\nBeijing          4673.453333\nChongqing        2477.712500\nFujian           4864.023333\nGansu            1397.832500\nGuangdong       15358.781667\nGuangxi          2924.104167\nGuizhou          1422.010833\nHainan            686.714167\nHebei            6936.825000\nHeilongjiang     4041.241667\nHenan            7208.966667\nHubei            4772.503333\nHunan            4765.891667\nJiangsu         10761.846667\nJiangxi          2460.782500\nJilin            2274.854167\nLiaoning         5231.135000\nNingxia           432.268333\nQinghai           383.099167\nShaanxi          2658.034167\nShandong        12324.002500\nShanghai         6432.454167\nShanxi           2817.210833\nSichuan          5377.790000\nTianjin          2528.665000\nTibet             170.426667\nXinjiang         1828.896667\nYunnan           2604.054167\nZhejiang         9138.151667\nName: gdp, dtype: float64\n\n\n\ngdp.compute().sort_values()\n\nprovince\nTibet             170.426667\nQinghai           383.099167\nNingxia           432.268333\nHainan            686.714167\nGansu            1397.832500\nGuizhou          1422.010833\nXinjiang         1828.896667\nJilin            2274.854167\nJiangxi          2460.782500\nChongqing        2477.712500\nTianjin          2528.665000\nYunnan           2604.054167\nShaanxi          2658.034167\nShanxi           2817.210833\nGuangxi          2924.104167\nAnhui            3905.870000\nHeilongjiang     4041.241667\nBeijing          4673.453333\nHunan            4765.891667\nHubei            4772.503333\nFujian           4864.023333\nLiaoning         5231.135000\nSichuan          5377.790000\nShanghai         6432.454167\nHebei            6936.825000\nHenan            7208.966667\nZhejiang         9138.151667\nJiangsu         10761.846667\nShandong        12324.002500\nGuangdong       15358.781667\nName: gdp, dtype: float64\n\n\n\nddf[ddf.reg.str.contains('East China')].head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.30\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.34\n26131\nNaN\nNaN\nNaN\n1646891\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\n\n\n\n\n\n\n\n\nec = ddf[ddf.reg.str.contains('East China')]\n\nmean_gdp_prov = ec.groupby('province').gdp.agg(['mean','count'])\nmean_gdp_prov.compute()\n\n\n\n\n\n\n\n\nmean\ncount\n\n\nprovince\n\n\n\n\n\n\nAnhui\n3905.870000\n12\n\n\nFujian\n4864.023333\n12\n\n\nJiangsu\n10761.846667\n12\n\n\nJiangxi\n2460.782500\n12\n\n\nShandong\n12324.002500\n12\n\n\nShanghai\n6432.454167\n12\n\n\nZhejiang\n9138.151667\n12\n\n\n\n\n\n\n\n\nmean_gdp_prov.nlargest(5, 'mean').compute()\n\n\n\n\n\n\n\n\nmean\ncount\n\n\nprovince\n\n\n\n\n\n\nShandong\n12324.002500\n12\n\n\nJiangsu\n10761.846667\n12\n\n\nZhejiang\n9138.151667\n12\n\n\nShanghai\n6432.454167\n12\n\n\nFujian\n4864.023333\n12\n\n\n\n\n\n\n\n\nmean_gdp_prov.to_csv('mean_gdp-*.csv') #the * is where the partition number will go\n\n['/home/jovyan/work/mean_gdp-0.csv']\n\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-11-10-Dask-api-eda.html#credit-code-from-httpsgithub.comcoileddata-science-at-scale",
    "href": "posts/2020-11-10-Dask-api-eda.html#credit-code-from-httpsgithub.comcoileddata-science-at-scale",
    "title": "Dask API for analytics",
    "section": "",
    "text": "from dask.distributed import Client\n\nclient = Client(n_workers=4)\n\nclient\n\n/opt/venv/lib/python3.7/site-packages/distributed/node.py:155: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 45123 instead\n  http_address[\"port\"], self.http_server.port\n\n\n\n\n\n\n\n\n\n\n\nScheduler: tcp://127.0.0.1:43829\nDashboard: http://127.0.0.1:45123/status\n\n\n\nWorkers: 4\nCores: 4\nMemory: 5.00 GB\n\n\n\n\n\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport dask.dataframe as dd\nimport aiohttp\n\nddf = dd.read_csv(\n    url,\n    blocksize=\"10 MiB\",\n).persist()\n\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\nnpartitions=1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nint64\nobject\nfloat64\nfloat64\nint64\nfloat64\nint64\nfloat64\nfloat64\nfloat64\nobject\nobject\nint64\n\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: read-csv, 1 tasks\n\n\n\n# See that we actually have a collection of Pandas DataFrames\nddf.map_partitions(type).compute()\n\n0    &lt;class 'pandas.core.frame.DataFrame'&gt;\ndtype: object\n\n\n\n# View head of Dask DataFrame\nddf.head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.30\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.34\n26131\nNaN\nNaN\nNaN\n1646891\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\n\n\n\n\n\n\n\n\ngdp = ddf.groupby('province').gdp.mean()\ngdp.compute()\n\nprovince\nAnhui            3905.870000\nBeijing          4673.453333\nChongqing        2477.712500\nFujian           4864.023333\nGansu            1397.832500\nGuangdong       15358.781667\nGuangxi          2924.104167\nGuizhou          1422.010833\nHainan            686.714167\nHebei            6936.825000\nHeilongjiang     4041.241667\nHenan            7208.966667\nHubei            4772.503333\nHunan            4765.891667\nJiangsu         10761.846667\nJiangxi          2460.782500\nJilin            2274.854167\nLiaoning         5231.135000\nNingxia           432.268333\nQinghai           383.099167\nShaanxi          2658.034167\nShandong        12324.002500\nShanghai         6432.454167\nShanxi           2817.210833\nSichuan          5377.790000\nTianjin          2528.665000\nTibet             170.426667\nXinjiang         1828.896667\nYunnan           2604.054167\nZhejiang         9138.151667\nName: gdp, dtype: float64\n\n\n\ngdp.compute().sort_values()\n\nprovince\nTibet             170.426667\nQinghai           383.099167\nNingxia           432.268333\nHainan            686.714167\nGansu            1397.832500\nGuizhou          1422.010833\nXinjiang         1828.896667\nJilin            2274.854167\nJiangxi          2460.782500\nChongqing        2477.712500\nTianjin          2528.665000\nYunnan           2604.054167\nShaanxi          2658.034167\nShanxi           2817.210833\nGuangxi          2924.104167\nAnhui            3905.870000\nHeilongjiang     4041.241667\nBeijing          4673.453333\nHunan            4765.891667\nHubei            4772.503333\nFujian           4864.023333\nLiaoning         5231.135000\nSichuan          5377.790000\nShanghai         6432.454167\nHebei            6936.825000\nHenan            7208.966667\nZhejiang         9138.151667\nJiangsu         10761.846667\nShandong        12324.002500\nGuangdong       15358.781667\nName: gdp, dtype: float64\n\n\n\nddf[ddf.reg.str.contains('East China')].head()\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.30\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.34\n26131\nNaN\nNaN\nNaN\n1646891\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\n\n\n\n\n\n\n\n\nec = ddf[ddf.reg.str.contains('East China')]\n\nmean_gdp_prov = ec.groupby('province').gdp.agg(['mean','count'])\nmean_gdp_prov.compute()\n\n\n\n\n\n\n\n\nmean\ncount\n\n\nprovince\n\n\n\n\n\n\nAnhui\n3905.870000\n12\n\n\nFujian\n4864.023333\n12\n\n\nJiangsu\n10761.846667\n12\n\n\nJiangxi\n2460.782500\n12\n\n\nShandong\n12324.002500\n12\n\n\nShanghai\n6432.454167\n12\n\n\nZhejiang\n9138.151667\n12\n\n\n\n\n\n\n\n\nmean_gdp_prov.nlargest(5, 'mean').compute()\n\n\n\n\n\n\n\n\nmean\ncount\n\n\nprovince\n\n\n\n\n\n\nShandong\n12324.002500\n12\n\n\nJiangsu\n10761.846667\n12\n\n\nZhejiang\n9138.151667\n12\n\n\nShanghai\n6432.454167\n12\n\n\nFujian\n4864.023333\n12\n\n\n\n\n\n\n\n\nmean_gdp_prov.to_csv('mean_gdp-*.csv') #the * is where the partition number will go\n\n['/home/jovyan/work/mean_gdp-0.csv']\n\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-11-20-Dask-Diab.html",
    "href": "posts/2020-11-20-Dask-Diab.html",
    "title": "Dask for Predicting Onset/Diagnosis of Chronic Conditions, Diabetes",
    "section": "",
    "text": "categories: [Big Data]"
  },
  {
    "objectID": "posts/2020-11-20-Dask-Diab.html#credit-httpmatthewrocklin.comblogwork20170328dask-xgboost-httpsexamples.dask.orgapplicationsforecasting-with-prophet.html",
    "href": "posts/2020-11-20-Dask-Diab.html#credit-httpmatthewrocklin.comblogwork20170328dask-xgboost-httpsexamples.dask.orgapplicationsforecasting-with-prophet.html",
    "title": "Dask for Predicting Onset/Diagnosis of Chronic Conditions, Diabetes",
    "section": "Credit: http://matthewrocklin.com/blog/work/2017/03/28/dask-xgboost, https://examples.dask.org/applications/forecasting-with-prophet.html",
    "text": "Credit: http://matthewrocklin.com/blog/work/2017/03/28/dask-xgboost, https://examples.dask.org/applications/forecasting-with-prophet.html\n\nimport dask\nfrom dask.distributed import Client, progress\n\n\nfrom dask.distributed import Client\n\nclient = Client(n_workers=4)\n\nclient\n\n\n\n\n\n\n\n\nClient\n\nScheduler: tcp://127.0.0.1:41309\nDashboard: http://127.0.0.1:8787/status\n\nCluster\n\nWorkers: 4\nCores: 8\nMemory: 16.57 GB\n\n\n\n\n\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n763\n10\n101\n76\n48\n180\n32.9\n0.171\n63\n0\n\n\n764\n2\n122\n70\n27\n0\n36.8\n0.340\n27\n0\n\n\n765\n5\n121\n72\n23\n112\n26.2\n0.245\n30\n0\n\n\n766\n1\n126\n60\n0\n0\n30.1\n0.349\n47\n1\n\n\n767\n1\n93\n70\n31\n0\n30.4\n0.315\n23\n0\n\n\n\n\n768 rows × 9 columns\n\n\n\n\nfrom dask import dataframe as dd \n\nddf = dd.from_pandas(df, npartitions=5)\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\nnpartitions=5\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nint64\nint64\nint64\nint64\nint64\nfloat64\nfloat64\nint64\nint64\n\n\n154\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n616\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n767\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: from_pandas, 5 tasks\n\n\n\nimport dask.dataframe as dd\n\n# Subset of the columns to use\ncols = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness',\n        'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age']\n\n\nddf2 = ddf.sample(frac=0.2) # XGBoost requires a bit of RAM, we need a larger cluster\n\n\nddf2\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\nnpartitions=5\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nint64\nint64\nint64\nint64\nint64\nfloat64\nfloat64\nint64\nint64\n\n\n154\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n616\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n767\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: sample, 10 tasks\n\n\n\ndiab_diag = (ddf.Outcome)  # column of labels\n\n\ndel ddf['Outcome']  # Remove delay information from training dataframe\n\n\nddf, diab_diag = dask.persist(ddf, diab_diag)  # start work in the background\n\n\ndiab_diag.head()\n\n0    1\n1    0\n2    1\n3    0\n4    1\nName: Outcome, dtype: int64\n\n\n\ndf2 = dd.get_dummies(ddf.categorize()).persist()\n\n\nlen(df2.columns)\n\n8\n\n\n\ndata_train, data_test = df2.random_split([0.9, 0.1],\n                                         random_state=1234)\nlabels_train, labels_test = diab_diag.random_split([0.9, 0.1],\n                                                    random_state=1234)\n\n\n%%time\nimport dask_xgboost as dxgb\n\nparams = {'objective': 'binary:logistic', 'nround': 1000,\n          'max_depth': 16, 'eta': 0.01, 'subsample': 0.5,\n          'min_child_weight': 1, 'tree_method': 'hist',\n          'grow_policy': 'lossguide'}\n\nbst = dxgb.train(client, params, data_train, labels_train)\n\nCPU times: user 1.23 s, sys: 607 ms, total: 1.84 s\nWall time: 3.56 s\n\n\n\nbst\n\n&lt;xgboost.core.Booster at 0x7fde80f05f98&gt;\n\n\n\nimport xgboost as xgb\npandas_df = data_test.head()\ndtest = xgb.DMatrix(pandas_df)\n\n\nbst.predict(dtest)\n\narray([0.52612805, 0.51560616, 0.47321838, 0.5084377 , 0.45707062],\n      dtype=float32)\n\n\n\npredictions = dxgb.predict(client, bst, data_test).persist()\n\n\npredictions\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\nunknown\nunknown\n\n\nShape\n(nan,)\n(nan,)\n\n\nCount\n5 Tasks\n5 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_auc_score, roc_curve\n\n\nprint(roc_auc_score(labels_test.compute(), predictions.compute()))\n\n0.7775157232704403\n\n\n\nimport matplotlib.pyplot as plt\nfpr, tpr, _ = roc_curve(labels_test.compute(), predictions.compute())\n# Taken from\n# http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\nplt.figure(figsize=(8, 8))\nlw = 2\nplt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic example')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n\n\n\n\nimport dask\nimport xgboost\nimport dask_xgboost\n\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\n\nax = xgboost.plot_importance(bst, height=0.8, max_num_features=9)\nax.grid(False, axis=\"y\")\nax.set_title('Estimated feature importance')\nplt.show()\n\n\n\n\n\ny_hat = dask_xgboost.predict(client, bst, data_test).persist()\ny_hat\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\nunknown\nunknown\n\n\nShape\n(nan,)\n(nan,)\n\n\nCount\n5 Tasks\n5 Chunks\n\n\nType\nfloat32\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.metrics import roc_curve\n\nlabels_test, y_hat = dask.compute(labels_test, y_hat)\nfpr, tpr, _ = roc_curve(labels_test, y_hat)\n\n\nfrom sklearn.metrics import auc\n\nfig, ax = plt.subplots(figsize=(5, 5))\nax.plot(fpr, tpr, lw=3,\n        label='ROC Curve (area = {:.2f})'.format(auc(fpr, tpr)))\nax.plot([0, 1], [0, 1], 'k--', lw=2)\nax.set(\n    xlim=(0, 1),\n    ylim=(0, 1),\n    title=\"ROC Curve\",\n    xlabel=\"False Positive Rate\",\n    ylabel=\"True Positive Rate\",\n)\nax.legend();\nplt.show()"
  },
  {
    "objectID": "posts/2021-01-23-voila-datasette-blog-post.html",
    "href": "posts/2021-01-23-voila-datasette-blog-post.html",
    "title": "Kearney Data Science",
    "section": "",
    "text": "# Set up \n\n\nimport pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport datetime\nimport sqlite3\nimport pandas as pd\nfrom sqlalchemy.sql.schema import Column\n\n\nstart = pd.to_datetime('2018-01-01')\nend = pd.to_datetime('today')\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nSP500_stock = web.DataReader('^GSPC', 'yahoo', start, end)\nSP500_stock.head()\n\nRussell_2000_stock = web.DataReader('^RUT', 'yahoo', start, end)\nRussell_2000_stock.head()\n\n\nstocks = pd.concat([SP500_stock['Open'], Russell_2000_stock['Open'], FXAIX_stock['Open']], axis = 1)\n\nstocks.columns = ['SP500_stock','Russell_2000_stock', 'FXAIX_stock']\n\nstocks.reset_index(level=0, inplace=True)\nstocks\n\nengine = db.create_engine('sqlite:///stocks.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nstocks_table = db.Table('index_stocks_table', metadata, \n    db.Column('Date',db.Date, nullable=True, index=False),\n    db.Column('SP500_stock',db.Integer, nullable=True),\n    db.Column('Russell_2000_stock',db.Integer, nullable=True),\n    db.Column('FXAIX_stock', db.Numeric, nullable=True)\n)\n\nmetadata.create_all(engine) \n\nstocks.to_sql('index_stocks_table', con=engine, if_exists='replace', index=False)\n\nsql = \"SELECT * FROM index_stocks_table LIMIT 10\"\ncnxn = connection\ndf = pd.read_sql(sql, cnxn)\nprint(df.head(10))\n\n                         Date  SP500_stock  Russell_2000_stock  FXAIX_stock\n0  2018-01-02 00:00:00.000000  2683.729980         1536.119995    94.230003\n1  2018-01-03 00:00:00.000000  2697.850098         1550.280029    94.830002\n2  2018-01-04 00:00:00.000000  2719.310059         1552.979980    95.230003\n3  2018-01-05 00:00:00.000000  2731.330078         1555.869995    95.900002\n4  2018-01-08 00:00:00.000000  2742.669922         1559.800049    96.059998\n5  2018-01-09 00:00:00.000000  2751.149902         1562.219971    96.220001\n6  2018-01-10 00:00:00.000000  2745.550049         1559.010010    96.110001\n7  2018-01-11 00:00:00.000000  2752.969971         1560.219971    96.790001\n8  2018-01-12 00:00:00.000000  2770.179932         1587.119995    97.449997\n9  2018-01-16 00:00:00.000000  2798.959961         1592.910034    97.110001\n\n\n\n#!/bin/bash\nsource ~/anaconda3/etc/profile.d/conda.sh\nconda activate base\npython stocks_fiax.py\ndatasette publish heroku -n stocks-snp-500 stocks.db --install=datasette-vega\n\n\n#snp_list_tables.columns = ['Symbol','Security', 'SEC filings', 'GICS Sector', 'GICS Sub-Industry', ' Headquarters Location', 'Date first added', 'CIK', 'Founded']\n#snp_list_tables.head(10)\n#\n#\n#snp_list_table = db.Table('snp_list_tables', metadata, \n#    db.Column('Symbol',db.String, nullable=True, index=False),\n#    db.Column('Security',db.String, nullable=True),\n#    db.Column('SEC filings',db.String, nullable=True),\n#    db.Column('GICS Sub-Industry',db.String, nullable=True),\n#    db.Column('Headquarters Location',db.String, nullable=True)\n#    , db.Column('Date first added', db.Date, nullable=True)\n#    , db.Column('CIK', db.String, nullable=True)\n#    , db.Column('Founded', db.Date, nullable=True)\n#)\n#\n#snp_list_table.to_sql('snp_list', con=engine, if_exists='replace', index=False)\n#\n#sql = \"SELECT * FROM snp_list LIMIT 10\"\n#cnxn = connection\n#df = pd.read_sql(sql, cnxn)\n#print(df.head(30))"
  },
  {
    "objectID": "posts/2020-09-24-AnalyzingUSRealInterestRate.html",
    "href": "posts/2020-09-24-AnalyzingUSRealInterestRate.html",
    "title": "Analyzing US Real Interest Rate From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n1959-03-31\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1959-06-30\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n1959-09-30\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n1959-12-31\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n1960-03-31\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\ndf['realint'].plot()\nplt.ylabel(\"realint\")\n\nText(0, 0.5, 'realint')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\ncycle, trend = sm.tsa.filters.hpfilter(df.realint)\n\ncycle\n\n1959-03-31   -1.195751\n1959-06-30   -0.505792\n1959-09-30   -0.205086\n1959-12-31    2.717430\n1960-03-31   -0.197051\n                ...   \n2008-09-30    4.330269\n2008-12-31    8.961987\n2009-03-31   -0.596183\n2009-06-30   -3.008487\n2009-09-30   -3.188797\nName: realint, Length: 203, dtype: float64\n\n\n\ntype(cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = trend\n\ndf[['trend','realint']].plot(figsize = (12, 8))\n\ndf[['trend','realint']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fd2a8100438&gt;"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html",
    "title": "Preprocessing example in Sklearn",
    "section": "",
    "text": "import seaborn as sns\nsns.set_theme(context=\"notebook\", font_scale=1.4,\n              rc={\"figure.constrained_layout.use\": True,\n                  \"figure.figsize\": [10, 6]})\n\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nX\n\n\n\n\n\n\n\n\nage\nhypertension\nheart_disease\navg_glucose_level\nbmi\n\n\n\n\n1\n58.0\n1\n0\n87.96\n39.2\n\n\n3\n70.0\n0\n0\n69.04\n35.9\n\n\n6\n52.0\n0\n0\n77.59\n17.7\n\n\n7\n75.0\n0\n1\n243.53\n27.0\n\n\n8\n32.0\n0\n0\n77.67\n32.3\n\n\n...\n...\n...\n...\n...\n...\n\n\n43395\n10.0\n0\n0\n58.64\n20.4\n\n\n43396\n56.0\n0\n0\n213.61\n55.4\n\n\n43397\n82.0\n1\n0\n91.94\n28.9\n\n\n43398\n40.0\n0\n0\n99.16\n33.2\n\n\n43399\n82.0\n0\n0\n79.48\n20.6\n\n\n\n\n29072 rows × 5 columns\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 10))\n\nfor name, ax in zip(df.drop(['id'], axis = 1), axes.ravel()):\n    sns.scatterplot(x=name, y='age', ax=ax, data=df)"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html#code-adapted-from-httpsgithub.comthomasjpfanml-workshop-intro",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html#code-adapted-from-httpsgithub.comthomasjpfanml-workshop-intro",
    "title": "Preprocessing example in Sklearn",
    "section": "",
    "text": "import seaborn as sns\nsns.set_theme(context=\"notebook\", font_scale=1.4,\n              rc={\"figure.constrained_layout.use\": True,\n                  \"figure.figsize\": [10, 6]})\n\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nX\n\n\n\n\n\n\n\n\nage\nhypertension\nheart_disease\navg_glucose_level\nbmi\n\n\n\n\n1\n58.0\n1\n0\n87.96\n39.2\n\n\n3\n70.0\n0\n0\n69.04\n35.9\n\n\n6\n52.0\n0\n0\n77.59\n17.7\n\n\n7\n75.0\n0\n1\n243.53\n27.0\n\n\n8\n32.0\n0\n0\n77.67\n32.3\n\n\n...\n...\n...\n...\n...\n...\n\n\n43395\n10.0\n0\n0\n58.64\n20.4\n\n\n43396\n56.0\n0\n0\n213.61\n55.4\n\n\n43397\n82.0\n1\n0\n91.94\n28.9\n\n\n43398\n40.0\n0\n0\n99.16\n33.2\n\n\n43399\n82.0\n0\n0\n79.48\n20.6\n\n\n\n\n29072 rows × 5 columns\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfig, axes = plt.subplots(3, 4, figsize=(20, 10))\n\nfor name, ax in zip(df.drop(['id'], axis = 1), axes.ravel()):\n    sns.scatterplot(x=name, y='age', ax=ax, data=df)"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html#model-without-scaling",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html#model-without-scaling",
    "title": "Preprocessing example in Sklearn",
    "section": "Model without scaling",
    "text": "Model without scaling\nRemove categories for this example\n\nfeature_names = X.columns\n\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknr = KNeighborsClassifier().fit(X_train, y_train)\nknr.score(X_train, y_train)\n\n0.9810126582278481\n\n\n\nknr.score(X_test, y_test)\n\n0.9821133736929004"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html#model-with-scaling",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html#model-with-scaling",
    "title": "Preprocessing example in Sklearn",
    "section": "Model with scaling",
    "text": "Model with scaling\n\nScale first!\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\n\n\nimport pandas as pd\nX_train_scaled_df = pd.DataFrame(X_train_scaled, columns=feature_names)\n\n\nX_train_scaled_df.plot(kind='box');\n\n\n\n\n\n\nTrain model on scaled data\n\nknr = KNeighborsClassifier().fit(X_train_scaled, y_train)\nknr.score(X_train_scaled, y_train)\n\n0.9805081636396992\n\n\n\nX_test_scaled = scaler.transform(X_test)\nknr.score(X_test_scaled, y_test)\n\n0.9815630159603742\n\n\n\n# %load solutions/03-ex1-solution.py\nfrom sklearn.svm import SVR\n\nsvr_unscaled = SVR()\n\nsvr_unscaled.fit(X_train, y_train)\n\nsvr_unscaled.score(X_test, y_test)\n\nsvr_scaled = SVR()\nsvr_scaled.fit(X_train_scaled, y_train)\n\nsvr_scaled.score(X_test_scaled, y_test)\n\n-0.38405905025130793"
  },
  {
    "objectID": "posts/2021-05-31-sklearn-preprocessing-example.html#tree-based-models",
    "href": "posts/2021-05-31-sklearn-preprocessing-example.html#tree-based-models",
    "title": "Preprocessing example in Sklearn",
    "section": "Tree based models",
    "text": "Tree based models\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ntree = DecisionTreeClassifier(random_state=0, max_depth=3).fit(X_train, y_train)\ntree.score(X_test, y_test)\n\n0.9822509631260319\n\n\n\ntree_scaled = DecisionTreeClassifier(random_state=0, max_depth=3).fit(X_train_scaled, y_train)\ntree_scaled.score(X_test_scaled, y_test)\n\n0.9822509631260319\n\n\n\nWhy are the scores the same?\n\nfrom sklearn.tree import plot_tree\nsns.reset_orig()\nfig, ax = plt.subplots(figsize=(20, 10))\n_ = plot_tree(tree, ax=ax, fontsize=16, feature_names=feature_names)\n\n\n\n\n\nfrom sklearn.tree import plot_tree\nsns.reset_orig()\nfig, ax = plt.subplots(figsize=(20, 10))\n_ = plot_tree(tree_scaled, ax=ax, fontsize=16, feature_names=feature_names)"
  },
  {
    "objectID": "posts/2020-09-19-StockMarketPortfolioAnaylsis.html",
    "href": "posts/2020-09-19-StockMarketPortfolioAnaylsis.html",
    "title": "Stock Market and Portfolio Anaylsis with pandas_datareader and quandl",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport pandas as pd\nfrom pandas_datareader import data, wb\nimport datetime\n\n\nstart = pd.to_datetime('2020-02-04')\nend = pd.to_datetime('today')\n\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f557129fa90&gt;\n\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\n# Grabbing a bunch of tech stocks for our portfolio\nCOST = quandl.get('WIKI/COST.11',\n                  start_date = start,\n                  end_date = end)\nNLSN = quandl.get('WIKI/NLSN.11',\n                   start_date = start,\n                   end_date = end)\nNKE = quandl.get('WIKI/NKE.11',\n                 start_date = start,\n                 end_date = end)\nDIS = quandl.get('WIKI/DIS.11',\n                  start_date = start,\n                  end_date = end)\n\n\n\n# Example\nCOST.iloc[0]['Adj. Close']\n\nfor stock_df in (COST, NLSN, NKE, DIS):\n    stock_df['Normed Return'] = stock_df['Adj. Close'] / stock_df.iloc[0]['Adj. Close']\n\nCOST.head()\n\nCOST.tail()\n\n## Allocations\n\nfor stock_df,allo in zip([COST,NLSN,NKE,DIS],[.2, .1, .4, .3]):\n    stock_df['Allocation'] = stock_df['Normed Return'] * allo\n\nCOST.head()\n\n## Investment\n\nfor stock_df in [COST,NLSN,NKE,DIS]:\n    stock_df['Position Values'] = stock_df['Allocation'] * 1000000\n\n## Total Portfolio Value\n\nportfolio_val = pd.concat([COST['Position Values'],\n                           NLSN['Position Values'],\n                           NKE['Position Values'],\n                           DIS['Position Values']],\n                          axis = 1)\n\nportfolio_val.head()\n\nportfolio_val.columns = ['COST Pos', 'NLSN Pos', 'NKE Pos', 'DIS Pos']\n\nportfolio_val.head()\n\nportfolio_val['Total Pos'] = portfolio_val.sum(axis = 1)\n\nportfolio_val.head()\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nportfolio_val['Total Pos'].plot(figsize = (12, 8))\nplt.title('Total Portfolio Value')\n\nportfolio_val.drop('Total Pos',\n                   axis = 1).plot(kind = 'line', figsize = (12, 8))\n\nportfolio_val.tail()\n\n\n\n\n\n\n\n\nCOST Pos\nNLSN Pos\nNKE Pos\nDIS Pos\nTotal Pos\n\n\nDate\n\n\n\n\n\n\n\n\n\n2018-03-21\n758153.014553\n144489.827125\n1.799185e+06\n1.054741e+06\n3.756569e+06\n\n\n2018-03-22\n744177.280475\n141728.307617\n1.746850e+06\n1.042104e+06\n3.674859e+06\n\n\n2018-03-23\n736843.076002\n140347.547864\n1.752545e+06\n1.020764e+06\n3.650500e+06\n\n\n2018-03-26\n762838.756299\n142663.660999\n1.786983e+06\n1.042622e+06\n3.735107e+06\n\n\n2018-03-27\n746255.305075\n142930.904822\n1.794304e+06\n1.029259e+06\n3.712749e+06"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#t-test-comparisons-uses-the-means-counts-and-standard-deviations-of-a-treatment-and-control-in-comparison-to-an-idealized-normal-distribution-to-calculate-a-p-value-which-by-intuition-is-the-likelihood-of-seeing-a-mean-difference-of-the-same-or-more-extreme-magnitude-between-treatment-and-control-as-a-result-of-chance.-this-is-done-through-a-comparison-to-an-idealized-normal-distribution-through-the-calculation-of-a-t-statistic.-while-the-test-statistic-is-assumed-to-follow-an-idealized-normal-distribution-if-the-scaling-term-but-where-the-scaling-term-is-unknown-and-it-is-instead-estimated-based-on-the-data-which-is-assumed-to-follow-the-students-t-distribution.this-process-can-be-thought-of-trying-to-disentangle-the-signal-mean-difference-and-counts-from-the-noise-variability.-here-the-mean-difference-is-the-direction-of-the-signal-and-the-counts-are-the-strength-of-the-signal.",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#t-test-comparisons-uses-the-means-counts-and-standard-deviations-of-a-treatment-and-control-in-comparison-to-an-idealized-normal-distribution-to-calculate-a-p-value-which-by-intuition-is-the-likelihood-of-seeing-a-mean-difference-of-the-same-or-more-extreme-magnitude-between-treatment-and-control-as-a-result-of-chance.-this-is-done-through-a-comparison-to-an-idealized-normal-distribution-through-the-calculation-of-a-t-statistic.-while-the-test-statistic-is-assumed-to-follow-an-idealized-normal-distribution-if-the-scaling-term-but-where-the-scaling-term-is-unknown-and-it-is-instead-estimated-based-on-the-data-which-is-assumed-to-follow-the-students-t-distribution.this-process-can-be-thought-of-trying-to-disentangle-the-signal-mean-difference-and-counts-from-the-noise-variability.-here-the-mean-difference-is-the-direction-of-the-signal-and-the-counts-are-the-strength-of-the-signal.",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "T test comparisons uses the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term, but where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the student’s t distribution.This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal.",
    "text": "T test comparisons uses the means, counts and standard deviations of a treatment and control in comparison to an idealized normal distribution to calculate a p value, which by intuition is the likelihood of seeing a mean difference of the same or more extreme magnitude between treatment and control as a result of chance. This is done through a comparison to an idealized normal distribution, through the calculation of a t-statistic. While the test statistic is assumed to follow an idealized normal distribution if the scaling term, but where the scaling term is unknown and it is instead estimated based on the data, which is assumed to follow the student’s t distribution.This process can be thought of trying to disentangle the signal (mean difference and counts) from the noise variability. Here the mean difference is the direction of the signal and the counts are the strength of the signal.\n\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\nimport seaborn as sns\n\nimport numpy as np\nimport pandas as pd\nfrom statsmodels.stats.power import NormalIndPower, TTestIndPower\nfrom scipy.stats import ttest_ind_from_stats\nimport numpy as np\nimport scipy\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n  import pandas.util.testing as tm\n\n\n\ndf = pd.read_csv('df_panel_fix.csv')\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\n\n\ndf=df_subset\ndf\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nEast China\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 7 columns\n\n\n\n\n# Add distributions by region\nimport matplotlib.pyplot as plt\n#fig, axes = plt.subplots(nrows=3, ncols=3)\n\ntest_cells = ['East China', 'North China']\nmetrics = ['gdp', 'fdi', 'it']\n\nfor test_cell in test_cells:\n    for metric in metrics:\n        df.loc[df[\"region\"] == test_cell].hist(column=[metric], bins=60)\n        print(test_cell)\n        print(metric)\n\n        \n\nEast China\ngdp\nEast China\nfdi\nEast China\nit\nNorth China\ngdp\nNorth China\nfdi\nNorth China\nit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec7cbdd8&gt;]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#distributions-of-dependant-variables",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#distributions-of-dependant-variables",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec6e00f0&gt;]],\n      dtype=object)\n\n\n\n\n\n\nsns.distplot(df['gdp'])\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec5d6a90&gt;\n\n\n\n\n\n\nsns.distplot(df['fdi'])\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4a4d30&gt;\n\n\n\n\n\n\nsns.distplot(df['it'])\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec4df278&gt;\n\n\n\n\n\n\nsns.distplot(df['specific'].dropna())\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fa7ec3e09e8&gt;\n\n\n\n\n\n\ndf.hist(column=['fdi'], bins=60)\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec31ccc0&gt;]],\n      dtype=object)"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are &gt; 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then &gt; 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])&gt;3].hist(column = ['gdp'])\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec873208&gt;]],\n      dtype=object)\n\n\n\n\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])&lt;3]\n\n\ndf_no_gdp_outliers\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n-0.521466\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n-0.464746\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n-0.421061\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n-0.383239\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n-0.340870\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n354\n2002\nEast China\nZhejiang\n8003.67\n307610\n1962633\n365437.0\n0.798274\n\n\n355\n2003\nEast China\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n1.178172\n\n\n356\n2004\nEast China\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n1.612181\n\n\n357\n2005\nEast China\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n2.007180\n\n\n358\n2006\nEast China\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n2.520929\n\n\n\n\n350 rows × 8 columns\n\n\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\narray([[&lt;matplotlib.axes._subplots.AxesSubplot object at 0x7fa7ec95e4e0&gt;]],\n      dtype=object)\n\n\n\n\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\n\n\n\n\n\n\nyear\nprovince\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\nregion\n\n\n\n\n\n\n\n\n\n\n\nEast China\n84\n84\n84\n84\n84\n84\n84\n\n\nNorth China\n48\n48\n48\n48\n48\n47\n48\n\n\nNortheast China\n36\n36\n36\n36\n36\n36\n36\n\n\nNorthwest China\n60\n60\n60\n60\n60\n60\n60\n\n\nSouth Central China\n72\n72\n72\n72\n72\n72\n72\n\n\nSouthwest China\n60\n60\n60\n60\n60\n57\n60\n\n\n\n\n\n\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n\n\n\n\n\n\nyear\nregion\ngdp\nfdi\nit\nspecific\ngdp_zscore\n\n\nprovince\n\n\n\n\n\n\n\n\n\n\n\nAnhui\n12\n12\n12\n12\n12\n12\n12\n\n\nBeijing\n12\n12\n12\n12\n12\n12\n12\n\n\nChongqing\n12\n12\n12\n12\n12\n9\n12\n\n\nFujian\n12\n12\n12\n12\n12\n12\n12\n\n\nGansu\n12\n12\n12\n12\n12\n12\n12\n\n\nGuangdong\n12\n12\n12\n12\n12\n12\n12\n\n\nGuangxi\n12\n12\n12\n12\n12\n12\n12\n\n\nGuizhou\n12\n12\n12\n12\n12\n12\n12\n\n\nHainan\n12\n12\n12\n12\n12\n12\n12\n\n\nHebei\n12\n12\n12\n12\n12\n11\n12\n\n\nHeilongjiang\n12\n12\n12\n12\n12\n12\n12\n\n\nHenan\n12\n12\n12\n12\n12\n12\n12\n\n\nHubei\n12\n12\n12\n12\n12\n12\n12\n\n\nHunan\n12\n12\n12\n12\n12\n12\n12\n\n\nJiangsu\n12\n12\n12\n12\n12\n12\n12\n\n\nJiangxi\n12\n12\n12\n12\n12\n12\n12\n\n\nJilin\n12\n12\n12\n12\n12\n12\n12\n\n\nLiaoning\n12\n12\n12\n12\n12\n12\n12\n\n\nNingxia\n12\n12\n12\n12\n12\n12\n12\n\n\nQinghai\n12\n12\n12\n12\n12\n12\n12\n\n\nShaanxi\n12\n12\n12\n12\n12\n12\n12\n\n\nShandong\n12\n12\n12\n12\n12\n12\n12\n\n\nShanghai\n12\n12\n12\n12\n12\n12\n12\n\n\nShanxi\n12\n12\n12\n12\n12\n12\n12\n\n\nSichuan\n12\n12\n12\n12\n12\n12\n12\n\n\nTianjin\n12\n12\n12\n12\n12\n12\n12\n\n\nTibet\n12\n12\n12\n12\n12\n12\n12\n\n\nXinjiang\n12\n12\n12\n12\n12\n12\n12\n\n\nYunnan\n12\n12\n12\n12\n12\n12\n12\n\n\nZhejiang\n12\n12\n12\n12\n12\n12\n12\n\n\n\n\n\n\n\n\n#df_no_gdp_outliers.pivot_table(index='grouping column 1', columns='grouping column 2', values='aggregating column', aggfunc='sum')\n\n\n#pd.crosstab(df_no_gdp_outliers, 'year')\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\n\n\n\n\n\n\nregion\ngdp\nfdi\nit\n\n\n\n\n0\nEast China\n2093.30\n50661\n631930\n\n\n1\nEast China\n2347.32\n43443\n657860\n\n\n2\nEast China\n2542.96\n27673\n889463\n\n\n3\nEast China\n2712.34\n26131\n1227364\n\n\n4\nEast China\n2902.09\n31847\n1499110\n\n\n...\n...\n...\n...\n...\n\n\n354\nEast China\n8003.67\n307610\n1962633\n\n\n355\nEast China\n9705.02\n498055\n2261631\n\n\n356\nEast China\n11648.70\n668128\n3162299\n\n\n357\nEast China\n13417.68\n772000\n2370200\n\n\n358\nEast China\n15718.47\n888935\n2553268\n\n\n\n\n350 rows × 4 columns\n\n\n\n\ndef aggregate_and_ttest(dataset, groupby_feature='region', alpha=.05, test_cells = [0, 1]):\n    #Imports\n    from tqdm import tqdm\n    from scipy.stats import ttest_ind_from_stats\n\n    \n    metrics = ['gdp', 'fdi', 'it']\n    \n    feature_size = 'size'\n    feature_mean = 'mean'\n    feature_std = 'std'    \n\n    for metric in tqdm(metrics):\n        \n        #print(metric)\n        crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg(['size', 'mean', 'std'])\n        print(crosstab)\n        \n        treatment = crosstab.index[test_cells[0]]\n        control = crosstab.index[test_cells[1]]\n        \n        counts_control = crosstab.loc[control, feature_size]\n        counts_treatment = crosstab.loc[treatment, feature_size]\n\n        mean_control = crosstab.loc[control, feature_mean]\n        mean_treatment = crosstab.loc[treatment, feature_mean]\n\n        standard_deviation_control = crosstab.loc[control, feature_std]\n        standard_deviation_treatment = crosstab.loc[treatment, feature_std]\n        \n        t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control)\n        \n        #fstring to print the p value and t statistic\n        print(f\"The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} is {t_statistic} and the p value is {p_value}.\")\n        \n        #f string to say of the comparison is significant at a given alpha level\n\n        if p_value &lt; alpha: \n            print(f'The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}') \n        else: \n            print(f'The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}')\n\n\naggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\n\n100%|██████████| 3/3 [00:00&lt;00:00, 115.78it/s]\n\n\n                     size         mean          std\nregion                                             \nEast China             78  6070.604231  3500.372702\nNorth China            48  4239.038542  2866.705149\nNortheast China        36  3849.076944  1948.531835\nNorthwest China        60  1340.026167  1174.399739\nSouth Central China    68  4835.540882  3697.129915\nSouthwest China        60  2410.398833  2144.589994\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size           mean            std\nregion                                                 \nEast China             78  355577.897436  275635.866746\nNorth China            48  169600.583333  127011.475909\nNortheast China        36  136623.750000  142734.495232\nNorthwest China        60   15111.133333   22954.193559\nSouth Central China    68  218931.426471  339981.399823\nSouthwest China        60   25405.083333   31171.373876\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size          mean           std\nregion                                               \nEast China             78  1.775615e+06  1.153030e+06\nNorth China            48  1.733719e+06  1.548794e+06\nNortheast China        36  2.665148e+06  1.768442e+06\nNorthwest China        60  1.703538e+06  1.446408e+06\nSouth Central China    68  2.500962e+06  2.196436e+06\nSouthwest China        60  2.424971e+06  2.002198e+06\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372.\nThe comparison between East China and North China is not statistically significant at the threshold of 0.05\n\n\n\nfrom tqdm import tqdm\nfor i in tqdm(range(10000)):\n    ...\n\n100%|██████████| 10000/10000 [00:00&lt;00:00, 2169617.21it/s]\n\n\n\nEastvNorth=pd.DataFrame()\nEastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\nEastvNorth\n\n100%|██████████| 3/3 [00:00&lt;00:00, 135.00it/s]\n\n\n                     size         mean          std\nregion                                             \nEast China             78  6070.604231  3500.372702\nNorth China            48  4239.038542  2866.705149\nNortheast China        36  3849.076944  1948.531835\nNorthwest China        60  1340.026167  1174.399739\nSouth Central China    68  4835.540882  3697.129915\nSouthwest China        60  2410.398833  2144.589994\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 3.0488753833171947 and the p value is 0.002808541335921234.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size           mean            std\nregion                                                 \nEast China             78  355577.897436  275635.866746\nNorth China            48  169600.583333  127011.475909\nNortheast China        36  136623.750000  142734.495232\nNorthwest China        60   15111.133333   22954.193559\nSouth Central China    68  218931.426471  339981.399823\nSouthwest China        60   25405.083333   31171.373876\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 4.391461461316698 and the p value is 2.3859390186769955e-05.\nThe comparison between East China and North China is statistically significant at the threshold of 0.05\n                     size          mean           std\nregion                                               \nEast China             78  1.775615e+06  1.153030e+06\nNorth China            48  1.733719e+06  1.548794e+06\nNortheast China        36  2.665148e+06  1.768442e+06\nNorthwest China        60  1.703538e+06  1.446408e+06\nSouth Central China    68  2.500962e+06  2.196436e+06\nSouthwest China        60  2.424971e+06  2.002198e+06\nThe t statistic of the comparison of the treatment test cell of East China compared to the control test cell of North China is 0.17339716493934587 and the p value is 0.862621991978372.\nThe comparison between East China and North China is not statistically significant at the threshold of 0.05"
  },
  {
    "objectID": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "href": "posts/2020-10-19-ttests-distributions-crosstabs-functions.html#genearate-an-experimental_crosstab-to-be-used-in-statistical-tests",
    "title": "Evaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python",
    "section": "Genearate an experimental_crosstab to be used in statistical tests",
    "text": "Genearate an experimental_crosstab to be used in statistical tests\n\nexperimental_crosstab = df_no_gdp_outliers_subset.groupby('region').agg(['size', 'mean', 'std'])\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\ndf = experimental_crosstab.T\ndf\n\n\n\n\n\n\n\n\nregion\nEast China\nNorth China\nNortheast China\nNorthwest China\nSouth Central China\nSouthwest China\n\n\n\n\ngdp\nsize\n7.800000e+01\n4.800000e+01\n3.600000e+01\n6.000000e+01\n6.800000e+01\n6.000000e+01\n\n\nmean\n6.070604e+03\n4.239039e+03\n3.849077e+03\n1.340026e+03\n4.835541e+03\n2.410399e+03\n\n\nstd\n3.500373e+03\n2.866705e+03\n1.948532e+03\n1.174400e+03\n3.697130e+03\n2.144590e+03\n\n\nfdi\nsize\n7.800000e+01\n4.800000e+01\n3.600000e+01\n6.000000e+01\n6.800000e+01\n6.000000e+01\n\n\nmean\n3.555779e+05\n1.696006e+05\n1.366238e+05\n1.511113e+04\n2.189314e+05\n2.540508e+04\n\n\nstd\n2.756359e+05\n1.270115e+05\n1.427345e+05\n2.295419e+04\n3.399814e+05\n3.117137e+04\n\n\nit\nsize\n7.800000e+01\n4.800000e+01\n3.600000e+01\n6.000000e+01\n6.800000e+01\n6.000000e+01\n\n\nmean\n1.775615e+06\n1.733719e+06\n2.665148e+06\n1.703538e+06\n2.500962e+06\n2.424971e+06\n\n\nstd\n1.153030e+06\n1.548794e+06\n1.768442e+06\n1.446408e+06\n2.196436e+06\n2.002198e+06\n\n\n\n\n\n\n\n\n#experimental_crosstab.reset_index().unstack()\n\n\nexperimental_crosstab.iloc[0,1]\n\n6070.604230769231\n\n\n\nexperimental_crosstab.index\n\nIndex(['East China', 'North China', 'Northeast China', 'Northwest China',\n       'South Central China', 'Southwest China'],\n      dtype='object', name='region')\n\n\n\nexperimental_crosstab\n\n\n\n\n\n\n\n\ngdp\nfdi\nit\n\n\n\nsize\nmean\nstd\nsize\nmean\nstd\nsize\nmean\nstd\n\n\nregion\n\n\n\n\n\n\n\n\n\n\n\n\n\nEast China\n78\n6070.604231\n3500.372702\n78\n355577.897436\n275635.866746\n78\n1.775615e+06\n1.153030e+06\n\n\nNorth China\n48\n4239.038542\n2866.705149\n48\n169600.583333\n127011.475909\n48\n1.733719e+06\n1.548794e+06\n\n\nNortheast China\n36\n3849.076944\n1948.531835\n36\n136623.750000\n142734.495232\n36\n2.665148e+06\n1.768442e+06\n\n\nNorthwest China\n60\n1340.026167\n1174.399739\n60\n15111.133333\n22954.193559\n60\n1.703538e+06\n1.446408e+06\n\n\nSouth Central China\n68\n4835.540882\n3697.129915\n68\n218931.426471\n339981.399823\n68\n2.500962e+06\n2.196436e+06\n\n\nSouthwest China\n60\n2410.398833\n2144.589994\n60\n25405.083333\n31171.373876\n60\n2.424971e+06\n2.002198e+06\n\n\n\n\n\n\n\n\nexperimental_crosstab.columns = ['_'.join(col) for col in experimental_crosstab.columns.values]\n\n\nexperimental_crosstab\n\n\n\n\n\n\n\n\ngdp_size\ngdp_mean\ngdp_std\nfdi_size\nfdi_mean\nfdi_std\nit_size\nit_mean\nit_std\n\n\nregion\n\n\n\n\n\n\n\n\n\n\n\n\n\nEast China\n78\n6070.604231\n3500.372702\n78\n355577.897436\n275635.866746\n78\n1.775615e+06\n1.153030e+06\n\n\nNorth China\n48\n4239.038542\n2866.705149\n48\n169600.583333\n127011.475909\n48\n1.733719e+06\n1.548794e+06\n\n\nNortheast China\n36\n3849.076944\n1948.531835\n36\n136623.750000\n142734.495232\n36\n2.665148e+06\n1.768442e+06\n\n\nNorthwest China\n60\n1340.026167\n1174.399739\n60\n15111.133333\n22954.193559\n60\n1.703538e+06\n1.446408e+06\n\n\nSouth Central China\n68\n4835.540882\n3697.129915\n68\n218931.426471\n339981.399823\n68\n2.500962e+06\n2.196436e+06\n\n\nSouthwest China\n60\n2410.398833\n2144.589994\n60\n25405.083333\n31171.373876\n60\n2.424971e+06\n2.002198e+06\n\n\n\n\n\n\n\n\nexperimental_crosstab.loc['East China', 'gdp_size']\n\n78\n\n\n\nexperimental_crosstab.to_csv('fiscal_experimental_crosstab.csv')"
  },
  {
    "objectID": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html",
    "href": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader\nimport datetime\nimport pandas_datareader.data as web\n\nimport statsmodels.api as sm\nimport quandl\nstart = datetime.datetime(1960, 1, 1)\nend = pd.to_datetime('today')\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n1960-01-01\n58.03\n\n\n1960-02-01\n55.78\n\n\n1960-03-01\n55.02\n\n\n1960-04-01\n55.73\n\n\n1960-05-01\n55.22\n\n\n...\n...\n\n\n2020-08-01\n3391.71\n\n\n2020-08-31\n3500.31\n\n\n2020-09-01\n3526.65\n\n\n2020-09-30\n3363.00\n\n\n2020-10-01\n3380.80\n\n\n\n\n740 rows × 1 columns\ndf = SP500\ndf.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n1960-01-01\n58.03\n\n\n1960-02-01\n55.78\n\n\n1960-03-01\n55.02\n\n\n1960-04-01\n55.73\n\n\n1960-05-01\n55.22\ndf.tail()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2020-08-01\n3391.71\n\n\n2020-08-31\n3500.31\n\n\n2020-09-01\n3526.65\n\n\n2020-09-30\n3363.00\n\n\n2020-10-01\n3380.80\ndf.columns = ['Value']\ndf.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n1960-01-01\n58.03\n\n\n1960-02-01\n55.78\n\n\n1960-03-01\n55.02\n\n\n1960-04-01\n55.73\n\n\n1960-05-01\n55.22\ndf.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n1960-01-01\n58.03\n\n\n1960-02-01\n55.78\n\n\n1960-03-01\n55.02\n\n\n1960-04-01\n55.73\n\n\n1960-05-01\n55.22\ndf.describe().transpose()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nValue\n740.0\n761.732932\n834.566138\n53.73\n100.9\n349.425\n1239.415\n3526.65"
  },
  {
    "objectID": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html#step-2-visualize-the-data",
    "href": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html#step-2-visualize-the-data",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Step 2: Visualize the Data",
    "text": "Step 2: Visualize the Data\nLet’s visualize this data with a few methods.\n\ndf.plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8873251b38&gt;\n\n\n\n\n\n\ntimeseries = df['Value']\n\n\ntimeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')\ntimeseries.rolling(12).std().plot(label='12 Month Rolling Std')\ntimeseries.plot()\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f88731e22e8&gt;\n\n\n\n\n\n\ntimeseries.rolling(12).mean().plot(label='12 Month Rolling Mean')\ntimeseries.plot()\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f887315f630&gt;"
  },
  {
    "objectID": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html#decomposition",
    "href": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html#decomposition",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Decomposition",
    "text": "Decomposition\nETS decomposition allows us to see the individual parts!\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(df['Value'], freq=12)  \nfig = plt.figure()  \nfig = decomposition.plot()  \nfig.set_size_inches(15, 8)\n\n&lt;Figure size 432x288 with 0 Axes&gt;"
  },
  {
    "objectID": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html#testing-for-stationarity",
    "href": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html#testing-for-stationarity",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Testing for Stationarity",
    "text": "Testing for Stationarity\n\ndf.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n1960-01-01\n58.03\n\n\n1960-02-01\n55.78\n\n\n1960-03-01\n55.02\n\n\n1960-04-01\n55.73\n\n\n1960-05-01\n55.22\n\n\n\n\n\n\n\n\nfrom statsmodels.tsa.stattools import adfuller\n\n\nresult = adfuller(df['Value'])\n\n\nprint('Augmented Dickey-Fuller Test:')\nlabels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\nfor value,label in zip(result,labels):\n    print(label+' : '+str(value) )\n    \nif result[1] &lt;= 0.05:\n    print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\nelse:\n    print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : 1.7247353245135\np-value : 0.9981874531215522\n#Lags Used : 20\nNumber of Observations Used : 719\nweak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \n\n\n\ndef adf_check(time_series):\n    \"\"\"\n    Pass in a time series, returns ADF report\n    \"\"\"\n    result = adfuller(time_series)\n    print('Augmented Dickey-Fuller Test:')\n    labels = ['ADF Test Statistic','p-value','#Lags Used','Number of Observations Used']\n\n    for value,label in zip(result,labels):\n        print(label+' : '+str(value) )\n    \n    if result[1] &lt;= 0.05:\n        print(\"strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\")\n    else:\n        print(\"weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary \")\n\n** First Difference **\n\ndf['Value First Difference'] = df['Value'] - df['Value'].shift(1)\n\n\nadf_check(df['Value First Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -4.267790128581322\np-value : 0.0005048563860225925\n#Lags Used : 20\nNumber of Observations Used : 718\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\ndf['Value First Difference'].plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f886d63feb8&gt;\n\n\n\n\n\n** Second Difference **\n\ndf['Value Second Difference'] = df['Value First Difference'] - df['Value First Difference'].shift(1)\n\n\nadf_check(df['Value Second Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -12.29955077642857\np-value : 7.504260735615441e-23\n#Lags Used : 18\nNumber of Observations Used : 719\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\ndf['Value Second Difference'].plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8877f726d8&gt;\n\n\n\n\n\n** Seasonal Difference **\n\ndf['Seasonal Difference'] = df['Value'] - df['Value'].shift(12)\ndf['Seasonal Difference'].plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8877fb79e8&gt;\n\n\n\n\n\n\nadf_check(df['Seasonal Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -5.239903673260254\np-value : 7.284266188346342e-06\n#Lags Used : 20\nNumber of Observations Used : 707\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n** Seasonal First Difference **\n\ndf['Seasonal First Difference'] = df['Value First Difference'] - df['Value First Difference'].shift(12)\ndf['Seasonal First Difference'].plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f88acb0c2b0&gt;\n\n\n\n\n\n\nadf_check(df['Seasonal First Difference'].dropna())\n\nAugmented Dickey-Fuller Test:\nADF Test Statistic : -6.196739887980032\np-value : 5.940155101037563e-08\n#Lags Used : 20\nNumber of Observations Used : 706\nstrong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root and is stationary\n\n\n\nfrom statsmodels.graphics.tsaplots import plot_acf,plot_pacf\n\n\n# Check out: https://stackoverflow.com/questions/21788593/statsmodels-duplicate-charts\n# https://github.com/statsmodels/statsmodels/issues/1265\nfig_first = plot_acf(df[\"Value First Difference\"].dropna())\n\n\n\n\n\nfig_seasonal_first = plot_acf(df[\"Seasonal First Difference\"].dropna())\n\n\n\n\nPandas also has this functionality built in, but only for ACF, not PACF. So I recommend using statsmodels, as ACF and PACF is more core to its functionality than it is to pandas’ functionality.\n\nfrom pandas.plotting import autocorrelation_plot\nautocorrelation_plot(df['Seasonal First Difference'].dropna())\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f886d660358&gt;\n\n\n\n\n\nWe can then plot this relationship:\n\nresult = plot_pacf(df[\"Seasonal First Difference\"].dropna())\n\n\n\n\n\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(df['Seasonal First Difference'].iloc[13:], lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(df['Seasonal First Difference'].iloc[13:], lags=40, ax=ax2)\n\n\n\n\n\nfrom statsmodels.tsa.arima_model import ARIMA\n\n\nhelp(ARIMA)\n\nHelp on class ARIMA in module statsmodels.tsa.arima_model:\n\nclass ARIMA(ARMA)\n |  ARIMA(endog, order, exog=None, dates=None, freq=None, missing='none')\n |  \n |  Autoregressive Integrated Moving Average ARIMA(p,d,q) Model\n |  \n |  Parameters\n |  ----------\n |  endog : array-like\n |      The endogenous variable.\n |  order : iterable\n |      The (p,d,q) order of the model for the number of AR parameters,\n |      differences, and MA parameters to use.\n |  exog : array-like, optional\n |      An optional array of exogenous variables. This should *not* include a\n |      constant or trend. You can specify this in the `fit` method.\n |  dates : array-like of datetime, optional\n |      An array-like object of datetime objects. If a pandas object is given\n |      for endog or exog, it is assumed to have a DateIndex.\n |  freq : str, optional\n |      The frequency of the time-series. A Pandas offset or 'B', 'D', 'W',\n |      'M', 'A', or 'Q'. This is optional if dates are given.\n |  \n |  \n |  Notes\n |  -----\n |  If exogenous variables are given, then the model that is fit is\n |  \n |  .. math::\n |  \n |     \\phi(L)(y_t - X_t\\beta) = \\theta(L)\\epsilon_t\n |  \n |  where :math:`\\phi` and :math:`\\theta` are polynomials in the lag\n |  operator, :math:`L`. This is the regression model with ARMA errors,\n |  or ARMAX model. This specification is used, whether or not the model\n |  is fit using conditional sum of square or maximum-likelihood, using\n |  the `method` argument in\n |  :meth:`statsmodels.tsa.arima_model.ARIMA.fit`. Therefore, for\n |  now, `css` and `mle` refer to estimation methods only. This may\n |  change for the case of the `css` model in future versions.\n |  \n |  Method resolution order:\n |      ARIMA\n |      ARMA\n |      statsmodels.tsa.base.tsa_model.TimeSeriesModel\n |      statsmodels.base.model.LikelihoodModel\n |      statsmodels.base.model.Model\n |      builtins.object\n |  \n |  Methods defined here:\n |  \n |  __getnewargs__(self)\n |  \n |  __init__(self, endog, order, exog=None, dates=None, freq=None, missing='none')\n |      Initialize self.  See help(type(self)) for accurate signature.\n |  \n |  fit(self, start_params=None, trend='c', method='css-mle', transparams=True, solver='lbfgs', maxiter=500, full_output=1, disp=5, callback=None, start_ar_lags=None, **kwargs)\n |      Fits ARIMA(p,d,q) model by exact maximum likelihood via Kalman filter.\n |      \n |      Parameters\n |      ----------\n |      start_params : array-like, optional\n |          Starting parameters for ARMA(p,q).  If None, the default is given\n |          by ARMA._fit_start_params.  See there for more information.\n |      transparams : bool, optional\n |          Whehter or not to transform the parameters to ensure stationarity.\n |          Uses the transformation suggested in Jones (1980).  If False,\n |          no checking for stationarity or invertibility is done.\n |      method : str {'css-mle','mle','css'}\n |          This is the loglikelihood to maximize.  If \"css-mle\", the\n |          conditional sum of squares likelihood is maximized and its values\n |          are used as starting values for the computation of the exact\n |          likelihood via the Kalman filter.  If \"mle\", the exact likelihood\n |          is maximized via the Kalman Filter.  If \"css\" the conditional sum\n |          of squares likelihood is maximized.  All three methods use\n |          `start_params` as starting parameters.  See above for more\n |          information.\n |      trend : str {'c','nc'}\n |          Whether to include a constant or not.  'c' includes constant,\n |          'nc' no constant.\n |      solver : str or None, optional\n |          Solver to be used.  The default is 'lbfgs' (limited memory\n |          Broyden-Fletcher-Goldfarb-Shanno).  Other choices are 'bfgs',\n |          'newton' (Newton-Raphson), 'nm' (Nelder-Mead), 'cg' -\n |          (conjugate gradient), 'ncg' (non-conjugate gradient), and\n |          'powell'. By default, the limited memory BFGS uses m=12 to\n |          approximate the Hessian, projected gradient tolerance of 1e-8 and\n |          factr = 1e2. You can change these by using kwargs.\n |      maxiter : int, optional\n |          The maximum number of function evaluations. Default is 500.\n |      tol : float\n |          The convergence tolerance.  Default is 1e-08.\n |      full_output : bool, optional\n |          If True, all output from solver will be available in\n |          the Results object's mle_retvals attribute.  Output is dependent\n |          on the solver.  See Notes for more information.\n |      disp : int, optional\n |          If True, convergence information is printed.  For the default\n |          l_bfgs_b solver, disp controls the frequency of the output during\n |          the iterations. disp &lt; 0 means no output in this case.\n |      callback : function, optional\n |          Called after each iteration as callback(xk) where xk is the current\n |          parameter vector.\n |      start_ar_lags : int, optional\n |          Parameter for fitting start_params. When fitting start_params,\n |          residuals are obtained from an AR fit, then an ARMA(p,q) model is\n |          fit via OLS using these residuals. If start_ar_lags is None, fit\n |          an AR process according to best BIC. If start_ar_lags is not None,\n |          fits an AR process with a lag length equal to start_ar_lags.\n |          See ARMA._fit_start_params_hr for more information.\n |      kwargs\n |          See Notes for keyword arguments that can be passed to fit.\n |      \n |      Returns\n |      -------\n |      `statsmodels.tsa.arima.ARIMAResults` class\n |      \n |      See Also\n |      --------\n |      statsmodels.base.model.LikelihoodModel.fit : for more information\n |          on using the solvers.\n |      ARIMAResults : results class returned by fit\n |      \n |      Notes\n |      -----\n |      If fit by 'mle', it is assumed for the Kalman Filter that the initial\n |      unknown state is zero, and that the initial variance is\n |      P = dot(inv(identity(m**2)-kron(T,T)),dot(R,R.T).ravel('F')).reshape(r,\n |      r, order = 'F')\n |  \n |  predict(self, params, start=None, end=None, exog=None, typ='linear', dynamic=False)\n |      ARIMA model in-sample and out-of-sample prediction\n |      \n |      Parameters\n |      ----------\n |      params : array-like\n |          The fitted parameters of the model.\n |      start : int, str, or datetime\n |          Zero-indexed observation number at which to start forecasting, ie.,\n |          the first forecast is start. Can also be a date string to\n |          parse or a datetime type.\n |      end : int, str, or datetime\n |          Zero-indexed observation number at which to end forecasting, ie.,\n |          the first forecast is start. Can also be a date string to\n |          parse or a datetime type. However, if the dates index does not\n |          have a fixed frequency, end must be an integer index if you\n |          want out of sample prediction.\n |      exog : array-like, optional\n |          If the model is an ARMAX and out-of-sample forecasting is\n |          requested, exog must be given. Note that you'll need to pass\n |          `k_ar` additional lags for any exogenous variables. E.g., if you\n |          fit an ARMAX(2, q) model and want to predict 5 steps, you need 7\n |          observations to do this.\n |      dynamic : bool, optional\n |          The `dynamic` keyword affects in-sample prediction. If dynamic\n |          is False, then the in-sample lagged values are used for\n |          prediction. If `dynamic` is True, then in-sample forecasts are\n |          used in place of lagged dependent variables. The first forecasted\n |          value is `start`.\n |      typ : str {'linear', 'levels'}\n |      \n |          - 'linear' : Linear prediction in terms of the differenced\n |            endogenous variables.\n |          - 'levels' : Predict the levels of the original endogenous\n |            variables.\n |      \n |      \n |      Returns\n |      -------\n |      predict : array\n |          The predicted values.\n |      \n |      \n |      \n |      Notes\n |      -----\n |      Use the results predict method instead.\n |  \n |  ----------------------------------------------------------------------\n |  Static methods defined here:\n |  \n |  __new__(cls, endog, order, exog=None, dates=None, freq=None, missing='none')\n |      Create and return a new object.  See help(type) for accurate signature.\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from ARMA:\n |  \n |  geterrors(self, params)\n |      Get the errors of the ARMA process.\n |      \n |      Parameters\n |      ----------\n |      params : array-like\n |          The fitted ARMA parameters\n |      order : array-like\n |          3 item iterable, with the number of AR, MA, and exogenous\n |          parameters, including the trend\n |  \n |  hessian(self, params)\n |      Compute the Hessian at params,\n |      \n |      Notes\n |      -----\n |      This is a numerical approximation.\n |  \n |  loglike(self, params, set_sigma2=True)\n |      Compute the log-likelihood for ARMA(p,q) model\n |      \n |      Notes\n |      -----\n |      Likelihood used depends on the method set in fit\n |  \n |  loglike_css(self, params, set_sigma2=True)\n |      Conditional Sum of Squares likelihood function.\n |  \n |  loglike_kalman(self, params, set_sigma2=True)\n |      Compute exact loglikelihood for ARMA(p,q) model by the Kalman Filter.\n |  \n |  score(self, params)\n |      Compute the score function at params.\n |      \n |      Notes\n |      -----\n |      This is a numerical approximation.\n |  \n |  ----------------------------------------------------------------------\n |  Class methods inherited from ARMA:\n |  \n |  from_formula(formula, data, subset=None, drop_cols=None, *args, **kwargs) from builtins.type\n |      Create a Model from a formula and dataframe.\n |      \n |      Parameters\n |      ----------\n |      formula : str or generic Formula object\n |          The formula specifying the model\n |      data : array-like\n |          The data for the model. See Notes.\n |      subset : array-like\n |          An array-like object of booleans, integers, or index values that\n |          indicate the subset of df to use in the model. Assumes df is a\n |          `pandas.DataFrame`\n |      drop_cols : array-like\n |          Columns to drop from the design matrix.  Cannot be used to\n |          drop terms involving categoricals.\n |      args : extra arguments\n |          These are passed to the model\n |      kwargs : extra keyword arguments\n |          These are passed to the model with one exception. The\n |          ``eval_env`` keyword is passed to patsy. It can be either a\n |          :class:`patsy:patsy.EvalEnvironment` object or an integer\n |          indicating the depth of the namespace to use. For example, the\n |          default ``eval_env=0`` uses the calling namespace. If you wish\n |          to use a \"clean\" environment set ``eval_env=-1``.\n |      \n |      Returns\n |      -------\n |      model : Model instance\n |      \n |      Notes\n |      -----\n |      data must define __getitem__ with the keys in the formula terms\n |      args and kwargs are passed on to the model instantiation. E.g.,\n |      a numpy structured or rec array, a dictionary, or a pandas DataFrame.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from statsmodels.tsa.base.tsa_model.TimeSeriesModel:\n |  \n |  exog_names\n |  \n |  ----------------------------------------------------------------------\n |  Methods inherited from statsmodels.base.model.LikelihoodModel:\n |  \n |  information(self, params)\n |      Fisher information matrix of model\n |      \n |      Returns -Hessian of loglike evaluated at params.\n |  \n |  initialize(self)\n |      Initialize (possibly re-initialize) a Model instance. For\n |      instance, the design matrix of a linear model may change\n |      and some things must be recomputed.\n |  \n |  ----------------------------------------------------------------------\n |  Data descriptors inherited from statsmodels.base.model.Model:\n |  \n |  __dict__\n |      dictionary for instance variables (if defined)\n |  \n |  __weakref__\n |      list of weak references to the object (if defined)\n |  \n |  endog_names\n |      Names of endogenous variables\n\n\n\n\nmodel = sm.tsa.statespace.SARIMAX(df['Value'],order=(0,1,0), seasonal_order=(1,1,1,12))\nresults = model.fit()\nprint(results.summary())\n\n/home/gao/anaconda3/lib/python3.7/site-packages/statsmodels/tsa/base/tsa_model.py:219: ValueWarning: A date index has been provided, but it has no associated frequency information and so will be ignored when e.g. forecasting.\n  ' ignored when e.g. forecasting.', ValueWarning)\n\n\n                                 Statespace Model Results                                 \n==========================================================================================\nDep. Variable:                              Value   No. Observations:                  740\nModel:             SARIMAX(0, 1, 0)x(1, 1, 1, 12)   Log Likelihood               -3719.108\nDate:                            Fri, 23 Oct 2020   AIC                           7444.215\nTime:                                    09:03:22   BIC                           7457.982\nSample:                                         0   HQIC                          7449.528\n                                            - 740                                         \nCovariance Type:                              opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nar.S.L12       0.0043      0.021      0.201      0.840      -0.037       0.046\nma.S.L12      -0.9513      0.018    -53.297      0.000      -0.986      -0.916\nsigma2      1563.8216     30.663     51.001      0.000    1503.724    1623.919\n===================================================================================\nLjung-Box (Q):                      116.83   Jarque-Bera (JB):              9251.13\nProb(Q):                              0.00   Prob(JB):                         0.00\nHeteroskedasticity (H):             410.23   Skew:                            -1.86\nProb(H) (two-sided):                  0.00   Kurtosis:                        20.08\n===================================================================================\n\nWarnings:\n[1] Covariance matrix calculated using the outer product of gradients (complex-step).\n\n\n\nresults.resid.plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872daf940&gt;\n\n\n\n\n\n\nresults.resid.plot(kind='kde')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872d863c8&gt;"
  },
  {
    "objectID": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html#prediction-of-future-values",
    "href": "posts/2020-10-25-ARIMA-ADF-FORECASTING.html#prediction-of-future-values",
    "title": "Stock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting",
    "section": "Prediction of Future Values",
    "text": "Prediction of Future Values\nFirts we can get an idea of how well our model performs by just predicting for values that we actually already know:\n\ndf['forecast'] = results.predict(start = 1, end= 720, dynamic= True)  \ndf[['Value','forecast']].plot(figsize=(12,8))\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f88ace0d7b8&gt;\n\n\n\n\n\n\nForecasting\n\ndf.tail()\n\n\n\n\n\n\n\n\nValue\nValue First Difference\nValue Second Difference\nSeasonal Difference\nSeasonal First Difference\nforecast\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-08-01\n3391.71\n120.59\n57.09\n166.67\n173.75\nNaN\n\n\n2020-08-31\n3500.31\n108.60\n-11.99\n223.00\n56.33\nNaN\n\n\n2020-09-01\n3526.65\n26.34\n-82.26\n571.84\n348.84\nNaN\n\n\n2020-09-30\n3363.00\n-163.65\n-189.99\n710.61\n138.77\nNaN\n\n\n2020-10-01\n3380.80\n17.80\n181.45\n796.21\n85.60\nNaN\n\n\n\n\n\n\n\n\nfrom pandas.tseries.offsets import DateOffset\n\n\nfuture_dates = [df.index[-1] + DateOffset(months=x) for x in range(0,24) ]\n\n\nfuture_dates\n\n[Timestamp('2020-10-01 00:00:00'),\n Timestamp('2020-11-01 00:00:00'),\n Timestamp('2020-12-01 00:00:00'),\n Timestamp('2021-01-01 00:00:00'),\n Timestamp('2021-02-01 00:00:00'),\n Timestamp('2021-03-01 00:00:00'),\n Timestamp('2021-04-01 00:00:00'),\n Timestamp('2021-05-01 00:00:00'),\n Timestamp('2021-06-01 00:00:00'),\n Timestamp('2021-07-01 00:00:00'),\n Timestamp('2021-08-01 00:00:00'),\n Timestamp('2021-09-01 00:00:00'),\n Timestamp('2021-10-01 00:00:00'),\n Timestamp('2021-11-01 00:00:00'),\n Timestamp('2021-12-01 00:00:00'),\n Timestamp('2022-01-01 00:00:00'),\n Timestamp('2022-02-01 00:00:00'),\n Timestamp('2022-03-01 00:00:00'),\n Timestamp('2022-04-01 00:00:00'),\n Timestamp('2022-05-01 00:00:00'),\n Timestamp('2022-06-01 00:00:00'),\n Timestamp('2022-07-01 00:00:00'),\n Timestamp('2022-08-01 00:00:00'),\n Timestamp('2022-09-01 00:00:00')]\n\n\n\nfuture_dates_df = pd.DataFrame(index=future_dates[1:],columns=df.columns)\n\n\nfuture_df = pd.concat([df,future_dates_df])\n\n\nfuture_df.head()\n\n\n\n\n\n\n\n\nValue\nValue First Difference\nValue Second Difference\nSeasonal Difference\nSeasonal First Difference\nforecast\n\n\n\n\n1960-01-01\n58.03\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1960-02-01\n55.78\n-2.25\nNaN\nNaN\nNaN\n58.03\n\n\n1960-03-01\n55.02\n-0.76\n1.49\nNaN\nNaN\n58.03\n\n\n1960-04-01\n55.73\n0.71\n1.47\nNaN\nNaN\n58.03\n\n\n1960-05-01\n55.22\n-0.51\n-1.22\nNaN\nNaN\n58.03\n\n\n\n\n\n\n\n\nfuture_df.tail()\n\n\n\n\n\n\n\n\nValue\nValue First Difference\nValue Second Difference\nSeasonal Difference\nSeasonal First Difference\nforecast\n\n\n\n\n2022-05-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-06-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-07-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-08-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2022-09-01\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n\n\n\n\nfuture_df['forecast'] = results.predict(start = 1, end = 720, dynamic= True)  \nfuture_df[['Value', 'forecast']].plot(figsize=(12, 8)) \n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f8872997fd0&gt;"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html",
    "title": "Regression and Classification with Pyspark ML",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\nfrom pyspark.sql.functions import *\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy('province').count().show()\n\n\n+------------+-----+\n    province|count|\n+------------+-----+\n   Guangdong|   12|\n       Hunan|   12|\n      Shanxi|   12|\n       Tibet|   12|\n       Hubei|   12|\n     Tianjin|   12|\n     Beijing|   12|\nHeilongjiang|   12|\n    Liaoning|   12|\n       Henan|   12|\n       Anhui|   12|\n    Xinjiang|   12|\n      Fujian|   12|\n     Jiangxi|   12|\n       Jilin|   12|\n   Chongqing|   12|\n     Shaanxi|   12|\n     Sichuan|   12|\n      Yunnan|   12|\n       Gansu|   12|\n+------------+-----+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#linear-regression-and-random-forestgbt-classification-with-pyspark",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#linear-regression-and-random-forestgbt-classification-with-pyspark",
    "title": "Regression and Classification with Pyspark ML",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\nfrom pyspark.sql.functions import *\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy('province').count().show()\n\n\n+------------+-----+\n    province|count|\n+------------+-----+\n   Guangdong|   12|\n       Hunan|   12|\n      Shanxi|   12|\n       Tibet|   12|\n       Hubei|   12|\n     Tianjin|   12|\n     Beijing|   12|\nHeilongjiang|   12|\n    Liaoning|   12|\n       Henan|   12|\n       Anhui|   12|\n    Xinjiang|   12|\n      Fujian|   12|\n     Jiangxi|   12|\n       Jilin|   12|\n   Chongqing|   12|\n     Shaanxi|   12|\n     Sichuan|   12|\n      Yunnan|   12|\n       Gansu|   12|\n+------------+-----+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#imputation-of-mean-values-to-prepare-the-data",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#imputation-of-mean-values-to-prepare-the-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Imputation of mean values to prepare the data",
    "text": "Imputation of mean values to prepare the data\n\nmean_val = df.select(mean(df['general'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"general\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['specific'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"specific\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['rr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"rr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['fr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"fr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['rnr'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"rnr\"])\n\n\n\n\n\n\nmean_val = df.select(mean(df['i'])).collect()\nmean_val[0][0]\nmean_gen = mean_val[0][0]\ndf = df.na.fill(mean_gen,[\"i\"])"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#creating-binary-target-feature-from-extant-column-for-classification",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#creating-binary-target-feature-from-extant-column-for-classification",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Creating binary target feature from extant column for classification",
    "text": "Creating binary target feature from extant column for classification\n\nfrom pyspark.sql.functions import *\ndf = df.withColumn('specific_classification',when(df.specific &gt;= 583470.7303370787,1).otherwise(0))"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#using-stringindexer-for-categorical-encoding-of-string-type-columns",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#using-stringindexer-for-categorical-encoding-of-string-type-columns",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Using StringIndexer for categorical encoding of string type columns",
    "text": "Using StringIndexer for categorical encoding of string type columns\n\nfrom pyspark.ml.feature import StringIndexer\n\n\n\n\n\n\nindexer = StringIndexer(inputCol=\"province\", outputCol=\"provinceIndex\")\ndf = indexer.fit(df).transform(df)\n\n\n\n\n\n\nindexer = StringIndexer(inputCol=\"reg\", outputCol=\"regionIndex\")\ndf = indexer.fit(df).transform(df)\n\n\n\n\n\n\ndf.show()\n\n\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\n_c0|province| specific|           general|year|    gdp|     fdi|               rnr|        rr|         i|     fr|        reg|     it|specific_classification|provinceIndex|regionIndex|\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661.0|               0.0|       0.0|       0.0|1128873| East China| 631930|                      0|          0.0|        0.0|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443.0|               0.0|       0.0|       0.0|1356287| East China| 657860|                      0|          0.0|        0.0|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673.0|               0.0|       0.0|       0.0|1518236| East China| 889463|                      0|          0.0|        0.0|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131.0|0.0355944252244898|0.05968862|0.08376352|1646891| East China|1227364|                      0|          0.0|        0.0|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847.0|               0.0|       0.0|       0.0|1601508| East China|1499110|                      0|          0.0|        0.0|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672.0|               0.0|       0.0|       0.0|1672445| East China|2165189|                      0|          0.0|        0.0|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375.0|               0.0|       0.0|       0.0|1677840| East China|2404936|                      0|          0.0|        0.0|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720.0|               0.0|       0.0|       0.0|1896479| East China|2815820|                      1|          0.0|        0.0|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669.0|               0.0|       0.0|       0.0|2522449| East China|3422176|                      1|          0.0|        0.0|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000.0|               0.0|       0.0| 0.3243243|2522449| East China|3874846|                      1|          0.0|        0.0|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354.0|               0.0|       0.0| 0.3243243|3434548| East China|5167300|                      1|          0.0|        0.0|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892.0|               0.0|       0.0| 0.3243243|4468640| East China|7040099|                      1|          0.0|        0.0|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290.0|0.0355944252244898|0.05968862|0.08376352| 634562|North China| 508135|                      0|          1.0|        4.0|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286.0|               0.0|       0.0|       0.6| 634562|North China| 569283|                      0|          1.0|        4.0|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800.0|               0.0|       0.0|      0.53| 938788|North China| 695528|                      0|          1.0|        4.0|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525.0|               0.0|       0.0|      0.53|2522449|North China| 944047|                      0|          1.0|        4.0|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368.0|               0.0|       0.0|      0.53|1667114|North China| 757990|                      0|          1.0|        4.0|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818.0|               0.0|       0.0|      0.53|2093925|North China|1194728|                      0|          1.0|        4.0|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464.0|               0.0|       0.0|      0.53|2511249|North China|1078754|                      0|          1.0|        4.0|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126.0|               0.0| 0.7948718|       0.0|2823366|North China|1426600|                      1|          1.0|        4.0|\n+---+--------+---------+------------------+----+-------+--------+------------------+----------+----------+-------+-----------+-------+-----------------------+-------------+-----------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#using-vectorassembler-to-prepare-features-for-machine-learning",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#using-vectorassembler-to-prepare-features-for-machine-learning",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Using VectorAssembler to prepare features for machine learning",
    "text": "Using VectorAssembler to prepare features for machine learning\n\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\n\n\n\n\n\n\ndf.columns\n\n\nOut[375]: ['_c0',\n 'province',\n 'specific',\n 'general',\n 'year',\n 'gdp',\n 'fdi',\n 'rnr',\n 'rr',\n 'i',\n 'fr',\n 'reg',\n 'it',\n 'specific_classification',\n 'provinceIndex',\n 'regionIndex']\n\n\n\nassembler = VectorAssembler(\n inputCols=[\n 'provinceIndex',\n# 'specific',\n 'general',\n 'year',\n 'gdp',\n 'fdi',\n #'rnr',\n #'rr',\n #'i',\n #'fr',\n 'regionIndex',\n 'it'\n ],\n outputCol=\"features\")\n\n\n\n\n\n\noutput = assembler.transform(df)\n\n\n\n\n\n\nfinal_data = output.select(\"features\", \"specific\")"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#spliting-data-into-train-and-test",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#spliting-data-into-train-and-test",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Spliting data into train and test",
    "text": "Spliting data into train and test\n\ntrain_data,test_data = final_data.randomSplit([0.7,0.3])"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#regression-with-pyspark-ml",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#regression-with-pyspark-ml",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Regression with Pyspark ML",
    "text": "Regression with Pyspark ML\n\nfrom pyspark.ml.regression import LinearRegression\nlr = LinearRegression(labelCol='specific')"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#fitting-the-linear-regression-model-to-the-training-data",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#fitting-the-linear-regression-model-to-the-training-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Fitting the linear regression model to the training data",
    "text": "Fitting the linear regression model to the training data\n\nlrModel = lr.fit(train_data)"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#coefficients-and-intercept-of-the-linear-regression-model",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#coefficients-and-intercept-of-the-linear-regression-model",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Coefficients and Intercept of the linear regression model",
    "text": "Coefficients and Intercept of the linear regression model\n\nprint(\"Coefficients: {} Intercept: {}\".format(lrModel.coefficients,lrModel.intercept))\n\n\nCoefficients: [-4936.461707001148,0.8007702471080539,-3994.683052325085,-7.5033201950338,0.42095493334994133,50994.51222529955,0.2531915644818595] Intercept: 7695214.561654471"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#evaluating-trained-linear-regression-model-on-the-test-data",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#evaluating-trained-linear-regression-model-on-the-test-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Evaluating trained linear regression model on the test data",
    "text": "Evaluating trained linear regression model on the test data\n\ntest_results = lrModel.evaluate(test_data)"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#metrics-of-trained-linear-regression-model-on-the-test-data-rmse-mse-r2",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#metrics-of-trained-linear-regression-model-on-the-test-data-rmse-mse-r2",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Metrics of trained linear regression model on the test data (RMSE, MSE, R2)",
    "text": "Metrics of trained linear regression model on the test data (RMSE, MSE, R2)\n\nprint(\"RMSE: {}\".format(test_results.rootMeanSquaredError))\nprint(\"MSE: {}\".format(test_results.meanSquaredError))\nprint(\"R2: {}\".format(test_results.r2))\n\n\nRMSE: 292695.0825058327\nMSE: 85670411323.0962\nR2: 0.7853651103073853"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#looking-at-correlations-with-corr",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#looking-at-correlations-with-corr",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Looking at correlations with corr",
    "text": "Looking at correlations with corr\n\nfrom pyspark.sql.functions import corr\n\n\n\n\n\n\ndf.select(corr('specific','gdp')).show()\n\n\n+-------------------+\ncorr(specific, gdp)|\n+-------------------+\n 0.5141876884991972|\n+-------------------+"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#classification-with-pyspark-ml",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#classification-with-pyspark-ml",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classification with Pyspark ML",
    "text": "Classification with Pyspark ML\n\nfrom pyspark.ml.classification import DecisionTreeClassifier,GBTClassifier,RandomForestClassifier\nfrom pyspark.ml import Pipeline"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#decisiontreeclassifier-randomforestclassifier-and-gbtclassifier",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#decisiontreeclassifier-randomforestclassifier-and-gbtclassifier",
    "title": "Regression and Classification with Pyspark ML",
    "section": "DecisionTreeClassifier, RandomForestClassifier and GBTClassifier",
    "text": "DecisionTreeClassifier, RandomForestClassifier and GBTClassifier\n\ndtc = DecisionTreeClassifier(labelCol='specific_classification',featuresCol='features')\nrfc = RandomForestClassifier(labelCol='specific_classification',featuresCol='features')\ngbt = GBTClassifier(labelCol='specific_classification',featuresCol='features')"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#selecting-features-and-binary-target",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#selecting-features-and-binary-target",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Selecting features and binary target",
    "text": "Selecting features and binary target\n\nfinal_data = output.select(\"features\", \"specific_classification\")\ntrain_data,test_data = final_data.randomSplit([0.7,0.3])"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#fitting-the-classifiers-to-the-training-data",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#fitting-the-classifiers-to-the-training-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Fitting the Classifiers to the Training Data",
    "text": "Fitting the Classifiers to the Training Data\n\nrfc_model = rfc.fit(train_data)\ngbt_model = gbt.fit(train_data)\ndtc_model = dtc.fit(train_data)"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#classifier-predictions-on-test-data",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#classifier-predictions-on-test-data",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classifier predictions on test data",
    "text": "Classifier predictions on test data\n\ndtc_predictions = dtc_model.transform(test_data)\nrfc_predictions = rfc_model.transform(test_data)\ngbt_predictions = gbt_model.transform(test_data)"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#evaluating-classifiers-using-pyspark.ml.evaluation-and-multiclassclassificationevaluator",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#evaluating-classifiers-using-pyspark.ml.evaluation-and-multiclassclassificationevaluator",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator",
    "text": "Evaluating Classifiers using pyspark.ml.evaluation and MulticlassClassificationEvaluator\n\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator\n\n\n\n\n\n\nClassifier Accuracy\n\nacc_evaluator = MulticlassClassificationEvaluator(labelCol=\"specific_classification\", predictionCol=\"prediction\", metricName=\"accuracy\")"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#classifier-accuracy-metrics",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#classifier-accuracy-metrics",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classifier Accuracy Metrics",
    "text": "Classifier Accuracy Metrics\n\ndtc_acc = acc_evaluator.evaluate(dtc_predictions)\nrfc_acc = acc_evaluator.evaluate(rfc_predictions)\ngbt_acc = acc_evaluator.evaluate(gbt_predictions)\n\n\n\n\n\n\nprint('-'*80)\nprint('Decision tree accuracy: {0:2.2f}%'.format(dtc_acc*100))\nprint('-'*80)\nprint('Random forest ensemble accuracy: {0:2.2f}%'.format(rfc_acc*100))\nprint('-'*80)\nprint('GBT accuracy: {0:2.2f}%'.format(gbt_acc*100))\nprint('-'*80)\n\n\n--------------------------------------------------------------------------------\nDecision tree accuracy: 81.98%\n--------------------------------------------------------------------------------\nRandom forest ensemble accuracy: 88.29%\n--------------------------------------------------------------------------------\nGBT accuracy: 81.08%\n--------------------------------------------------------------------------------"
  },
  {
    "objectID": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#classification-correlation-with-corr",
    "href": "posts/2020-08-25-Linear Regression and Random Forest_GBT Classification with Pyspark.html#classification-correlation-with-corr",
    "title": "Regression and Classification with Pyspark ML",
    "section": "Classification Correlation with Corr",
    "text": "Classification Correlation with Corr\n\ndf.select(corr('specific_classification','fdi')).show()\n\n\n+----------------------------------+\ncorr(specific_classification, fdi)|\n+----------------------------------+\n                 0.307429849493392|\n+----------------------------------+\n\n\n\n\n\ndf.select(corr('specific_classification','gdp')).show()\n\n\n+----------------------------------+\ncorr(specific_classification, gdp)|\n+----------------------------------+\n                 0.492176921599151|\n+----------------------------------+\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-11-09-dask-sql.html#dask-sql-docs-httpsdask-sql.readthedocs.io",
    "href": "posts/2020-11-09-dask-sql.html#dask-sql-docs-httpsdask-sql.readthedocs.io",
    "title": "Using Dask and dask-sql",
    "section": "Dask SQL docs https://dask-sql.readthedocs.io/",
    "text": "Dask SQL docs https://dask-sql.readthedocs.io/\ndask-sql\n\nimport numpy as np\nimport pandas as pd\n\n\ndf = pd.read_csv('df_panel_fix.csv')\n\n\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.30\n50661\n0.000000\n0.000000\n0.000000\n1128873\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.32\n43443\n0.000000\n0.000000\n0.000000\n1356287\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.96\n27673\n0.000000\n0.000000\n0.000000\n1518236\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.34\n26131\nNaN\nNaN\nNaN\n1646891\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.000000\n0.000000\n0.000000\n1601508\nEast China\n1499110\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n355\nZhejiang\n391292.0\n260313.0\n2003\n9705.02\n498055\n1.214286\n0.035714\n0.035714\n6217715\nEast China\n2261631\n\n\n356\n356\nZhejiang\n656175.0\n276652.0\n2004\n11648.70\n668128\n1.214286\n0.035714\n0.035714\nNaN\nEast China\n3162299\n\n\n357\n357\nZhejiang\n656175.0\nNaN\n2005\n13417.68\n772000\n1.214286\n0.035714\n0.035714\nNaN\nEast China\n2370200\n\n\n358\n358\nZhejiang\n1017303.0\n394795.0\n2006\n15718.47\n888935\n1.214286\n0.035714\n0.035714\n11537149\nEast China\n2553268\n\n\n359\n359\nZhejiang\n844647.0\n0.0\n2007\n18753.73\n1036576\n0.047619\n0.000000\n0.000000\n16494981\nEast China\n2939778\n\n\n\n\n360 rows × 13 columns\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\nnpartitions=5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nint64\nobject\nfloat64\nfloat64\nint64\nfloat64\nint64\nfloat64\nfloat64\nfloat64\nobject\nobject\nint64\n\n\n72\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n288\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n359\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: from_pandas, 5 tasks\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/21359/1\nDashboard: http://192.168.1.71:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\n# client.restart()\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/21359/1\nDashboard: http://192.168.1.71:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nddf.groupby(\"province\").gdp.mean().compute()\n\nprovince\nAnhui            3905.870000\nBeijing          4673.453333\nChongqing        2477.712500\nFujian           4864.023333\nGansu            1397.832500\nGuangdong       15358.781667\nGuangxi          2924.104167\nGuizhou          1422.010833\nHainan            686.714167\nHebei            6936.825000\nHeilongjiang     4041.241667\nHenan            7208.966667\nHubei            4772.503333\nHunan            4765.891667\nJiangsu         10761.846667\nJiangxi          2460.782500\nJilin            2274.854167\nLiaoning         5231.135000\nNingxia           432.268333\nQinghai           383.099167\nShaanxi          2658.034167\nShandong        12324.002500\nShanghai         6432.454167\nShanxi           2817.210833\nSichuan          5377.790000\nTianjin          2528.665000\nTibet             170.426667\nXinjiang         1828.896667\nYunnan           2604.054167\nZhejiang         9138.151667\nName: gdp, dtype: float64\n\n\n\nfrom dask_sql import Context\n\nc = Context()\n\n\nddf\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\nnpartitions=5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nint64\nobject\nfloat64\nfloat64\nint64\nfloat64\nint64\nfloat64\nfloat64\nfloat64\nobject\nobject\nint64\n\n\n72\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n288\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n359\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: from_pandas, 5 tasks\n\n\n\nc.register_dask_table(ddf, \"fiscal\")\n\n\nresult = c.sql('SELECT count(1) FROM fiscal')\n\n\nresult\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nCOUNT(1)\n\n\nnpartitions=1\n\n\n\n\n\n\nint64\n\n\n\n...\n\n\n\n\n\nDask Name: getitem, 22 tasks\n\n\n\nresult.compute()\n\n\n\n\n\n\n\n\nCOUNT(1)\n\n\n\n\n0\n360\n\n\n\n\n\n\n\n\nresult = c.sql(\"\"\"\n    SELECT\n        province,\n        gdp,\n        fdi\n    FROM\n        fiscal AS fiscal\n\"\"\")\n\n\nresult.compute()\n\n\n\n\n\n\n\n\nprovince\ngdp\nfdi\n\n\n\n\n0\nAnhui\n2093.30\n50661\n\n\n1\nAnhui\n2347.32\n43443\n\n\n2\nAnhui\n2542.96\n27673\n\n\n3\nAnhui\n2712.34\n26131\n\n\n4\nAnhui\n2902.09\n31847\n\n\n...\n...\n...\n...\n\n\n355\nZhejiang\n9705.02\n498055\n\n\n356\nZhejiang\n11648.70\n668128\n\n\n357\nZhejiang\n13417.68\n772000\n\n\n358\nZhejiang\n15718.47\n888935\n\n\n359\nZhejiang\n18753.73\n1036576\n\n\n\n\n360 rows × 3 columns\n\n\n\n\nprint(result.compute())\n\n     province       gdp      fdi\n0       Anhui   2093.30    50661\n1       Anhui   2347.32    43443\n2       Anhui   2542.96    27673\n3       Anhui   2712.34    26131\n4       Anhui   2902.09    31847\n..        ...       ...      ...\n355  Zhejiang   9705.02   498055\n356  Zhejiang  11648.70   668128\n357  Zhejiang  13417.68   772000\n358  Zhejiang  15718.47   888935\n359  Zhejiang  18753.73  1036576\n\n[360 rows x 3 columns]\n\n\n\nfrom dask_sql import Context\nfrom dask.datasets import timeseries\n\n\nprint(result.gdp.mean().compute())\n\n4428.653416666667\n\n\n\nresult\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nprovince\ngdp\nfdi\n\n\nnpartitions=5\n\n\n\n\n\n\n\n0\nobject\nfloat64\nint64\n\n\n72\n...\n...\n...\n\n\n...\n...\n...\n...\n\n\n288\n...\n...\n...\n\n\n359\n...\n...\n...\n\n\n\n\n\nDask Name: getitem, 30 tasks\n\n\n\n%%time\nddf.groupby(\"province\").fdi.mean().compute()\n\nCPU times: user 96.7 ms, sys: 1.99 ms, total: 98.7 ms\nWall time: 87.4 ms\n\n\nprovince\nAnhui           7.095308e+04\nBeijing         2.573693e+05\nChongqing       4.112783e+04\nFujian          3.744664e+05\nGansu           5.295500e+03\nGuangdong       1.194950e+06\nGuangxi         5.514783e+04\nGuizhou         5.812333e+03\nHainan          6.436600e+04\nHebei           1.322308e+05\nHeilongjiang    8.271933e+04\nHenan           9.442600e+04\nHubei           1.497132e+05\nHunan           1.321102e+05\nJiangsu         8.736957e+05\nJiangxi         1.037352e+05\nJilin           4.122658e+04\nLiaoning        2.859253e+05\nNingxia         3.950417e+03\nQinghai         1.098408e+04\nShaanxi         5.089258e+04\nShandong        5.455843e+05\nShanghai        5.082483e+05\nShanxi          3.862883e+04\nSichuan         6.219717e+04\nTianjin         2.501733e+05\nTibet           8.397500e+02\nXinjiang        4.433083e+03\nYunnan          1.704833e+04\nZhejiang        4.259302e+05\nName: fdi, dtype: float64\n\n\n\n%%time\nc.sql('SELECT avg(fdi) FROM fiscal GROUP BY province').compute()\n\nCPU times: user 206 ms, sys: 5.58 ms, total: 212 ms\nWall time: 176 ms\n\n\n\n\n\n\n\n\n\nAVG(\"fiscal\".\"fdi\")\n\n\n\n\n0\n70953\n\n\n1\n257369\n\n\n2\n41127\n\n\n3\n374466\n\n\n4\n5295\n\n\n5\n1194950\n\n\n6\n55147\n\n\n7\n5812\n\n\n8\n64366\n\n\n9\n132230\n\n\n10\n82719\n\n\n11\n94426\n\n\n12\n149713\n\n\n13\n132110\n\n\n14\n873695\n\n\n15\n103735\n\n\n16\n41226\n\n\n17\n285925\n\n\n18\n3950\n\n\n19\n10984\n\n\n20\n50892\n\n\n21\n545584\n\n\n22\n508248\n\n\n23\n38628\n\n\n24\n62197\n\n\n25\n250173\n\n\n26\n839\n\n\n27\n4433\n\n\n28\n17048\n\n\n29\n425930\n\n\n\n\n\n\n\n\ndfp = ddf.persist()\n\n\nimport distributed\n\n\ncached_tasks = distributed.wait(dfp)\nprint(f'cached {len(cached_tasks[0])} results')\n\ncached 5 results\n\n\n\nc.register_dask_table(dfp, \"fiscal_cached\")\n\n\nresult = c.sql('SELECT count(1) FROM fiscal_cached')\nresult.compute()\n\n\n\n\n\n\n\n\nCOUNT(1)\n\n\n\n\n0\n360\n\n\n\n\n\n\n\n\n%%time\n\nc.sql('SELECT avg(fdi) FROM fiscal GROUP BY province').compute()\n\nCPU times: user 208 ms, sys: 12.9 ms, total: 221 ms\nWall time: 162 ms\n\n\n\n\n\n\n\n\n\nAVG(\"fiscal\".\"fdi\")\n\n\n\n\n0\n70953\n\n\n1\n257369\n\n\n2\n41127\n\n\n3\n374466\n\n\n4\n5295\n\n\n5\n1194950\n\n\n6\n55147\n\n\n7\n5812\n\n\n8\n64366\n\n\n9\n132230\n\n\n10\n82719\n\n\n11\n94426\n\n\n12\n149713\n\n\n13\n132110\n\n\n14\n873695\n\n\n15\n103735\n\n\n16\n41226\n\n\n17\n285925\n\n\n18\n3950\n\n\n19\n10984\n\n\n20\n50892\n\n\n21\n545584\n\n\n22\n508248\n\n\n23\n38628\n\n\n24\n62197\n\n\n25\n250173\n\n\n26\n839\n\n\n27\n4433\n\n\n28\n17048\n\n\n29\n425930\n\n\n\n\n\n\n\n\nc.sql('SELECT floor(3.14)').compute()\n\n\n\n\n\n\n\n\nFLOOR(3.14)\n\n\n\n\n0\n3.0\n\n\n\n\n\n\n\n\n%%time\n\nc.sql(\"\"\"\n    SELECT floor(fdi) AS fdi, avg(gdp) as gdp, count(1) as fiscal_count\n    FROM fiscal_cached \n    WHERE fdi &gt; 50 AND gdp &gt;= 0 \n    GROUP BY floor(fdi)\n\"\"\").compute()\n\ndistributed.utils_perf - WARNING - full garbage collections took 92% CPU time recently (threshold: 10%)\n\n\nCPU times: user 356 ms, sys: 20.4 ms, total: 376 ms\nWall time: 308 ms\n\n\n\n\n\n\n\n\n\nfdi\ngdp\nfiscal_count\n\n\n\n\n0\n2000\n1933.98\n1\n\n\n1\n2342\n1399.83\n1\n\n\n2\n2954\n2277.35\n1\n\n\n3\n3539\n1688.49\n1\n\n\n4\n3864\n887.67\n1\n\n\n...\n...\n...\n...\n\n\n354\n527776\n5252.76\n1\n\n\n355\n668128\n11648.70\n1\n\n\n356\n772000\n13417.68\n1\n\n\n357\n888935\n15718.47\n1\n\n\n358\n1036576\n18753.73\n1\n\n\n\n\n359 rows × 3 columns\n\n\n\nAnd now we can run a query and immediately plot a visualization of the result using Pandas plotting syntax!\n\nc.sql(\"\"\"\n    SELECT floor(fdi) AS fdi, avg(gdp) as gdp\n    FROM fiscal_cached \n    WHERE fdi &gt; 50 AND gdp &gt;= 0 \n    GROUP BY floor(fdi)\n\"\"\").compute().plot(x='fdi', y='gdp')\n\n&lt;AxesSubplot:xlabel='fdi'&gt;"
  },
  {
    "objectID": "posts/2020-11-02-pandas_prof_shap_values.html",
    "href": "posts/2020-11-02-pandas_prof_shap_values.html",
    "title": "Pandas profiling and Shap values for European Soccer Match Data",
    "section": "",
    "text": "This post includes code and notes from this gist and this post.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nimport sqlalchemy as db\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\n\n%load_ext sql\n\n\nengine = db.create_engine('sqlite:///database.sqlite')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\ntables = pd.read_sql(\"\"\"SELECT *\n                        FROM sqlite_master\n                        WHERE type='table';\"\"\", connection)\ntables\n\n\n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nsqlite_sequence\nsqlite_sequence\n4\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n1\ntable\nPlayer_Attributes\nPlayer_Attributes\n11\nCREATE TABLE \"Player_Attributes\" (\\n\\t`id`\\tIN...\n\n\n2\ntable\nPlayer\nPlayer\n14\nCREATE TABLE `Player` (\\n\\t`id`\\tINTEGER PRIMA...\n\n\n3\ntable\nMatch\nMatch\n18\nCREATE TABLE `Match` (\\n\\t`id`\\tINTEGER PRIMAR...\n\n\n4\ntable\nLeague\nLeague\n24\nCREATE TABLE `League` (\\n\\t`id`\\tINTEGER PRIMA...\n\n\n5\ntable\nCountry\nCountry\n26\nCREATE TABLE `Country` (\\n\\t`id`\\tINTEGER PRIM...\n\n\n6\ntable\nTeam\nTeam\n29\nCREATE TABLE \"Team\" (\\n\\t`id`\\tINTEGER PRIMARY...\n\n\n7\ntable\nTeam_Attributes\nTeam_Attributes\n2\nCREATE TABLE `Team_Attributes` (\\n\\t`id`\\tINTE...\n\n\n8\ntable\nMatch_df\nMatch_df\n3\nCREATE TABLE Match_df(\\n id INT,\\n country_n...\n\n\n9\ntable\nMatch_Wins\nMatch_Wins\n308451\nCREATE TABLE Match_Wins(\\n id INT,\\n country...\n\n\n\n\n\n\n\n\n%%sql\nSELECT * \nFROM Match \nLIMIT 3;\n\nEnvironment variable $DATABASE_URL not set, and no connect string given.\nConnection info needed in SQLAlchemy format, example:\n               postgresql://username:password@hostname/dbname\n               or an existing connection: dict_keys([])\n\n\n\nconnection\n\n&lt;sqlalchemy.engine.base.Connection at 0x7f785f788c50&gt;\n\n\n\nmatch_wins = pd.read_sql(\"\"\"SELECT *\n                        FROM Match_Wins;\"\"\", connection)\n\n\n# sql_query = %sql SELECT * FROM Match_Wins\n# df = sql_query.DataFrame()\n# df\n\n\nmatch_wins\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\nhome_team_win\n\n\n\n\n0\n24559\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-18 00:00:00\nBSC Young Boys\nFC Basel\n1\n2\n0\n\n\n1\n24560\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-19 00:00:00\nFC Aarau\nFC Sion\n3\n1\n1\n\n\n2\n24561\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nFC Luzern\nFC Vaduz\n1\n2\n0\n\n\n3\n24562\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nNeuchâtel Xamax\nFC Zürich\n1\n2\n0\n\n\n4\n24613\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Basel\nGrasshopper Club Zürich\n1\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n25974\n25945\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Basel\nGrasshopper Club Zürich\n0\n1\n0\n\n\n25975\n25946\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nLugano\nFC St. Gallen\n3\n0\n1\n\n\n25976\n25947\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Luzern\nFC Sion\n2\n2\n0\n\n\n25977\n25948\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Thun\nBSC Young Boys\n0\n3\n0\n\n\n25978\n25949\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Zürich\nFC Vaduz\n3\n1\n1\n\n\n\n\n25979 rows × 11 columns\n\n\n\n\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nprofile = ProfileReport(match_wins, title='Pandas Profiling Report')\n\n\nprofile.to_widgets()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\nhome_team_win\n\n\n\n\n0\n24559\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-18 00:00:00\nBSC Young Boys\nFC Basel\n1\n2\n0\n\n\n1\n24560\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-19 00:00:00\nFC Aarau\nFC Sion\n3\n1\n1\n\n\n2\n24561\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nFC Luzern\nFC Vaduz\n1\n2\n0\n\n\n3\n24562\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nNeuchâtel Xamax\nFC Zürich\n1\n2\n0\n\n\n4\n24613\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Basel\nGrasshopper Club Zürich\n1\n0\n1\n\n\n5\n24614\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nAC Bellinzona\nNeuchâtel Xamax\n1\n2\n0\n\n\n6\n24615\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Zürich\nFC Luzern\n1\n0\n1\n\n\n7\n24616\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-24 00:00:00\nFC Sion\nBSC Young Boys\n2\n1\n1\n\n\n8\n24617\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-24 00:00:00\nFC Vaduz\nFC Aarau\n0\n2\n0\n\n\n9\n24668\nSwitzerland\nSwitzerland Super League\n2008/2009\n3\n2008-07-26 00:00:00\nFC Basel\nAC Bellinzona\n2\n0\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\nhome_team_win\n\n\n\n\n25969\n25940\nSwitzerland\nSwitzerland Super League\n2015/2016\n35\n2016-05-22 00:00:00\nGrasshopper Club Zürich\nFC Thun\n0\n0\n0\n\n\n25970\n25941\nSwitzerland\nSwitzerland Super League\n2015/2016\n35\n2016-05-22 00:00:00\nFC Sion\nFC Zürich\n2\n2\n0\n\n\n25971\n25942\nSwitzerland\nSwitzerland Super League\n2015/2016\n35\n2016-05-22 00:00:00\nFC Vaduz\nLugano\n0\n0\n0\n\n\n25972\n25943\nSwitzerland\nSwitzerland Super League\n2015/2016\n35\n2016-05-22 00:00:00\nBSC Young Boys\nFC Basel\n2\n3\n0\n\n\n25973\n25944\nSwitzerland\nSwitzerland Super League\n2015/2016\n35\n2016-05-22 00:00:00\nFC St. Gallen\nFC Luzern\n1\n4\n0\n\n\n25974\n25945\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Basel\nGrasshopper Club Zürich\n0\n1\n0\n\n\n25975\n25946\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nLugano\nFC St. Gallen\n3\n0\n1\n\n\n25976\n25947\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Luzern\nFC Sion\n2\n2\n0\n\n\n25977\n25948\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Thun\nBSC Young Boys\n0\n3\n0\n\n\n25978\n25949\nSwitzerland\nSwitzerland Super League\n2015/2016\n36\n2016-05-25 00:00:00\nFC Zürich\nFC Vaduz\n3\n1\n1\n\n\n\n\n\n\n\n\n\n\n\n\nprofile.to_notebook_iframe()\n\n\n\n\n\n\n\n\n\n\n\n\nprofile.to_file(output_file=\"pandas_profiling.html\")\n\n\n\n\n\n\n\n\nmatch_wins.head()\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\nhome_team_win\n\n\n\n\n0\n24559\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-18 00:00:00\nBSC Young Boys\nFC Basel\n1\n2\n0\n\n\n1\n24560\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-19 00:00:00\nFC Aarau\nFC Sion\n3\n1\n1\n\n\n2\n24561\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nFC Luzern\nFC Vaduz\n1\n2\n0\n\n\n3\n24562\nSwitzerland\nSwitzerland Super League\n2008/2009\n1\n2008-07-20 00:00:00\nNeuchâtel Xamax\nFC Zürich\n1\n2\n0\n\n\n4\n24613\nSwitzerland\nSwitzerland Super League\n2008/2009\n2\n2008-07-23 00:00:00\nFC Basel\nGrasshopper Club Zürich\n1\n0\n1\n\n\n\n\n\n\n\n\ncols = match_wins.columns \ncolours = ['darkblue', 'red'] \nsns.heatmap(match_wins[cols].isnull(), cmap=sns.color_palette(colours))\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\n# top = match_wins[\"home_team_win\"].describe()['top'] # impute with the most frequent value.\n# match_wins[\"home_team_win\"] = match_wins[\"home_team_win\"].fillna(top)\n\n\npct_list = []\nfor col in match_wins.columns:\n    pct_missing = np.mean(match_wins[col].isnull())\n    if round(pct_missing*100) &gt;0:\n        pct_list.append([col, round(pct_missing*100)])\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\nid - 0%\ncountry_name - 0%\nleague_name - 0%\nseason - 0%\nstage - 0%\ndate - 0%\nhome_team - 0%\naway_team - 0%\nhome_team_goal - 0%\naway_team_goal - 0%\nhome_team_win - 0%\n\n\n\nmatch_wins.country_name\n\n0        Switzerland\n1        Switzerland\n2        Switzerland\n3        Switzerland\n4        Switzerland\n            ...     \n25974    Switzerland\n25975    Switzerland\n25976    Switzerland\n25977    Switzerland\n25978    Switzerland\nName: country_name, Length: 25979, dtype: object\n\n\n\n# # extracting the titles from the names:\n# Title = []\n# for name in match_wins.country_name:\n#     Title.append(name.split(\",\")[1].split(\".\")[0])\n    \n# match_wins[\"Team\"] = Title\n\n\nmatch_wins.groupby([\"home_team\", 'season'])['home_team_win'].agg(['sum']).round(0)\n\n\n\n\n\n\n\n\n\nsum\n\n\nhome_team\nseason\n\n\n\n\n\n1. FC Kaiserslautern\n2010/2011\n6\n\n\n2011/2012\n2\n\n\n1. FC Köln\n2008/2009\n4\n\n\n2009/2010\n3\n\n\n2010/2011\n11\n\n\n...\n...\n...\n\n\nŚląsk Wrocław\n2011/2012\n9\n\n\n2012/2013\n9\n\n\n2013/2014\n5\n\n\n2014/2015\n9\n\n\n2015/2016\n5\n\n\n\n\n1478 rows × 1 columns\n\n\n\n\ndf = df.drop(columns = [\"Name\"])\ndf = df.drop(columns = [\"PassengerId\"])\ndf = df.drop(columns = [\"Ticket\"])\n\n\nmatch_wins.dtypes\n\nid                int64\ncountry_name       int8\nleague_name        int8\nseason             int8\nstage             int64\ndate              int16\nhome_team         int16\naway_team         int16\nhome_team_goal    int64\naway_team_goal    int64\nhome_team_win     int64\ndtype: object\n\n\n\nmatch_wins.country_name = pd.Categorical(match_wins.country_name)\nmatch_wins.league_name = pd.Categorical(match_wins.league_name)\nmatch_wins.season = pd.Categorical(match_wins.season)\nmatch_wins.date = pd.Categorical(match_wins.date)\n\n\nmatch_wins[\"country_name\"] = match_wins.country_name.cat.codes\n\n\nmatch_wins[\"league_name\"] = match_wins.league_name.cat.codes\nmatch_wins[\"season\"] = match_wins.season.cat.codes\nmatch_wins[\"date\"] = match_wins.date.cat.codes\n\n\nmatch_wins.home_team = pd.Categorical(match_wins.home_team)\n\n\nmatch_wins.away_team = pd.Categorical(match_wins.away_team)\n\n\nmatch_wins[\"away_team\"] = match_wins.away_team.cat.codes\n\n\nmatch_wins[\"home_team\"] = match_wins.home_team.cat.codes\n\n\nmatch_wins[\"home_team\"]\n\n0         24\n1         72\n2         84\n3        173\n4         76\n        ... \n25974     76\n25975    160\n25976     84\n25977     95\n25978    100\nName: home_team, Length: 25979, dtype: int16\n\n\n\nmatch_wins.date = pd.Categorical(match_wins.date)\n\n\nmatch_wins[\"date\"] = match_wins.date.cat.codes\n\n\nmatch_wins\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\nhome_team_win\n\n\n\n\n0\n24559\n10\n10\n0\n1\n0\n24\n76\n1\n2\n0\n\n\n1\n24560\n10\n10\n0\n1\n1\n72\n91\n3\n1\n1\n\n\n2\n24561\n10\n10\n0\n1\n2\n84\n98\n1\n2\n0\n\n\n3\n24562\n10\n10\n0\n1\n2\n173\n100\n1\n2\n0\n\n\n4\n24613\n10\n10\n0\n2\n3\n76\n117\n1\n0\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n25974\n25945\n10\n10\n7\n36\n1693\n76\n117\n0\n1\n0\n\n\n25975\n25946\n10\n10\n7\n36\n1693\n160\n93\n3\n0\n1\n\n\n25976\n25947\n10\n10\n7\n36\n1693\n84\n91\n2\n2\n0\n\n\n25977\n25948\n10\n10\n7\n36\n1693\n95\n24\n0\n3\n0\n\n\n25978\n25949\n10\n10\n7\n36\n1693\n100\n98\n3\n1\n1\n\n\n\n\n25979 rows × 11 columns\n\n\n\n\nmatch_wins.dtypes\n\nid                int64\ncountry_name       int8\nleague_name        int8\nseason             int8\nstage             int64\ndate              int16\nhome_team         int16\naway_team         int16\nhome_team_goal    int64\naway_team_goal    int64\nhome_team_win     int64\ndtype: object\n\n\n\n#match_wins = match_wins.drop(columns = [\"Title\"])\ntarget = match_wins.home_team_win.values\nmatch_wins = match_wins.drop(columns =[\"home_team_win\"])\n\n\nmatch_wins\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\n\n\n\n\n0\n24559\n10\n10\n0\n1\n0\n24\n76\n1\n2\n\n\n1\n24560\n10\n10\n0\n1\n1\n72\n91\n3\n1\n\n\n2\n24561\n10\n10\n0\n1\n2\n84\n98\n1\n2\n\n\n3\n24562\n10\n10\n0\n1\n2\n173\n100\n1\n2\n\n\n4\n24613\n10\n10\n0\n2\n3\n76\n117\n1\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n25974\n25945\n10\n10\n7\n36\n1693\n76\n117\n0\n1\n\n\n25975\n25946\n10\n10\n7\n36\n1693\n160\n93\n3\n0\n\n\n25976\n25947\n10\n10\n7\n36\n1693\n84\n91\n2\n2\n\n\n25977\n25948\n10\n10\n7\n36\n1693\n95\n24\n0\n3\n\n\n25978\n25949\n10\n10\n7\n36\n1693\n100\n98\n3\n1\n\n\n\n\n25979 rows × 10 columns\n\n\n\n\ntarget\n\narray([0, 1, 0, ..., 0, 0, 1])\n\n\n\nfrom sklearn.model_selection import train_test_split\n\n\nx_train, x_test, y_train, y_test = train_test_split(match_wins, target, test_size=0.2, random_state=0)\n\n\nfrom sklearn.linear_model import LogisticRegression\n\n\nLR = LogisticRegression()\nLR.fit(x_train, y_train)\n\n/home/gao/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n\n\nLogisticRegression()\n\n\n\nLR.score(x_test, y_test)\n\n0.9736335642802155\n\n\n\nimport shap\nexplainer = shap.LinearExplainer(LR, x_train, feature_perturbation=\"interventional\")\nshap_values = explainer.shap_values(x_test)\nshap.summary_plot(shap_values, x_test)\n\n\n\n\n\nshap.dependence_plot(\"home_team\", shap_values, x_test)\n\n\n\n\n\nshap.summary_plot(shap_values, x_train, plot_type=\"bar\")\n\n\n\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values, x_test, link=\"logit\")\n\n\n\n\n\nshap.plots.force is slow for many thousands of rows, try subsampling your data.\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[0,:], x_test.iloc[0,:], link=\"logit\")\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nshap.initjs()\nshap.force_plot(explainer.expected_value, shap_values[3,:], x_test.iloc[3,:], link=\"logit\")\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written."
  },
  {
    "objectID": "posts/2020-10-04-StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
    "href": "posts/2020-10-04-StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
    "title": "Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020 with pandas_datareader and writing to at sqlite database",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here\n\nimport pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n# # start = datetime.datetime(2016, 1, 1)\n# # end = datetime.datetime(2017, 5, 17)\n\n# start = datetime.datetime(2010, 1, 1)\n# end = datetime.datetime(2020, 1, 1)\n\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nFXAIX_stock['Open'].plot(label='SNP_500')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nFXAIX_stock['Volume'].plot(label='SNP_500')\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7fae549b8ba8&gt;\n\n\n\n\n\n\n\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-09-16\n319.0\n231.110001\n245.000000\n253.929993\n36099700\n253.929993\n\n\n2020-09-17\n241.5\n215.240005\n230.759995\n227.539993\n11907500\n227.539993\n\n\n2020-09-18\n249.0\n218.589996\n235.000000\n240.000000\n7475400\n240.000000\n\n\n2020-09-21\n241.5\n218.600006\n230.000000\n228.850006\n5524900\n228.850006\n\n\n2020-09-22\n239.0\n225.149994\n238.500000\n235.160004\n3889100\n235.160004\n\n\n\n\n\n\n\n\nstocks = pd.concat([MSFT_stock['Open'], ZOOM_stock['Open'], SNOW_stock['Open'], FXAIX_stock['Open']],\n                   axis = 1)\n\n\nstocks\n\n\n\n\n\n\n\n\nOpen\nOpen\nOpen\nOpen\n\n\nDate\n\n\n\n\n\n\n\n\n2020-01-02\n158.779999\n68.800003\nNaN\n112.980003\n\n\n2020-01-03\n158.320007\n67.620003\nNaN\n112.190002\n\n\n2020-01-06\n157.080002\n66.629997\nNaN\n112.589996\n\n\n2020-01-07\n159.320007\n70.290001\nNaN\n112.290001\n\n\n2020-01-08\n158.929993\n71.809998\nNaN\n112.839996\n\n\n...\n...\n...\n...\n...\n\n\n2020-09-28\n210.880005\n502.410004\n235.929993\n116.650002\n\n\n2020-09-29\n209.350006\n488.130005\n255.000000\n116.099998\n\n\n2020-09-30\n207.729996\n464.209991\n261.500000\n117.070000\n\n\n2020-10-01\n213.490005\n477.000000\n255.250000\n117.699997\n\n\n2020-10-02\n208.000000\n485.005005\n232.440002\n116.120003\n\n\n\n\n191 rows × 4 columns\n\n\n\n\nstocks.columns = ['MSFT_stock','ZOOM_stock','SNOW_stock','FXAIX_stock']\n\n\nstocks\n\n\n\n\n\n\n\n\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\nDate\n\n\n\n\n\n\n\n\n2020-01-02\n158.779999\n68.800003\nNaN\n112.980003\n\n\n2020-01-03\n158.320007\n67.620003\nNaN\n112.190002\n\n\n2020-01-06\n157.080002\n66.629997\nNaN\n112.589996\n\n\n2020-01-07\n159.320007\n70.290001\nNaN\n112.290001\n\n\n2020-01-08\n158.929993\n71.809998\nNaN\n112.839996\n\n\n...\n...\n...\n...\n...\n\n\n2020-09-28\n210.880005\n502.410004\n235.929993\n116.650002\n\n\n2020-09-29\n209.350006\n488.130005\n255.000000\n116.099998\n\n\n2020-09-30\n207.729996\n464.209991\n261.500000\n117.070000\n\n\n2020-10-01\n213.490005\n477.000000\n255.250000\n117.699997\n\n\n2020-10-02\n208.000000\n485.005005\n232.440002\n116.120003\n\n\n\n\n191 rows × 4 columns\n\n\n\n\nmean_daily_ret = stocks.pct_change(1).mean()\nmean_daily_ret\n\nMSFT_stock     0.001751\nZOOM_stock     0.011973\nSNOW_stock    -0.002546\nFXAIX_stock    0.000440\ndtype: float64\n\n\n\nstocks.pct_change(1).corr()\n\n\n\n\n\n\n\n\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\n\n\nMSFT_stock\n1.000000\n0.209041\n0.661827\n0.382807\n\n\nZOOM_stock\n0.209041\n1.000000\n0.095052\n0.127526\n\n\nSNOW_stock\n0.661827\n0.095052\n1.000000\n0.292117\n\n\nFXAIX_stock\n0.382807\n0.127526\n0.292117\n1.000000\n\n\n\n\n\n\n\n\nstock_normed = stocks/stocks.iloc[0]\nstock_normed.plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fae54a74a90&gt;\n\n\n\n\n\n\nstock_daily_ret = stocks.pct_change(1)\nstock_daily_ret.head()\n\n\n\n\n\n\n\n\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\nDate\n\n\n\n\n\n\n\n\n2020-01-02\nNaN\nNaN\nNaN\nNaN\n\n\n2020-01-03\n-0.002897\n-0.017151\nNaN\n-0.006992\n\n\n2020-01-06\n-0.007832\n-0.014641\nNaN\n0.003565\n\n\n2020-01-07\n0.014260\n0.054930\nNaN\n-0.002664\n\n\n2020-01-08\n-0.002448\n0.021625\nNaN\n0.004898\n\n\n\n\n\n\n\n\nlog_ret = np.log(stocks / stocks.shift(1))\nlog_ret.head()\n\n\n\n\n\n\n\n\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\nDate\n\n\n\n\n\n\n\n\n2020-01-02\nNaN\nNaN\nNaN\nNaN\n\n\n2020-01-03\n-0.002901\n-0.017300\nNaN\n-0.007017\n\n\n2020-01-06\n-0.007863\n-0.014749\nNaN\n0.003559\n\n\n2020-01-07\n0.014160\n0.053475\nNaN\n-0.002668\n\n\n2020-01-08\n-0.002451\n0.021394\nNaN\n0.004886\n\n\n\n\n\n\n\n\nlog_ret.hist(bins = 100,\n             figsize = (12, 6));\nplt.tight_layout()\n\n\n\n\n\nlog_ret.describe().transpose()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nMSFT_stock\n190.0\n0.001421\n0.025752\n-0.087821\n-0.012115\n0.004000\n0.016980\n0.081248\n\n\nZOOM_stock\n190.0\n0.010279\n0.056461\n-0.142569\n-0.017014\n0.011119\n0.035968\n0.368600\n\n\nSNOW_stock\n12.0\n-0.004386\n0.063753\n-0.131433\n-0.033113\n0.019477\n0.034320\n0.077728\n\n\nFXAIX_stock\n190.0\n0.000144\n0.024461\n-0.127150\n-0.007774\n0.002806\n0.010082\n0.089894\n\n\n\n\n\n\n\n\nlog_ret.mean() * 252\n\nMSFT_stock     0.358130\nZOOM_stock     2.590236\nSNOW_stock    -1.105148\nFXAIX_stock    0.036359\ndtype: float64\n\n\n\nlog_ret.cov()\n\n\n\n\n\n\n\n\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\n\n\nMSFT_stock\n0.000663\n0.000323\n0.001291\n0.000245\n\n\nZOOM_stock\n0.000323\n0.003188\n0.000290\n0.000184\n\n\nSNOW_stock\n0.001291\n0.000290\n0.004064\n0.000231\n\n\nFXAIX_stock\n0.000245\n0.000184\n0.000231\n0.000598\n\n\n\n\n\n\n\n\n# Set seed (optional)\nnp.random.seed(101)\n\n# Stock Columns\nprint('Stocks')\nprint(stocks.columns)\nprint('\\n')\n\n# Create Random Weights\nprint('Creating Random Weights')\nweights = np.array(np.random.random(4))\nprint(weights)\nprint('\\n')\n\n# Rebalance Weights\nprint('Rebalance to sum to 1.0')\nweights = weights / np.sum(weights)\nprint(weights)\nprint('\\n')\n\n# Expected Return\nprint('Expected Portfolio Return')\nexp_ret = np.sum(log_ret.mean() * weights) *252\nprint(exp_ret)\nprint('\\n')\n\n# Expected Variance\nprint('Expected Volatility')\nexp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\nprint(exp_vol)\nprint('\\n')\n\n# Sharpe Ratio\nSR = exp_ret/exp_vol\nprint('Sharpe Ratio')\nprint(SR)\n\nStocks\nIndex(['MSFT_stock', 'ZOOM_stock', 'SNOW_stock', 'FXAIX_stock'], dtype='object')\n\n\nCreating Random Weights\n[0.51639863 0.57066759 0.02847423 0.17152166]\n\n\nRebalance to sum to 1.0\n[0.40122278 0.44338777 0.02212343 0.13326603]\n\n\nExpected Portfolio Return\n1.272564336318203\n\n\nExpected Volatility\n0.4864366288684257\n\n\nSharpe Ratio\n2.6160948020680697\n\n\n\nnum_ports = 15000\n\nall_weights = np.zeros((num_ports, len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    # Create Random Weights\n    weights = np.array(np.random.random(4))\n\n    # Rebalance Weights\n    weights = weights / np.sum(weights)\n    \n    # Save Weights\n    all_weights[ind,:] = weights\n\n    # Expected Return\n    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n\n    # Expected Variance\n    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n\n    # Sharpe Ratio\n    sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind]\n\n\nsharpe_arr.max()\n\n2.8667995807841824\n\n\n\nsharpe_arr.argmax()\n\n5483\n\n\n\nall_weights[10619,:]\n\narray([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01])\n\n\n\nmax_sr_ret = ret_arr[1419]\nmax_sr_vol = vol_arr[1419]\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n# Add red dot for max SR\nplt.scatter(max_sr_vol,\n            max_sr_ret,\n            c = 'red',\n            s = 50,\n            edgecolors = 'black')\n\n&lt;matplotlib.collections.PathCollection at 0x7fae54366048&gt;\n\n\n\n\n\n\ndef get_ret_vol_sr(weights):\n    \"\"\"\n    Takes in weights, returns array or return,volatility, sharpe ratio\n    \"\"\"\n    weights = np.array(weights)\n    ret = np.sum(log_ret.mean() * weights) * 252\n    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n    sr = ret/vol\n    return np.array([ret, vol, sr])\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ndef neg_sharpe(weights):\n    return  get_ret_vol_sr(weights)[2] * -1\n\n# Contraints\ndef check_sum(weights):\n    '''\n    Returns 0 if sum of weights is 1.0\n    '''\n    return np.sum(weights) - 1\n\n# By convention of minimize function it should be a function that returns zero for conditions\ncons = ({'type' : 'eq', 'fun': check_sum})\n\n# 0-1 bounds for each weight\nbounds = ((0, 1), (0, 1), (0, 1), (0, 1))\n\n# Initial Guess (equal distribution)\ninit_guess = [0.25, 0.25, 0.25, 0.25]\n\n# Sequential Least Squares \nopt_results = minimize(neg_sharpe,\n                       init_guess,\n                       method = 'SLSQP',\n                       bounds = bounds,\n                       constraints = cons)\n\nopt_results\n\n     fun: -2.8998675936504807\n     jac: array([-3.57061625e-04,  6.75618649e-05,  1.98669076e+00,  1.90789163e-01])\n message: 'Optimization terminated successfully.'\n    nfev: 42\n     nit: 7\n    njev: 7\n  status: 0\n success: True\n       x: array([1.59222977e-01, 8.40777023e-01, 7.68699340e-16, 0.00000000e+00])\n\n\n\nopt_results.x\n\nget_ret_vol_sr(opt_results.x)\n\narray([2.23483308, 0.77066728, 2.89986759])\n\n\n\nfrontier_y = np.linspace(0, 0.3, 100)\n\n\ndef minimize_volatility(weights):\n    return  get_ret_vol_sr(weights)[1] \n\nfrontier_volatility = []\n\nfor possible_return in frontier_y:\n    # function for return\n    cons = ({'type':'eq','fun': check_sum},\n            {'type':'eq','fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n    \n    result = minimize(minimize_volatility,\n                      init_guess,\n                      method = 'SLSQP',\n                      bounds = bounds,\n                      constraints = cons)\n    \n    frontier_volatility.append(result['fun'])\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n\n\n# Add frontier line\nplt.plot(frontier_volatility,\n         frontier_y,\n         'g--',\n         linewidth = 3)\n\n\n\n\n\nstocks['FXAIX_stock'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 in 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 in 2020 Value')\n\n\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nstocks\n\n\n\n\n\n\n\n\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\nDate\n\n\n\n\n\n\n\n\n2020-01-02\n158.779999\n68.800003\nNaN\n112.980003\n\n\n2020-01-03\n158.320007\n67.620003\nNaN\n112.190002\n\n\n2020-01-06\n157.080002\n66.629997\nNaN\n112.589996\n\n\n2020-01-07\n159.320007\n70.290001\nNaN\n112.290001\n\n\n2020-01-08\n158.929993\n71.809998\nNaN\n112.839996\n\n\n...\n...\n...\n...\n...\n\n\n2020-09-28\n210.880005\n502.410004\n235.929993\n116.650002\n\n\n2020-09-29\n209.350006\n488.130005\n255.000000\n116.099998\n\n\n2020-09-30\n207.729996\n464.209991\n261.500000\n117.070000\n\n\n2020-10-01\n213.490005\n477.000000\n255.250000\n117.699997\n\n\n2020-10-02\n208.000000\n485.005005\n232.440002\n116.120003\n\n\n\n\n191 rows × 4 columns\n\n\n\n\nengine = db.create_engine('sqlite:///stocks.sqlite')\n\n\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nstocks.to_sql('stocks', con=engine, if_exists='append', index=True)\n\n\nengine.execute(\"SELECT * FROM stocks LIMIT 10\").fetchall()\n\n[(158.77999877929688, 68.80000305175781, None, 112.9800033569336),\n (158.32000732421875, 67.62000274658203, None, 112.19000244140625),\n (157.0800018310547, 66.62999725341797, None, 112.58999633789062),\n (159.32000732421875, 70.29000091552734, None, 112.29000091552734),\n (158.92999267578125, 71.80999755859375, None, 112.83999633789062),\n (161.83999633789062, 73.98999786376953, None, 113.62000274658203),\n (162.82000732421875, 73.08000183105469, None, 113.30000305175781),\n (161.75999450683594, 73.88999938964844, None, 114.08999633789062),\n (163.38999938964844, 74.31999969482422, None, 113.93000030517578),\n (162.6199951171875, 73.27999877929688, None, 114.13999938964844)]\n\n\n\nengine.execute(\"SELECT FXAIX_stock FROM stocks LIMIT 10\").fetchall()\n\n[(112.9800033569336,),\n (112.19000244140625,),\n (112.58999633789062,),\n (112.29000091552734,),\n (112.83999633789062,),\n (113.62000274658203,),\n (113.30000305175781,),\n (114.08999633789062,),\n (113.93000030517578,),\n (114.13999938964844,)]\n\n\n\n# df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']})\n# df\n\n# df.to_sql('users', con=engine)\n\n# engine.execute(\"SELECT * FROM users\").fetchall()"
  },
  {
    "objectID": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html",
    "href": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n\n\n\n\n_c0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\nAnhui\n147002.0\nnull\n1996\n2093.3\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\nAnhui\n151981.0\nnull\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\nAnhui\n174930.0\nnull\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\nAnhui\n285324.0\nnull\n1999\n2712.34\n26131\nnull\nnull\nnull\n1646891\nEast China\n1227364\n\n\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\ndf.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: double (nullable = true)\n-- general: double (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: double (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: double (nullable = true)\n-- rr: double (nullable = true)\n-- i: double (nullable = true)\n-- fr: string (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)\ndf.columns\n\n\nOut[64]: ['_c0',\n 'province',\n 'specific',\n 'general',\n 'year',\n 'gdp',\n 'fdi',\n 'rnr',\n 'rr',\n 'i',\n 'fr',\n 'reg',\n 'it']\ndf.describe()\n\n\nOut[65]: DataFrame[summary: string, _c0: string, province: string, specific: string, general: string, year: string, gdp: string, fdi: string, rnr: string, rr: string, i: string, fr: string, reg: string, it: string]"
  },
  {
    "objectID": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#setting-data-schema-and-data-types",
    "href": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#setting-data-schema-and-data-types",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Setting Data Schema and Data Types",
    "text": "Setting Data Schema and Data Types\n\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType\n\n\n\n\n\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", IntegerType(), True)\n,StructField(\"general\", IntegerType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", IntegerType(), True)\n,StructField(\"fdi\", IntegerType(), True)\n,StructField(\"rnr\", IntegerType(), True)\n,StructField(\"rr\", IntegerType(), True)\n,StructField(\"i\", IntegerType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\n\n\n\n\n\nfinal_struc = StructType(fields=data_schema)"
  },
  {
    "objectID": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#applying-the-data-schemadata-types-while-reading-in-a-csv",
    "href": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#applying-the-data-schemadata-types-while-reading-in-a-csv",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Applying the Data Schema/Data Types while reading in a CSV",
    "text": "Applying the Data Schema/Data Types while reading in a CSV\n\ndf = spark.read.format(\"CSV\").schema(final_struc).load(file_location)\n\n\n\n\n\n\ndf.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: integer (nullable = true)\n-- general: integer (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: integer (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: integer (nullable = true)\n-- rr: integer (nullable = true)\n-- i: integer (nullable = true)\n-- fr: integer (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)\n\n\n\n\n\ndf.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf['fr']\n\n\nOut[72]: Column&lt;b'fr'&gt;\n\n\n\ntype(df['fr'])\n\n\nOut[73]: pyspark.sql.column.Column\n\n\n\ndf.select('fr')\n\n\nOut[74]: DataFrame[fr: int]\n\n\n\ntype(df.select('fr'))\n\n\nOut[75]: pyspark.sql.dataframe.DataFrame\n\n\n\ndf.select('fr').show()\n\n\n+-------+\n     fr|\n+-------+\n   null|\n1128873|\n1356287|\n1518236|\n1646891|\n1601508|\n1672445|\n1677840|\n1896479|\n   null|\n   null|\n3434548|\n4468640|\n 634562|\n 634562|\n 938788|\n   null|\n1667114|\n2093925|\n2511249|\n+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.head(2)\n\n\nOut[77]: [Row(_c0=None, province='province', specific=None, general=None, year=None, gdp=None, fdi=None, rnr=None, rr=None, i=None, fr=None, reg='reg', it=None),\n Row(_c0=0, province='Anhui', specific=None, general=None, year=1996, gdp=None, fdi=50661, rnr=None, rr=None, i=None, fr=1128873, reg='East China', it=631930)]\n\n\n\ndf.select(['reg','fr'])\n\n\nOut[78]: DataFrame[reg: string, fr: int]"
  },
  {
    "objectID": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#using-select-with-rdds",
    "href": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#using-select-with-rdds",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Using select with RDDs",
    "text": "Using select with RDDs\n\ndf.select(['reg','fr']).show()\n\n\n+-----------+-------+\n        reg|     fr|\n+-----------+-------+\n        reg|   null|\n East China|1128873|\n East China|1356287|\n East China|1518236|\n East China|1646891|\n East China|1601508|\n East China|1672445|\n East China|1677840|\n East China|1896479|\n East China|   null|\n East China|   null|\n East China|3434548|\n East China|4468640|\nNorth China| 634562|\nNorth China| 634562|\nNorth China| 938788|\nNorth China|   null|\nNorth China|1667114|\nNorth China|2093925|\nNorth China|2511249|\n+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('fiscal_revenue',df['fr']).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|          null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|       1128873|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|       1356287|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|       1518236|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|       1646891|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|       1601508|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|       1672445|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|       1677840|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|       1896479|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|          null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|          null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|       3434548|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|       4468640|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|        634562|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|        634562|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|        938788|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|          null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|       1667114|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|       2093925|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|       2511249|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+--------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#renaming-columns-using-withcolumnrenamed",
    "href": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#renaming-columns-using-withcolumnrenamed",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Renaming Columns using withColumnRenamed",
    "text": "Renaming Columns using withColumnRenamed\n\ndf.withColumnRenamed('fr','new_fiscal_revenue').show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|new_fiscal_revenue|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|              null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|           1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|           1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|           1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|           1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|           1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|           1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|           1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|           1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|              null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|              null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|           3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|           4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null|            634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null|            634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null|            938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|              null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|           1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|           2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|           2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+------------------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#new-columns-by-transforming-extant-columns-using-withcolumn",
    "href": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#new-columns-by-transforming-extant-columns-using-withcolumn",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "New Columns by Transforming extant Columns using withColumn",
    "text": "New Columns by Transforming extant Columns using withColumn\n\ndf.withColumn('double_fiscal_revenue',df['fr']*2).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|double_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|                 null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|              2257746|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|              2712574|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|              3036472|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|              3293782|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|              3203016|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|              3344890|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|              3355680|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|              3792958|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|                 null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|                 null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|              6869096|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|              8937280|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|              1269124|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|              1269124|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|              1877576|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|                 null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|              3334228|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|              4187850|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|              5022498|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+---------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('add_fiscal_revenue',df['fr']+1).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|add_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|              null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|           1128874|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|           1356288|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|           1518237|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|           1646892|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|           1601509|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|           1672446|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|           1677841|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|           1896480|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|              null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|              null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|           3434549|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|           4468641|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|            634563|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|            634563|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|            938789|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|              null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|           1667115|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|           2093926|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|           2511250|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('half_fiscal_revenue',df['fr']/2).show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|half_fiscal_revenue|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|               null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|           564436.5|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|           678143.5|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|           759118.0|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|           823445.5|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|           800754.0|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|           836222.5|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|           838920.0|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|           948239.5|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|               null|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|               null|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|          1717274.0|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|          2234320.0|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|           317281.0|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|           317281.0|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|           469394.0|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|               null|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|           833557.0|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|          1046962.5|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|          1255624.5|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+-------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.withColumn('half_fr',df['fr']/2)\n\n\nOut[86]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int, half_fr: double]"
  },
  {
    "objectID": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#spark-sql-for-sql-functionality-using-createorreplacetempview",
    "href": "posts/2020-08-21-RDDs and Schemas and Data Types with Pyspark.html#spark-sql-for-sql-functionality-using-createorreplacetempview",
    "title": "RDDs and Schemas and Data Types with Pyspark",
    "section": "Spark SQL for SQL functionality using createOrReplaceTempView",
    "text": "Spark SQL for SQL functionality using createOrReplaceTempView\n\ndf.createOrReplaceTempView(\"economic_data\")\n\n\n\n\n\n\nsql_results = spark.sql(\"SELECT * FROM economic_data\")\n\n\n\n\n\n\nsql_results\n\n\nOut[89]: DataFrame[_c0: int, province: string, specific: int, general: int, year: int, gdp: int, fdi: int, rnr: int, rr: int, i: int, fr: int, reg: string, it: int]\n\n\n\nsql_results.show()\n\n\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\n _c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|     fr|        reg|     it|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nnull|province|    null|   null|null|null|  null|null|null|null|   null|        reg|   null|\n   0|   Anhui|    null|   null|1996|null| 50661|null|null|null|1128873| East China| 631930|\n   1|   Anhui|    null|   null|1997|null| 43443|null|null|null|1356287| East China| 657860|\n   2|   Anhui|    null|   null|1998|null| 27673|null|null|null|1518236| East China| 889463|\n   3|   Anhui|    null|   null|1999|null| 26131|null|null|null|1646891| East China|1227364|\n   4|   Anhui|    null|   null|2000|null| 31847|null|null|null|1601508| East China|1499110|\n   5|   Anhui|    null|   null|2001|null| 33672|null|null|null|1672445| East China|2165189|\n   6|   Anhui|    null|   null|2002|null| 38375|null|null|null|1677840| East China|2404936|\n   7|   Anhui|    null|   null|2003|null| 36720|null|null|null|1896479| East China|2815820|\n   8|   Anhui|    null|   null|2004|null| 54669|null|null|null|   null| East China|3422176|\n   9|   Anhui|    null|   null|2005|null| 69000|null|null|null|   null| East China|3874846|\n  10|   Anhui|    null|   null|2006|null|139354|null|null|null|3434548| East China|5167300|\n  11|   Anhui|    null|   null|2007|null|299892|null|null|null|4468640| East China|7040099|\n  12| Beijing|    null|   null|1996|null|155290|null|null|null| 634562|North China| 508135|\n  13| Beijing|    null|   null|1997|null|159286|null|null|null| 634562|North China| 569283|\n  14| Beijing|    null|   null|1998|null|216800|null|null|null| 938788|North China| 695528|\n  15| Beijing|    null|   null|1999|null|197525|null|null|null|   null|North China| 944047|\n  16| Beijing|    null|   null|2000|null|168368|null|null|null|1667114|North China| 757990|\n  17| Beijing|    null|   null|2001|null|176818|null|null|null|2093925|North China|1194728|\n  18| Beijing|    null|   null|2002|null|172464|null|null|null|2511249|North China|1078754|\n+----+--------+--------+-------+----+----+------+----+----+----+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\nspark.sql(\"SELECT * FROM economic_data WHERE fr=634562\").show()\n\n\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n_c0|province|specific|general|year| gdp|   fdi| rnr|  rr|   i|    fr|        reg|    it|\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n 12| Beijing|    null|   null|1996|null|155290|null|null|null|634562|North China|508135|\n 13| Beijing|    null|   null|1997|null|159286|null|null|null|634562|North China|569283|\n+---+--------+--------+-------+----+----+------+----+----+----+------+-----------+------+\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-11-14-Text_Generation_LSTM_Dostoevsky.html",
    "href": "posts/2020-11-14-Text_Generation_LSTM_Dostoevsky.html",
    "title": "Text Generation - LSTM",
    "section": "",
    "text": "Credit: Code from https://github.com/jeffheaton/t81_558_deep_learning\n\ntry:\n    %tensorflow_version 2.x\n    COLAB = True\n    print(\"Note: using Google CoLab\")\nexcept:\n    print(\"Note: not using Google CoLab\")\n    COLAB = False\n\nNote: using Google CoLab\n\n\n\nfrom tensorflow.keras.callbacks import LambdaCallback\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.optimizers import RMSprop\nfrom tensorflow.keras.utils import get_file\nimport numpy as np\nimport random\nimport sys\nimport io\nimport requests\nimport re\n\n\nr = requests.get(\"https://www.gutenberg.org/cache/epub/600/pg600.txt\")\nraw_text = r.text\nprint(raw_text[0:1000])\n\n﻿Project Gutenberg's Notes from the Underground, by Feodor Dostoevsky\n\nThis eBook is for the use of anyone anywhere at no cost and with\nalmost no restrictions whatsoever.  You may copy it, give it away or\nre-use it under the terms of the Project Gutenberg License included\nwith this eBook or online at www.gutenberg.net\n\n\nTitle: Notes from the Underground\n\nAuthor: Feodor Dostoevsky\n\nPosting Date: September 13, 2008 [EBook #600]\nRelease Date: July, 1996\n\nLanguage: English\n\n\n*** START OF THIS PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND ***\n\n\n\n\nProduced by Judith Boss.  HTML version by Al Haines.\n\n\n\n\n\n\n\n\nNotes from the Underground\n\nFYODOR DOSTOYEVSKY\n\n\n\n\n\nPART I\n\nUnderground*\n\n     *The author of the diary and the diary itself\n     are, of course, imaginary.  Nevertheless it is clear\n     that such persons as the writer of these notes\n     not only may, but positively must, exist in our\n     society, when we consider the circumstances i\n\n\n\nprocessed_text = raw_text.lower()\nprocessed_text = re.sub(r'[^\\x00-\\x7f]',r'', processed_text) \n\n\nprint('corpus length:', len(processed_text))\n\nchars = sorted(list(set(processed_text)))\nprint('total chars:', len(chars))\nchar_indices = dict((c, i) for i, c in enumerate(chars))\nindices_char = dict((i, c) for i, c in enumerate(chars))\n\ncorpus length: 265582\ntotal chars: 58\n\n\n\n# cut the text in semi-redundant sequences of maxlen characters\nmaxlen = 40\nstep = 3\nsentences = []\nnext_chars = []\nfor i in range(0, len(processed_text) - maxlen, step):\n    sentences.append(processed_text[i: i + maxlen])\n    next_chars.append(processed_text[i + maxlen])\nprint('nb sequences:', len(sentences))\n\nnb sequences: 88514\n\n\n\nsentences\n\n[\"project gutenberg's notes from the under\",\n \"ject gutenberg's notes from the undergro\",\n \"t gutenberg's notes from the underground\",\n \"utenberg's notes from the underground, b\",\n \"nberg's notes from the underground, by f\",\n \"rg's notes from the underground, by feod\",\n 's notes from the underground, by feodor ',\n 'otes from the underground, by feodor dos',\n 's from the underground, by feodor dostoe',\n 'rom the underground, by feodor dostoevsk',\n ' the underground, by feodor dostoevsky\\r\\n',\n 'e underground, by feodor dostoevsky\\r\\n\\r\\nt',\n 'nderground, by feodor dostoevsky\\r\\n\\r\\nthis',\n 'rground, by feodor dostoevsky\\r\\n\\r\\nthis eb',\n 'ound, by feodor dostoevsky\\r\\n\\r\\nthis ebook',\n 'd, by feodor dostoevsky\\r\\n\\r\\nthis ebook is',\n 'by feodor dostoevsky\\r\\n\\r\\nthis ebook is fo',\n 'feodor dostoevsky\\r\\n\\r\\nthis ebook is for t',\n 'dor dostoevsky\\r\\n\\r\\nthis ebook is for the ',\n ' dostoevsky\\r\\n\\r\\nthis ebook is for the use',\n 'stoevsky\\r\\n\\r\\nthis ebook is for the use of',\n 'evsky\\r\\n\\r\\nthis ebook is for the use of an',\n 'ky\\r\\n\\r\\nthis ebook is for the use of anyon',\n '\\n\\r\\nthis ebook is for the use of anyone a',\n 'this ebook is for the use of anyone anyw',\n 's ebook is for the use of anyone anywher',\n 'book is for the use of anyone anywhere a',\n 'k is for the use of anyone anywhere at n',\n 's for the use of anyone anywhere at no c',\n 'or the use of anyone anywhere at no cost',\n 'the use of anyone anywhere at no cost an',\n ' use of anyone anywhere at no cost and w',\n 'e of anyone anywhere at no cost and with',\n 'f anyone anywhere at no cost and with\\r\\na',\n 'nyone anywhere at no cost and with\\r\\nalmo',\n 'ne anywhere at no cost and with\\r\\nalmost ',\n 'anywhere at no cost and with\\r\\nalmost no ',\n 'where at no cost and with\\r\\nalmost no res',\n 're at no cost and with\\r\\nalmost no restri',\n 'at no cost and with\\r\\nalmost no restricti',\n 'no cost and with\\r\\nalmost no restrictions',\n 'cost and with\\r\\nalmost no restrictions wh',\n 't and with\\r\\nalmost no restrictions whats',\n 'nd with\\r\\nalmost no restrictions whatsoev',\n 'with\\r\\nalmost no restrictions whatsoever.',\n 'h\\r\\nalmost no restrictions whatsoever.  y',\n 'almost no restrictions whatsoever.  you ',\n 'ost no restrictions whatsoever.  you may',\n ' no restrictions whatsoever.  you may co',\n ' restrictions whatsoever.  you may copy ',\n 'strictions whatsoever.  you may copy it,',\n 'ictions whatsoever.  you may copy it, gi',\n 'ions whatsoever.  you may copy it, give ',\n 's whatsoever.  you may copy it, give it ',\n 'hatsoever.  you may copy it, give it awa',\n 'soever.  you may copy it, give it away o',\n 'ver.  you may copy it, give it away or\\r\\n',\n '.  you may copy it, give it away or\\r\\nre-',\n 'you may copy it, give it away or\\r\\nre-use',\n ' may copy it, give it away or\\r\\nre-use it',\n 'y copy it, give it away or\\r\\nre-use it un',\n 'opy it, give it away or\\r\\nre-use it under',\n ' it, give it away or\\r\\nre-use it under th',\n ', give it away or\\r\\nre-use it under the t',\n 'ive it away or\\r\\nre-use it under the term',\n ' it away or\\r\\nre-use it under the terms o',\n ' away or\\r\\nre-use it under the terms of t',\n 'ay or\\r\\nre-use it under the terms of the ',\n 'or\\r\\nre-use it under the terms of the pro',\n '\\nre-use it under the terms of the projec',\n '-use it under the terms of the project g',\n 'e it under the terms of the project gute',\n 't under the terms of the project gutenbe',\n 'nder the terms of the project gutenberg ',\n 'r the terms of the project gutenberg lic',\n 'he terms of the project gutenberg licens',\n 'terms of the project gutenberg license i',\n 'ms of the project gutenberg license incl',\n 'of the project gutenberg license include',\n 'the project gutenberg license included\\r\\n',\n ' project gutenberg license included\\r\\nwit',\n 'oject gutenberg license included\\r\\nwith t',\n 'ct gutenberg license included\\r\\nwith this',\n 'gutenberg license included\\r\\nwith this eb',\n 'enberg license included\\r\\nwith this ebook',\n 'erg license included\\r\\nwith this ebook or',\n ' license included\\r\\nwith this ebook or on',\n 'cense included\\r\\nwith this ebook or onlin',\n 'se included\\r\\nwith this ebook or online a',\n 'included\\r\\nwith this ebook or online at w',\n 'luded\\r\\nwith this ebook or online at www.',\n 'ed\\r\\nwith this ebook or online at www.gut',\n '\\nwith this ebook or online at www.gutenb',\n 'th this ebook or online at www.gutenberg',\n 'this ebook or online at www.gutenberg.ne',\n 's ebook or online at www.gutenberg.net\\r\\n',\n 'book or online at www.gutenberg.net\\r\\n\\r\\n\\r',\n 'k or online at www.gutenberg.net\\r\\n\\r\\n\\r\\nti',\n 'r online at www.gutenberg.net\\r\\n\\r\\n\\r\\ntitle',\n 'nline at www.gutenberg.net\\r\\n\\r\\n\\r\\ntitle: n',\n 'ne at www.gutenberg.net\\r\\n\\r\\n\\r\\ntitle: note',\n 'at www.gutenberg.net\\r\\n\\r\\n\\r\\ntitle: notes f',\n 'www.gutenberg.net\\r\\n\\r\\n\\r\\ntitle: notes from',\n '.gutenberg.net\\r\\n\\r\\n\\r\\ntitle: notes from th',\n 'tenberg.net\\r\\n\\r\\n\\r\\ntitle: notes from the u',\n 'berg.net\\r\\n\\r\\n\\r\\ntitle: notes from the unde',\n 'g.net\\r\\n\\r\\n\\r\\ntitle: notes from the undergr',\n 'et\\r\\n\\r\\n\\r\\ntitle: notes from the undergroun',\n '\\n\\r\\n\\r\\ntitle: notes from the underground\\r\\n',\n '\\r\\ntitle: notes from the underground\\r\\n\\r\\na',\n 'itle: notes from the underground\\r\\n\\r\\nauth',\n 'e: notes from the underground\\r\\n\\r\\nauthor:',\n 'notes from the underground\\r\\n\\r\\nauthor: fe',\n 'es from the underground\\r\\n\\r\\nauthor: feodo',\n 'from the underground\\r\\n\\r\\nauthor: feodor d',\n 'm the underground\\r\\n\\r\\nauthor: feodor dost',\n 'he underground\\r\\n\\r\\nauthor: feodor dostoev',\n 'underground\\r\\n\\r\\nauthor: feodor dostoevsky',\n 'erground\\r\\n\\r\\nauthor: feodor dostoevsky\\r\\n\\r',\n 'round\\r\\n\\r\\nauthor: feodor dostoevsky\\r\\n\\r\\npo',\n 'nd\\r\\n\\r\\nauthor: feodor dostoevsky\\r\\n\\r\\nposti',\n '\\n\\r\\nauthor: feodor dostoevsky\\r\\n\\r\\nposting ',\n 'author: feodor dostoevsky\\r\\n\\r\\nposting dat',\n 'hor: feodor dostoevsky\\r\\n\\r\\nposting date: ',\n ': feodor dostoevsky\\r\\n\\r\\nposting date: sep',\n 'eodor dostoevsky\\r\\n\\r\\nposting date: septem',\n 'or dostoevsky\\r\\n\\r\\nposting date: september',\n 'dostoevsky\\r\\n\\r\\nposting date: september 13',\n 'toevsky\\r\\n\\r\\nposting date: september 13, 2',\n 'vsky\\r\\n\\r\\nposting date: september 13, 2008',\n 'y\\r\\n\\r\\nposting date: september 13, 2008 [e',\n '\\r\\nposting date: september 13, 2008 [eboo',\n 'osting date: september 13, 2008 [ebook #',\n 'ing date: september 13, 2008 [ebook #600',\n ' date: september 13, 2008 [ebook #600]\\r\\n',\n 'te: september 13, 2008 [ebook #600]\\r\\nrel',\n ' september 13, 2008 [ebook #600]\\r\\nreleas',\n 'ptember 13, 2008 [ebook #600]\\r\\nrelease d',\n 'mber 13, 2008 [ebook #600]\\r\\nrelease date',\n 'r 13, 2008 [ebook #600]\\r\\nrelease date: j',\n '3, 2008 [ebook #600]\\r\\nrelease date: july',\n '2008 [ebook #600]\\r\\nrelease date: july, 1',\n '8 [ebook #600]\\r\\nrelease date: july, 1996',\n 'ebook #600]\\r\\nrelease date: july, 1996\\r\\n\\r',\n 'ok #600]\\r\\nrelease date: july, 1996\\r\\n\\r\\nla',\n '#600]\\r\\nrelease date: july, 1996\\r\\n\\r\\nlangu',\n '0]\\r\\nrelease date: july, 1996\\r\\n\\r\\nlanguage',\n '\\nrelease date: july, 1996\\r\\n\\r\\nlanguage: e',\n 'lease date: july, 1996\\r\\n\\r\\nlanguage: engl',\n 'se date: july, 1996\\r\\n\\r\\nlanguage: english',\n 'date: july, 1996\\r\\n\\r\\nlanguage: english\\r\\n\\r',\n 'e: july, 1996\\r\\n\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n',\n 'july, 1996\\r\\n\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n***',\n 'y, 1996\\r\\n\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n*** st',\n '1996\\r\\n\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n*** start',\n '6\\r\\n\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n*** start of',\n '\\r\\nlanguage: english\\r\\n\\r\\n\\r\\n*** start of th',\n 'anguage: english\\r\\n\\r\\n\\r\\n*** start of this ',\n 'uage: english\\r\\n\\r\\n\\r\\n*** start of this pro',\n 'e: english\\r\\n\\r\\n\\r\\n*** start of this projec',\n 'english\\r\\n\\r\\n\\r\\n*** start of this project g',\n 'lish\\r\\n\\r\\n\\r\\n*** start of this project gute',\n 'h\\r\\n\\r\\n\\r\\n*** start of this project gutenbe',\n '\\r\\n\\r\\n*** start of this project gutenberg ',\n '\\n*** start of this project gutenberg ebo',\n '* start of this project gutenberg ebook ',\n 'tart of this project gutenberg ebook not',\n 't of this project gutenberg ebook notes ',\n 'f this project gutenberg ebook notes fro',\n 'his project gutenberg ebook notes from t',\n ' project gutenberg ebook notes from the ',\n 'oject gutenberg ebook notes from the und',\n 'ct gutenberg ebook notes from the underg',\n 'gutenberg ebook notes from the undergrou',\n 'enberg ebook notes from the underground ',\n 'erg ebook notes from the underground ***',\n ' ebook notes from the underground ***\\r\\n\\r',\n 'ook notes from the underground ***\\r\\n\\r\\n\\r\\n',\n ' notes from the underground ***\\r\\n\\r\\n\\r\\n\\r\\n\\r',\n 'tes from the underground ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npr',\n ' from the underground ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nprodu',\n 'om the underground ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced',\n 'the underground ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by',\n ' underground ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by ju',\n 'derground ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by judit',\n 'ground ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by judith b',\n 'und ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by judith boss',\n ' ***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by judith boss.  ',\n '*\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nproduced by judith boss.  htm',\n '\\r\\n\\r\\n\\r\\n\\r\\nproduced by judith boss.  html v',\n '\\n\\r\\n\\r\\nproduced by judith boss.  html vers',\n '\\r\\nproduced by judith boss.  html version',\n 'roduced by judith boss.  html version by',\n 'uced by judith boss.  html version by al',\n 'd by judith boss.  html version by al ha',\n 'y judith boss.  html version by al haine',\n 'udith boss.  html version by al haines.\\r',\n 'th boss.  html version by al haines.\\r\\n\\r\\n',\n 'boss.  html version by al haines.\\r\\n\\r\\n\\r\\n\\r',\n 's.  html version by al haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n ' html version by al haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r',\n 'ml version by al haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n 'version by al haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nn',\n 'sion by al haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnote',\n 'n by al haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes f',\n 'y al haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes from',\n 'l haines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes from th',\n 'aines.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes from the u',\n 'es.\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes from the unde',\n '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes from the undergr',\n '\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes from the undergroun',\n '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes from the underground\\r\\n',\n '\\n\\r\\n\\r\\n\\r\\n\\r\\nnotes from the underground\\r\\n\\r\\nf',\n '\\r\\n\\r\\n\\r\\nnotes from the underground\\r\\n\\r\\nfyod',\n '\\n\\r\\nnotes from the underground\\r\\n\\r\\nfyodor ',\n 'notes from the underground\\r\\n\\r\\nfyodor dos',\n 'es from the underground\\r\\n\\r\\nfyodor dostoy',\n 'from the underground\\r\\n\\r\\nfyodor dostoyevs',\n 'm the underground\\r\\n\\r\\nfyodor dostoyevsky\\r',\n 'he underground\\r\\n\\r\\nfyodor dostoyevsky\\r\\n\\r\\n',\n 'underground\\r\\n\\r\\nfyodor dostoyevsky\\r\\n\\r\\n\\r\\n\\r',\n 'erground\\r\\n\\r\\nfyodor dostoyevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n',\n 'round\\r\\n\\r\\nfyodor dostoyevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\np',\n 'nd\\r\\n\\r\\nfyodor dostoyevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart',\n '\\n\\r\\nfyodor dostoyevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r',\n 'fyodor dostoyevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\n',\n 'dor dostoyevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\nund',\n ' dostoyevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\nunderg',\n 'stoyevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\nundergrou',\n 'yevsky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\nunderground*',\n 'sky\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\nunderground*\\r\\n\\r',\n '\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\nunderground*\\r\\n\\r\\n  ',\n '\\n\\r\\n\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\nunderground*\\r\\n\\r\\n     ',\n '\\r\\n\\r\\n\\r\\npart i\\r\\n\\r\\nunderground*\\r\\n\\r\\n     *th',\n '\\n\\r\\npart i\\r\\n\\r\\nunderground*\\r\\n\\r\\n     *the a',\n 'part i\\r\\n\\r\\nunderground*\\r\\n\\r\\n     *the auth',\n 't i\\r\\n\\r\\nunderground*\\r\\n\\r\\n     *the author ',\n '\\r\\n\\r\\nunderground*\\r\\n\\r\\n     *the author of ',\n '\\nunderground*\\r\\n\\r\\n     *the author of the',\n 'derground*\\r\\n\\r\\n     *the author of the di',\n 'ground*\\r\\n\\r\\n     *the author of the diary',\n 'und*\\r\\n\\r\\n     *the author of the diary an',\n '*\\r\\n\\r\\n     *the author of the diary and t',\n '\\r\\n     *the author of the diary and the ',\n '    *the author of the diary and the dia',\n ' *the author of the diary and the diary ',\n 'he author of the diary and the diary its',\n 'author of the diary and the diary itself',\n 'hor of the diary and the diary itself\\r\\n ',\n ' of the diary and the diary itself\\r\\n    ',\n ' the diary and the diary itself\\r\\n     ar',\n 'e diary and the diary itself\\r\\n     are, ',\n 'iary and the diary itself\\r\\n     are, of ',\n 'y and the diary itself\\r\\n     are, of cou',\n 'nd the diary itself\\r\\n     are, of course',\n 'the diary itself\\r\\n     are, of course, i',\n ' diary itself\\r\\n     are, of course, imag',\n 'ary itself\\r\\n     are, of course, imagina',\n ' itself\\r\\n     are, of course, imaginary.',\n 'self\\r\\n     are, of course, imaginary.  n',\n 'f\\r\\n     are, of course, imaginary.  neve',\n '     are, of course, imaginary.  neverth',\n '  are, of course, imaginary.  neverthele',\n 're, of course, imaginary.  nevertheless ',\n ' of course, imaginary.  nevertheless it ',\n ' course, imaginary.  nevertheless it is ',\n 'urse, imaginary.  nevertheless it is cle',\n 'e, imaginary.  nevertheless it is clear\\r',\n 'imaginary.  nevertheless it is clear\\r\\n  ',\n 'ginary.  nevertheless it is clear\\r\\n     ',\n 'ary.  nevertheless it is clear\\r\\n     tha',\n '.  nevertheless it is clear\\r\\n     that s',\n 'nevertheless it is clear\\r\\n     that such',\n 'ertheless it is clear\\r\\n     that such pe',\n 'heless it is clear\\r\\n     that such perso',\n 'ess it is clear\\r\\n     that such persons ',\n ' it is clear\\r\\n     that such persons as ',\n ' is clear\\r\\n     that such persons as the',\n ' clear\\r\\n     that such persons as the wr',\n 'ear\\r\\n     that such persons as the write',\n '\\r\\n     that such persons as the writer o',\n '    that such persons as the writer of t',\n ' that such persons as the writer of thes',\n 'at such persons as the writer of these n',\n 'such persons as the writer of these note',\n 'h persons as the writer of these notes\\r\\n',\n 'ersons as the writer of these notes\\r\\n   ',\n 'ons as the writer of these notes\\r\\n     n',\n ' as the writer of these notes\\r\\n     not ',\n ' the writer of these notes\\r\\n     not onl',\n 'e writer of these notes\\r\\n     not only m',\n 'riter of these notes\\r\\n     not only may,',\n 'er of these notes\\r\\n     not only may, bu',\n 'of these notes\\r\\n     not only may, but p',\n 'these notes\\r\\n     not only may, but posi',\n 'se notes\\r\\n     not only may, but positiv',\n 'notes\\r\\n     not only may, but positively',\n 'es\\r\\n     not only may, but positively mu',\n '\\n     not only may, but positively must,',\n '   not only may, but positively must, ex',\n 'not only may, but positively must, exist',\n ' only may, but positively must, exist in',\n 'ly may, but positively must, exist in ou',\n 'may, but positively must, exist in our\\r\\n',\n ', but positively must, exist in our\\r\\n   ',\n 'ut positively must, exist in our\\r\\n     s',\n 'positively must, exist in our\\r\\n     soci',\n 'itively must, exist in our\\r\\n     society',\n 'vely must, exist in our\\r\\n     society, w',\n 'y must, exist in our\\r\\n     society, when',\n 'ust, exist in our\\r\\n     society, when we',\n ', exist in our\\r\\n     society, when we co',\n 'xist in our\\r\\n     society, when we consi',\n 't in our\\r\\n     society, when we consider',\n 'n our\\r\\n     society, when we consider th',\n 'ur\\r\\n     society, when we consider the c',\n '\\n     society, when we consider the circ',\n '   society, when we consider the circums',\n 'society, when we consider the circumstan',\n 'iety, when we consider the circumstances',\n 'y, when we consider the circumstances in',\n 'when we consider the circumstances in\\r\\n ',\n 'n we consider the circumstances in\\r\\n    ',\n 'e consider the circumstances in\\r\\n     th',\n 'onsider the circumstances in\\r\\n     the m',\n 'ider the circumstances in\\r\\n     the mids',\n 'r the circumstances in\\r\\n     the midst o',\n 'he circumstances in\\r\\n     the midst of w',\n 'circumstances in\\r\\n     the midst of whic',\n 'cumstances in\\r\\n     the midst of which o',\n 'stances in\\r\\n     the midst of which our ',\n 'nces in\\r\\n     the midst of which our soc',\n 's in\\r\\n     the midst of which our societ',\n 'n\\r\\n     the midst of which our society i',\n '     the midst of which our society is f',\n '  the midst of which our society is form',\n 'he midst of which our society is formed.',\n 'midst of which our society is formed.  i',\n 'st of which our society is formed.  i ha',\n 'of which our society is formed.  i have\\r',\n 'which our society is formed.  i have\\r\\n  ',\n 'ch our society is formed.  i have\\r\\n     ',\n 'our society is formed.  i have\\r\\n     tri',\n ' society is formed.  i have\\r\\n     tried ',\n 'ciety is formed.  i have\\r\\n     tried to ',\n 'ty is formed.  i have\\r\\n     tried to exp',\n 'is formed.  i have\\r\\n     tried to expose',\n 'formed.  i have\\r\\n     tried to expose to',\n 'med.  i have\\r\\n     tried to expose to th',\n '.  i have\\r\\n     tried to expose to the v',\n 'i have\\r\\n     tried to expose to the view',\n 'ave\\r\\n     tried to expose to the view of',\n '\\r\\n     tried to expose to the view of th',\n '    tried to expose to the view of the p',\n ' tried to expose to the view of the publ',\n 'ied to expose to the view of the public ',\n ' to expose to the view of the public mor',\n ' expose to the view of the public more\\r\\n',\n 'pose to the view of the public more\\r\\n   ',\n 'e to the view of the public more\\r\\n     d',\n 'o the view of the public more\\r\\n     dist',\n 'he view of the public more\\r\\n     distinc',\n 'view of the public more\\r\\n     distinctly',\n 'w of the public more\\r\\n     distinctly th',\n 'f the public more\\r\\n     distinctly than ',\n 'he public more\\r\\n     distinctly than is ',\n 'public more\\r\\n     distinctly than is com',\n 'lic more\\r\\n     distinctly than is common',\n ' more\\r\\n     distinctly than is commonly ',\n 're\\r\\n     distinctly than is commonly don',\n '\\n     distinctly than is commonly done, ',\n '   distinctly than is commonly done, one',\n 'distinctly than is commonly done, one of',\n 'tinctly than is commonly done, one of th',\n 'ctly than is commonly done, one of the\\r\\n',\n 'y than is commonly done, one of the\\r\\n   ',\n 'han is commonly done, one of the\\r\\n     c',\n ' is commonly done, one of the\\r\\n     char',\n ' commonly done, one of the\\r\\n     charact',\n 'mmonly done, one of the\\r\\n     characters',\n 'nly done, one of the\\r\\n     characters of',\n ' done, one of the\\r\\n     characters of th',\n 'ne, one of the\\r\\n     characters of the r',\n ' one of the\\r\\n     characters of the rece',\n 'e of the\\r\\n     characters of the recent ',\n 'f the\\r\\n     characters of the recent pas',\n 'he\\r\\n     characters of the recent past. ',\n '\\n     characters of the recent past.  he',\n '   characters of the recent past.  he is',\n 'characters of the recent past.  he is on',\n 'racters of the recent past.  he is one o',\n 'ters of the recent past.  he is one of t',\n 's of the recent past.  he is one of the\\r',\n 'f the recent past.  he is one of the\\r\\n  ',\n 'he recent past.  he is one of the\\r\\n     ',\n 'recent past.  he is one of the\\r\\n     rep',\n 'ent past.  he is one of the\\r\\n     repres',\n ' past.  he is one of the\\r\\n     represent',\n 'st.  he is one of the\\r\\n     representati',\n '  he is one of the\\r\\n     representatives',\n 'e is one of the\\r\\n     representatives of',\n 's one of the\\r\\n     representatives of a ',\n 'ne of the\\r\\n     representatives of a gen',\n 'of the\\r\\n     representatives of a genera',\n 'the\\r\\n     representatives of a generatio',\n '\\r\\n     representatives of a generation s',\n '    representatives of a generation stil',\n ' representatives of a generation still l',\n 'presentatives of a generation still livi',\n 'sentatives of a generation still living.',\n 'tatives of a generation still living.  i',\n 'ives of a generation still living.  in t',\n 's of a generation still living.  in this',\n 'f a generation still living.  in this\\r\\n ',\n ' generation still living.  in this\\r\\n    ',\n 'neration still living.  in this\\r\\n     fr',\n 'ation still living.  in this\\r\\n     fragm',\n 'on still living.  in this\\r\\n     fragment',\n 'still living.  in this\\r\\n     fragment, e',\n 'll living.  in this\\r\\n     fragment, enti',\n 'living.  in this\\r\\n     fragment, entitle',\n 'ing.  in this\\r\\n     fragment, entitled \"',\n '.  in this\\r\\n     fragment, entitled \"und',\n 'in this\\r\\n     fragment, entitled \"underg',\n 'this\\r\\n     fragment, entitled \"undergrou',\n 's\\r\\n     fragment, entitled \"underground,',\n '     fragment, entitled \"underground,\" t',\n '  fragment, entitled \"underground,\" this',\n 'ragment, entitled \"underground,\" this pe',\n 'ment, entitled \"underground,\" this perso',\n 't, entitled \"underground,\" this person\\r\\n',\n 'entitled \"underground,\" this person\\r\\n   ',\n 'itled \"underground,\" this person\\r\\n     i',\n 'ed \"underground,\" this person\\r\\n     intr',\n '\"underground,\" this person\\r\\n     introdu',\n 'derground,\" this person\\r\\n     introduces',\n 'ground,\" this person\\r\\n     introduces hi',\n 'und,\" this person\\r\\n     introduces himse',\n ',\" this person\\r\\n     introduces himself ',\n 'this person\\r\\n     introduces himself and',\n 's person\\r\\n     introduces himself and hi',\n 'erson\\r\\n     introduces himself and his v',\n 'on\\r\\n     introduces himself and his view',\n '\\n     introduces himself and his views, ',\n '   introduces himself and his views, and',\n 'introduces himself and his views, and, a',\n 'roduces himself and his views, and, as i',\n 'uces himself and his views, and, as it w',\n 's himself and his views, and, as it were',\n 'imself and his views, and, as it were,\\r\\n',\n 'elf and his views, and, as it were,\\r\\n   ',\n ' and his views, and, as it were,\\r\\n     t',\n 'd his views, and, as it were,\\r\\n     trie',\n 'is views, and, as it were,\\r\\n     tries t',\n 'views, and, as it were,\\r\\n     tries to e',\n 'ws, and, as it were,\\r\\n     tries to expl',\n ' and, as it were,\\r\\n     tries to explain',\n 'd, as it were,\\r\\n     tries to explain th',\n 'as it were,\\r\\n     tries to explain the c',\n 'it were,\\r\\n     tries to explain the caus',\n 'were,\\r\\n     tries to explain the causes ',\n 'e,\\r\\n     tries to explain the causes owi',\n '\\n     tries to explain the causes owing ',\n '   tries to explain the causes owing to ',\n 'tries to explain the causes owing to whi',\n 'es to explain the causes owing to which ',\n 'to explain the causes owing to which he ',\n 'explain the causes owing to which he has',\n 'lain the causes owing to which he has\\r\\n ',\n 'n the causes owing to which he has\\r\\n    ',\n 'he causes owing to which he has\\r\\n     ma',\n 'causes owing to which he has\\r\\n     made ',\n 'ses owing to which he has\\r\\n     made his',\n ' owing to which he has\\r\\n     made his ap',\n 'ing to which he has\\r\\n     made his appea',\n ' to which he has\\r\\n     made his appearan',\n ' which he has\\r\\n     made his appearance ',\n 'ich he has\\r\\n     made his appearance and',\n ' he has\\r\\n     made his appearance and wa',\n ' has\\r\\n     made his appearance and was b',\n 's\\r\\n     made his appearance and was boun',\n '     made his appearance and was bound t',\n '  made his appearance and was bound to m',\n 'ade his appearance and was bound to make',\n ' his appearance and was bound to make hi',\n 's appearance and was bound to make his\\r\\n',\n 'ppearance and was bound to make his\\r\\n   ',\n 'arance and was bound to make his\\r\\n     a',\n 'nce and was bound to make his\\r\\n     appe',\n ' and was bound to make his\\r\\n     appeara',\n 'd was bound to make his\\r\\n     appearance',\n 'as bound to make his\\r\\n     appearance in',\n 'bound to make his\\r\\n     appearance in ou',\n 'nd to make his\\r\\n     appearance in our m',\n 'to make his\\r\\n     appearance in our mids',\n 'make his\\r\\n     appearance in our midst. ',\n 'e his\\r\\n     appearance in our midst.  in',\n 'is\\r\\n     appearance in our midst.  in th',\n '\\n     appearance in our midst.  in the s',\n '   appearance in our midst.  in the seco',\n 'appearance in our midst.  in the second ',\n 'earance in our midst.  in the second fra',\n 'ance in our midst.  in the second fragme',\n 'e in our midst.  in the second fragment\\r',\n 'n our midst.  in the second fragment\\r\\n  ',\n 'ur midst.  in the second fragment\\r\\n     ',\n 'midst.  in the second fragment\\r\\n     the',\n 'st.  in the second fragment\\r\\n     there ',\n '  in the second fragment\\r\\n     there are',\n 'n the second fragment\\r\\n     there are ad',\n 'he second fragment\\r\\n     there are added',\n 'second fragment\\r\\n     there are added th',\n 'ond fragment\\r\\n     there are added the a',\n ' fragment\\r\\n     there are added the actu',\n 'agment\\r\\n     there are added the actual ',\n 'ent\\r\\n     there are added the actual not',\n '\\r\\n     there are added the actual notes ',\n '    there are added the actual notes of ',\n ' there are added the actual notes of thi',\n 'ere are added the actual notes of this p',\n ' are added the actual notes of this pers',\n 'e added the actual notes of this person\\r',\n 'dded the actual notes of this person\\r\\n  ',\n 'd the actual notes of this person\\r\\n     ',\n 'he actual notes of this person\\r\\n     con',\n 'actual notes of this person\\r\\n     concer',\n 'ual notes of this person\\r\\n     concernin',\n ' notes of this person\\r\\n     concerning c',\n 'tes of this person\\r\\n     concerning cert',\n ' of this person\\r\\n     concerning certain',\n ' this person\\r\\n     concerning certain ev',\n 'is person\\r\\n     concerning certain event',\n 'person\\r\\n     concerning certain events i',\n 'son\\r\\n     concerning certain events in h',\n '\\r\\n     concerning certain events in his ',\n '    concerning certain events in his lif',\n ' concerning certain events in his life.-',\n 'ncerning certain events in his life.--au',\n 'rning certain events in his life.--autho',\n \"ng certain events in his life.--author's\",\n \"certain events in his life.--author's no\",\n \"tain events in his life.--author's note.\",\n \"n events in his life.--author's note.\\r\\n\\r\",\n \"vents in his life.--author's note.\\r\\n\\r\\n\\r\\n\",\n \"ts in his life.--author's note.\\r\\n\\r\\n\\r\\n\\r\\ni\",\n \"in his life.--author's note.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\",\n \"his life.--author's note.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni \",\n \" life.--author's note.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am \",\n \"fe.--author's note.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am a s\",\n \"--author's note.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am a sick\",\n \"uthor's note.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am a sick ma\",\n \"or's note.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am a sick man..\",\n 's note.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am a sick man.... ',\n 'ote.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am a sick man....  i ',\n '.\\r\\n\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am a sick man....  i am ',\n '\\r\\n\\r\\n\\r\\ni\\r\\n\\r\\ni am a sick man....  i am a s',\n '\\n\\r\\ni\\r\\n\\r\\ni am a sick man....  i am a spit',\n 'i\\r\\n\\r\\ni am a sick man....  i am a spitefu',\n '\\r\\ni am a sick man....  i am a spiteful m',\n ' am a sick man....  i am a spiteful man.',\n ' a sick man....  i am a spiteful man.  i',\n 'sick man....  i am a spiteful man.  i am',\n 'k man....  i am a spiteful man.  i am an',\n 'an....  i am a spiteful man.  i am an un',\n '...  i am a spiteful man.  i am an unatt',\n '  i am a spiteful man.  i am an unattrac',\n ' am a spiteful man.  i am an unattractiv',\n ' a spiteful man.  i am an unattractive m',\n 'spiteful man.  i am an unattractive man.',\n 'teful man.  i am an unattractive man.  i',\n 'ul man.  i am an unattractive man.  i\\r\\nb',\n 'man.  i am an unattractive man.  i\\r\\nbeli',\n '.  i am an unattractive man.  i\\r\\nbelieve',\n 'i am an unattractive man.  i\\r\\nbelieve my',\n 'm an unattractive man.  i\\r\\nbelieve my li',\n 'n unattractive man.  i\\r\\nbelieve my liver',\n 'nattractive man.  i\\r\\nbelieve my liver is',\n 'tractive man.  i\\r\\nbelieve my liver is di',\n 'ctive man.  i\\r\\nbelieve my liver is disea',\n 've man.  i\\r\\nbelieve my liver is diseased',\n 'man.  i\\r\\nbelieve my liver is diseased.  ',\n '.  i\\r\\nbelieve my liver is diseased.  how',\n 'i\\r\\nbelieve my liver is diseased.  howeve',\n 'believe my liver is diseased.  however, ',\n 'ieve my liver is diseased.  however, i k',\n 'e my liver is diseased.  however, i know',\n 'y liver is diseased.  however, i know no',\n 'iver is diseased.  however, i know nothi',\n 'r is diseased.  however, i know nothing ',\n 's diseased.  however, i know nothing at ',\n 'iseased.  however, i know nothing at all',\n 'ased.  however, i know nothing at all ab',\n 'd.  however, i know nothing at all about',\n ' however, i know nothing at all about my',\n 'wever, i know nothing at all about my\\r\\nd',\n 'er, i know nothing at all about my\\r\\ndise',\n ' i know nothing at all about my\\r\\ndisease',\n 'know nothing at all about my\\r\\ndisease, a',\n 'w nothing at all about my\\r\\ndisease, and ',\n 'othing at all about my\\r\\ndisease, and do ',\n 'ing at all about my\\r\\ndisease, and do not',\n ' at all about my\\r\\ndisease, and do not kn',\n ' all about my\\r\\ndisease, and do not know ',\n 'l about my\\r\\ndisease, and do not know for',\n 'bout my\\r\\ndisease, and do not know for ce',\n 't my\\r\\ndisease, and do not know for certa',\n 'y\\r\\ndisease, and do not know for certain ',\n 'disease, and do not know for certain wha',\n 'ease, and do not know for certain what a',\n 'e, and do not know for certain what ails',\n 'and do not know for certain what ails me',\n ' do not know for certain what ails me.  ',\n ' not know for certain what ails me.  i d',\n \"t know for certain what ails me.  i don'\",\n \"now for certain what ails me.  i don't c\",\n \" for certain what ails me.  i don't cons\",\n \"r certain what ails me.  i don't consult\",\n \"ertain what ails me.  i don't consult a\\r\",\n \"ain what ails me.  i don't consult a\\r\\ndo\",\n \" what ails me.  i don't consult a\\r\\ndocto\",\n \"at ails me.  i don't consult a\\r\\ndoctor f\",\n \"ails me.  i don't consult a\\r\\ndoctor for \",\n \"s me.  i don't consult a\\r\\ndoctor for it,\",\n \"e.  i don't consult a\\r\\ndoctor for it, an\",\n \" i don't consult a\\r\\ndoctor for it, and n\",\n \"don't consult a\\r\\ndoctor for it, and neve\",\n \"'t consult a\\r\\ndoctor for it, and never h\",\n 'consult a\\r\\ndoctor for it, and never have',\n 'sult a\\r\\ndoctor for it, and never have, t',\n 't a\\r\\ndoctor for it, and never have, thou',\n '\\r\\ndoctor for it, and never have, though ',\n 'octor for it, and never have, though i h',\n 'or for it, and never have, though i have',\n 'for it, and never have, though i have a ',\n ' it, and never have, though i have a res',\n ', and never have, though i have a respec',\n 'nd never have, though i have a respect f',\n 'never have, though i have a respect for ',\n 'er have, though i have a respect for med',\n 'have, though i have a respect for medici',\n 'e, though i have a respect for medicine ',\n 'though i have a respect for medicine and',\n 'ugh i have a respect for medicine and\\r\\nd',\n ' i have a respect for medicine and\\r\\ndoct',\n 'have a respect for medicine and\\r\\ndoctors',\n 'e a respect for medicine and\\r\\ndoctors. b',\n ' respect for medicine and\\r\\ndoctors. besi',\n 'spect for medicine and\\r\\ndoctors. besides',\n 'ct for medicine and\\r\\ndoctors. besides, i',\n 'for medicine and\\r\\ndoctors. besides, i am',\n ' medicine and\\r\\ndoctors. besides, i am ex',\n 'dicine and\\r\\ndoctors. besides, i am extre',\n 'ine and\\r\\ndoctors. besides, i am extremel',\n ' and\\r\\ndoctors. besides, i am extremely s',\n 'd\\r\\ndoctors. besides, i am extremely supe',\n 'doctors. besides, i am extremely superst',\n 'tors. besides, i am extremely superstiti',\n 's. besides, i am extremely superstitious',\n 'besides, i am extremely superstitious, s',\n 'ides, i am extremely superstitious, suff',\n 's, i am extremely superstitious, suffici',\n 'i am extremely superstitious, sufficient',\n 'm extremely superstitious, sufficiently ',\n 'xtremely superstitious, sufficiently so ',\n 'emely superstitious, sufficiently so to\\r',\n 'ly superstitious, sufficiently so to\\r\\nre',\n 'superstitious, sufficiently so to\\r\\nrespe',\n 'erstitious, sufficiently so to\\r\\nrespect ',\n 'titious, sufficiently so to\\r\\nrespect med',\n 'ious, sufficiently so to\\r\\nrespect medici',\n 's, sufficiently so to\\r\\nrespect medicine,',\n 'sufficiently so to\\r\\nrespect medicine, an',\n 'ficiently so to\\r\\nrespect medicine, anywa',\n 'iently so to\\r\\nrespect medicine, anyway (',\n 'tly so to\\r\\nrespect medicine, anyway (i a',\n ' so to\\r\\nrespect medicine, anyway (i am w',\n ' to\\r\\nrespect medicine, anyway (i am well',\n '\\r\\nrespect medicine, anyway (i am well-ed',\n 'espect medicine, anyway (i am well-educa',\n 'ect medicine, anyway (i am well-educated',\n ' medicine, anyway (i am well-educated en',\n 'dicine, anyway (i am well-educated enoug',\n 'ine, anyway (i am well-educated enough n',\n ', anyway (i am well-educated enough not ',\n 'nyway (i am well-educated enough not to ',\n 'ay (i am well-educated enough not to be\\r',\n '(i am well-educated enough not to be\\r\\nsu',\n 'am well-educated enough not to be\\r\\nsuper',\n 'well-educated enough not to be\\r\\nsupersti',\n 'l-educated enough not to be\\r\\nsuperstitio',\n 'ducated enough not to be\\r\\nsuperstitious,',\n 'ated enough not to be\\r\\nsuperstitious, bu',\n 'd enough not to be\\r\\nsuperstitious, but i',\n 'nough not to be\\r\\nsuperstitious, but i am',\n 'gh not to be\\r\\nsuperstitious, but i am su',\n 'not to be\\r\\nsuperstitious, but i am super',\n ' to be\\r\\nsuperstitious, but i am supersti',\n ' be\\r\\nsuperstitious, but i am superstitio',\n '\\r\\nsuperstitious, but i am superstitious)',\n 'uperstitious, but i am superstitious).  ',\n 'rstitious, but i am superstitious).  no,',\n 'itious, but i am superstitious).  no, i ',\n 'ous, but i am superstitious).  no, i ref',\n ', but i am superstitious).  no, i refuse',\n 'ut i am superstitious).  no, i refuse to',\n 'i am superstitious).  no, i refuse to co',\n 'm superstitious).  no, i refuse to consu',\n 'uperstitious).  no, i refuse to consult ',\n 'rstitious).  no, i refuse to consult a\\r\\n',\n 'itious).  no, i refuse to consult a\\r\\ndoc',\n 'ous).  no, i refuse to consult a\\r\\ndoctor',\n ').  no, i refuse to consult a\\r\\ndoctor fr',\n ' no, i refuse to consult a\\r\\ndoctor from ',\n ', i refuse to consult a\\r\\ndoctor from spi',\n ' refuse to consult a\\r\\ndoctor from spite.',\n 'fuse to consult a\\r\\ndoctor from spite.  t',\n 'e to consult a\\r\\ndoctor from spite.  that',\n 'o consult a\\r\\ndoctor from spite.  that yo',\n 'onsult a\\r\\ndoctor from spite.  that you p',\n 'ult a\\r\\ndoctor from spite.  that you prob',\n ' a\\r\\ndoctor from spite.  that you probabl',\n '\\ndoctor from spite.  that you probably w',\n 'ctor from spite.  that you probably will',\n 'r from spite.  that you probably will no',\n 'rom spite.  that you probably will not u',\n ' spite.  that you probably will not unde',\n 'ite.  that you probably will not underst',\n '.  that you probably will not understand',\n 'that you probably will not understand.  ',\n 't you probably will not understand.  wel',\n 'ou probably will not understand.  well, ',\n 'probably will not understand.  well, i\\r\\n',\n 'bably will not understand.  well, i\\r\\nund',\n 'ly will not understand.  well, i\\r\\nunders',\n 'will not understand.  well, i\\r\\nunderstan',\n 'l not understand.  well, i\\r\\nunderstand i',\n 'ot understand.  well, i\\r\\nunderstand it, ',\n 'understand.  well, i\\r\\nunderstand it, tho',\n 'erstand.  well, i\\r\\nunderstand it, though',\n 'tand.  well, i\\r\\nunderstand it, though.  ',\n 'd.  well, i\\r\\nunderstand it, though.  of ',\n ' well, i\\r\\nunderstand it, though.  of cou',\n 'll, i\\r\\nunderstand it, though.  of course',\n ' i\\r\\nunderstand it, though.  of course, i',\n '\\nunderstand it, though.  of course, i ca',\n \"derstand it, though.  of course, i can't\",\n \"stand it, though.  of course, i can't ex\",\n \"nd it, though.  of course, i can't expla\",\n \"it, though.  of course, i can't explain \",\n \" though.  of course, i can't explain who\",\n \"ough.  of course, i can't explain who it\",\n \"h.  of course, i can't explain who it is\",\n \" of course, i can't explain who it is pr\",\n \" course, i can't explain who it is preci\",\n \"urse, i can't explain who it is precisel\",\n \"e, i can't explain who it is precisely\\r\\n\",\n \"i can't explain who it is precisely\\r\\ntha\",\n \"an't explain who it is precisely\\r\\nthat i\",\n 't explain who it is precisely\\r\\nthat i am',\n 'xplain who it is precisely\\r\\nthat i am mo',\n 'ain who it is precisely\\r\\nthat i am morti',\n ' who it is precisely\\r\\nthat i am mortifyi',\n 'o it is precisely\\r\\nthat i am mortifying ',\n 't is precisely\\r\\nthat i am mortifying in ',\n 's precisely\\r\\nthat i am mortifying in thi',\n 'recisely\\r\\nthat i am mortifying in this c',\n 'isely\\r\\nthat i am mortifying in this case',\n 'ly\\r\\nthat i am mortifying in this case by',\n '\\nthat i am mortifying in this case by my',\n 'at i am mortifying in this case by my sp',\n 'i am mortifying in this case by my spite',\n 'm mortifying in this case by my spite: i',\n 'ortifying in this case by my spite: i am',\n 'ifying in this case by my spite: i am pe',\n 'ing in this case by my spite: i am perfe',\n ' in this case by my spite: i am perfectl',\n ' this case by my spite: i am perfectly w',\n 'is case by my spite: i am perfectly well',\n 'case by my spite: i am perfectly well\\r\\na',\n 'e by my spite: i am perfectly well\\r\\nawar',\n 'y my spite: i am perfectly well\\r\\naware t',\n 'y spite: i am perfectly well\\r\\naware that',\n 'pite: i am perfectly well\\r\\naware that i ',\n 'e: i am perfectly well\\r\\naware that i can',\n 'i am perfectly well\\r\\naware that i cannot',\n 'm perfectly well\\r\\naware that i cannot \"p',\n 'erfectly well\\r\\naware that i cannot \"pay ',\n 'ectly well\\r\\naware that i cannot \"pay out',\n 'ly well\\r\\naware that i cannot \"pay out\" t',\n 'well\\r\\naware that i cannot \"pay out\" the ',\n 'l\\r\\naware that i cannot \"pay out\" the doc',\n 'aware that i cannot \"pay out\" the doctor',\n 're that i cannot \"pay out\" the doctors b',\n 'that i cannot \"pay out\" the doctors by n',\n 't i cannot \"pay out\" the doctors by not ',\n ' cannot \"pay out\" the doctors by not con',\n 'nnot \"pay out\" the doctors by not consul',\n 't \"pay out\" the doctors by not consultin',\n 'pay out\" the doctors by not consulting t',\n ' out\" the doctors by not consulting them',\n 't\" the doctors by not consulting them; i',\n 'the doctors by not consulting them; i\\r\\nk',\n ' doctors by not consulting them; i\\r\\nknow',\n 'ctors by not consulting them; i\\r\\nknow be',\n 'rs by not consulting them; i\\r\\nknow bette',\n 'by not consulting them; i\\r\\nknow better t',\n 'not consulting them; i\\r\\nknow better than',\n ' consulting them; i\\r\\nknow better than an',\n 'nsulting them; i\\r\\nknow better than anyon',\n 'lting them; i\\r\\nknow better than anyone t',\n 'ng them; i\\r\\nknow better than anyone that',\n 'them; i\\r\\nknow better than anyone that by',\n 'm; i\\r\\nknow better than anyone that by al',\n 'i\\r\\nknow better than anyone that by all t',\n 'know better than anyone that by all this',\n 'w better than anyone that by all this i ',\n 'etter than anyone that by all this i am ',\n 'er than anyone that by all this i am onl',\n 'than anyone that by all this i am only i',\n 'n anyone that by all this i am only inju',\n 'nyone that by all this i am only injurin',\n 'ne that by all this i am only injuring m',\n 'that by all this i am only injuring myse',\n 't by all this i am only injuring myself ',\n 'y all this i am only injuring myself and',\n 'll this i am only injuring myself and\\r\\nn',\n 'this i am only injuring myself and\\r\\nno o',\n 's i am only injuring myself and\\r\\nno one ',\n ' am only injuring myself and\\r\\nno one els',\n ' only injuring myself and\\r\\nno one else. ',\n 'ly injuring myself and\\r\\nno one else.  bu',\n 'injuring myself and\\r\\nno one else.  but s',\n 'uring myself and\\r\\nno one else.  but stil',\n 'ng myself and\\r\\nno one else.  but still, ',\n 'myself and\\r\\nno one else.  but still, if ',\n 'elf and\\r\\nno one else.  but still, if i d',\n \" and\\r\\nno one else.  but still, if i don'\",\n \"d\\r\\nno one else.  but still, if i don't c\",\n \"no one else.  but still, if i don't cons\",\n \"one else.  but still, if i don't consult\",\n \" else.  but still, if i don't consult a \",\n \"se.  but still, if i don't consult a doc\",\n \"  but still, if i don't consult a doctor\",\n \"ut still, if i don't consult a doctor it\",\n \"still, if i don't consult a doctor it is\",\n \"ll, if i don't consult a doctor it is fr\",\n \" if i don't consult a doctor it is from \",\n \" i don't consult a doctor it is from spi\",\n \"don't consult a doctor it is from spite.\",\n \"'t consult a doctor it is from spite.\\r\\nm\",\n 'consult a doctor it is from spite.\\r\\nmy l',\n 'sult a doctor it is from spite.\\r\\nmy live',\n 't a doctor it is from spite.\\r\\nmy liver i',\n ' doctor it is from spite.\\r\\nmy liver is b',\n 'ctor it is from spite.\\r\\nmy liver is bad,',\n 'r it is from spite.\\r\\nmy liver is bad, we',\n 't is from spite.\\r\\nmy liver is bad, well-',\n 's from spite.\\r\\nmy liver is bad, well--le',\n 'rom spite.\\r\\nmy liver is bad, well--let i',\n ' spite.\\r\\nmy liver is bad, well--let it g',\n 'ite.\\r\\nmy liver is bad, well--let it get ',\n '.\\r\\nmy liver is bad, well--let it get wor',\n 'my liver is bad, well--let it get worse!',\n 'liver is bad, well--let it get worse!\\r\\n\\r',\n 'er is bad, well--let it get worse!\\r\\n\\r\\ni ',\n 'is bad, well--let it get worse!\\r\\n\\r\\ni hav',\n 'bad, well--let it get worse!\\r\\n\\r\\ni have b',\n ', well--let it get worse!\\r\\n\\r\\ni have been',\n 'ell--let it get worse!\\r\\n\\r\\ni have been go',\n '--let it get worse!\\r\\n\\r\\ni have been going',\n 'et it get worse!\\r\\n\\r\\ni have been going on',\n 'it get worse!\\r\\n\\r\\ni have been going on li',\n 'get worse!\\r\\n\\r\\ni have been going on like ',\n ' worse!\\r\\n\\r\\ni have been going on like tha',\n 'rse!\\r\\n\\r\\ni have been going on like that f',\n '!\\r\\n\\r\\ni have been going on like that for ',\n '\\r\\ni have been going on like that for a l',\n ' have been going on like that for a long',\n 've been going on like that for a long ti',\n 'been going on like that for a long time-',\n 'n going on like that for a long time--tw',\n 'oing on like that for a long time--twent',\n 'g on like that for a long time--twenty y',\n 'n like that for a long time--twenty year',\n 'ike that for a long time--twenty years. ',\n ' that for a long time--twenty years.  no',\n 'at for a long time--twenty years.  now i',\n 'for a long time--twenty years.  now i am',\n ' a long time--twenty years.  now i am\\r\\nf',\n 'long time--twenty years.  now i am\\r\\nfort',\n 'g time--twenty years.  now i am\\r\\nforty. ',\n 'ime--twenty years.  now i am\\r\\nforty.  i ',\n '--twenty years.  now i am\\r\\nforty.  i use',\n 'wenty years.  now i am\\r\\nforty.  i used t',\n 'ty years.  now i am\\r\\nforty.  i used to b',\n 'years.  now i am\\r\\nforty.  i used to be i',\n 'rs.  now i am\\r\\nforty.  i used to be in t',\n '  now i am\\r\\nforty.  i used to be in the ',\n 'ow i am\\r\\nforty.  i used to be in the gov',\n 'i am\\r\\nforty.  i used to be in the govern',\n 'm\\r\\nforty.  i used to be in the governmen',\n 'forty.  i used to be in the government s',\n 'ty.  i used to be in the government serv',\n '  i used to be in the government service',\n ' used to be in the government service, b',\n 'ed to be in the government service, but ',\n 'to be in the government service, but am ',\n 'be in the government service, but am no ',\n 'in the government service, but am no lon',\n 'the government service, but am no longer',\n ' government service, but am no longer.  ',\n 'vernment service, but am no longer.  i\\r\\n',\n 'nment service, but am no longer.  i\\r\\nwas',\n 'nt service, but am no longer.  i\\r\\nwas a ',\n 'service, but am no longer.  i\\r\\nwas a spi',\n 'vice, but am no longer.  i\\r\\nwas a spitef',\n 'e, but am no longer.  i\\r\\nwas a spiteful ',\n 'but am no longer.  i\\r\\nwas a spiteful off',\n ' am no longer.  i\\r\\nwas a spiteful offici',\n ' no longer.  i\\r\\nwas a spiteful official.',\n ' longer.  i\\r\\nwas a spiteful official.  i',\n 'nger.  i\\r\\nwas a spiteful official.  i wa',\n 'r.  i\\r\\nwas a spiteful official.  i was r',\n ' i\\r\\nwas a spiteful official.  i was rude',\n '\\nwas a spiteful official.  i was rude an',\n 's a spiteful official.  i was rude and t',\n ' spiteful official.  i was rude and took',\n 'iteful official.  i was rude and took pl',\n 'ful official.  i was rude and took pleas',\n ' official.  i was rude and took pleasure',\n 'ficial.  i was rude and took pleasure in',\n 'ial.  i was rude and took pleasure in be',\n '.  i was rude and took pleasure in being',\n 'i was rude and took pleasure in being so',\n 'as rude and took pleasure in being so.  ',\n 'rude and took pleasure in being so.  i\\r\\n',\n 'e and took pleasure in being so.  i\\r\\ndid',\n 'nd took pleasure in being so.  i\\r\\ndid no',\n 'took pleasure in being so.  i\\r\\ndid not t',\n 'k pleasure in being so.  i\\r\\ndid not take',\n 'leasure in being so.  i\\r\\ndid not take br',\n 'sure in being so.  i\\r\\ndid not take bribe',\n 'e in being so.  i\\r\\ndid not take bribes, ',\n 'n being so.  i\\r\\ndid not take bribes, you',\n 'eing so.  i\\r\\ndid not take bribes, you se',\n 'g so.  i\\r\\ndid not take bribes, you see, ',\n 'o.  i\\r\\ndid not take bribes, you see, so ',\n ' i\\r\\ndid not take bribes, you see, so i w',\n '\\ndid not take bribes, you see, so i was ',\n 'd not take bribes, you see, so i was bou',\n 'ot take bribes, you see, so i was bound ',\n 'take bribes, you see, so i was bound to ',\n 'e bribes, you see, so i was bound to fin',\n 'ribes, you see, so i was bound to find a',\n 'es, you see, so i was bound to find a re',\n ' you see, so i was bound to find a recom',\n 'u see, so i was bound to find a recompen',\n 'ee, so i was bound to find a recompense ',\n ' so i was bound to find a recompense in\\r',\n ' i was bound to find a recompense in\\r\\nth',\n 'was bound to find a recompense in\\r\\nthat,',\n ' bound to find a recompense in\\r\\nthat, at',\n 'und to find a recompense in\\r\\nthat, at le',\n ' to find a recompense in\\r\\nthat, at least',\n ' find a recompense in\\r\\nthat, at least.  ',\n 'nd a recompense in\\r\\nthat, at least.  (a ',\n 'a recompense in\\r\\nthat, at least.  (a poo',\n 'ecompense in\\r\\nthat, at least.  (a poor j',\n 'mpense in\\r\\nthat, at least.  (a poor jest',\n 'nse in\\r\\nthat, at least.  (a poor jest, b',\n ' in\\r\\nthat, at least.  (a poor jest, but ',\n '\\r\\nthat, at least.  (a poor jest, but i w',\n 'hat, at least.  (a poor jest, but i will',\n ', at least.  (a poor jest, but i will no',\n 't least.  (a poor jest, but i will not s',\n 'east.  (a poor jest, but i will not scra',\n 't.  (a poor jest, but i will not scratch',\n ' (a poor jest, but i will not scratch it',\n ' poor jest, but i will not scratch it ou',\n 'or jest, but i will not scratch it out. ',\n 'jest, but i will not scratch it out.  i ',\n 't, but i will not scratch it out.  i wro',\n 'but i will not scratch it out.  i wrote\\r',\n ' i will not scratch it out.  i wrote\\r\\nit',\n 'will not scratch it out.  i wrote\\r\\nit th',\n 'l not scratch it out.  i wrote\\r\\nit think',\n 'ot scratch it out.  i wrote\\r\\nit thinking',\n 'scratch it out.  i wrote\\r\\nit thinking it',\n 'atch it out.  i wrote\\r\\nit thinking it wo',\n 'h it out.  i wrote\\r\\nit thinking it would',\n 't out.  i wrote\\r\\nit thinking it would so',\n 'ut.  i wrote\\r\\nit thinking it would sound',\n '  i wrote\\r\\nit thinking it would sound ve',\n ' wrote\\r\\nit thinking it would sound very ',\n 'ote\\r\\nit thinking it would sound very wit',\n '\\r\\nit thinking it would sound very witty;',\n 't thinking it would sound very witty; bu',\n 'hinking it would sound very witty; but n',\n 'king it would sound very witty; but now ',\n 'g it would sound very witty; but now tha',\n ...]\n\n\n\nprint('Vectorization...')\nx = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\ny = np.zeros((len(sentences), len(chars)), dtype=np.bool)\nfor i, sentence in enumerate(sentences):\n    for t, char in enumerate(sentence):\n        x[i, t, char_indices[char]] = 1\n    y[i, char_indices[next_chars[i]]] = 1\n\nVectorization...\n\n\n\nx.shape\n\n(88514, 40, 58)\n\n\n\ny.shape\n\n(88514, 58)\n\n\n\ny[0:10]\n\narray([[False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False,  True, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False,  True, False,\n        False, False, False, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False,  True, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False,  True, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n         True, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False,  True, False, False, False, False, False, False, False,\n        False, False, False, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False,  True,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False,  True, False, False,\n        False, False, False, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False,  True,\n        False, False, False, False],\n       [False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False, False, False, False, False, False, False, False,\n        False, False,  True, False]])\n\n\n\n# build the model: a single LSTM\nprint('Build model...')\nmodel = Sequential()\nmodel.add(LSTM(128, input_shape=(maxlen, len(chars))))\nmodel.add(Dense(len(chars), activation='softmax'))\n\noptimizer = RMSprop(lr=0.01)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\nBuild model...\n\n\n\nmodel.summary()\n\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nlstm (LSTM)                  (None, 128)               95744     \n_________________________________________________________________\ndense (Dense)                (None, 58)                7482      \n=================================================================\nTotal params: 103,226\nTrainable params: 103,226\nNon-trainable params: 0\n_________________________________________________________________\n\n\n\ndef sample(preds, temperature=1.0):\n    # helper function to sample an index from a probability array\n    preds = np.asarray(preds).astype('float64')\n    preds = np.log(preds) / temperature\n    exp_preds = np.exp(preds)\n    preds = exp_preds / np.sum(exp_preds)\n    probas = np.random.multinomial(1, preds, 1)\n    return np.argmax(probas)\n\n\ndef on_epoch_end(epoch, _):\n    # Function invoked at end of each epoch. Prints generated text.\n    print(\"******************************************************\")\n    print('----- Generating text after Epoch: %d' % epoch)\n\n    start_index = random.randint(0, len(processed_text) - maxlen - 1)\n    for temperature in [0.2, 0.5, 1.0, 1.2]:\n        print('----- temperature:', temperature)\n\n        generated = ''\n        sentence = processed_text[start_index: start_index + maxlen]\n        generated += sentence\n        print('----- Generating with seed: \"' + sentence + '\"')\n        sys.stdout.write(generated)\n\n        for i in range(400):\n            x_pred = np.zeros((1, maxlen, len(chars)))\n            for t, char in enumerate(sentence):\n                x_pred[0, t, char_indices[char]] = 1.\n\n            preds = model.predict(x_pred, verbose=0)[0]\n            next_index = sample(preds, temperature)\n            next_char = indices_char[next_index]\n\n            generated += next_char\n            sentence = sentence[1:] + next_char\n\n            sys.stdout.write(next_char)\n            sys.stdout.flush()\n        print()\n\n\n# Ignore useless W0819 warnings generated by TensorFlow 2.0.  Hopefully can remove this ignore in the future.\n# See https://github.com/tensorflow/tensorflow/issues/31308\nimport logging, os\nlogging.disable(logging.WARNING)\nos.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n\n# Fit the model\nprint_callback = LambdaCallback(on_epoch_end=on_epoch_end)\n\nmodel.fit(x, y,\n          batch_size=128,\n          epochs=60,\n          callbacks=[print_callback])\n\nEpoch 1/60\n692/692 [==============================] - ETA: 0s - loss: 2.1576******************************************************\n----- Generating text after Epoch: 0\n----- temperature: 0.2\n----- Generating with seed: \"cried.  \"but i\nwill make up for it or p\"\ncried.  \"but i\nwill make up for it or pored the stine the derent of the dook and the stand to the store and the doon the sare the dong the dened to the dook the sare the stare the street of the seres at all the sall the deared the derent of the street in i am in the derent to the stan to the street the stine the sered to the dran that is all the stan the stare the dook the forest of the stan stand of the dook all the stand the streed t\n----- temperature: 0.5\n----- Generating with seed: \"cried.  \"but i\nwill make up for it or p\"\ncried.  \"but i\n\n\n\nhe know conse the lake the s\n----- temperature: 1.0\n----- Generating with seed: \"cried.  \"but i\nwill make up for it or p\"\ncried.  \"but i\n\n\n\n\n\n\n\n\"yes kle the lafine as the prosies netc\n----- temperature: 1.2\n----- Generating with seed: \"cried.  \"but i\nwill make up for it or p\"\ncried.  \"but i\n\n\n\n\n\n\n\n\n\n\n\n\nanp, !\n692/692 [==============================] - 155s 224ms/step - loss: 2.1576\nEpoch 2/60\n572/692 [=======================&gt;......] - ETA: 13s - loss: 1.7386"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html",
    "href": "posts/2021-05-31-cross-validation.html",
    "title": "Cross-Validation in scikit-learn example",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2\nimport seaborn as sns\nimport sklearn\nsns.set_theme(context=\"notebook\", font_scale=1.2,\n              rc={\"figure.figsize\": [10, 6]})\nsklearn.set_config(display=\"diagram\")"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#load-sample-data",
    "href": "posts/2021-05-31-cross-validation.html#load-sample-data",
    "title": "Cross-Validation in scikit-learn example",
    "section": "Load sample data",
    "text": "Load sample data\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['it', 'Unnamed: 0'], axis = 1), df['it']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#cross-validation-for-model-selection",
    "href": "posts/2021-05-31-cross-validation.html#cross-validation-for-model-selection",
    "title": "Cross-Validation in scikit-learn example",
    "section": "Cross validation for model selection",
    "text": "Cross validation for model selection\n\nTry DummyClassifier\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.dummy import DummyClassifier\n\n\ndummy_clf = DummyClassifier(strategy=\"prior\")\ndummy_scores = cross_val_score(dummy_clf, X_train, y_train)\n\n\ndummy_scores\n\narray([0.98119697, 0.98119697, 0.98119697, 0.98096767, 0.98119266])\n\n\n\ndummy_scores.mean()\n\n0.981150249606079\n\n\n\n\nTry KNeighborsClassifier\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nknc = make_pipeline(StandardScaler(), KNeighborsClassifier())\nknc_scores = cross_val_score(knc, X_train, y_train)\n\n\nknc_scores\n\narray([0.98073836, 0.98096767, 0.98050906, 0.98096767, 0.9809633 ])\n\n\n\nknc_scores.mean()\n\n0.980829211800172\n\n\n\n\nTry LogisticRegression\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\n\n\nlog_reg = make_pipeline(\n    StandardScaler(),\n    LogisticRegression(random_state=0)\n)\n\n\nlog_reg_scores = cross_val_score(log_reg, X_train, y_train)\n\n\nlog_reg_scores\n\narray([0.98119697, 0.98119697, 0.98119697, 0.98096767, 0.98119266])\n\n\n\nlog_reg_scores.mean()\n\n0.981150249606079\n\n\n\n\nWhich model do we choose?\n\nDummy\nKNeighborsClassifier\nLogisticRegression\n\n\ny.value_counts()\n\n\ndummy_scores = cross_val_score(dummy_clf, X_train, y_train, scoring=\"roc_auc\")\ndummy_scores.mean()\n\nknc_scores = cross_val_score(knc, X_train, y_train, scoring=\"roc_auc\")\nknc_scores.mean()\n\n0.5880984927926187"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#cross-validation-strategies",
    "href": "posts/2021-05-31-cross-validation.html#cross-validation-strategies",
    "title": "Cross-Validation in scikit-learn example",
    "section": "Cross validation Strategies",
    "text": "Cross validation Strategies\n\nKFold\n\nfrom sklearn.model_selection import KFold\n\ncross_val_score(log_reg, X_train, y_train, cv=KFold(n_splits=4))\n\narray([0.97982022, 0.98275546, 0.98183819, 0.98018712])"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#repeated-kfold",
    "href": "posts/2021-05-31-cross-validation.html#repeated-kfold",
    "title": "Cross-Validation in scikit-learn example",
    "section": "Repeated KFold",
    "text": "Repeated KFold\n\nfrom sklearn.model_selection import RepeatedKFold\n\nscores = cross_val_score(log_reg, X_train, y_train,\n                         cv=RepeatedKFold(n_splits=4, n_repeats=2))\n\n\nscores\n\narray([0.98238855, 0.9787195 , 0.98348927, 0.98000367, 0.98018712,\n       0.98018712, 0.98128784, 0.98293891])\n\n\n\nscores.shape\n\n(8,)"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#stratifiedkfold",
    "href": "posts/2021-05-31-cross-validation.html#stratifiedkfold",
    "title": "Cross-Validation in scikit-learn example",
    "section": "StratifiedKFold",
    "text": "StratifiedKFold\n\nfrom sklearn.model_selection import StratifiedKFold\n\nscores = cross_val_score(log_reg, X_train, y_train,\n                         cv=StratifiedKFold(n_splits=4))\n\n\nscores\n\narray([0.98128784, 0.98110438, 0.98110438, 0.98110438])\n\n\nThis is a binary classification problem:\n\ny.value_counts()\n\n0    28524\n1      548\nName: stroke, dtype: int64\n\n\nScikit-learn will use StratifiedKFold by default:\n\ncross_val_score(log_reg, X_train, y_train, cv=4)\n\narray([0.98128784, 0.98110438, 0.98110438, 0.98110438])"
  },
  {
    "objectID": "posts/2021-05-31-cross-validation.html#repeatedstratifiedkfold",
    "href": "posts/2021-05-31-cross-validation.html#repeatedstratifiedkfold",
    "title": "Cross-Validation in scikit-learn example",
    "section": "RepeatedStratifiedKFold",
    "text": "RepeatedStratifiedKFold\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\n\nscores = cross_val_score(\n    log_reg, X_train, y_train,\n    cv=RepeatedStratifiedKFold(n_splits=4, n_repeats=3))\n\n\nscores\n\narray([0.98128784, 0.98110438, 0.98110438, 0.98110438, 0.98128784,\n       0.98110438, 0.98110438, 0.98110438, 0.98128784, 0.98110438,\n       0.98110438, 0.98110438])\n\n\n\nscores.shape\n\n(12,)\n\n\n\n# %load solutions/01-ex02-solutions.py\nfrom sklearn.model_selection import cross_validate\n\nresults = cross_validate(log_reg, X_train, y_train, cv=4)\nresults\n\nimport pandas as pd\npd.DataFrame(results)\n\nmore_results = cross_validate(log_reg, X_train, y_train, cv=4, scoring=[\"f1\", \"accuracy\"])\n\npd.DataFrame(more_results)\n\n\n\n\n\n\n\n\nfit_time\nscore_time\ntest_f1\ntest_accuracy\n\n\n\n\n0\n0.039663\n0.007991\n0.0\n0.981288\n\n\n1\n0.038163\n0.007970\n0.0\n0.981104\n\n\n2\n0.040289\n0.008049\n0.0\n0.981104\n\n\n3\n0.038140\n0.007756\n0.0\n0.981104\n\n\n\n\n\n\n\n\nAppendix: TimeSeriesSplit\n\nfrom sklearn.model_selection import TimeSeriesSplit\nimport numpy as np\n\nX = np.arange(10)\n\n\ntscv = TimeSeriesSplit(n_splits=3)\nfor train_index, test_index in tscv.split(X):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n\nTRAIN: [0 1 2 3] TEST: [4 5]\nTRAIN: [0 1 2 3 4 5] TEST: [6 7]\nTRAIN: [0 1 2 3 4 5 6 7] TEST: [8 9]\n\n\nWith gap=2:"
  },
  {
    "objectID": "posts/2021-06-10-regression-pycaret-2.html",
    "href": "posts/2021-06-10-regression-pycaret-2.html",
    "title": "Regression using Fiscal Data with PyCaret",
    "section": "",
    "text": "# check version\nfrom pycaret.utils import version\nversion()\n\n'2.3.1'\n\n\n\n# %load solutions/regression_example.py\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\ndf=df.drop(['Unnamed: 0'], axis = 1)\n\n\n\n\n\nfrom pycaret.regression import *\nreg1 = setup(df, target = 'specific', session_id=153, log_experiment=True, experiment_name='fiscal')\n\n\n\n\n\n\nDescription\nValue\n\n\n\n\n0\nsession_id\n153\n\n\n1\nTarget\nspecific\n\n\n2\nOriginal Data\n(118, 12)\n\n\n3\nMissing Values\nFalse\n\n\n4\nNumeric Features\n8\n\n\n5\nCategorical Features\n3\n\n\n6\nOrdinal Features\nFalse\n\n\n7\nHigh Cardinality Features\nFalse\n\n\n8\nHigh Cardinality Method\nNone\n\n\n9\nTransformed Train Set\n(82, 47)\n\n\n10\nTransformed Test Set\n(36, 47)\n\n\n11\nShuffle Train-Test\nTrue\n\n\n12\nStratify Train-Test\nFalse\n\n\n13\nFold Generator\nKFold\n\n\n14\nFold Number\n10\n\n\n15\nCPU Jobs\n-1\n\n\n16\nUse GPU\nFalse\n\n\n17\nLog Experiment\nTrue\n\n\n18\nExperiment Name\nfiscal\n\n\n19\nUSI\n0884\n\n\n20\nImputation Type\nsimple\n\n\n21\nIterative Imputation Iteration\nNone\n\n\n22\nNumeric Imputer\nmean\n\n\n23\nIterative Imputation Numeric Model\nNone\n\n\n24\nCategorical Imputer\nconstant\n\n\n25\nIterative Imputation Categorical Model\nNone\n\n\n26\nUnknown Categoricals Handling\nleast_frequent\n\n\n27\nNormalize\nFalse\n\n\n28\nNormalize Method\nNone\n\n\n29\nTransformation\nFalse\n\n\n30\nTransformation Method\nNone\n\n\n31\nPCA\nFalse\n\n\n32\nPCA Method\nNone\n\n\n33\nPCA Components\nNone\n\n\n34\nIgnore Low Variance\nFalse\n\n\n35\nCombine Rare Levels\nFalse\n\n\n36\nRare Level Threshold\nNone\n\n\n37\nNumeric Binning\nFalse\n\n\n38\nRemove Outliers\nFalse\n\n\n39\nOutliers Threshold\nNone\n\n\n40\nRemove Multicollinearity\nFalse\n\n\n41\nMulticollinearity Threshold\nNone\n\n\n42\nClustering\nFalse\n\n\n43\nClustering Iteration\nNone\n\n\n44\nPolynomial Features\nFalse\n\n\n45\nPolynomial Degree\nNone\n\n\n46\nTrignometry Features\nFalse\n\n\n47\nPolynomial Threshold\nNone\n\n\n48\nGroup Features\nFalse\n\n\n49\nFeature Selection\nFalse\n\n\n50\nFeature Selection Method\nclassic\n\n\n51\nFeatures Selection Threshold\nNone\n\n\n52\nFeature Interaction\nFalse\n\n\n53\nFeature Ratio\nFalse\n\n\n54\nInteraction Threshold\nNone\n\n\n55\nTransform Target\nFalse\n\n\n56\nTransform Target Method\nbox-cox\n\n\n\n\n\n\nbest_model = compare_models(fold=5)\n\n\n\n\n\n\nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\nridge\nRidge Regression\n203104.6812\n70889717760.0000\n264074.3844\n0.8726\n0.5540\n0.3956\n0.0080\n\n\nen\nElastic Net\n214984.3531\n85422610841.6000\n290426.9344\n0.8517\n0.4986\n0.4002\n0.0100\n\n\nbr\nBayesian Ridge\n220166.1782\n95589994393.9887\n304401.6991\n0.8301\n0.4481\n0.3928\n0.0120\n\n\nhuber\nHuber Regressor\n220856.7956\n112309915361.3680\n329346.7142\n0.8236\n0.4063\n0.3861\n0.0240\n\n\nlr\nLinear Regression\n232120.2812\n103810244608.0000\n317848.8281\n0.8138\n0.5000\n0.4204\n0.4660\n\n\net\nExtra Trees Regressor\n221900.6764\n127140774212.6801\n338961.7060\n0.7945\n0.3815\n0.3499\n0.0580\n\n\nrf\nRandom Forest Regressor\n237185.4749\n140134962286.5481\n359066.7448\n0.7677\n0.3845\n0.3603\n0.0680\n\n\ngbr\nGradient Boosting Regressor\n238720.3298\n145838870195.5470\n366741.3828\n0.7624\n0.3810\n0.3619\n0.0200\n\n\nknn\nK Neighbors Regressor\n285577.7062\n149621195571.2000\n378386.5938\n0.7535\n0.4782\n0.4564\n0.0080\n\n\nomp\nOrthogonal Matching Pursuit\n238278.1124\n126779634746.9780\n340431.6364\n0.7507\n0.6785\n0.4087\n0.0060\n\n\nada\nAdaBoost Regressor\n286133.9032\n178448925624.8169\n409351.8897\n0.7261\n0.4671\n0.4826\n0.0380\n\n\npar\nPassive Aggressive Regressor\n333654.6862\n255709611689.0604\n478365.7421\n0.6396\n0.6616\n0.4911\n0.0080\n\n\nlightgbm\nLight Gradient Boosting Machine\n333751.2407\n246645596801.5230\n489542.3762\n0.6196\n0.4881\n0.4594\n0.0140\n\n\ndt\nDecision Tree Regressor\n331466.4338\n251572931731.8265\n484401.1935\n0.5996\n0.4895\n0.4942\n0.0080\n\n\nlasso\nLasso Regression\n472806.4594\n1744652831948.8000\n924353.6562\n-2.9647\n0.9793\n0.7323\n0.3020\n\n\nllar\nLasso Least Angle Regression\n557614.3428\n2757565135711.8994\n1218269.0934\n-4.1517\n0.9882\n0.9808\n0.0120\n\n\nlar\nLeast Angle Regression\n523505032166121.1875\n8777827809541126967434359603200.0000\n1651376809056778.0000\n-21875898822041108480.0000\n12.5860\n2953087708.0914\n0.0120\n\n\n\n\n\n\ngbr = create_model('gbr')\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n259539.4234\n188010814764.0497\n433602.1388\n0.2035\n0.3725\n0.3658\n\n\n1\n344439.6555\n261157479760.8762\n511035.6932\n0.7712\n0.2734\n0.2021\n\n\n2\n269448.7502\n89799376760.5959\n299665.4414\n0.2559\n0.5134\n0.5621\n\n\n3\n156389.8428\n56101010341.3762\n236856.5185\n0.9215\n0.3037\n0.2752\n\n\n4\n197734.1876\n68770895511.2340\n262242.0552\n0.8442\n0.4341\n0.4405\n\n\n5\n316382.5762\n190021156955.1915\n435914.1624\n0.8431\n0.3250\n0.3036\n\n\n6\n132877.7936\n48011457619.3648\n219115.1698\n0.9377\n0.1445\n0.1184\n\n\n7\n63780.4855\n6638335948.5179\n81475.9839\n0.9926\n0.2004\n0.1484\n\n\n8\n84622.6556\n19489890756.1842\n139606.1988\n0.8903\n0.5672\n0.5448\n\n\n9\n312499.9655\n219320548133.8201\n468316.7178\n0.6284\n0.4557\n0.4542\n\n\nMean\n213771.5336\n114732096655.1211\n308783.0080\n0.7288\n0.3590\n0.3415\n\n\nSD\n95820.4252\n86484686219.4249\n139230.5665\n0.2673\n0.1285\n0.1501\n\n\n\n\n\n\nimport numpy as np\ngbrs = [create_model('gbr', learning_rate=i) for i in np.arange(0.1,1,0.1)]\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n214161.7167\n66589303052.3318\n258049.0323\n0.7179\n0.5064\n0.3315\n\n\n1\n383023.4934\n211259341300.2875\n459629.5697\n0.8149\n0.3382\n0.2795\n\n\n2\n239279.0742\n87706228150.9347\n296152.3732\n0.2732\n0.5829\n0.5918\n\n\n3\n192434.2246\n95811714139.4744\n309534.6736\n0.8660\n0.4334\n0.4130\n\n\n4\n142428.6249\n51054538060.0787\n225952.5128\n0.8843\n0.2410\n0.1901\n\n\n5\n367843.9369\n206432485479.9056\n454348.4186\n0.8295\n0.5008\n0.4725\n\n\n6\n228995.4083\n126313136112.6237\n355405.5938\n0.8362\n0.2683\n0.2024\n\n\n7\n158840.0937\n40230541391.6350\n200575.5254\n0.9550\n0.4181\n0.2684\n\n\n8\n195178.1770\n46865758291.9661\n216485.0071\n0.7363\n0.5635\n0.6440\n\n\n9\n371923.4898\n271120545391.5568\n520692.3712\n0.5406\n0.5683\n0.5885\n\n\nMean\n249410.8239\n120338359137.0794\n329682.5078\n0.7454\n0.4421\n0.3982\n\n\nSD\n86305.1580\n77214568547.1943\n107924.9888\n0.1907\n0.1184\n0.1603\n\n\n\n\n\n\nprint(len(gbrs))\n\n9\n\n\n\ntuned_gbr = tune_model(gbr, n_iter=50, optimize = 'RMSE')\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n154942.6734\n74997708663.9170\n273857.0953\n0.6823\n0.2835\n0.2430\n\n\n1\n356026.5185\n259197991105.0069\n509114.9095\n0.7729\n0.3057\n0.2048\n\n\n2\n196245.5979\n50062838730.3433\n223747.2653\n0.5852\n0.4460\n0.4092\n\n\n3\n130813.1472\n28724978265.9940\n169484.4484\n0.9598\n0.3879\n0.3551\n\n\n4\n150379.1478\n58016543407.9028\n240866.2355\n0.8686\n0.3318\n0.2935\n\n\n5\n360842.5596\n256418976969.6155\n506378.2943\n0.7883\n0.3802\n0.3776\n\n\n6\n101664.5162\n23660563887.4224\n153819.9073\n0.9693\n0.1738\n0.1305\n\n\n7\n121078.1094\n22126447635.5874\n148749.6139\n0.9753\n0.3012\n0.2768\n\n\n8\n204892.4698\n59961129586.8548\n244869.6175\n0.6626\n0.7065\n0.9038\n\n\n9\n247798.4343\n119125737964.7928\n345145.9662\n0.7982\n0.3572\n0.3649\n\n\nMean\n202468.3174\n95229291621.7437\n281603.3353\n0.8062\n0.3674\n0.3559\n\n\nSD\n88121.5888\n85677172457.2040\n126209.5604\n0.1300\n0.1326\n0.2000\n\n\n\n\n\n\ntuned_gbr\n\nGradientBoostingRegressorGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.001, min_impurity_split=None,\n                          min_samples_leaf=3, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=260,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\ndt = create_model('dt')\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n207956.4444\n64209396573.1111\n253395.7312\n0.7280\n0.2948\n0.3014\n\n\n1\n524236.3333\n569991781768.5555\n754978.0009\n0.5006\n0.4063\n0.3254\n\n\n2\n329215.7500\n190393519675.5000\n436341.0589\n-0.5777\n0.5801\n0.6555\n\n\n3\n191451.2500\n124860637323.7500\n353356.2470\n0.8254\n0.4116\n0.3334\n\n\n4\n213423.5000\n76306439743.2500\n276236.2028\n0.8271\n0.5254\n0.3705\n\n\n5\n370690.3750\n199965742624.8750\n447175.2929\n0.8349\n0.5442\n0.4049\n\n\n6\n246669.8750\n129638363043.8750\n360053.2781\n0.8319\n0.2617\n0.2171\n\n\n7\n178672.0000\n77945893211.5000\n279187.9174\n0.9128\n0.3495\n0.3173\n\n\n8\n166587.8750\n46127063745.6250\n214772.1205\n0.7404\n0.4736\n0.4654\n\n\n9\n295026.8750\n185238337215.6250\n430393.2356\n0.6861\n0.3864\n0.3666\n\n\nMean\n272393.0278\n166467717492.5666\n380588.9085\n0.6310\n0.4234\n0.3757\n\n\nSD\n105664.3855\n144523328671.4254\n147036.7308\n0.4171\n0.1010\n0.1121\n\n\n\n\n\n\nbagged_dt = ensemble_model(dt, n_estimators=50)\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n196727.1578\n90285018684.0473\n300474.6556\n0.6175\n0.3087\n0.2890\n\n\n1\n400549.3422\n340151201313.8998\n583224.8291\n0.7020\n0.3095\n0.2039\n\n\n2\n225912.9050\n72423782552.9881\n269116.6709\n0.3999\n0.4933\n0.5057\n\n\n3\n118783.1250\n22526053317.3862\n150086.8193\n0.9685\n0.3121\n0.2808\n\n\n4\n202532.9275\n80967074782.4978\n284547.1398\n0.8166\n0.4177\n0.4139\n\n\n5\n341289.0375\n234909197221.4275\n484674.3208\n0.8060\n0.3879\n0.3623\n\n\n6\n141661.2425\n37608256142.0725\n193928.4820\n0.9512\n0.1464\n0.1355\n\n\n7\n126158.8400\n34803232314.1118\n186556.2444\n0.9611\n0.3018\n0.2442\n\n\n8\n183361.3550\n44385100050.7344\n210677.7161\n0.7502\n0.5438\n0.5929\n\n\n9\n260316.8175\n176470763137.0123\n420084.2334\n0.7010\n0.3999\n0.3821\n\n\nMean\n219729.2750\n113452967951.6178\n308337.1111\n0.7674\n0.3621\n0.3410\n\n\nSD\n87376.1209\n99179989784.9265\n135577.2615\n0.1680\n0.1066\n0.1322\n\n\n\n\n\n\nboosted_dt = ensemble_model(dt, method = 'Boosting')\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n261162.8889\n114911582041.3333\n338986.1089\n0.5132\n0.3872\n0.3925\n\n\n1\n422328.3333\n333782428795.6667\n577739.0664\n0.7076\n0.3211\n0.2542\n\n\n2\n232284.1250\n77562868468.1250\n278501.1104\n0.3573\n0.5087\n0.5015\n\n\n3\n197047.1250\n112221331803.8750\n334994.5250\n0.8431\n0.3502\n0.3395\n\n\n4\n285119.7500\n161644974606.0000\n402050.9602\n0.6338\n0.5596\n0.5738\n\n\n5\n473330.2500\n599114250508.0000\n774024.7092\n0.5053\n0.5119\n0.5217\n\n\n6\n108483.0000\n20435489036.7500\n142952.7511\n0.9735\n0.2098\n0.1500\n\n\n7\n157960.0000\n69455073830.5000\n263543.3054\n0.9223\n0.2735\n0.2394\n\n\n8\n120478.7500\n23354347455.5000\n152821.2925\n0.8686\n0.5655\n0.5893\n\n\n9\n231595.7500\n117602419885.0000\n342932.0922\n0.8007\n0.4401\n0.4379\n\n\nMean\n248978.9972\n163008476643.0750\n360854.5921\n0.7125\n0.4128\n0.4000\n\n\nSD\n113864.7400\n167985632403.4423\n181086.8299\n0.1940\n0.1176\n0.1435\n\n\n\n\n\n\nplot_model(dt)\n\n\n\n\n\nplot_model(dt, plot = 'error')\n\n\n\n\n\nplot_model(dt, plot = 'feature')\n\n\n\n\n\nevaluate_model(dt)\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\nccp_alpha\n0.0\n\n\ncriterion\nmse\n\n\nmax_depth\nNone\n\n\nmax_features\nNone\n\n\nmax_leaf_nodes\nNone\n\n\nmin_impurity_decrease\n0.0\n\n\nmin_impurity_split\nNone\n\n\nmin_samples_leaf\n1\n\n\nmin_samples_split\n2\n\n\nmin_weight_fraction_leaf\n0.0\n\n\npresort\ndeprecated\n\n\nrandom_state\n153\n\n\nsplitter\nbest\n\n\n\n\n\n\n\n\ninterpret_model(dt)\n\n\n\n\n\ninterpret_model(dt, plot = 'correlation')\n\n\n\n\n\ninterpret_model(dt, plot = 'reason', observation = 12)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\nbest = automl(optimize = 'MAE')\nbest\n\nGradientBoostingRegressorGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.05, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.001, min_impurity_split=None,\n                          min_samples_leaf=3, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=260,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=1.0, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\npred_holdouts = predict_model(dt)\npred_holdouts.head()\n\n\n\n\n\n\nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nDecision Tree Regressor\n330931.5556\n395886787646.2778\n629195.3494\n0.3277\n0.4581\n0.4602\n\n\n\n\n\n\n\n\n\n\n\n\ngeneral\ngdp\nfdi\nrnr\nrr\ni\nfr\nit\nprovince_Anhui\nprovince_Beijing\n...\nyear_2006\nyear_2007\nreg_East China\nreg_North China\nreg_Northeast China\nreg_Northwest China\nreg_South Central China\nreg_Southwest China\nspecific\nLabel\n\n\n\n\n0\n123546.0\n2011.189941\n12812.0\n0.0\n0.0\n0.000000\n1514364.0\n2254281.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n436189.0\n472786.0\n\n\n1\n36670.0\n2312.820068\n11169.0\n0.0\n0.0\n0.000000\n1600475.0\n3035767.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n615593.0\n601485.0\n\n\n2\n241282.0\n6867.700195\n53903.0\n0.0\n0.0\n0.516129\n2823413.0\n3586373.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n685732.0\n681676.0\n\n\n3\n581800.0\n25776.910156\n1101159.0\n0.0\n0.0\n0.000000\n16753980.0\n6357869.0\n0.0\n0.0\n...\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2121243.0\n3860764.0\n\n\n4\n36946.0\n445.359985\n1743.0\n0.0\n0.0\n0.000000\n233299.0\n736165.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n133858.0\n107687.0\n\n\n\n\n5 rows × 49 columns\n\n\n\n\nnew_data = df.copy()\nnew_data.drop(['specific'], axis=1, inplace=True)\npredict_new = predict_model(best, data=new_data)\npredict_new.head()\n\n\n\n\n\n\n\n\nprovince\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\nLabel\n\n\n\n\n4\nAnhui\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.000000\n1601508\nEast China\n1499110\n2.000834e+05\n\n\n6\nAnhui\n66529.0\n2002\n3519.72\n38375\n0.0\n0.0\n0.000000\n1677840\nEast China\n2404936\n4.365530e+05\n\n\n7\nAnhui\n52108.0\n2003\n3923.11\n36720\n0.0\n0.0\n0.000000\n1896479\nEast China\n2815820\n6.096731e+05\n\n\n10\nAnhui\n279052.0\n2006\n6112.50\n139354\n0.0\n0.0\n0.324324\n3434548\nEast China\n5167300\n1.455109e+06\n\n\n11\nAnhui\n178705.0\n2007\n7360.92\n299892\n0.0\n0.0\n0.324324\n4468640\nEast China\n7040099\n2.000116e+06\n\n\n\n\n\n\n\n\nsave_model(best, model_name='best-model')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n(Pipeline(memory=None,\n          steps=[('dtypes',\n                  DataTypes_Auto_infer(categorical_features=[],\n                                       display_types=True, features_todrop=[],\n                                       id_columns=[], ml_usecase='regression',\n                                       numerical_features=[], target='specific',\n                                       time_features=[])),\n                 ('imputer',\n                  Simple_Imputer(categorical_strategy='not_available',\n                                 fill_value_categorical=None,\n                                 fill_value_numerical=None,\n                                 numeric_strateg...\n                                            learning_rate=0.05, loss='ls',\n                                            max_depth=8, max_features='sqrt',\n                                            max_leaf_nodes=None,\n                                            min_impurity_decrease=0.001,\n                                            min_impurity_split=None,\n                                            min_samples_leaf=3,\n                                            min_samples_split=10,\n                                            min_weight_fraction_leaf=0.0,\n                                            n_estimators=260,\n                                            n_iter_no_change=None,\n                                            presort='deprecated',\n                                            random_state=153, subsample=1.0,\n                                            tol=0.0001, validation_fraction=0.1,\n                                            verbose=0, warm_start=False)]],\n          verbose=False),\n 'best-model.pkl')\n\n\n\nloaded_bestmodel = load_model('best-model')\nprint(loaded_bestmodel)\n\nTransformation Pipeline and Model Successfully Loaded\nPipeline(memory=None,\n         steps=[('dtypes',\n                 DataTypes_Auto_infer(categorical_features=[],\n                                      display_types=True, features_todrop=[],\n                                      id_columns=[], ml_usecase='regression',\n                                      numerical_features=[], target='specific',\n                                      time_features=[])),\n                ('imputer',\n                 Simple_Imputer(categorical_strategy='not_available',\n                                fill_value_categorical=None,\n                                fill_value_numerical=None,\n                                numeric_strateg...\n                                           learning_rate=0.05, loss='ls',\n                                           max_depth=8, max_features='sqrt',\n                                           max_leaf_nodes=None,\n                                           min_impurity_decrease=0.001,\n                                           min_impurity_split=None,\n                                           min_samples_leaf=3,\n                                           min_samples_split=10,\n                                           min_weight_fraction_leaf=0.0,\n                                           n_estimators=260,\n                                           n_iter_no_change=None,\n                                           presort='deprecated',\n                                           random_state=153, subsample=1.0,\n                                           tol=0.0001, validation_fraction=0.1,\n                                           verbose=0, warm_start=False)]],\n         verbose=False)\n\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\nloaded_bestmodel[0]\n\nDataTypes_Auto_inferDataTypes_Auto_infer(categorical_features=[], display_types=True,\n                     features_todrop=[], id_columns=[], ml_usecase='regression',\n                     numerical_features=[], target='specific',\n                     time_features=[])\n\n\n\nfrom sklearn import set_config\nset_config(display='text')\n\n\nX_train = get_config('X_train')\nX_train.head()\n\n\n\n\n\n\n\n\ngeneral\ngdp\nfdi\nrnr\nrr\ni\nfr\nit\nprovince_Anhui\nprovince_Beijing\n...\nyear_2002\nyear_2003\nyear_2006\nyear_2007\nreg_East China\nreg_North China\nreg_Northeast China\nreg_Northwest China\nreg_South Central China\nreg_Southwest China\n\n\n\n\n343\n66100.0\n2556.020020\n8384.0\n0.0\n0.000000\n0.000000\n1807967.0\n3388449.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n259\n116000.0\n12078.150391\n601617.0\n0.0\n0.000000\n0.000000\n6166904.0\n2940367.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n190\n655919.0\n4056.760010\n242000.0\n0.0\n0.410256\n0.000000\n2525301.0\n3343228.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n319\n50097.0\n185.089996\n467.0\n0.0\n0.000000\n0.324324\n70048.0\n1333133.0\n0.0\n0.0\n...\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n258\n113000.0\n10275.500000\n473404.0\n0.0\n0.000000\n0.000000\n5145006.0\n2455900.0\n0.0\n0.0\n...\n1.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n5 rows × 47 columns\n\n\n\n\nget_config('seed')\n\n153\n\n\n\nfrom pycaret.regression import set_config\nset_config('seed', 999)\n\n\nget_config('seed')\n\n999\n\n\n\n!mlflow ui \n\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Starting gunicorn 20.0.4\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Listening at: http://127.0.0.1:5000 (56453)\n[2021-05-31 20:13:02 -0500] [56453] [INFO] Using worker: sync\n[2021-05-31 20:13:02 -0500] [56455] [INFO] Booting worker with pid: 56455\n^C\n[2021-05-31 20:13:35 -0500] [56453] [INFO] Handling signal: int\n[2021-05-31 20:13:35 -0500] [56455] [INFO] Worker exiting (pid: 56455)"
  },
  {
    "objectID": "posts/2020-11-08-nlp_tieng_viet.html",
    "href": "posts/2020-11-08-nlp_tieng_viet.html",
    "title": "NLP Example bằng tiếng Việt using StackNetClassifier",
    "section": "",
    "text": "https://www.aivivn.com/\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.sparse import hstack, csr_matrix, vstack\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.ensemble import *\nfrom sklearn.linear_model import *\n\nfrom tqdm import *\n\nimport wordcloud\nimport matplotlib.pyplot as plt\nimport gc\n\nimport lightgbm as lgb\n%matplotlib inline\n\n\n# Load data\ntrain_df = pd.read_csv(\"./data/train.csv\")\ntest_df = pd.read_csv(\"./data/test.csv\")\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\ncomment\nlabel\n\n\n\n\n0\ntrain_000000\nDung dc sp tot cam on \\nshop Đóng gói sản phẩm...\n0\n\n\n1\ntrain_000001\nChất lượng sản phẩm tuyệt vời . Son mịn nhưng...\n0\n\n\n2\ntrain_000002\nChất lượng sản phẩm tuyệt vời nhưng k có hộp ...\n0\n\n\n3\ntrain_000003\n:(( Mình hơi thất vọng 1 chút vì mình đã kỳ vọ...\n1\n\n\n4\ntrain_000004\nLần trước mình mua áo gió màu hồng rất ok mà đ...\n1\n\n\n\n\n\n\n\n\ntest_df.head()\n\n\n\n\n\n\n\n\nid\ncomment\n\n\n\n\n0\ntest_000000\nChưa dùng thử nên chưa biết\n\n\n1\ntest_000001\nKhông đáng tiềnVì ngay đợt sale nên mới mua n...\n\n\n2\ntest_000002\nCám ơn shop. Đóng gói sản phẩm rất đẹp và chắc...\n\n\n3\ntest_000003\nVải đẹp.phom oki luôn.quá ưng\n\n\n4\ntest_000004\nChuẩn hàng đóng gói đẹp\n\n\n\n\n\n\n\n\ndf = pd.concat([train_df, test_df], axis=0)\n# del train_df, test_df\n# gc.collect()\n\n\nimport emoji\n\ndef extract_emojis(str):\n    return [c for c in str if c in emoji.UNICODE_EMOJI]\n\n\ngood_df = train_df[train_df['label'] == 0]\ngood_comment = good_df['comment'].values\ngood_emoji = []\nfor c in good_comment:\n    good_emoji += extract_emojis(c)\n\ngood_emoji = np.unique(np.asarray(good_emoji))\n\n\nbad_df = train_df[train_df['label'] == 1]\nbad_comment = bad_df['comment'].values\n\nbad_emoji = []\nfor c in bad_comment:\n    bad_emoji += extract_emojis(c)\n\nbad_emoji = np.unique(np.asarray(bad_emoji))\n\n\ngood_emoji\n\narray(['↖', '↗', '☀', '☺', '♀', '♥', '✌', '✨', '❌', '❣', '❤', '⭐', '🆗',\n       '🌝', '🌟', '🌧', '🌷', '🌸', '🌺', '🌼', '🍓', '🎈', '🎉', '🏻', '🏼', '🏿',\n       '🐅', '🐾', '👉', '👌', '👍', '👏', '💋', '💌', '💐', '💓', '💕', '💖', '💗',\n       '💙', '💚', '💛', '💜', '💞', '💟', '💥', '💪', '💮', '💯', '💰', '📑', '🖤',\n       '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😉', '😊', '😋', '😌', '😍',\n       '😎', '😑', '😓', '😔', '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞',\n       '😟', '😡', '😢', '😣', '😥', '😩', '😪', '😫', '😬', '😭', '😯', '😰', '😱',\n       '😲', '😳', '😻', '😿', '🙁', '🙂', '🙃', '🙄', '🙆', '🙌', '🤑', '🤔', '🤗',\n       '🤙', '🤝', '🤣', '🤤', '🤨', '🤪', '🤭'], dtype='&lt;U1')\n\n\n\n# Just remove \"sad, bad\" emoji :D\ngood_emoji_fix = [\n    '↖', '↗', '☀', '☺', '♀', '♥', '✌', '✨', '❣', '❤', '⭐', '🆗',\n       '🌝', '🌟', '🌧', '🌷', '🌸', '🌺', '🌼', '🍓', '🎈', '🎉', '🐅', '🐾', '👉',\n       '👌', '👍', '👏', '💋', '💌', '💐', '💓', '💕', '💖', '💗', '💙', '💚', '💛',\n       '💜', '💞', '💟', '💥', '💪', '💮', '💯', '💰', '📑', '🖤', '😀', '😁', '😂',\n       '😃', '😄', '😅', '😆', '😇', '😉', '😊', '😋', '😌', '😍', '😎', '😑', '😓', '😔', \n    '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😟', '😡', '😯', '😰', '😱', '😲', '😳', '😻', '🙂', '🙃', '🙄', '🙆', '🙌', '🤑', '🤔', '🤗',\n]\n\n\nbad_emoji\n\narray(['☹', '✋', '❌', '❓', '❤', '⭐', '🎃', '🏻', '🏼', '🏿', '👌', '👍', '👎',\n       '👶', '💀', '💋', '😁', '😂', '😈', '😊', '😌', '😏', '😐', '😑', '😒', '😓',\n       '😔', '😖', '😚', '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😧', '😩',\n       '😪', '😫', '😬', '😭', '😳', '😵', '😶', '🙁', '🙂', '🙄', '🤔', '🤚', '🤤'],\n      dtype='&lt;U1')\n\n\n\n# Just remove \"good\" emoji :D\nbad_emoji_fix = [\n    '☹', '✋', '❌', '❓', '👎', '👶', '💀',\n       '😐', '😑', '😒', '😓', '😔',\n       '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😧', '😩', '😪', '😫', '😬',\n       '😭', '😳', '😵', '😶', '🙁', '🙄', '🤔',\n]\n\n\ndef count_good_bad_emoji(row):\n    comment = row['comment']\n    n_good_emoji = 0\n    n_bad_emoji = 0\n    for c in comment:\n        if c in good_emoji_fix:\n            n_good_emoji += 1\n        if c in bad_emoji_fix:\n            n_bad_emoji += 1\n    \n    row['n_good_emoji'] = n_good_emoji\n    row['n_bad_emoji'] = n_bad_emoji\n    \n    return row\n\n\n# Some features\ndf['comment'] = df['comment'].astype(str).fillna(' ')\ndf['comment'] = df['comment'].str.lower()\ndf['num_words'] = df['comment'].apply(lambda s: len(s.split()))\ndf['num_unique_words'] = df['comment'].apply(lambda s: len(set(w for w in s.split())))\ndf['words_vs_unique'] = df['num_unique_words'] / df['num_words'] * 100\ndf = df.apply(count_good_bad_emoji, axis=1)\n\n\ndf['good_bad_emoji_ratio'] = df['n_good_emoji'] / df['n_bad_emoji']\ndf['good_bad_emoji_ratio'] = df['good_bad_emoji_ratio'].replace(np.nan, 0)\ndf['good_bad_emoji_ratio'] = df['good_bad_emoji_ratio'].replace(np.inf, 99)\ndf['good_bad_emoji_diff'] = df['n_good_emoji'] - df['n_bad_emoji']\ndf['good_bad_emoji_sum'] = df['n_good_emoji'] + df['n_bad_emoji']\n\n\ntrain_df = df[~df['label'].isnull()]\ntest_df = df[df['label'].isnull()]\n\ntrain_comments = train_df['comment'].fillna(\"none\").values\ntest_comments = test_df['comment'].fillna(\"none\").values\n\ny_train = train_df['label'].values\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\ncomment\nlabel\nnum_words\nnum_unique_words\nwords_vs_unique\nn_good_emoji\nn_bad_emoji\ngood_bad_emoji_ratio\ngood_bad_emoji_diff\ngood_bad_emoji_sum\n\n\n\n\n0\ntrain_000000\ndung dc sp tot cam on \\nshop đóng gói sản phẩm...\n0.0\n22\n20\n90.909091\n0\n0\n0.0\n0\n0\n\n\n1\ntrain_000001\nchất lượng sản phẩm tuyệt vời . son mịn nhưng...\n0.0\n18\n18\n100.000000\n0\n0\n0.0\n0\n0\n\n\n2\ntrain_000002\nchất lượng sản phẩm tuyệt vời nhưng k có hộp ...\n0.0\n18\n14\n77.777778\n0\n0\n0.0\n0\n0\n\n\n3\ntrain_000003\n:(( mình hơi thất vọng 1 chút vì mình đã kỳ vọ...\n1.0\n114\n91\n79.824561\n0\n0\n0.0\n0\n0\n\n\n4\ntrain_000004\nlần trước mình mua áo gió màu hồng rất ok mà đ...\n1.0\n26\n24\n92.307692\n0\n0\n0.0\n0\n0\n\n\n\n\n\n\n\nTạo feature TFIDF đơn giản\n\ntfidf = TfidfVectorizer(\n    min_df = 5, \n    max_df = 0.8, \n    max_features=10000,\n    sublinear_tf=True\n)\n\n\nX_train_tfidf = tfidf.fit_transform(train_comments)\nX_test_tfidf = tfidf.transform(test_comments)\n\n\nEXCLUED_COLS = ['id', 'comment', 'label']\nstatic_cols = [c for c in train_df.columns if not c in EXCLUED_COLS]\nX_train_static = train_df[static_cols].values\nX_test_static = test_df[static_cols].values\n\n\nX_train = hstack([X_train_tfidf, csr_matrix(X_train_static)]).tocsr()\nX_test = hstack([X_test_tfidf, csr_matrix(X_test_static)]).tocsr()\n# X_train = X_train_tfidf\n# X_test = X_test_tfidf\n\n\nX_train.shape, X_test.shape, y_train.shape\n\n((16087, 2687), (10981, 2687), (16087,))"
  },
  {
    "objectID": "posts/2020-11-08-nlp_tieng_viet.html#credit-code-and-notebooks-from-httpsgithub.comngxbacaivivn_phanloaisacthaibinhluan",
    "href": "posts/2020-11-08-nlp_tieng_viet.html#credit-code-and-notebooks-from-httpsgithub.comngxbacaivivn_phanloaisacthaibinhluan",
    "title": "NLP Example bằng tiếng Việt using StackNetClassifier",
    "section": "",
    "text": "https://www.aivivn.com/\n\nimport pandas as pd\nimport numpy as np\n\nfrom scipy.sparse import hstack, csr_matrix, vstack\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.ensemble import *\nfrom sklearn.linear_model import *\n\nfrom tqdm import *\n\nimport wordcloud\nimport matplotlib.pyplot as plt\nimport gc\n\nimport lightgbm as lgb\n%matplotlib inline\n\n\n# Load data\ntrain_df = pd.read_csv(\"./data/train.csv\")\ntest_df = pd.read_csv(\"./data/test.csv\")\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\ncomment\nlabel\n\n\n\n\n0\ntrain_000000\nDung dc sp tot cam on \\nshop Đóng gói sản phẩm...\n0\n\n\n1\ntrain_000001\nChất lượng sản phẩm tuyệt vời . Son mịn nhưng...\n0\n\n\n2\ntrain_000002\nChất lượng sản phẩm tuyệt vời nhưng k có hộp ...\n0\n\n\n3\ntrain_000003\n:(( Mình hơi thất vọng 1 chút vì mình đã kỳ vọ...\n1\n\n\n4\ntrain_000004\nLần trước mình mua áo gió màu hồng rất ok mà đ...\n1\n\n\n\n\n\n\n\n\ntest_df.head()\n\n\n\n\n\n\n\n\nid\ncomment\n\n\n\n\n0\ntest_000000\nChưa dùng thử nên chưa biết\n\n\n1\ntest_000001\nKhông đáng tiềnVì ngay đợt sale nên mới mua n...\n\n\n2\ntest_000002\nCám ơn shop. Đóng gói sản phẩm rất đẹp và chắc...\n\n\n3\ntest_000003\nVải đẹp.phom oki luôn.quá ưng\n\n\n4\ntest_000004\nChuẩn hàng đóng gói đẹp\n\n\n\n\n\n\n\n\ndf = pd.concat([train_df, test_df], axis=0)\n# del train_df, test_df\n# gc.collect()\n\n\nimport emoji\n\ndef extract_emojis(str):\n    return [c for c in str if c in emoji.UNICODE_EMOJI]\n\n\ngood_df = train_df[train_df['label'] == 0]\ngood_comment = good_df['comment'].values\ngood_emoji = []\nfor c in good_comment:\n    good_emoji += extract_emojis(c)\n\ngood_emoji = np.unique(np.asarray(good_emoji))\n\n\nbad_df = train_df[train_df['label'] == 1]\nbad_comment = bad_df['comment'].values\n\nbad_emoji = []\nfor c in bad_comment:\n    bad_emoji += extract_emojis(c)\n\nbad_emoji = np.unique(np.asarray(bad_emoji))\n\n\ngood_emoji\n\narray(['↖', '↗', '☀', '☺', '♀', '♥', '✌', '✨', '❌', '❣', '❤', '⭐', '🆗',\n       '🌝', '🌟', '🌧', '🌷', '🌸', '🌺', '🌼', '🍓', '🎈', '🎉', '🏻', '🏼', '🏿',\n       '🐅', '🐾', '👉', '👌', '👍', '👏', '💋', '💌', '💐', '💓', '💕', '💖', '💗',\n       '💙', '💚', '💛', '💜', '💞', '💟', '💥', '💪', '💮', '💯', '💰', '📑', '🖤',\n       '😀', '😁', '😂', '😃', '😄', '😅', '😆', '😇', '😉', '😊', '😋', '😌', '😍',\n       '😎', '😑', '😓', '😔', '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞',\n       '😟', '😡', '😢', '😣', '😥', '😩', '😪', '😫', '😬', '😭', '😯', '😰', '😱',\n       '😲', '😳', '😻', '😿', '🙁', '🙂', '🙃', '🙄', '🙆', '🙌', '🤑', '🤔', '🤗',\n       '🤙', '🤝', '🤣', '🤤', '🤨', '🤪', '🤭'], dtype='&lt;U1')\n\n\n\n# Just remove \"sad, bad\" emoji :D\ngood_emoji_fix = [\n    '↖', '↗', '☀', '☺', '♀', '♥', '✌', '✨', '❣', '❤', '⭐', '🆗',\n       '🌝', '🌟', '🌧', '🌷', '🌸', '🌺', '🌼', '🍓', '🎈', '🎉', '🐅', '🐾', '👉',\n       '👌', '👍', '👏', '💋', '💌', '💐', '💓', '💕', '💖', '💗', '💙', '💚', '💛',\n       '💜', '💞', '💟', '💥', '💪', '💮', '💯', '💰', '📑', '🖤', '😀', '😁', '😂',\n       '😃', '😄', '😅', '😆', '😇', '😉', '😊', '😋', '😌', '😍', '😎', '😑', '😓', '😔', \n    '😖', '😗', '😘', '😙', '😚', '😛', '😜', '😝', '😞', '😟', '😡', '😯', '😰', '😱', '😲', '😳', '😻', '🙂', '🙃', '🙄', '🙆', '🙌', '🤑', '🤔', '🤗',\n]\n\n\nbad_emoji\n\narray(['☹', '✋', '❌', '❓', '❤', '⭐', '🎃', '🏻', '🏼', '🏿', '👌', '👍', '👎',\n       '👶', '💀', '💋', '😁', '😂', '😈', '😊', '😌', '😏', '😐', '😑', '😒', '😓',\n       '😔', '😖', '😚', '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😧', '😩',\n       '😪', '😫', '😬', '😭', '😳', '😵', '😶', '🙁', '🙂', '🙄', '🤔', '🤚', '🤤'],\n      dtype='&lt;U1')\n\n\n\n# Just remove \"good\" emoji :D\nbad_emoji_fix = [\n    '☹', '✋', '❌', '❓', '👎', '👶', '💀',\n       '😐', '😑', '😒', '😓', '😔',\n       '😞', '😟', '😠', '😡', '😢', '😣', '😤', '😥', '😧', '😩', '😪', '😫', '😬',\n       '😭', '😳', '😵', '😶', '🙁', '🙄', '🤔',\n]\n\n\ndef count_good_bad_emoji(row):\n    comment = row['comment']\n    n_good_emoji = 0\n    n_bad_emoji = 0\n    for c in comment:\n        if c in good_emoji_fix:\n            n_good_emoji += 1\n        if c in bad_emoji_fix:\n            n_bad_emoji += 1\n    \n    row['n_good_emoji'] = n_good_emoji\n    row['n_bad_emoji'] = n_bad_emoji\n    \n    return row\n\n\n# Some features\ndf['comment'] = df['comment'].astype(str).fillna(' ')\ndf['comment'] = df['comment'].str.lower()\ndf['num_words'] = df['comment'].apply(lambda s: len(s.split()))\ndf['num_unique_words'] = df['comment'].apply(lambda s: len(set(w for w in s.split())))\ndf['words_vs_unique'] = df['num_unique_words'] / df['num_words'] * 100\ndf = df.apply(count_good_bad_emoji, axis=1)\n\n\ndf['good_bad_emoji_ratio'] = df['n_good_emoji'] / df['n_bad_emoji']\ndf['good_bad_emoji_ratio'] = df['good_bad_emoji_ratio'].replace(np.nan, 0)\ndf['good_bad_emoji_ratio'] = df['good_bad_emoji_ratio'].replace(np.inf, 99)\ndf['good_bad_emoji_diff'] = df['n_good_emoji'] - df['n_bad_emoji']\ndf['good_bad_emoji_sum'] = df['n_good_emoji'] + df['n_bad_emoji']\n\n\ntrain_df = df[~df['label'].isnull()]\ntest_df = df[df['label'].isnull()]\n\ntrain_comments = train_df['comment'].fillna(\"none\").values\ntest_comments = test_df['comment'].fillna(\"none\").values\n\ny_train = train_df['label'].values\n\n\ntrain_df.head()\n\n\n\n\n\n\n\n\nid\ncomment\nlabel\nnum_words\nnum_unique_words\nwords_vs_unique\nn_good_emoji\nn_bad_emoji\ngood_bad_emoji_ratio\ngood_bad_emoji_diff\ngood_bad_emoji_sum\n\n\n\n\n0\ntrain_000000\ndung dc sp tot cam on \\nshop đóng gói sản phẩm...\n0.0\n22\n20\n90.909091\n0\n0\n0.0\n0\n0\n\n\n1\ntrain_000001\nchất lượng sản phẩm tuyệt vời . son mịn nhưng...\n0.0\n18\n18\n100.000000\n0\n0\n0.0\n0\n0\n\n\n2\ntrain_000002\nchất lượng sản phẩm tuyệt vời nhưng k có hộp ...\n0.0\n18\n14\n77.777778\n0\n0\n0.0\n0\n0\n\n\n3\ntrain_000003\n:(( mình hơi thất vọng 1 chút vì mình đã kỳ vọ...\n1.0\n114\n91\n79.824561\n0\n0\n0.0\n0\n0\n\n\n4\ntrain_000004\nlần trước mình mua áo gió màu hồng rất ok mà đ...\n1.0\n26\n24\n92.307692\n0\n0\n0.0\n0\n0\n\n\n\n\n\n\n\nTạo feature TFIDF đơn giản\n\ntfidf = TfidfVectorizer(\n    min_df = 5, \n    max_df = 0.8, \n    max_features=10000,\n    sublinear_tf=True\n)\n\n\nX_train_tfidf = tfidf.fit_transform(train_comments)\nX_test_tfidf = tfidf.transform(test_comments)\n\n\nEXCLUED_COLS = ['id', 'comment', 'label']\nstatic_cols = [c for c in train_df.columns if not c in EXCLUED_COLS]\nX_train_static = train_df[static_cols].values\nX_test_static = test_df[static_cols].values\n\n\nX_train = hstack([X_train_tfidf, csr_matrix(X_train_static)]).tocsr()\nX_test = hstack([X_test_tfidf, csr_matrix(X_test_static)]).tocsr()\n# X_train = X_train_tfidf\n# X_test = X_test_tfidf\n\n\nX_train.shape, X_test.shape, y_train.shape\n\n((16087, 2687), (10981, 2687), (16087,))"
  },
  {
    "objectID": "posts/2020-06-07-kwargs-decorators.html",
    "href": "posts/2020-06-07-kwargs-decorators.html",
    "title": "A timer for ML functions",
    "section": "",
    "text": "“A timer for ML functions”\n\n\ntoc: true- branch: master- badges: true\ncomments: true\nauthor: David Kearney\ncategories: [timer, jupyter]\ndescription: A timer for ML functions\ntitle: A timer for ML functions\n\n\n#collapse-hide\n\nfrom functools import wraps\nimport time\n\n\ndef timer(func):\n    \"\"\"[This decorator is a timer for functions]\n\n    Args:\n        func ([function]): [This decorator takes a function as argument]\n\n    Returns:\n        [string]: [states the duration of time between the function begining and ending]\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"{func.__name__!r} begins\")\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__!r} ends in {time.time()-start_time}  secs\")\n        return result\n    return wrapper\n\n\n@timer\ndef model_metrics(*args, **kwargs):\n    \"\"\"[This is a function to print model metrics of interest]\n    \"\"\"\n    print(\"Model ID Number:\", args)\n    print(\"Metric of Interest:\", kwargs)\n\n\nmodel_metrics(1, 2, 10, key=\"word\", key2=\"word2\", numtrees=\"200\")\n\n\nfrom collections import Counter\nimport math, random\n\n#\n# data splitting\n#\n\ndef split_data(data, prob):\n    \"\"\"split data into fractions [prob, 1 - prob]\"\"\"\n    results = [], []\n    for row in data:\n        results[0 if random.random() &lt; prob else 1].append(row)\n    return results\n\ndef train_test_split(x, y, test_pct):\n    data = list(zip(x, y))                        # pair corresponding values\n    train, test = split_data(data, 1 - test_pct)  # split the dataset of pairs\n    x_train, y_train = list(zip(*train))          # magical un-zip trick\n    x_test, y_test = list(zip(*test))\n    return x_train, x_test, y_train, y_test\n\n#\n# correctness\n#\n\ndef accuracy(tp, fp, fn, tn):\n    correct = tp + tn\n    total = tp + fp + fn + tn\n    return correct / total\n\ndef precision(tp, fp, fn, tn):\n    return tp / (tp + fp)\n\ndef recall(tp, fp, fn, tn):\n    return tp / (tp + fn)\n\ndef f1_score(tp, fp, fn, tn):\n    p = precision(tp, fp, fn, tn)\n    r = recall(tp, fp, fn, tn)\n\n    return 2 * p * r / (p + r)\n\nif __name__ == \"__main__\":\n\n    print(\"accuracy(70, 4930, 13930, 981070)\", accuracy(70, 4930, 13930, 981070))\n    print(\"precision(70, 4930, 13930, 981070)\", precision(70, 4930, 13930, 981070))\n    print(\"recall(70, 4930, 13930, 981070)\", recall(70, 4930, 13930, 981070))\n    print(\"f1_score(70, 4930, 13930, 981070)\", f1_score(70, 4930, 13930, 981070))\n\n\nfavorite_number = 7\n\n\ndef add(a, b):\n    return a + b\n\n\ndef sub(a, b):\n    return a - b\n\n\ndef multiply(a, b):\n    return a * b\n\n\ndef divide(a, b):\n    return a / b\n\n\ndef count_vowels(word):\n    count = 0\n    for letter in word.lower():\n        count += letter in 'aeiou'\n\n    return count\n\n\n# import example_module as sm\n\n# print(sm.favorite_number)\n\n# # add two numbers together\n# print(sm.add(3, 8))\n\n# # count the number of vowels in a string\n# print(sm.count_vowels('Testing'))\n\n\nimport pandas as pd\nfrom alive_progress import alive_bar, showtime, show_bars, show_spinners, config_handler\nconfig_handler.set_global(theme='ascii', spinner='notes', bar='solid')\n\nwith alive_bar(3) as bar:\n    df = pd.read_csv('https://gist.githubusercontent.com/davidrkearney/bb461ba351da484336a19bd00a2612e2/raw/18dd90b57fec46a247248d161ffd8085de2a00db/china_province_economicdata_1996_2007.csv')\n    bar('file read, printing file')\n    print(df.head)\n    bar('data printed ok, printing methods of data')\n    print(dir(df))\n    bar('process complete')\n\n\nfrom functools import wraps\nimport time\n\n\ndef timer(func):\n    \"\"\"[This decorator is a timer for functions]\n\n    Args:\n        func ([function]): [This decorator takes a function as argument]\n\n    Returns:\n        [string]: [states the duration of time between the function begining and ending]\n    \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        print(f\"{func.__name__!r} begins\")\n        start_time = time.time()\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__!r} ends in {time.time()-start_time}  secs\")\n        return result\n    return wrapper\n\n\n\n@timer\ndef model_metrics(*args, **kwargs):\n    \"\"\"[This is a function to print model metrics of interest]\n    \"\"\"\n    print(\"Model ID Number:\", args)\n    print(\"Metric of Interest:\", kwargs)\n\n\nmodel_metrics(1, 2, 10, key=\"word\", key2=\"word2\", numtrees=\"200\")\n\nThis post includes code adapted from Data Science from Scratch"
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/2020-09-30-StockMarketPortfolioAnaylsis_snp.html",
    "href": "posts/2020-09-30-StockMarketPortfolioAnaylsis_snp.html",
    "title": "Stock Market and Portfolio Anaylsis of the S&P 500 with pandas and quandl",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport pandas as pd\nimport quandl\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\n\n\nSP500.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2010-01-01\n1123.58\n\n\n2010-02-01\n1089.16\n\n\n2010-03-01\n1152.05\n\n\n2010-04-01\n1197.32\n\n\n2010-05-01\n1125.06\n\n\n\n\n\n\n\n\nSP500.tail()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2020-07-01\n3207.62\n\n\n2020-07-31\n3271.12\n\n\n2020-08-01\n3391.71\n\n\n2020-08-31\n3500.31\n\n\n2020-09-01\n3526.65\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nSP500['Value'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 from 2010 to 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 from 2010 to 2020 Value')"
  },
  {
    "objectID": "posts/2021-01-01-SNP-trend-minus-cycle.html",
    "href": "posts/2021-01-01-SNP-trend-minus-cycle.html",
    "title": "Stock Market Analysis of the S&P 500 Index",
    "section": "",
    "text": "This post includes code and notes adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader\nimport datetime\nimport pandas_datareader.data as web\nimport statsmodels.api as sm\nimport quandl\nstart = datetime.datetime(2019, 1, 1)\nend = pd.to_datetime('today')\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2019-01-01\n2607.39\n\n\n2019-02-01\n2754.86\n\n\n2019-03-01\n2803.98\n\n\n2019-04-01\n2903.80\n\n\n2019-05-01\n2854.71\n\n\n2019-05-31\n2752.08\n\n\n2019-06-01\n2890.17\n\n\n2019-07-01\n2996.11\n\n\n2019-08-01\n2897.50\n\n\n2019-09-01\n2982.16\n\n\n2019-10-01\n2977.68\n\n\n2019-11-01\n3104.90\n\n\n2019-12-01\n3176.75\n\n\n2019-12-31\n3230.58\n\n\n2020-01-01\n3278.20\n\n\n2020-01-31\n3225.04\n\n\n2020-02-01\n3277.31\n\n\n2020-02-28\n2954.81\n\n\n2020-03-01\n2652.39\n\n\n2020-03-31\n2584.59\n\n\n2020-04-01\n2761.98\n\n\n2020-04-30\n2912.43\n\n\n2020-05-01\n2919.61\n\n\n2020-06-01\n3104.66\n\n\n2020-06-30\n3100.29\n\n\n2020-07-01\n3207.62\n\n\n2020-07-31\n3271.12\n\n\n2020-08-01\n3391.71\n\n\n2020-08-31\n3500.31\n\n\n2020-09-01\n3365.52\n\n\n2020-09-30\n3363.00\n\n\n2020-10-01\n3418.70\n\n\n2020-11-01\n3429.33\n\n\n2020-11-30\n3621.63\n\n\n2020-12-01\n3662.45\nSP500.plot()\n\n&lt;AxesSubplot:xlabel='Date'&gt;\nSP500.index\n\nDatetimeIndex(['2019-01-01', '2019-02-01', '2019-03-01', '2019-04-01',\n               '2019-05-01', '2019-05-31', '2019-06-01', '2019-07-01',\n               '2019-08-01', '2019-09-01', '2019-10-01', '2019-11-01',\n               '2019-12-01', '2019-12-31', '2020-01-01', '2020-01-31',\n               '2020-02-01', '2020-02-28', '2020-03-01', '2020-03-31',\n               '2020-04-01', '2020-04-30', '2020-05-01', '2020-06-01',\n               '2020-06-30', '2020-07-01', '2020-07-31', '2020-08-01',\n               '2020-08-31', '2020-09-01', '2020-09-30', '2020-10-01',\n               '2020-11-01', '2020-11-30', '2020-12-01'],\n              dtype='datetime64[ns]', name='Date', freq=None)\nSP500.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2019-01-01\n2607.39\n\n\n2019-02-01\n2754.86\n\n\n2019-03-01\n2803.98\n\n\n2019-04-01\n2903.80\n\n\n2019-05-01\n2854.71\nSP500['Value'].plot()\nplt.ylabel(\"SP500 Value\")\n\nText(0, 0.5, 'SP500 Value')"
  },
  {
    "objectID": "posts/2021-01-01-SNP-trend-minus-cycle.html#getting-at-the-trend-by-removing-the-cyclical-elements-of-the-sp-500",
    "href": "posts/2021-01-01-SNP-trend-minus-cycle.html#getting-at-the-trend-by-removing-the-cyclical-elements-of-the-sp-500",
    "title": "Stock Market Analysis of the S&P 500 Index",
    "section": "Getting at the trend by removing the cyclical elements of the S&P 500",
    "text": "Getting at the trend by removing the cyclical elements of the S&P 500\n\n# Tuple unpacking\nSP500_cycle, SP500_trend = sm.tsa.filters.hpfilter(SP500.Value)\n\n\nSP500_cycle\n\nDate\n2019-01-01   -159.626002\n2019-02-01    -32.234117\n2019-03-01     -3.092466\n2019-04-01     76.968864\n2019-05-01      8.461718\n2019-05-31   -113.170164\n2019-06-01      6.401669\n2019-07-01     94.446400\n2019-08-01    -21.300789\n2019-09-01     47.056253\n2019-10-01     27.196993\n2019-11-01    140.021486\n2019-12-01    198.502790\n2019-12-31    239.946447\n2020-01-01    275.993938\n2020-01-31    211.756776\n2020-02-01    252.953978\n2020-02-28    -81.237788\n2020-03-01   -396.749949\n2020-03-31   -479.773159\n2020-04-01   -320.220102\n2020-04-30   -190.403605\n2020-05-01   -206.636358\n2020-06-01    -47.642048\n2020-06-30    -80.445214\n2020-07-01     -3.630618\n2020-07-31     27.617253\n2020-08-01    114.566186\n2020-08-31    188.466703\n2020-09-01     18.177725\n2020-09-30    -20.499622\n2020-10-01     -1.485569\n2020-11-01    -27.927537\n2020-11-30    127.057981\n2020-12-01    130.481948\nName: Value_cycle, dtype: float64\n\n\n\ntype(SP500_cycle)\n\npandas.core.series.Series\n\n\n\nSP500[\"trend\"] = SP500_trend\n\n\nSP500[['trend','Value']].plot()\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\nSP500[['trend','Value']][\"2000-03-31\":].plot(figsize=(12,8))\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "posts/2020-10-11-dask-xgboost-fiscal-data.html",
    "href": "posts/2020-10-11-dask-xgboost-fiscal-data.html",
    "title": "Moving Fiscal Data from a sqlite db to a dask dataframe",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\n\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\n# engine = db.create_engine('sqlite:///fiscal_data.db')\n# connection = engine.connect()\n# metadata = db.MetaData()\n\n\n# engine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n\n# sql = \"\"\"\n# SELECT year\n# , region\n# , province\n# , gdp\n# , fdi\n# , it\n# , specific\n# FROM fiscal_data\n# \"\"\"\n\n# cnxn = connection\n\n\n# df = pd.read_sql(sql, cnxn)\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf = df[['year', 'province', 'gdp', 'fdi', 'it', 'specific']]\ndf\n\n/tmp/ipykernel_191516/2939603070.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n\n\n  df = pd.read_csv(url, error_bad_lines=False)\n\n\n\n\n\n\n\n\n\nyear\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n\n\n357\n2005\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 6 columns\n\n\n\n\ndf.columns\n\nIndex(['year', 'province', 'gdp', 'fdi', 'it', 'specific'], dtype='object')\n\n\n\ndf.gdp.hist()\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n/home/gao/anaconda3/lib/python3.9/site-packages/distributed/node.py:180: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 38997 instead\n  warnings.warn(\n\n\n\n     \n    \n        Client\n        Client-39280590-426e-11ed-ac1c-e12ff7d3d0e4\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://192.168.0.198:38997/status\n\n\n\n\n\nCluster Info\n\n\n\n\n\n\nLocalCluster\ndb5f5d30\n\n\n\nDashboard: http://192.168.0.198:38997/status\nWorkers: 3\n\n\nTotal threads: 6\nTotal memory: 11.18 GiB\n\n\nStatus: running\nUsing processes: False\n\n\n\n\n\nScheduler Info\n\n\n\n\n\n\nScheduler\nScheduler-72c24d14-1897-4884-b09e-482fcc8891ad\n\n\n\nComm: inproc://192.168.0.198/191516/1\nWorkers: 3\n\n\nDashboard: http://192.168.0.198:38997/status\nTotal threads: 6\n\n\nStarted: Just now\nTotal memory: 11.18 GiB\n\n\n\n\n\n\nWorkers\n\n\n\n\n\nWorker: 0\n\n\n\nComm: inproc://192.168.0.198/191516/6\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:41221/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-4gdzm5d1\n\n\n\n\n\n\n\n\n\n\nWorker: 1\n\n\n\nComm: inproc://192.168.0.198/191516/4\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:32863/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-h48e5dop\n\n\n\n\n\n\n\n\n\n\nWorker: 2\n\n\n\nComm: inproc://192.168.0.198/191516/5\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:46041/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-53xs8_i_"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html",
    "href": "posts/2021-05-18-groupby_pivot.html",
    "title": "Groupby and Pivot Tables in Python",
    "section": "",
    "text": "import pandas as pd\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\n#df\n\n\ndf.pivot_table(index='province', columns='year', values='it', aggfunc='mean').round(-1).style.highlight_max(color='blue').highlight_max(axis=1, color='green')\ndf_subset = df[[\"year\", \"reg\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]]\ndf_subset.columns = [\"year\", \"region\", \"province\", \"gdp\", \"fdi\", 'it',\"specific\"]\ndf=df_subset\ndf\n# Add distributions by region\nimport matplotlib.pyplot as plt\n#fig, axes = plt.subplots(nrows=3, ncols=3)\n\ntest_cells = ['East China', 'North China']\nmetrics = ['gdp', 'fdi', 'it']\n\nfor test_cell in test_cells:\n    for metric in metrics:\n        df.loc[df[\"region\"] == test_cell].hist(column=[metric], bins=60)\n        print(test_cell)\n        print(metric)\ndf.hist(column=['fdi'], bins=60)"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html#distributions-of-dependant-variables",
    "href": "posts/2021-05-18-groupby_pivot.html#distributions-of-dependant-variables",
    "title": "Groupby and Pivot Tables in Python",
    "section": "Distributions of Dependant Variables",
    "text": "Distributions of Dependant Variables\n\nRight skew\n\ndf.hist(column=['fdi'], bins=60)\n\n\nsns.histplot(df['fdi'])\n\n\nsns.displot(df['gdp'])\n\n\nsns.displot(df['fdi'])\n\n\nsns.displot(df['it'])\n\n\nsns.displot(df['specific'].dropna())\n\n\ndf.hist(column=['fdi'], bins=60)"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "href": "posts/2021-05-18-groupby_pivot.html#removal-of-gdp-value-outliers-more-than-3-standard-deviations-away-from-the-mean",
    "title": "Groupby and Pivot Tables in Python",
    "section": "Removal of GDP value outliers more than 3 standard deviations away from the mean",
    "text": "Removal of GDP value outliers more than 3 standard deviations away from the mean"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "href": "posts/2021-05-18-groupby_pivot.html#outlier-removal-of-rows-with-gdp-values-that-are-3-standard-deviations-away-form-the-mean",
    "title": "Groupby and Pivot Tables in Python",
    "section": "outlier removal of rows with GDP values that are > 3 standard deviations away form the mean",
    "text": "outlier removal of rows with GDP values that are &gt; 3 standard deviations away form the mean\n\nimport scipy.stats as stats\n\n\ndf['gdp_zscore'] = stats.zscore(df['gdp'])"
  },
  {
    "objectID": "posts/2021-05-18-groupby_pivot.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "href": "posts/2021-05-18-groupby_pivot.html#these-are-the-observations-more-then-3-sds-away-from-the-mean-of-gdp-that-will-be-dropped",
    "title": "Groupby and Pivot Tables in Python",
    "section": "these are the observations more then > 3 SDs away from the mean of gdp that will be dropped",
    "text": "these are the observations more then &gt; 3 SDs away from the mean of gdp that will be dropped\n\ndf[abs(df['gdp_zscore'])&gt;3].hist(column = ['gdp'])\n\n\ndf_no_gdp_outliers=df[abs(df['gdp_zscore'])&lt;3]\n\n\ndf_no_gdp_outliers\n\n\ndf_no_gdp_outliers.hist(column=['gdp'], bins=60)\n\n\ncounts_fiscal=df.groupby('region').count()\ncounts_fiscal\n\n\ncounts_fiscal=df.groupby('province').count()\ncounts_fiscal\n\n\n#df_no_gdp_outliers.pivot_table(index='grouping column 1', columns='grouping column 2', values='aggregating column', aggfunc='sum')\n\n\n#pd.crosstab(df_no_gdp_outliers, 'year')\n\n\ndf_no_gdp_outliers_subset = df_no_gdp_outliers[['region', 'gdp', 'fdi', 'it']]\ndf_no_gdp_outliers_subset\n\n\ndef aggregate_and_ttest(dataset, groupby_feature='province', alpha=.05, test_cells = [0, 1]):\n    #Imports\n    from tqdm import tqdm\n    from scipy.stats import ttest_ind_from_stats\n\n    \n    metrics = ['gdp', 'fdi', 'it']\n    \n    feature_size = 'size'\n    feature_mean = 'mean'\n    feature_std = 'std'    \n\n    for metric in tqdm(metrics):\n        \n        #print(metric)\n        crosstab = dataset.groupby(groupby_feature, as_index=False)[metric].agg(['size', 'mean', 'std'])\n        print(crosstab)\n        \n        treatment = crosstab.index[test_cells[0]]\n        control = crosstab.index[test_cells[1]]\n        \n        counts_control = crosstab.loc[control, feature_size]\n        counts_treatment = crosstab.loc[treatment, feature_size]\n\n        mean_control = crosstab.loc[control, feature_mean]\n        mean_treatment = crosstab.loc[treatment, feature_mean]\n\n        standard_deviation_control = crosstab.loc[control, feature_std]\n        standard_deviation_treatment = crosstab.loc[treatment, feature_std]\n        \n        t_statistic, p_value = ttest_ind_from_stats(mean1=mean_treatment, std1=standard_deviation_treatment, nobs1=counts_treatment,mean2=mean_control,std2=standard_deviation_control,nobs2=counts_control)\n        \n        #fstring to print the p value and t statistic\n        print(f\"The t statistic of the comparison of the treatment test cell of {treatment} compared to the control test cell of {control} for the metric of {metric} is {t_statistic} and the p value is {p_value}.\")\n        \n        #f string to say of the comparison is significant at a given alpha level\n\n        if p_value &lt; alpha: \n            print(f'The comparison between {treatment} and {control} is statistically significant at the threshold of {alpha}') \n        else: \n            print(f'The comparison between {treatment} and {control} is not statistically significant at the threshold of {alpha}')\n\n\naggregate_and_ttest(df_no_gdp_outliers, test_cells = [0,2])\n\n\nEastvNorth=pd.DataFrame()\nEastvNorth= aggregate_and_ttest(df_no_gdp_outliers_subset, test_cells = [0,1])\nEastvNorth\n\n\nimport numpy as np\nimport bootstrapped.bootstrap as bs\nimport bootstrapped.stats_functions as bs_stats\n\n\ntest_1=df_no_gdp_outliers[df_no_gdp_outliers['province']=='Beijing']\ntest=test_1['gdp'].to_numpy()\ntest\n\ncontrol_1=df_no_gdp_outliers[df_no_gdp_outliers['province']=='Shanxi']\ncontrol=control_1['gdp'].to_numpy()\ncontrol\n\n\nbins = np.linspace(0, 40, 20)\n\nplt.hist(control, label='Control')\nplt.hist(test, label='Test', color='orange')\nplt.title('Test/Ctrl Data')\nplt.legend()\n\n\nbs.bootstrap_ab(test, control, stat_func=bs_stats.sum, compare_func=bs_compare.percent_change)\n\n\n# run an a/b test simulation considering the lengths of the series (sum)\n# consider the full 'volume' of values that are passed in\n\nprint(bs_compare.percent_change(test.sum(), control.sum()))\n\nprint(bs.bootstrap_ab(\n    test, \n    control, \n    stat_func=bs_stats.sum,\n    compare_func=bs_compare.percent_change\n))\n\n\n# run an a/b test simulation ignoring the lengths of the series (average)\n# just what is the 'typical' value\n# use percent change to compare test and control\n\nprint(bs_compare.difference(test.mean(), control.mean()))\n\n\nprint(bs.bootstrap_ab(test, control, bs_stats.mean, bs_compare.difference))"
  },
  {
    "objectID": "posts/2020-11-18-stocks-timeseries-viz-altair.html",
    "href": "posts/2020-11-18-stocks-timeseries-viz-altair.html",
    "title": "Timeseries, Stocks and Altair",
    "section": "",
    "text": "toc: true\nbadges: true\ncomments: true\nsticky_rank: 1\ncategories: [Timeseries]\n\n\n#collapse-hide\nimport pandas as pd\nimport altair as alt\n\n\n#collapse-show\nsp500 = 'https://vega.github.io/vega-datasets/data/sp500.csv'\nstocks = 'https://vega.github.io/vega-datasets/data/stocks.csv'\n\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=sp500\n).properties(\n    width=1000,\n    height=900\n)\n\n\n\n\n\n\n\n# select a point for which to provide details-on-demand\nlabel = alt.selection_single(\n    encodings=['x'], # limit selection to x-axis value\n    on='mouseover',  # select on mouseover events\n    nearest=True,    # select data point nearest the cursor\n    empty='none'     # empty selection includes no data points\n)\n\n# define our base line chart of stock prices\nbase = alt.Chart().mark_line().encode(\n    alt.X('date:T'),\n    alt.Y('price:Q', scale=alt.Scale(type='log')),\n    alt.Color('symbol:N')\n)\n\nalt.layer(\n    base, # base line chart\n    \n    # add a rule mark to serve as a guide line\n    alt.Chart().mark_rule(color='#aaa').encode(\n        x='date:T'\n    ).transform_filter(label),\n    \n    # add circle marks for selected time points, hide unselected points\n    base.mark_circle().encode(\n        opacity=alt.condition(label, alt.value(1), alt.value(0))\n    ).add_selection(label),\n\n    # add white stroked text to provide a legible background for labels\n    base.mark_text(align='left', dx=5, dy=-5, stroke='white', strokeWidth=2).encode(\n        text='price:Q'\n    ).transform_filter(label),\n\n    # add text labels for stock prices\n    base.mark_text(align='left', dx=5, dy=-5).encode(\n        text='price:Q'\n    ).transform_filter(label),\n    \n    data=stocks\n).properties(\n    width=1000,\n    height=900\n)"
  },
  {
    "objectID": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html",
    "href": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n\n\n\n\n_c0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\nAnhui\n147002.0\nnull\n1996\n2093.3\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\nAnhui\n151981.0\nnull\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\nAnhui\n174930.0\nnull\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\nAnhui\n285324.0\nnull\n1999\n2712.34\n26131\nnull\nnull\nnull\n1646891\nEast China\n1227364\n\n\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\ndf.columns\n\n\nOut[85]: ['_c0',\n 'province',\n 'specific',\n 'general',\n 'year',\n 'gdp',\n 'fdi',\n 'rnr',\n 'rr',\n 'i',\n 'fr',\n 'reg',\n 'it']\ndf.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: double (nullable = true)\n-- general: double (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: double (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: double (nullable = true)\n-- rr: double (nullable = true)\n-- i: double (nullable = true)\n-- fr: string (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)\n# for row in df.head(5):\n#     print(row)\n#     print('\\n')\ndf.describe().show()\n\n\n+-------+------------------+--------+-----------------+------------------+------------------+-----------------+------------------+-------------------+--------------------+-------------------+------------------+---------------+------------------+\nsummary|               _c0|province|         specific|           general|              year|              gdp|               fdi|                rnr|                  rr|                  i|                fr|            reg|                it|\n+-------+------------------+--------+-----------------+------------------+------------------+-----------------+------------------+-------------------+--------------------+-------------------+------------------+---------------+------------------+\n  count|               360|     360|              356|               169|               360|              360|               360|                294|                 296|                287|               295|            360|               360|\n   mean|             179.5|    null|583470.7303370787|309127.53846153844|            2001.5|4428.653416666667|196139.38333333333| 0.0355944252244898|0.059688621057432424|0.08376351662369343|2522449.0034013605|           null|2165819.2583333333|\n stddev|104.06728592598157|    null|654055.3290782663| 355423.5760674793|3.4568570586927794|4484.668659976412|303043.97011891654|0.16061503029299648| 0.15673351824073453| 0.1838933104683607|3491329.8613106664|           null|1769294.2935487411|\n    min|                 0|   Anhui|           8964.0|               0.0|              1996|            64.98|                 2|                0.0|                 0.0|                0.0|             #REF!|     East China|            147897|\n    max|               359|Zhejiang|        3937966.0|         1737800.0|              2007|         31777.01|           1743140|        1.214285714|                0.84|               1.05|           9898522|Southwest China|          10533312|\n+-------+------------------+--------+-----------------+------------------+------------------+-----------------+------------------+-------------------+--------------------+-------------------+------------------+---------------+------------------+\ndf.describe().printSchema()\n\n\nroot\n-- summary: string (nullable = true)\n-- _c0: string (nullable = true)\n-- province: string (nullable = true)\n-- specific: string (nullable = true)\n-- general: string (nullable = true)\n-- year: string (nullable = true)\n-- gdp: string (nullable = true)\n-- fdi: string (nullable = true)\n-- rnr: string (nullable = true)\n-- rr: string (nullable = true)\n-- i: string (nullable = true)\n-- fr: string (nullable = true)\n-- reg: string (nullable = true)\n-- it: string (nullable = true)"
  },
  {
    "objectID": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html#casting-data-types-and-formatting-significant-digits",
    "href": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html#casting-data-types-and-formatting-significant-digits",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Casting Data Types and Formatting Significant Digits",
    "text": "Casting Data Types and Formatting Significant Digits\n\nfrom pyspark.sql.functions import format_number\n\n\n\n\n\n\nresult = df.describe()\nresult.select(result['province']\n,format_number(result['specific'].cast('float'),2).alias('specific')\n,format_number(result['general'].cast('float'),2).alias('general')\n,format_number(result['year'].cast('int'),2).alias('year'),format_number(result['gdp'].cast('float'),2).alias('gdp')\n,format_number(result['rnr'].cast('int'),2).alias('rnr'),format_number(result['rr'].cast('float'),2).alias('rr')\n,format_number(result['fdi'].cast('int'),2).alias('fdi'),format_number(result['it'].cast('float'),2).alias('it')\n,result['reg'].cast('string').alias('reg')\n             ).show()\n\n\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+\nprovince|    specific|     general|    year|      gdp|   rnr|    rr|         fdi|           it|            reg|\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+\n     360|      356.00|      169.00|  360.00|   360.00|294.00|296.00|      360.00|       360.00|            360|\n    null|  583,470.75|  309,127.53|2,001.00| 4,428.65|  0.00|  0.06|  196,139.00| 2,165,819.25|           null|\n    null|  654,055.31|  355,423.56|    3.00| 4,484.67|  0.00|  0.16|  303,043.00| 1,769,294.25|           null|\n   Anhui|    8,964.00|        0.00|1,996.00|    64.98|  0.00|  0.00|        2.00|   147,897.00|     East China|\nZhejiang|3,937,966.00|1,737,800.00|2,007.00|31,777.01|  1.00|  0.84|1,743,140.00|10,533,312.00|Southwest China|\n+--------+------------+------------+--------+---------+------+------+------------+-------------+---------------+"
  },
  {
    "objectID": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html#new-columns-generated-from-extant-columns-using-withcolumn",
    "href": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html#new-columns-generated-from-extant-columns-using-withcolumn",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "New Columns generated from extant columns using withColumn",
    "text": "New Columns generated from extant columns using withColumn\n\ndf2 = df.withColumn(\"specific_gdp_ratio\",df[\"specific\"]/(df[\"gdp\"]*100))#.show()\n\n\n\n\n\n\ndf2.select('specific_gdp_ratio').show()\n\n\n+------------------+\nspecific_gdp_ratio|\n+------------------+\n0.7022500358285959|\n0.6474660463848132|\n0.6878991411583352|\n1.0519477646607727|\n 0.673928100093381|\n0.7727761333780966|\n 1.233475958314866|\n1.5783421826051272|\n1.8877587040110941|\n1.6792756118029895|\n2.3850666666666664|\n3.0077639751552794|\n0.9275486250838364|\n0.7989880072601573|\n1.0314658544998698|\n 1.448708759827088|\n0.8912058855158366|\n1.1918224576316896|\n1.2944820393974508|\n 1.283311464867661|\n+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.orderBy(df[\"specific\"].asc()).head(1)[0][0]\n\n\nOut[94]: 24"
  },
  {
    "objectID": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html#finding-the-mean-max-and-min",
    "href": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html#finding-the-mean-max-and-min",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Finding the Mean, Max, and Min",
    "text": "Finding the Mean, Max, and Min\n\nfrom pyspark.sql.functions import mean\ndf.select(mean(\"specific\")).show()\n\n\n+-----------------+\n    avg(specific)|\n+-----------------+\n583470.7303370787|\n+-----------------+\n\n\n\n\n\nfrom pyspark.sql.functions import max,min\n\n\n\n\n\n\ndf.select(max(\"specific\"),min(\"specific\")).show()\n\n\n+-------------+-------------+\nmax(specific)|min(specific)|\n+-------------+-------------+\n    3937966.0|       8964.0|\n+-------------+-------------+\n\n\n\n\n\ndf.filter(\"specific &lt; 60000\").count()\n\n\nOut[98]: 23\n\n\n\ndf.filter(df['specific'] &lt; 60000).count()\n\n\nOut[99]: 23\n\n\n\nfrom pyspark.sql.functions import count\nresult = df.filter(df['specific'] &lt; 60000)\nresult.select(count('specific')).show()\n\n\n+---------------+\ncount(specific)|\n+---------------+\n             23|\n+---------------+\n\n\n\n\n\n(df.filter(df[\"gdp\"]&gt;8000).count()*1.0/df.count())*100\n\n\nOut[101]: 14.444444444444443\n\n\n\nfrom pyspark.sql.functions import corr\ndf.select(corr(\"gdp\",\"fdi\")).show()\n\n\n+------------------+\n    corr(gdp, fdi)|\n+------------------+\n0.8366328478935896|\n+------------------+"
  },
  {
    "objectID": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html#finding-the-max-value-by-year",
    "href": "posts/2020-08-20-Pyspark-Dataframes-Data-Types.html#finding-the-max-value-by-year",
    "title": "Dataframes, Formatting, Casting Data Type and Correlation with Pyspark",
    "section": "Finding the max value by Year",
    "text": "Finding the max value by Year\n\nfrom pyspark.sql.functions import year\n#yeardf = df.withColumn(\"Year\",year(df[\"year\"]))\n\n\n\n\n\n\nmax_df = df.groupBy('year').max()\n\n\n\n\n\n\nmax_df.select('year','max(gdp)').show()\n\n\n+----+--------+\nyear|max(gdp)|\n+----+--------+\n2003|15844.64|\n2007|31777.01|\n2006|26587.76|\n1997| 7774.53|\n2004|18864.62|\n1996| 6834.97|\n1998| 8530.88|\n2001|12039.25|\n2005|22557.37|\n2000|10741.25|\n1999| 9250.68|\n2002|13502.42|\n+----+--------+\n\n\n\n\n\nfrom pyspark.sql.functions import month\n\n\n\n\n\n\n#df.select(\"year\",\"avg(gdp)\").orderBy('year').show()\n\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-11-16-automl-capabilities-of-h2o-library.html",
    "href": "posts/2020-11-16-automl-capabilities-of-h2o-library.html",
    "title": "AutoML, Xgboost and H2O",
    "section": "",
    "text": "Credit: code from https://www.kaggle.com/paradiselost/tutorial-automl-capabilities-of-h2o-library\n\n\n\nh2o.ai\n\n\nAutomated machine learning (AutoML) is the process of automating the end-to-end process of applying machine learning to real-world problems. In a typical machine learning application, the typical stages (and sub-stages) of work are the following: 1. Data preparation * data pre-processing * feature engineering * feature extraction * feature selection 2. Model selection 3. Hyperparameter optimization (to maximize the performance of the final model)\nMany of these steps are often beyond the abilities of non-experts. AutoML was proposed as an artificial intelligence-based solution to the ever-growing challenge of applying machine learning.\nSome of the notable platforms tackling various stages of AutoML are the following: * auto-sklearn is a Bayesian hyperparameter optimization layer on top of scikit-learn. * TPOT (TeaPOT) is a Python library that automatically creates and optimizes full machine learning pipelines using genetic programming. * TransmogrifAI is a Scala/SparkML library created by Salesforce for automated data cleansing, feature engineering, model selection, and hyperparameter optimization. * H2O AutoML performs (simple) data preprocessing, automates the process of training a large selection of candidate models, tunes hyperparameters of the models and creates stacked ensembles. * H2O Driverless AI is a commercial software package that automates lots of aspects of machine learning applications. It has a strong focus on automatic feature engineering.\nAn overview of AutoML capabilities of H2O library is presented in this tutorial. The library can be installed simply by\n\n#!pip install h2o\n\nLet’s import the required packages and call h2o.init(). The specified arguments (nthreads and max_mem_size) are optional.\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ execution_count=2}\nimport sys, os, os.path\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\nimport pickle\n\nimport h2o\nfrom h2o.automl import H2OAutoML\n\nh2o.init(\n    nthreads=-1,     # number of threads when launching a new H2O server\n    max_mem_size=12  # in gigabytes\n)\n\nChecking whether there is an H2O instance running at http://localhost:54321 . connected.\n\n\n\n\n      \n\n  \n\n\n\nH2O_cluster_uptime:\n3 mins 51 secs\n\n\nH2O_cluster_timezone:\nAmerica/Los_Angeles\n\n\nH2O_data_parsing_timezone:\nUTC\n\n\nH2O_cluster_version:\n3.38.0.1\n\n\nH2O_cluster_version_age:\n12 days\n\n\nH2O_cluster_name:\nH2O_from_python_gao_qldgcm\n\n\nH2O_cluster_total_nodes:\n1\n\n\nH2O_cluster_free_memory:\n3.838 Gb\n\n\nH2O_cluster_total_cores:\n8\n\n\nH2O_cluster_allowed_cores:\n8\n\n\nH2O_cluster_status:\nlocked, healthy\n\n\nH2O_connection_url:\nhttp://localhost:54321\n\n\nH2O_connection_proxy:\n{\"http\": null, \"https\": null}\n\n\nH2O_internal_security:\nFalse\n\n\nPython_version:\n3.9.12 final\n\n\n\n\n\n\n:::\nExample 1: a classification task\nLet’s apply the power of H2O AutoML to the “Flight delays” competition (it’s a binary classification task) from mlcourse.ai.\n::: {.cell _cell_guid=‘79c7e3d0-c299-4dcb-8224-4455121ee9b0’ _uuid=‘d629ff2d2480ee46fbb7e2d37f6b5fab8052498a’ execution_count=11}\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/bikes.csv'\ntrain_df = pd.read_csv(url, error_bad_lines=False)\ntest_df = pd.read_csv(url, error_bad_lines=False)\n:::\n\nprint('train_df cols:', list(train_df.columns))\nprint('test_df cols: ', list(test_df.columns))\ntrain_df.head()\n\ntrain_df cols: ['trip_id', 'gender', 'starttime', 'stoptime', 'tripduration', 'from_station_name', 'dpcapacity_start', 'to_station_name', 'dpcapacity_end', 'temperature', 'visibility', 'wind_speed', 'precipitation', 'events']\ntest_df cols:  ['trip_id', 'gender', 'starttime', 'stoptime', 'tripduration', 'from_station_name', 'dpcapacity_start', 'to_station_name', 'dpcapacity_end', 'temperature', 'visibility', 'wind_speed', 'precipitation', 'events']\n\n\n\n\n\n\n\n\n\ntrip_id\ngender\nstarttime\nstoptime\ntripduration\nfrom_station_name\ndpcapacity_start\nto_station_name\ndpcapacity_end\ntemperature\nvisibility\nwind_speed\nprecipitation\nevents\n\n\n\n\n0\n7147\nMale\n2013-06-28 19:01:00\n2013-06-28 19:17:00\n993\nLake Shore Dr & Monroe St\n11.0\nMichigan Ave & Oak St\n15.0\n73.9\n10.0\n12.7\n-9999.0\nmostlycloudy\n\n\n1\n7524\nMale\n2013-06-28 22:53:00\n2013-06-28 23:03:00\n623\nClinton St & Washington Blvd\n31.0\nWells St & Walton St\n19.0\n69.1\n10.0\n6.9\n-9999.0\npartlycloudy\n\n\n2\n10927\nMale\n2013-06-30 14:43:00\n2013-06-30 15:01:00\n1040\nSheffield Ave & Kingsbury St\n15.0\nDearborn St & Monroe St\n23.0\n73.0\n10.0\n16.1\n-9999.0\nmostlycloudy\n\n\n3\n12907\nMale\n2013-07-01 10:05:00\n2013-07-01 10:16:00\n667\nCarpenter St & Huron St\n19.0\nClark St & Randolph St\n31.0\n72.0\n10.0\n16.1\n-9999.0\nmostlycloudy\n\n\n4\n13168\nMale\n2013-07-01 11:16:00\n2013-07-01 11:18:00\n130\nDamen Ave & Pierce Ave\n19.0\nDamen Ave & Pierce Ave\n19.0\n73.0\n10.0\n17.3\n-9999.0\npartlycloudy\n\n\n\n\n\n\n\n\ntrain_df.dtypes\n\ntrip_id                int64\ngender                object\nstarttime             object\nstoptime              object\ntripduration           int64\nfrom_station_name     object\ndpcapacity_start     float64\nto_station_name       object\ndpcapacity_end       float64\ntemperature          float64\nvisibility           float64\nwind_speed           float64\nprecipitation        float64\nevents                object\ndtype: object\n\n\nThe features Month, DayofMonth, DayOfWeek, DepTime, Distance can be represented as numbers. Let’s convert those features to numerical type (a new feature HourFloat is added):\n\n# for df in [train_df, test_df]:\n#     df['Month'] = df['Month'].apply(lambda s: s.split('-')[1]).astype('int')\n#     df['DayofMonth'] = df['DayofMonth'].apply(lambda s: s.split('-')[1]).astype('int')\n#     df['DayOfWeek'] = df['DayOfWeek'].apply(lambda s: s.split('-')[1]).astype('int')\n    \n#     df['HourFloat'] = df['DepTime'].apply(\n#         lambda t: (t // 100) % 24 + ((t % 100) % 60) / 60\n#     ).astype('float')\n\nLet’s also introduce a new feature Route that is the concatenation of Origin and Dest:\n\n# for df in [train_df, test_df]:\n#     df['Route'] = df[['Origin', 'Dest']].apply(\n#         lambda pair: ''.join([str(a) for a in pair]),\n#         axis='columns'\n#     ).astype('str')\n\nWe will not use the column DepTime anymore. Split the target column from the features columns in train_df:\n\ntarget = train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0})\n\nfeature_cols = [\n    'Month', 'DayofMonth', 'DayOfWeek', 'HourFloat', \n    'UniqueCarrier', 'Origin', 'Dest', 'Route', 'Distance',]\ntrain_df_modif = train_df[feature_cols]\ntest_df_modif = test_df[feature_cols]\n\nThe features UniqueCarrier, Origin, Dest, Route should be categorical:\n\nN_train = train_df_modif.shape[0]\ntrain_test_X = pd.concat([train_df_modif, test_df_modif], axis='index')\n\nfor feat in ['UniqueCarrier', 'Origin', 'Dest', 'Route']:\n    train_test_X[feat] = train_test_X[feat].astype('category')\n\n\nX_train = train_test_X[:N_train]\nX_test = train_test_X[N_train:]\ny_train = target\n\nPandas DataFrames should be converted to H2O dataframes before calling H2OAutoML().\nNote: if you don’t have to preprocess the data, you can get H2O dataframes directly from the data files by a call like df = h2o.import_file(datafile_path) (where datafile_path is a filesystem path or a URL).\n\nX_y_train_h = h2o.H2OFrame(pd.concat([X_train, y_train], axis='columns'))\nX_y_train_h['dep_delayed_15min'] = X_y_train_h['dep_delayed_15min'].asfactor()\n# ^ the target column should have categorical type for classification tasks\n#   (numerical type for regression tasks)\n\nX_test_h = h2o.H2OFrame(X_test)\n\nX_y_train_h.describe()\n\n\naml = H2OAutoML(\n    max_runtime_secs=(3600 * 8),  # 8 hours\n    max_models=None,  # no limit\n    seed=17\n)\n\nAmong the most important arguments (with their default values) of H2OAutoML() are the following: * nfolds=5 – number of folds for k-fold cross-validation (nfolds=0 disables cross-validation) * balance_classes=False – balance training data class counts via over/under-sampling * max_runtime_secs=3600 – how long the AutoML run will execute (in seconds) * max_models=None – the maximum number of models to build in an AutoML run (None means no limitation) * include_algos=None – list of algorithms to restrict to during the model-building phase (cannot be used in combination with exclude_algos parameter; None means that all appropriate H2O algorithms will be used) * exclude_algos=None – list of algorithms to skip during the model-building phase (None means that all appropriate H2O algorithms will be used) * seed=None – a random seed for reproducibility (AutoML can only guarantee reproducibility if max_models or early stopping is used because max_runtime_secs is resource limited, meaning that if the resources are not the same between runs, AutoML may be able to train more models on one run vs another)\nH2O AutoML trains and cross-validates: * a default Random Forest (DRF), * an Extremely-Randomized Forest (XRT), * a random grid of Generalized Linear Models (GLM), * a random grid of XGBoost (XGBoost), * a random grid of Gradient Boosting Machines (GBM), * a random grid of Deep Neural Nets (DeepLearning), * and 2 Stacked Ensembles, one of all the models, and one of only the best models of each kind.\nIn the cell below, I call aml.train(), save the leaderboard and all individual models. The running time is about 8 hours, so after running it once I saved the output files as a new dataset, connected the dataset to this kernel and commented out the code in the cell.\n\n%%time\n\n# aml.train(\n#     x=feature_cols,\n#     y='dep_delayed_15min',\n#     training_frame=X_y_train_h\n# )\n\n# lb = aml.leaderboard\n# model_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\n# out_path = \".\"\n\n# for m_id in model_ids:\n#     mdl = h2o.get_model(m_id)\n#     h2o.save_model(model=mdl, path=out_path, force=True)\n\n# h2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)\n\nSome of the arguments for H2OAutoML.train() are the following: * training_frame – the H2OFrame having the columns indicated by x and y * x – list of feature column names in training_frame * y – a column name indicating the target * validation_frame – the H2OFrame with validation data (by default and when nfolds &gt; 1, validation_frame will be ignored) * leaderboard_frame – the H2OFrame with test data for scoring the leaderboard (optinal; by default (leaderboard_frame=None) the cross-validation metric on training_frame will be used to generate the leaderboard rankings)\nLet’s take a look at the leaderboard:\n\nmodels_path = \"../input/h2o-automl-saved-models-classif/\"\n\nlb = h2o.import_file(path=os.path.join(models_path, \"aml_leaderboard.h2o\"))\n\nlb.head(rows=10)\n#lb.head(rows=lb.nrows)\n# ^ to see the entire leaderboard\n\nAmong the individual models, XGBoost is the leader (auc = 0.749523) for this task. Best individual GBM has auc = 0.741785, best XRT has auc = 0.731317, best DRF has auc = 0.725166, best DNN has auc = 0.706676.\nStackedEnsemble_AllModels is usually the leader, StackedEnsemble_BestOfFamily is usually at the 2nd place. Let’s look inside the StackedEnsemble_AllModels. It is an ensemble of all of the individual models in the AutoML run.\n\nse_all = h2o.load_model(os.path.join(models_path, \"StackedEnsemble_AllModels_AutoML_20190414_112210\"))\n# Get the Stacked Ensemble metalearner model\nmetalearner = h2o.get_model(se_all.metalearner()['name'])\n\nThe AutoML Stacked Ensembles use the GLM with non-negative weights as the default metalearner (combiner) algorithm. Let’s examine the variable importance of the metalearner algorithm in the ensemble. This shows us how much each base learner is contributing to the ensemble. Intercept represents the constant term in a linear model.\n\n%matplotlib inline\nmetalearner.std_coef_plot(num_of_features=20)\n# ^ all importance values starting from the 16th are zero\n\n#metalearner.coef_norm()\n# ^ to see the table in the text form\n\nStackedEnsemble_BestOfFamily shows the following:\n\nse_best_of_family = h2o.load_model(os.path.join(models_path, \"StackedEnsemble_BestOfFamily_AutoML_20190414_112210\"))\n# Get the Stacked Ensemble metalearner model\nmetalearner = h2o.get_model(se_best_of_family.metalearner()['name'])\n\n%matplotlib inline\nmetalearner.std_coef_plot(num_of_features=10)\n#metalearner.coef_norm()\n\nLet’s reproduce the result (auc) of a few best individual models.\n\nfrom h2o.estimators.xgboost import H2OXGBoostEstimator\n\nmodel_01 = h2o.load_model(os.path.join(models_path, \"XGBoost_grid_1_AutoML_20190414_112210_model_19\"))\n\nexcluded_params = ['model_id', 'response_column', 'ignored_columns']\nmodel_01_actual_params = {k: v['actual'] for k, v in model_01.params.items() if k not in excluded_params}\n\nreprod_model_01 = H2OXGBoostEstimator(**model_01_actual_params)\nreprod_model_01.train(\n    x=feature_cols,\n    y='dep_delayed_15min',\n    training_frame=X_y_train_h\n)\nreprod_model_01.auc(xval=True)\n# ^ 0.749453, slightly worse compared to the leaderboard value\n\n\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\n\nmodel_12 = h2o.load_model(os.path.join(models_path, \"GBM_grid_1_AutoML_20190414_112210_model_85\"))\n\nexcluded_params = ['model_id', 'response_column', 'ignored_columns']\nmodel_12_actual_params = {k: v['actual'] for k, v in model_12.params.items() if k not in excluded_params}\n\nreprod_model_12 = H2OGradientBoostingEstimator(**model_12_actual_params)\nreprod_model_12.train(\n    x=feature_cols,\n    y='dep_delayed_15min',\n    training_frame=X_y_train_h\n)\nreprod_model_12.auc(xval=True)\n# ^ 0.741785, the same as at the leaderboard\n\n\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.grid.grid_search import H2OGridSearch\n\nmodel_93 = h2o.load_model(os.path.join(models_path, \"GLM_grid_1_AutoML_20190414_112210_model_1\"))\n\nexcluded_params = ['model_id', 'response_column', 'ignored_columns', 'lambda']\nmodel_93_actual_params = {k: v['actual'] for k, v in model_93.params.items() if k not in excluded_params}\n\nreprod_model_93 = H2OGeneralizedLinearEstimator(**model_93_actual_params)\nreprod_model_93.train(\n    x=feature_cols,\n    y='dep_delayed_15min',\n    training_frame=X_y_train_h\n)\nreprod_model_93.auc(xval=True)\n# ^ 0.699418, the same as at the leaderboard\n\nLet’s train the CatBoostClassifier with the default parameters and compare its results with AutoML run results.\n\nfrom catboost import Pool, CatBoostClassifier, cv\n\ncb_model = CatBoostClassifier(\n    eval_metric='AUC',\n    use_best_model=True,\n    random_seed=17\n)\n\ncv_data = cv(\n    Pool(X_train, y_train, cat_features=[4,5,6,7]),\n    cb_model.get_params(),\n    fold_count=5,\n    verbose=False\n)\n\nprint(\"CatBoostClassifier: the best cv auc is\", np.max(cv_data['test-AUC-mean']))\n\nThe CatBoostClassifier cross-validation auc result is 0.749009. This value falls between the 2nd (auc = 0.749523) and 3rd (auc = 0.749192) places among the individual models at the leaderboard.\nExample 2: a regression task\nLet’s consider a regression task from the “New York City Taxi Trip Duration” competition. The challenge is to build a model that predicts the total ride duration of taxi trips in New York City. The features include pickup time, geo-coordinates, number of passengers, and a few other variables.\n\ndf_train = pd.read_csv('../input/nyc-taxi-trip-duration/train.csv', index_col=0)\ndf_test  = pd.read_csv('../input/nyc-taxi-trip-duration/test.csv',  index_col=0)\n\nWe will use only df_train (perform 5-fold cross-validation on it). Convert the date- and time-related features to the datetime format; take the logarithm (log(1 + x)) of the target value (trip duration). After the logarithm transform, the distribution of the target variable is close to normal (see this kernel).\n\ndf_train['pickup_datetime'] = pd.to_datetime(df_train.pickup_datetime)\ndf_train.loc[:, 'pickup_date'] = df_train['pickup_datetime'].dt.date\ndf_train['dropoff_datetime'] = pd.to_datetime(df_train.dropoff_datetime)\ndf_train['store_and_fwd_flag'] = 1 * (df_train.store_and_fwd_flag.values == 'Y')\ndf_train['check_trip_duration'] = (df_train['dropoff_datetime'] - df_train['pickup_datetime']).map(\n    lambda x: x.total_seconds()\n)\ndf_train['log_trip_duration'] = np.log1p(df_train['trip_duration'].values)\n\ncnd = np.abs(df_train['check_trip_duration'].values  - df_train['trip_duration'].values) &gt; 1\nduration_difference = df_train[cnd]\n\nif len(duration_difference[['pickup_datetime', 'dropoff_datetime', 'trip_duration', 'check_trip_duration']]) == 0:\n    print('Trip_duration and datetimes are ok.')\nelse:\n    print('Ooops.')\n\nSelect the columns common to the train set and test set; convert pd.DataFrame to H2OFrame:\n\ncommon_cols = [\n    'vendor_id', \n    'pickup_datetime', \n    'passenger_count', \n    'pickup_longitude', 'pickup_latitude', \n    'dropoff_longitude', 'dropoff_latitude',\n    'store_and_fwd_flag',\n]\n\nX_y_train_h = h2o.H2OFrame(\n    pd.concat(\n        [df_train[common_cols], df_train['log_trip_duration']],\n        axis='columns'\n    )\n)\n\nfor ft in ['vendor_id', 'store_and_fwd_flag']:\n    X_y_train_h[ft] = X_y_train_h[ft].asfactor()\n    \nX_y_train_h.describe()\n\nI have run the cell below (~8 hours), saved all models and the leaderboard, then commented out the code:\n\n# aml = H2OAutoML(\n#     max_runtime_secs=(3600 * 8),  # 8 hours\n#     max_models=None,  # no limit\n#     seed=SEED,\n# )\n\n# aml.train(\n#     x=common_cols,\n#     y='log_trip_duration',\n#     training_frame=X_y_train_h\n# )\n\n# lb = aml.leaderboard\n# model_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\n# out_path = \".\"\n\n# for m_id in model_ids:\n#     mdl = h2o.get_model(m_id)\n#     h2o.save_model(model=mdl, path=out_path, force=True)\n\n# h2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)\n\nInterestingly, there is only one model at the leaderboard:\n\nmodels_path = \"../input/h2o-automl-saved-models-regress/\"\n\nlb = h2o.import_file(path=os.path.join(models_path, \"aml_leaderboard.h2o\"))\nlb.head(rows=10)\n\nLet’s compare the result of the model XGBoost_1_AutoML_20190417_212831 with that of the CatBoostRegressor with the default parameters.\n\nfrom catboost import Pool, CatBoostRegressor, cv\n\ncb_model = CatBoostRegressor(\n    eval_metric='RMSE',\n    use_best_model=True,\n    random_seed=17\n)\n\ncv_data = cv(\n    Pool(df_train[common_cols], df_train['log_trip_duration'], cat_features=[0,7]),\n    cb_model.get_params(),\n    fold_count=5,\n    verbose=False\n)\n\n\nprint(\"CatBoostRegressor: the best cv rmse is\", np.min(cv_data['test-RMSE-mean']))\n\nDefault CatBoost’s RMSE is slightly worse than that of the XGBoost model from the H2O AutoML run.\nConclusion\nI think that H2O AutoML is worth a try. And I hope you have found this tutorial useful.\nThere are extremely useful “H2O AutoML Pro Tips” in the presentation “Scalable Automatic Machine Learning in H2O” mentioned in the References below.\nReferences\n\nH2O.ai\nH2O AutoML documentation\nAutoML Tutorial: R and Python notebooks\nIntro to AutoML + Hands-on Lab: 1 hour video, slides\nScalable Automatic Machine Learning in H2O: 1 hour video, slides\nH2O for GPU (H2O4GPU)"
  },
  {
    "objectID": "posts/2020-09-25-AnalyzingSizeofArmedForces.html",
    "href": "posts/2020-09-25-AnalyzingSizeofArmedForces.html",
    "title": "Analyzing Size of Armed Forces From 1947 - 1963 with statsmodels",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\n\ndf = sm.datasets.longley.load_pandas().data\n\n#print(sm.datasets.longley.NOTE)\n\n\ndf.head()\n\n\n\n\n\n\n\n\nTOTEMP\nGNPDEFL\nGNP\nUNEMP\nARMED\nPOP\nYEAR\n\n\n\n\n0\n60323.0\n83.0\n234289.0\n2356.0\n1590.0\n107608.0\n1947.0\n\n\n1\n61122.0\n88.5\n259426.0\n2325.0\n1456.0\n108632.0\n1948.0\n\n\n2\n60171.0\n88.2\n258054.0\n3682.0\n1616.0\n109773.0\n1949.0\n\n\n3\n61187.0\n89.5\n284599.0\n3351.0\n1650.0\n110929.0\n1950.0\n\n\n4\n63221.0\n96.2\n328975.0\n2099.0\n3099.0\n112075.0\n1951.0\n\n\n\n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1947', '1962'))\ndf.index = index\ndf.head()\n\n\n\n\n\n\n\n\nTOTEMP\nGNPDEFL\nGNP\nUNEMP\nARMED\nPOP\nYEAR\n\n\n\n\n1947-12-31\n60323.0\n83.0\n234289.0\n2356.0\n1590.0\n107608.0\n1947.0\n\n\n1948-12-31\n61122.0\n88.5\n259426.0\n2325.0\n1456.0\n108632.0\n1948.0\n\n\n1949-12-31\n60171.0\n88.2\n258054.0\n3682.0\n1616.0\n109773.0\n1949.0\n\n\n1950-12-31\n61187.0\n89.5\n284599.0\n3351.0\n1650.0\n110929.0\n1950.0\n\n\n1951-12-31\n63221.0\n96.2\n328975.0\n2099.0\n3099.0\n112075.0\n1951.0\n\n\n\n\n\n\n\n\ndf['ARMED'].plot()\nplt.ylabel(\"ARMED\")\n\nText(0, 0.5, 'ARMED')\n\n\n\n\n\n\n# unpacking\ncycle, trend = sm.tsa.filters.hpfilter(df.ARMED)\n\ncycle\n\n1947-12-31    -497.642333\n1948-12-31    -713.661033\n1949-12-31    -635.368706\n1950-12-31    -682.008289\n1951-12-31     688.574390\n1952-12-31    1108.959755\n1953-12-31     992.297873\n1954-12-31     731.045710\n1955-12-31     370.040046\n1956-12-31     124.660757\n1957-12-31      15.056446\n1958-12-31    -193.702199\n1959-12-31    -324.553899\n1960-12-31    -407.316313\n1961-12-31    -393.604252\n1962-12-31    -182.777954\nName: ARMED, dtype: float64\n\n\n\ntype(cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = trend\n\ndf[['trend','ARMED']].plot(figsize = (12, 8))\n\ndf[['trend','ARMED']][\"1950-01-01\":\"1955-01-01\"].plot(figsize = (12, 8))\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f2d5af13940&gt;"
  },
  {
    "objectID": "posts/2020-08-18-Pyspark-NAs.html",
    "href": "posts/2020-08-18-Pyspark-NAs.html",
    "title": "Handling Missing Data with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\nfrom pyspark.sql.functions import countDistinct, avg,stddev\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n\n\n\n\n_c0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\nAnhui\n147002.0\nnull\n1996\n2093.3\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\nAnhui\n151981.0\nnull\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\nAnhui\n174930.0\nnull\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\nAnhui\n285324.0\nnull\n1999\n2712.34\n26131\nnull\nnull\nnull\n1646891\nEast China\n1227364\n\n\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-18-Pyspark-NAs.html#dropping-columns-without-non-null-values",
    "href": "posts/2020-08-18-Pyspark-NAs.html#dropping-columns-without-non-null-values",
    "title": "Handling Missing Data with Pyspark",
    "section": "Dropping Columns without non-null values",
    "text": "Dropping Columns without non-null values\n\n# Has to have at least 2 NON-null values\ndf.na.drop(thresh=2).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-18-Pyspark-NAs.html#dropping-any-row-that-contains-missing-data",
    "href": "posts/2020-08-18-Pyspark-NAs.html#dropping-any-row-that-contains-missing-data",
    "title": "Handling Missing Data with Pyspark",
    "section": "Dropping any row that contains missing data",
    "text": "Dropping any row that contains missing data\n\ndf.na.drop().show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n 46|   Fujian| 397517.0| 149549.0|2006|7583.85|322047|                0.4|        0.0|        0.0| 4830320|     East China|2135224|\n 47|   Fujian| 753552.0| 317700.0|2007|9248.53|406058|                0.4|        0.0|        0.0| 6994577|     East China|2649011|\n 52|    Gansu| 223984.0|  58533.0|2000|1052.88|  6235|                0.0|0.153846154|        0.0|  505196|Northwest China|1258100|\n 54|    Gansu| 337894.0| 129791.0|2002|1232.03|  6121|                0.0|       0.13|        0.0|  597159|Northwest China|1898911|\n 58|    Gansu| 833430.0| 516342.0|2006|2277.35|  2954|                0.0|        0.0|0.128205128|  924080|Northwest China|3847158|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(subset=[\"general\"]).show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n  8|    Anhui| 898441.0| 349699.0|2004| 4759.3| 54669|                0.0|        0.0|        0.0|    null|     East China|3422176|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 20|  Beijing|1009936.0| 309025.0|2004|6033.21|308354|                0.0|0.794871795|        0.0|    null|    North China|1644601|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 30|Chongqing| 311770.0|  41907.0|2002|2232.86| 19576|               null|       null|       null|  762806|Southwest China|1906968|\n 31|Chongqing| 335715.0|  18700.0|2003|2555.72| 26083|               null|       null|       null|  929935|Southwest China|1778125|\n 32|Chongqing| 568835.0|  97500.0|2004|3034.58| 40508|               null|       null|       null|    null|Southwest China|2197948|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(how='any').show()\n\n\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n_c0| province| specific|  general|year|    gdp|   fdi|                rnr|         rr|          i|      fr|            reg|     it|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\n  4|    Anhui| 195580.0|  32100.0|2000|2902.09| 31847|                0.0|        0.0|        0.0| 1601508|     East China|1499110|\n  6|    Anhui| 434149.0|  66529.0|2002|3519.72| 38375|                0.0|        0.0|        0.0| 1677840|     East China|2404936|\n  7|    Anhui| 619201.0|  52108.0|2003|3923.11| 36720|                0.0|        0.0|        0.0| 1896479|     East China|2815820|\n 10|    Anhui|1457872.0| 279052.0|2006| 6112.5|139354|                0.0|        0.0|0.324324324| 3434548|     East China|5167300|\n 11|    Anhui|2213991.0| 178705.0|2007|7360.92|299892|                0.0|        0.0|0.324324324| 4468640|     East China|7040099|\n 16|  Beijing| 281769.0| 188633.0|2000|3161.66|168368|                0.0|        0.0|       0.53| 1667114|    North China| 757990|\n 18|  Beijing| 558569.0| 280277.0|2002| 4315.0|172464|                0.0|        0.0|       0.53| 2511249|    North China|1078754|\n 19|  Beijing| 642581.0| 269596.0|2003|5007.21|219126|                0.0|0.794871795|        0.0| 2823366|    North China|1426600|\n 22|  Beijing|1315102.0| 405966.0|2006|8117.78|455191|                0.0|0.794871795|        0.0| 4830392|    North China|1782317|\n 23|  Beijing| 752279.0|1023453.0|2007|9846.81|506572|                0.0|0.794871795|        0.0|14926380|    North China|1962192|\n 34|Chongqing| 833423.0| 150000.0|2006|3907.23| 69595|0.09677419400000001|        0.0|        0.0| 1762409|Southwest China|3124234|\n 35|Chongqing|1791403.0| 447900.0|2007|4676.13|108534|0.09677419400000001|        0.0|        0.0| 4427000|Southwest China|3923569|\n 40|   Fujian| 142650.0|  71807.0|2000|3764.54|343191|                0.0|        0.0|        0.0| 2110577|     East China| 819028|\n 42|   Fujian| 137190.0|  59263.0|2002|4467.55|383837|               0.22|        0.3|        0.0| 2373047|     East China|1184990|\n 43|   Fujian| 148812.0|  68142.0|2003|4983.67|259903|                0.0|        0.0|        0.3| 2648861|     East China|1364980|\n 46|   Fujian| 397517.0| 149549.0|2006|7583.85|322047|                0.4|        0.0|        0.0| 4830320|     East China|2135224|\n 47|   Fujian| 753552.0| 317700.0|2007|9248.53|406058|                0.4|        0.0|        0.0| 6994577|     East China|2649011|\n 52|    Gansu| 223984.0|  58533.0|2000|1052.88|  6235|                0.0|0.153846154|        0.0|  505196|Northwest China|1258100|\n 54|    Gansu| 337894.0| 129791.0|2002|1232.03|  6121|                0.0|       0.13|        0.0|  597159|Northwest China|1898911|\n 58|    Gansu| 833430.0| 516342.0|2006|2277.35|  2954|                0.0|        0.0|0.128205128|  924080|Northwest China|3847158|\n+---+---------+---------+---------+----+-------+------+-------------------+-----------+-----------+--------+---------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.drop(how='all').show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-18-Pyspark-NAs.html#imputation-of-null-values",
    "href": "posts/2020-08-18-Pyspark-NAs.html#imputation-of-null-values",
    "title": "Handling Missing Data with Pyspark",
    "section": "Imputation of Null Values",
    "text": "Imputation of Null Values\n\ndf.na.fill('example').show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|example| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|example| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|example|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\nImputation of 0\n\ndf.na.fill(0).show()\n\n\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi|rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|     0.0|1996| 2093.3| 50661|0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|     0.0|1997|2347.32| 43443|0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|     0.0|1998|2542.96| 27673|0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|     0.0|1999|2712.34| 26131|0.0|        0.0|        0.0|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847|0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|     0.0|2001|3246.71| 33672|0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375|0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720|0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669|0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|     0.0|2005|5350.17| 69000|0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354|0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892|0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|     0.0|1996| 1789.2|155290|0.0|        0.0|        0.0| 634562|North China| 508135|\n 13| Beijing| 165957.0|     0.0|1997|2077.09|159286|0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|     0.0|1998|2377.18|216800|0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|     0.0|1999|2678.82|197525|0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368|0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|     0.0|2001|3707.96|176818|0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464|0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126|0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+---+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill('example',subset=['fr']).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|example| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000| 0.0|        0.0|0.324324324|example| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525| 0.0|        0.0|       0.53|example|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill(0,subset=['general']).show()\n\n\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|     0.0|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|     0.0|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|     0.0|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|     0.0|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|     0.0|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|     0.0|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|     0.0|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|     0.0|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|     0.0|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|     0.0|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|     0.0|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\n\nImputation of the Mean\n\nfrom pyspark.sql.functions import mean\nmean_val = df.select(mean(df['general'])).collect()\n\n\n\n\n\n\nmean_val[0][0]\n\n\nOut[19]: 309127.53846153844\n\n\n\nmean_gen = mean_val[0][0]\n\n\n\n\n\n\ndf.na.fill(mean_gen,[\"general\"]).show()\n\n\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific|           general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.na.fill(df.select(mean(df['general'])).collect()[0][0],['general']).show()\n\n\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n_c0|province| specific|           general|year|    gdp|   fdi| rnr|         rr|          i|     fr|        reg|     it|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\n  0|   Anhui| 147002.0|309127.53846153844|1996| 2093.3| 50661| 0.0|        0.0|        0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|309127.53846153844|1997|2347.32| 43443| 0.0|        0.0|        0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|309127.53846153844|1998|2542.96| 27673| 0.0|        0.0|        0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|309127.53846153844|1999|2712.34| 26131|null|       null|       null|1646891| East China|1227364|\n  4|   Anhui| 195580.0|           32100.0|2000|2902.09| 31847| 0.0|        0.0|        0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|309127.53846153844|2001|3246.71| 33672| 0.0|        0.0|        0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0|           66529.0|2002|3519.72| 38375| 0.0|        0.0|        0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0|           52108.0|2003|3923.11| 36720| 0.0|        0.0|        0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|          349699.0|2004| 4759.3| 54669| 0.0|        0.0|        0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|309127.53846153844|2005|5350.17| 69000| 0.0|        0.0|0.324324324|   null| East China|3874846|\n 10|   Anhui|1457872.0|          279052.0|2006| 6112.5|139354| 0.0|        0.0|0.324324324|3434548| East China|5167300|\n 11|   Anhui|2213991.0|          178705.0|2007|7360.92|299892| 0.0|        0.0|0.324324324|4468640| East China|7040099|\n 12| Beijing| 165957.0|309127.53846153844|1996| 1789.2|155290|null|       null|       null| 634562|North China| 508135|\n 13| Beijing| 165957.0|309127.53846153844|1997|2077.09|159286| 0.0|        0.0|        0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|309127.53846153844|1998|2377.18|216800| 0.0|        0.0|       0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|309127.53846153844|1999|2678.82|197525| 0.0|        0.0|       0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|          188633.0|2000|3161.66|168368| 0.0|        0.0|       0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|309127.53846153844|2001|3707.96|176818| 0.0|        0.0|       0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|          280277.0|2002| 4315.0|172464| 0.0|        0.0|       0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|          269596.0|2003|5007.21|219126| 0.0|0.794871795|        0.0|2823366|North China|1426600|\n+---+--------+---------+------------------+----+-------+------+----+-----------+-----------+-------+-----------+-------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2021-02-05-emails-python.html",
    "href": "posts/2021-02-05-emails-python.html",
    "title": "Python Email Example",
    "section": "",
    "text": "import csv, smtplib, ssl\nfrom datetime import date\nimport pandas as pd\nimport email, smtplib, ssl\nfrom email import encoders\nfrom email.mime.base import MIMEBase\nfrom email.mime.multipart import MIMEMultipart\nfrom email.mime.text import MIMEText\n\n\ntoday = date.today()\nprint(\"Today's date is\", today)\nprint(\"Date components\", today.year, today.month, today.day)\n\n\ndf = pd.read_csv(\"~/csvs/example.csv\")\n\n\nexample_filter_cols = df[\n    [\n        \"example\",\n        \"example\"\n    ]\n]\n\nexample_filter_cols\n\nfilter1 = df[\"month\"] == today.month\nfilter2 = df[\"day\"] == today.day\n\nexample_filter = filter1 & filter2\n\nexample_final_df = example_filter_cols[example_filter]\nexample_final_df\n\n\nexample_final_df_cols = example_final_df[\n    [\"full_name\", \"name\", \"birthday\", \"email\", \"phone\", \"gift_ideas\", \"address\"]\n]\n\n\nexample_final_df_cols.to_csv(\"example_final_df.csv\", index=False)\n\n\n\n# Email:\n\nsubject = \"Example\"\nbody = \"Example\"\n\n\n\nsender_email = \"email@gmail.com\"\nreceiver_email = \"email@gmail.com\"\n\n\n# Create a multipart message and set headers\nmessage = MIMEMultipart()\nmessage[\"From\"] = sender_email\nmessage[\"To\"] = receiver_email\nmessage[\"Subject\"] = subject\nmessage[\"Bcc\"] = receiver_email  # Recommended for mass emails\n\n# Add body to email\nmessage.attach(MIMEText(body, \"plain\"))\n\nfilename = \"example_final_df.csv\"  # In same directory as script\n\n# Open PDF file in binary mode\nwith open(filename, \"rb\") as attachment:\n    # Add file as application/octet-stream\n    # Email client can usually download this automatically as attachment\n    part = MIMEBase(\"application\", \"octet-stream\")\n    part.set_payload(attachment.read())\n\n# Encode file in ASCII characters to send by email\nencoders.encode_base64(part)\n\n# Add header as key/value pair to attachment part\npart.add_header(\n    \"Content-Disposition\",\n    f\"attachment; filename= {filename}\",\n)\n\n# Add attachment to message and convert message to string\nmessage.attach(part)\ntext = message.as_string()\n\n# Log in to server using secure context and send email\ncontext = ssl.create_default_context()\nwith smtplib.SMTP_SSL(\"smtp.gmail.com\", 465, context=context) as server:\n    server.login(sender_email, password)\n    server.sendmail(sender_email, receiver_email, text)"
  },
  {
    "objectID": "posts/2021-05-31-parameter-tuning.html",
    "href": "posts/2021-05-31-parameter-tuning.html",
    "title": "Parameter tuning",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2\n\n\nimport seaborn as sns\nsns.set_theme(context=\"notebook\", font_scale=1.4,\n              rc={\"figure.figsize\": [10, 6]})\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['it', 'Unnamed: 0'], axis = 1), df['it']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nX.fdi\n\n4        31847\n6        38375\n7        36720\n10      139354\n11      299892\n        ...   \n347      39453\n354     307610\n355     498055\n358     888935\n359    1036576\nName: fdi, Length: 118, dtype: int64\n\n\n\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nimport numpy as np\n\n\nparams = {\n    'max_depth': [2, 4, 8, 12, 16],\n    'max_features': [4, 8, 16, 32]\n}\n\n\ngrid_search = GridSearchCV(\n    RandomForestRegressor(random_state=42), param_grid=params,\n    verbose=1,\n    n_jobs=8)  # Update to the number of physical cpu cores\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 20 candidates, totalling 100 fits\n\n\n[Parallel(n_jobs=8)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=8)]: Done  34 tasks      | elapsed:    2.4s\n[Parallel(n_jobs=8)]: Done 100 out of 100 | elapsed:    3.6s finished\n\n\nGridSearchCVGridSearchCV(estimator=RandomForestRegressor(random_state=42), n_jobs=8,\n             param_grid={'max_depth': [2, 4, 8, 12, 16],\n                         'max_features': [4, 8, 16, 32]},\n             verbose=1)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\ngrid_search.best_score_\n\n0.6771979740200512\n\n\n\ngrid_search.best_params_\n\n{'max_depth': 12, 'max_features': 4}\n\n\n\ngrid_search.score(X_test, y_test)\n\n0.7816726342611063\n\n\n\nimport pandas as pd\ncv_df = pd.DataFrame(grid_search.cv_results_)\n\n\nres = (cv_df.pivot(index='param_max_depth', columns='param_max_features', values='mean_test_score')\n            .rename_axis(index='max_depth', columns='max_features'))\n\n\nimport seaborn as sns\n_ = sns.heatmap(res, cmap='viridis')\n\n\n\n\n\n# %load solutions/02-ex01-solutions.py\n\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint\n\nparam_dist = {\n    \"max_features\": randint(1, 11),\n    \"min_samples_split\": randint(2, 11)\n}\n\nrandom_search = RandomizedSearchCV(RandomForestRegressor(random_state=0),\n                                   param_distributions=param_dist,\n                                   verbose=1,\n                                   random_state=0)\n\nrandom_search.fit(X_train, y_train)\n\nrandom_search.best_params_\n\nrandom_search.best_score_\n\nrandom_search.score(X_test, y_test)\n\nfrom sklearn.svm import SVC\n\nsvm_grid = GridSearchCV(\n    SVC(random_state=42), param_grid={'kernel': ['linear', 'poly', 'rbf', 'sigmoid']},\n    verbose=1, n_jobs=8\n)\n\nsvm_grid.fit(X_train, y_train)\n\nsvm_grid.best_score_\n\nsvm_grid.best_params_\n\nsvm_grid.score(X_test, y_test)\n\nFitting 5 folds for each of 10 candidates, totalling 50 fits\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in &lt;listcomp&gt;\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in &lt;listcomp&gt;\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in &lt;listcomp&gt;\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in &lt;listcomp&gt;\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:548: FitFailedWarning: Estimator fit failed. The score on this train-test partition for these parameters will be set to nan. Details: \nTraceback (most recent call last):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py\", line 531, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 386, in fit\n    trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 1048, in __call__\n    if self.dispatch_one_batch(iterator):\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 866, in dispatch_one_batch\n    self._dispatch(tasks)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 784, in _dispatch\n    job = self._backend.apply_async(batch, callback=cb)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 208, in apply_async\n    result = ImmediateResult(func)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/_parallel_backends.py\", line 572, in __init__\n    self.results = batch()\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in __call__\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/joblib/parallel.py\", line 262, in &lt;listcomp&gt;\n    return [func(*args, **kwargs)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\", line 168, in _parallel_build_trees\n    tree.fit(X, y, sample_weight=curr_sample_weight, check_input=False)\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 1242, in fit\n    super().fit(\n  File \"/home/david/anaconda3/lib/python3.8/site-packages/sklearn/tree/_classes.py\", line 279, in fit\n    raise ValueError(\"max_features must be in (0, n_features]\")\nValueError: max_features must be in (0, n_features]\n\n  warnings.warn(\"Estimator fit failed. The score on this train-test\"\n[Parallel(n_jobs=1)]: Done  50 out of  50 | elapsed:    4.7s finished\n\n\nValueError: n_splits=5 cannot be greater than the number of members in each class."
  },
  {
    "objectID": "posts/2020-10-08-Dask-Day-1.html",
    "href": "posts/2020-10-08-Dask-Day-1.html",
    "title": "Using Dask for Arrays",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask.\n\nimport numpy as np\nimport dask.array as da\n\n\nnp_arr = np.random.randint(20, size=20)\nnp_arr\n\narray([12, 18, 17,  7,  5,  9, 11,  3,  5, 15, 13, 13,  5, 12, 11, 16,  4,\n       10,  9,  7])\n\n\n\ndask_arr = da.random.randint(20, size=20, chunks=5)\n\n\ndask_arr\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n160 B\n40 B\n\n\nShape\n(20,)\n(5,)\n\n\nCount\n4 Tasks\n4 Chunks\n\n\nType\nint64\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\n## This is simply because Dask does lazy evaluaion.   \n### You need to call `compute()` to start the execution\n\n\ndask_arr.compute()\n\narray([ 3, 17,  5, 11, 19, 14, 14, 11,  9, 18,  9,  7, 10, 13, 10, 10, 11,\n       10,  9,  2])\n\n\n\ndask_arr.chunks\n\n((5, 5, 5, 5),)\n\n\n\ndask_arr_from_np = da.from_array(np_arr, chunks=5)\n\n\ndask_arr_from_np\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n160 B\n40 B\n\n\nShape\n(20,)\n(5,)\n\n\nCount\n5 Tasks\n4 Chunks\n\n\nType\nint64\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\ndask_arr_from_np.compute()\n\narray([12, 18, 17,  7,  5,  9, 11,  3,  5, 15, 13, 13,  5, 12, 11, 16,  4,\n       10,  9,  7])\n\n\n\n### array operations into a graph to tasks\n#### See : http://docs.dask.org/en/latest/graphviz.html\n\n\ndask_arr_from_np.sum().visualize()\n\n\n\n\n\ndask_arr_from_np.sum().visualize(rankdir=\"LR\")\n\n\n\n\n\n(dask_arr_from_np+1).visualize(rankdir=\"LR\")\n\n\n\n\n\ndask_arr_mean = da.mean(dask_arr_from_np)\ndask_arr_mean.compute()\n\n10.1\n\n\n\ndask_arr_mean.visualize(rankdir=\"LR\")\n\n\n\n\n\nx = da.random.random(10, chunks=2)\ny = da.random.random(10, chunks=2)\n\nsum_x_y = da.add(x, y) #similar to numpy.add\nmean_x_y = da.mean(sum_x_y)\n\n\nsum_x_y.compute()\n\narray([0.96028343, 0.55946179, 1.11161829, 1.28233368, 0.53130934,\n       0.86805782, 0.20173099, 0.77596276, 0.92576765, 1.04750609])\n\n\n\nsum_x_y.visualize()\n\n\n\n\n\nmean_x_y.visualize()\n\n\n\n\n\nda_arr_large = da.random.randint(10000, size=(50000, 50000),\n                                 chunks=(5000, 1000))   \nda_sum_large = da_arr_large.sum()   \n\n\n### Get no. bytes using `nbytes` : http://docs.dask.org/en/latest/array-api.html#dask.array.Array.nbytes\n\n\nda_arr_large.nbytes  \n\n20000000000\n\n\n\n### Convert bytes to GB, 1Gb = 1e+9 bytes\n\n\nda_arr_large.nbytes/1e+9\n\n20.0\n\n\n\nda_sum_large.compute()\n\n12498643590734\n\n\n\n# Dask 2\n\n\nsize_tuple = (500,500)\nchunks_tuple = (10,500)\n\n\nda_arr = da.random.randint(10, size=size_tuple,\n                           chunks=chunks_tuple)\nda_arr2 = da.random.randint(10, size=size_tuple,\n                            chunks=chunks_tuple)\n\n\ndef random_func(x):\n    return np.mean((((x * 2).T)**2),axis=0)\n\n\ngufoo = da.gufunc(random_func, signature=\"(i)-&gt;()\",\n                  output_dtypes=float,\n                  vectorize=True)\n\n\nrandom_op_arr = gufoo(da_arr)\nrandom_op_arr.compute()\n\narray([112.056, 107.44 , 111.024, 109.656, 118.832, 109.84 , 117.2  ,\n       111.952, 116.312, 117.368, 128.568, 111.144, 110.656, 112.648,\n       115.24 , 114.624, 113.912, 109.632, 112.864, 113.488, 119.248,\n       121.4  , 108.272, 118.784, 114.968, 115.216, 107.872, 113.6  ,\n       112.456, 112.48 , 114.864, 119.28 , 112.656, 110.208, 109.728,\n       120.576, 119.632, 118.12 , 112.888, 116.384, 113.192, 106.84 ,\n       111.72 , 115.928, 106.08 , 114.568, 121.512, 115.384, 113.864,\n       107.104, 114.32 , 116.176, 117.28 , 116.976, 117.784, 110.088,\n       121.696, 114.2  , 113.864, 116.072, 112.344, 113.808, 113.968,\n       110.472, 119.536, 113.84 , 109.328, 116.552, 119.056, 113.84 ,\n       117.872, 114.928, 116.336, 115.192, 115.808, 106.984, 116.984,\n       114.536, 116.496, 111.968, 115.216, 108.24 , 119.52 , 116.136,\n       111.144, 111.712, 119.224, 114.312, 110.464, 110.216, 111.288,\n       119.6  , 108.264, 114.456, 119.016, 107.032, 114.832, 108.056,\n       105.712, 110.64 , 103.4  , 106.768, 118.216, 112.44 , 113.728,\n       114.6  , 117.832, 108.288, 117.92 , 113.12 , 121.984, 112.776,\n       123.144, 115.968, 112.44 , 115.712, 112.144, 108.448, 114.752,\n       108.376, 101.296, 102.992, 117.872, 114.056, 115.736, 115.528,\n       122.072, 130.168, 106.992, 109.912, 117.872, 112.152, 112.184,\n       113.544, 116.496, 112.832, 108.712, 116.96 , 120.984, 117.808,\n       112.272, 111.816, 118.872, 116.376, 118.992, 112.344, 124.672,\n        97.576, 112.496, 117.92 , 102.392, 109.992, 112.016, 117.92 ,\n       108.352, 112.376, 121.008, 117.808, 113.504, 125.592, 114.936,\n       111.456, 116.488, 104.744, 114.136, 114.   , 107.256, 117.84 ,\n       111.872, 109.152, 118.752, 112.32 , 116.16 , 106.696, 109.472,\n       111.968, 118.264, 115.088, 112.864, 110.016, 111.888, 111.84 ,\n       118.488, 107.952, 121.52 , 126.52 , 112.12 , 110.952, 115.328,\n       110.064, 106.36 , 118.96 , 109.68 , 117.776, 107.112, 111.152,\n       113.888, 113.408, 114.992, 117.632, 116.648, 117.112, 118.2  ,\n       116.36 , 113.104, 113.6  , 112.208, 112.592, 117.192, 102.832,\n       112.08 , 113.744, 116.048, 117.368, 113.96 , 111.24 , 121.824,\n       112.56 , 110.192, 130.776, 111.656, 119.984, 113.592, 113.592,\n       106.664, 125.192, 113.6  , 117.12 , 106.24 , 112.856, 114.544,\n       117.16 , 108.344, 112.208, 109.112, 124.824, 109.824, 106.352,\n       115.568, 112.64 , 112.904, 112.736, 112.52 , 124.808, 120.32 ,\n       114.472, 119.528, 113.456, 112.448, 118.672, 110.016, 116.16 ,\n       122.048, 111.088, 114.56 , 107.448, 115.328, 111.656, 108.688,\n       116.904, 110.8  , 108.896, 112.136, 115.896, 111.848, 108.808,\n       114.504, 124.552, 116.248, 114.576, 110.56 , 112.152, 117.576,\n       125.44 , 110.72 , 108.072, 115.192, 116.048, 107.76 , 111.376,\n       121.608, 115.256, 113.84 , 105.672, 115.024, 115.864, 114.304,\n       123.344, 114.624, 115.696, 113.288, 116.688, 109.048, 125.264,\n       118.8  , 112.2  , 114.312, 109.728, 116.064, 113.808, 106.912,\n       109.288, 117.   , 114.632, 114.456, 110.168, 111.976, 117.816,\n       110.04 , 103.048, 113.656, 112.504, 113.8  , 120.04 , 120.224,\n       110.68 , 110.096, 116.12 , 113.424, 107.408, 111.296, 111.512,\n       117.432, 105.96 , 115.992, 118.44 , 110.024, 119.216, 111.664,\n       119.184, 109.824, 116.736, 116.76 , 107.544, 120.44 , 115.08 ,\n       110.136, 112.144, 113.888, 111.32 , 109.952, 117.096, 111.152,\n       115.728, 110.832, 113.312, 113.664, 112.016, 111.952, 114.896,\n       114.728, 107.848, 108.832, 122.384, 111.824, 107.384, 117.504,\n       117.344, 110.144, 109.568, 101.36 , 111.944, 105.512, 115.792,\n       112.08 , 104.568, 109.008, 108.992, 114.936, 113.008, 120.088,\n       117.328, 117.008, 107.584, 111.688, 115.664, 108.416, 119.48 ,\n       107.336, 120.184, 111.952, 115.824, 113.928, 117.064, 114.296,\n       111.56 , 120.04 , 112.256, 115.368, 109.112, 112.184, 112.128,\n       111.288, 117.856, 109.184, 113.128, 119.888, 110.656, 111.992,\n       116.704, 107.696, 111.608, 121.504, 110.296, 111.008, 112.072,\n       117.072, 115.68 , 108.888, 117.704, 113.112, 101.144, 112.36 ,\n       122.688, 112.016, 111.64 , 113.992, 117.08 , 109.976, 108.048,\n       110.504, 112.936, 111.776, 117.392, 116.568, 106.896, 105.224,\n       115.512, 117.   , 116.192, 113.344, 111.776, 114.312, 113.008,\n       114.768, 121.712, 112.528, 108.976, 106.648, 107.8  , 122.696,\n       104.064, 117.072, 119.064, 111.472, 112.752, 109.52 , 123.712,\n       114.032, 120.888, 109.84 , 123.36 , 111.576, 118.56 , 116.328,\n       113.048, 111.68 , 106.072, 109.752, 112.32 , 114.344, 114.976,\n       114.072, 121.792, 113.024, 109.864, 115.84 , 115.752, 118.648,\n       107.52 , 116.104, 112.464, 123.232, 112.32 , 116.952, 106.32 ,\n       110.992, 111.256, 113.616, 111.344, 115.216, 121.504, 117.504,\n       115.816, 116.6  , 111.08 , 108.776, 110.672, 109.464, 107.096,\n       112.928, 106.8  , 110.4  , 112.576, 114.648, 113.272, 112.504,\n       112.888, 112.84 , 111.496])\n\n\n\nrandom_op_arr.shape\n\n(500,)\n\n\n\n@da.as_gufunc(signature=\"(m,n),(n,j)-&gt;(m,j)\", output_dtypes=int, allow_rechunk=True)\ndef random_func(x, y):\n    return np.matmul(x, y)**2\n\n\nda_arr3 = da.random.randint(10, size=(200, 100), chunks=(10, 100))\nda_arr4 = da.random.randint(10, size=(100, 300), chunks=(5,5))\n\n\n# random_matmul = random_func(da_arr3, da_arr4)\n# random_matmul.compute()\n\n\nrandom_matmul.shape\n\n(200, 300)\n\n\n\n# Dask 3\n\n\nmy_arr = da.random.randint(10, size=20, chunks=3)\n\n\nmy_arr.compute()\n\narray([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5])\n\n\n\nmy_hundred_arr = my_arr + 100\nmy_hundred_arr.compute()\n\narray([103, 108, 108, 107, 104, 101, 105, 107, 102, 107, 104, 104, 108,\n       100, 109, 103, 106, 107, 101, 105])\n\n\n\n(my_arr * (-1)).compute()\n\narray([-3, -8, -8, -7, -4, -1, -5, -7, -2, -7, -4, -4, -8,  0, -9, -3, -6,\n       -7, -1, -5])\n\n\n\ndask_sum = my_arr.sum()\ndask_sum\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n8 B\n8 B\n\n\nShape\n()\n()\n\n\nCount\n17 Tasks\n1 Chunks\n\n\nType\nint64\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\nmy_arr.compute()\n\narray([3, 8, 8, 7, 4, 1, 5, 7, 2, 7, 4, 4, 8, 0, 9, 3, 6, 7, 1, 5])\n\n\n\ndask_sum.compute()\n\n99\n\n\n\nmy_ones_arr = da.ones((10,10), chunks=2, dtype=int)\n\n\nmy_ones_arr.compute()\n\narray([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n\n\n\nmy_ones_arr.mean(axis=0).compute()\n\narray([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])\n\n\n\nmy_custom_array = da.random.randint(10, size=(4,4), chunks=(1,4))\n\n\nmy_custom_array.compute()\n\narray([[0, 1, 7, 6],\n       [0, 1, 2, 4],\n       [6, 3, 5, 3],\n       [3, 2, 2, 6]])\n\n\n\nmy_custom_array.mean(axis=0).compute()\n\narray([2.25, 1.75, 4.  , 4.75])\n\n\n\nmy_custom_array.mean(axis=1).compute()\n\narray([3.5 , 1.75, 4.25, 3.25])\n\n\n\n## Slicing\n\n\nmy_custom_array[1:3, 2:4]\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n32 B\n16 B\n\n\nShape\n(2, 2)\n(1, 2)\n\n\nCount\n6 Tasks\n2 Chunks\n\n\nType\nint64\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\nmy_custom_array[1:3, 2:4].compute()\n\narray([[2, 4],\n       [5, 3]])\n\n\n\n## Broadcasting\n\n\nmy_custom_array.compute()\n\narray([[0, 1, 7, 6],\n       [0, 1, 2, 4],\n       [6, 3, 5, 3],\n       [3, 2, 2, 6]])\n\n\n\nmy_small_arr = da.ones(4, chunks=2)\nmy_small_arr.compute()\n\narray([1., 1., 1., 1.])\n\n\n\nbrd_example1 = da.add(my_custom_array, my_small_arr)\n\n\nbrd_example1.compute()\n\narray([[1., 2., 8., 7.],\n       [1., 2., 3., 5.],\n       [7., 4., 6., 4.],\n       [4., 3., 3., 7.]])\n\n\n\nten_arr = da.full_like(my_small_arr, 10)\n\n\nten_arr.compute()\n\narray([10., 10., 10., 10.])\n\n\n\nbrd_example2 = da.add(my_custom_array, ten_arr)\n\n\nbrd_example2.compute()\n\narray([[10., 11., 17., 16.],\n       [10., 11., 12., 14.],\n       [16., 13., 15., 13.],\n       [13., 12., 12., 16.]])\n\n\n\n## Reshaping\n\n\nmy_custom_array.shape\n\n(4, 4)\n\n\n\ncustom_arr_1d = my_custom_array.reshape(16)\n\n\ncustom_arr_1d\n\n\n\n\n\n\n\n\n\n\n\n\nArray\nChunk\n\n\n\n\nBytes\n128 B\n32 B\n\n\nShape\n(16,)\n(4,)\n\n\nCount\n8 Tasks\n4 Chunks\n\n\nType\nint64\nnumpy.ndarray\n\n\n\n\n\n\n\n\n\n\ncustom_arr_1d.compute()\n\narray([0, 1, 7, 6, 0, 1, 2, 4, 6, 3, 5, 3, 3, 2, 2, 6])\n\n\n\n# Stacking\n\n\nstacked_arr = da.stack([brd_example1, brd_example2])\n\n\nstacked_arr.compute()\n\narray([[[ 1.,  2.,  8.,  7.],\n        [ 1.,  2.,  3.,  5.],\n        [ 7.,  4.,  6.,  4.],\n        [ 4.,  3.,  3.,  7.]],\n\n       [[10., 11., 17., 16.],\n        [10., 11., 12., 14.],\n        [16., 13., 15., 13.],\n        [13., 12., 12., 16.]]])\n\n\n\nanother_stacked = da.stack([brd_example1, brd_example2], axis=1)\n\n\nanother_stacked.compute()\n\narray([[[ 1.,  2.,  8.,  7.],\n        [10., 11., 17., 16.]],\n\n       [[ 1.,  2.,  3.,  5.],\n        [10., 11., 12., 14.]],\n\n       [[ 7.,  4.,  6.,  4.],\n        [16., 13., 15., 13.]],\n\n       [[ 4.,  3.,  3.,  7.],\n        [13., 12., 12., 16.]]])\n\n\n\n# Concatenate\n\n\nconcate_arr = da.concatenate([brd_example1, brd_example2])\n\n\nconcate_arr.compute()\n\narray([[ 1.,  2.,  8.,  7.],\n       [ 1.,  2.,  3.,  5.],\n       [ 7.,  4.,  6.,  4.],\n       [ 4.,  3.,  3.,  7.],\n       [10., 11., 17., 16.],\n       [10., 11., 12., 14.],\n       [16., 13., 15., 13.],\n       [13., 12., 12., 16.]])\n\n\n\nanother_concate_arr = da.concatenate([brd_example1, brd_example2],axis=1)\n\n\nanother_concate_arr.compute()\n\narray([[ 1.,  2.,  8.,  7., 10., 11., 17., 16.],\n       [ 1.,  2.,  3.,  5., 10., 11., 12., 14.],\n       [ 7.,  4.,  6.,  4., 16., 13., 15., 13.],\n       [ 4.,  3.,  3.,  7., 13., 12., 12., 16.]])\n\n\n\n# Dask 4\n\n\nimport numpy as np\nimport dask.array as da\n\n\nsize_tuple = (18000,18000)\nnp_arr = np.random.randint(10, size=size_tuple)\nnp_arr2 = np.random.randint(10, size=size_tuple)\n\n\n%time (((np_arr * 2).T)**2 + np_arr2 + 100).sum(axis=1).mean()\n\nMemoryError: \n\n\n\nchunks_tuple = (500, 500)\nda_arr = da.from_array(np_arr, chunks=chunks_tuple)\nda_arr2 = da.from_array(np_arr2, chunks=chunks_tuple)\n\n\n%time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute()\n\nCPU times: user 10.1 s, sys: 362 ms, total: 10.5 s\nWall time: 2.47 s\n\n\n3933124.5174444444\n\n\n\nsize_tuple = (50000, 50000)\nnp_arr = np.random.randint(10, size=size_tuple)\nnp_arr2 = np.random.randint(10, size=size_tuple)\n\nMemoryError: \n\n\n\nchunks_tuple = (5000, 5000)\nda_arr = da.random.randint(10, size=size_tuple,\n                           chunks=chunks_tuple)\nda_arr2 = da.random.randint(10, size=size_tuple,\n                            chunks=chunks_tuple)\n\n\n%time (((da_arr * 2).T)**2 + da_arr2 + 100).sum(axis=1).mean().compute()\n\nCPU times: user 3min 10s, sys: 10.5 s, total: 3min 20s\nWall time: 28.2 s\n\n\n10925051.41748\n\n\n\nda_arr.nbytes/1e+9\n\n20.0"
  },
  {
    "objectID": "posts/2020-10-14-dask-xgboost-HT-OHE.html",
    "href": "posts/2020-10-14-dask-xgboost-HT-OHE.html",
    "title": "Using dask_ml.preprocessing and OneHotEncoder for categorical encoding with Dask",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\n\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine(\"sqlite:///fiscal.db\")\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\n#engine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n\n\nsql = \"\"\"\nSELECT year\n, region\n, province\n, gdp\n, fdi\n, it\n, specific\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/9390/1\nDashboard: http://192.168.1.71:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nclient.restart()\n\n\n\n\n\n\n\n\nClient\n\nScheduler: inproc://192.168.1.71/9390/1\nDashboard: http://192.168.1.71:8787/status\n\nCluster\n\nWorkers: 3\nCores: 6\nMemory: 12.00 GB\n\n\n\n\n\n\n\nfrom dask import dataframe as dd \n\n\nddf = dd.from_pandas(df, npartitions=5)\n\n\nprint(ddf)\n\nDask DataFrame Structure:\n                year  region province      gdp    fdi     it specific\nnpartitions=5                                                        \n0              int64  object   object  float64  int64  int64  float64\n72               ...     ...      ...      ...    ...    ...      ...\n...              ...     ...      ...      ...    ...    ...      ...\n288              ...     ...      ...      ...    ...    ...      ...\n359              ...     ...      ...      ...    ...    ...      ...\nDask Name: from_pandas, 5 tasks\n\n\n\nddf.head()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nEast China\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nEast China\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nEast China\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nEast China\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n\n\n\n\n\n\nclient.id\n\n'Client-0ac0cc94-0e22-11eb-a4ae-d71460f30774'\n\n\n\n# Selecting Features and Target\n\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"region\", \"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\n#OHE\nfrom dask_ml.preprocessing import OneHotEncoder\n\n\nddf = ddf.categorize(cat_feat_list)\n\n\nohe = OneHotEncoder(sparse=False)\n\n\nohe_ddf = ohe.fit_transform(ddf[cat_feat_list])\n\n\nfeat_list = feat_list + ohe_ddf.columns.tolist()\nfeat_list = [f for f in feat_list if f not in cat_feat_list]\n#client.close()\n\n\nddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target])\n\n\nddf_processed.compute()\n\n\n\n\n\n\n\n\nyear\nfdi\nregion_East China\nregion_North China\nregion_Southwest China\nregion_Northwest China\nregion_South Central China\nregion_Northeast China\nprovince_Anhui\nprovince_Beijing\n...\nprovince_Shandong\nprovince_Shanghai\nprovince_Shanxi\nprovince_Sichuan\nprovince_Tianjin\nprovince_Tibet\nprovince_Xinjiang\nprovince_Yunnan\nprovince_Zhejiang\ngdp\n\n\n\n\n0\n1996\n50661.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2093.30\n\n\n1\n1997\n43443.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2347.32\n\n\n2\n1998\n27673.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2542.96\n\n\n3\n1999\n26131.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2712.34\n\n\n4\n2000\n31847.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2902.09\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\n498055.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n9705.02\n\n\n356\n2004\n668128.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n11648.70\n\n\n357\n2005\n772000.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n13417.68\n\n\n358\n2006\n888935.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n15718.47\n\n\n359\n2007\n1036576.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n18753.73\n\n\n\n\n360 rows × 39 columns\n\n\n\n\nclient.restart()\n\n\nclient.close()"
  },
  {
    "objectID": "posts/2020-09-22-AnalyzingUSUnemployment.html",
    "href": "posts/2020-09-22-AnalyzingUSUnemployment.html",
    "title": "Analyzing US Unemployment From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n1959-03-31\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1959-06-30\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n1959-09-30\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n1959-12-31\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n1960-03-31\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\ndf['unemp'].plot()\nplt.ylabel(\"unemp\")\n\nText(0, 0.5, 'unemp')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\nunemp_cycle, unemp_trend = sm.tsa.filters.hpfilter(df.unemp)\n\nunemp_cycle\n\n1959-03-31    0.011338\n1959-06-30   -0.702548\n1959-09-30   -0.516441\n1959-12-31   -0.229910\n1960-03-31   -0.642198\n                ...   \n2008-09-30   -0.481666\n2008-12-31    0.198598\n2009-03-31    1.171440\n2009-06-30    2.040247\n2009-09-30    2.207674\nName: unemp, Length: 203, dtype: float64\n\n\n\ntype(unemp_cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = unemp_trend\n\ndf[['trend','unemp']].plot(figsize = (12, 8))\n\ndf[['trend','unemp']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fdb5c2ecdd8&gt;"
  },
  {
    "objectID": "posts/2020-08-29-Clustering with Pyspark.html",
    "href": "posts/2020-08-29-Clustering with Pyspark.html",
    "title": "Clustering with Pyspark",
    "section": "",
    "text": "This post includes code from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks.\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler\nfeat_cols = ['year', 'gdp', 'fdi','fr']\nfeat_cols = ['gdp']\nvec_assembler = VectorAssembler(inputCols = feat_cols, outputCol='features')\nfinal_df = vec_assembler.transform(df)"
  },
  {
    "objectID": "posts/2020-08-29-Clustering with Pyspark.html#using-the-standardscaler",
    "href": "posts/2020-08-29-Clustering with Pyspark.html#using-the-standardscaler",
    "title": "Clustering with Pyspark",
    "section": "Using the StandardScaler",
    "text": "Using the StandardScaler\n\nfrom pyspark.ml.feature import StandardScaler\n\n\n\n\n\n\nscaler = StandardScaler(inputCol=\"features\", outputCol=\"scaledFeatures\", withStd=True, withMean=False)"
  },
  {
    "objectID": "posts/2020-08-29-Clustering with Pyspark.html#fitting-the-standardscaler",
    "href": "posts/2020-08-29-Clustering with Pyspark.html#fitting-the-standardscaler",
    "title": "Clustering with Pyspark",
    "section": "Fitting the StandardScaler",
    "text": "Fitting the StandardScaler\n\n# Compute summary statistics by fitting the StandardScaler\nscalerModel = scaler.fit(final_df)\n\n\n\n\n\n\n# Normalize each feature to have unit standard deviation.\ncluster_final_data = scalerModel.transform(final_df)\n\n\n\n\n\n\nkmeans3 = KMeans(featuresCol='scaledFeatures',k=3)\nkmeans2 = KMeans(featuresCol='scaledFeatures',k=2)\n\n\n\n\n\n\nmodel_k3 = kmeans3.fit(cluster_final_data)\nmodel_k2 = kmeans2.fit(cluster_final_data)\n\n\n\n\n\n\nmodel_k3.transform(cluster_final_data).groupBy('prediction').count().show()\n\n\n+----------+-----+\nprediction|count|\n+----------+-----+\n         1|   15|\n         2|   86|\n         0|  259|\n+----------+-----+\n\n\n\n\n\nmodel_k2.transform(cluster_final_data).groupBy('prediction').count().show()\n\n\n+----------+-----+\nprediction|count|\n+----------+-----+\n         1|  308|\n         0|   52|\n+----------+-----+"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html",
    "href": "posts/2021-06-02-model-calibration.html",
    "title": "Model Calibration",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport sklearn\n\nplt.rcParams['font.size'] = 16\nplt.rcParams['figure.figsize'] = [12, 8]\nplt.rcParams['savefig.bbox'] = 'tight'\nplt.rcParams[\"savefig.dpi\"] = 300\n\nsklearn.set_config(display='diagram')\ndef plot_calibration_curve(y_true, y_prob, n_bins=5, ax=None, hist=True, normalize=False):\n    prob_true, prob_pred = calibration_curve(y_true, y_prob, n_bins=n_bins, normalize=normalize)\n    if ax is None:\n        ax = plt.gca()\n    if hist:\n        ax.hist(y_prob, weights=np.ones_like(y_prob) / len(y_prob), alpha=.4,\n               bins=np.maximum(10, n_bins))\n    ax.plot([0, 1], [0, 1], ':', c='k')\n    curve = ax.plot(prob_pred, prob_true, marker=\"o\")\n\n    ax.set_xlabel(\"predicted probability\")\n    ax.set_ylabel(\"fraction of positive samples\")\n\n    ax.set(aspect='equal')\n    return curve"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html#create-dummy-dataset",
    "href": "posts/2021-06-02-model-calibration.html#create-dummy-dataset",
    "title": "Model Calibration",
    "section": "Create dummy dataset",
    "text": "Create dummy dataset\n\n# %load solutions/classifier_example.py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7139\n           1       0.17      0.01      0.01       129\n\n    accuracy                           0.98      7268\n   macro avg       0.57      0.50      0.50      7268\nweighted avg       0.97      0.98      0.97      7268\n\n\n\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\nTrain linear model\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\n\nlr = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\nlr.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('logisticregression', LogisticRegression(random_state=42))])StandardScalerStandardScaler()LogisticRegressionLogisticRegression(random_state=42)"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html#calibration-curve",
    "href": "posts/2021-06-02-model-calibration.html#calibration-curve",
    "title": "Model Calibration",
    "section": "Calibration curve",
    "text": "Calibration curve\n\nfrom sklearn.calibration import calibration_curve\n\n\nlr_proba = lr.predict_proba(X_test)\n\n\nprob_true, prod_pred = calibration_curve(y_test, lr_proba[:, 1], n_bins=5)\n\nprint(prob_true)\nprint(prod_pred)\n\n[0.01750517 0.15384615]\n[0.01906858 0.22387283]\n\n\n\nplot_calibration_curve(y_test, lr_proba[:, 1]);\n\n\n\n\n\nfrom sklearn.metrics import brier_score_loss\n\n\nlr_brier = brier_score_loss(y_test, lr_proba[:, 1])\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\nplot_calibration_curve(y_test, lr_proba[:, 1], n_bins=5, ax=ax1)\nax1.set_title(\"n_bins=5\")\nplot_calibration_curve(y_test, lr_proba[:, 1], n_bins=10, ax=ax2)\nax2.set_title(\"n_bins=10\")\nplot_calibration_curve(y_test, lr_proba[:, 1], n_bins=30, ax=ax3)\nax3.set_title(\"n_bins=30\")\n\nText(0.5, 1.0, 'n_bins=30')\n\n\n\n\n\n\nTrain Random Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nrf = RandomForestClassifier(random_state=0)\nrf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\nrf_proba = rf.predict_proba(X_test)\n\n\nrf_brier = brier_score_loss(y_test, rf_proba[:, 1])\nrf_brier\n\n0.0188862960924601\n\n\n\n\nTrain Single Tree\n\nfrom sklearn.tree import DecisionTreeClassifier\n\n\ntree = DecisionTreeClassifier(random_state=0)\ntree.fit(X_train, y_train)\n\nDecisionTreeClassifierDecisionTreeClassifier(random_state=0)\n\n\n\ntree_proba = tree.predict_proba(X_test)\n\n\ntree_brier = brier_score_loss(y_test, tree_proba[:, 1])\ntree_brier\n\n0.03880022014309301\n\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\nplot_calibration_curve(y_test, lr_proba[:, 1], n_bins=10, ax=ax1)\nax1.set_title(f\"LogisticRegression: {lr_brier:0.4f}\")\nplot_calibration_curve(y_test, tree_proba[:, 1], n_bins=10, ax=ax2)\nax2.set_title(f\"DecisionTreeClassifier: {tree_brier:0.4f}\")\nplot_calibration_curve(y_test, rf_proba[:, 1], n_bins=10, ax=ax3)\nax3.set_title(f\"RandomForestClassifier: {rf_brier:0.4f}\");"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html#exercise-1",
    "href": "posts/2021-06-02-model-calibration.html#exercise-1",
    "title": "Model Calibration",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nTrain a sklearn.naive_bayes.GaussianNB on the training set.\nCompute the brier score loss on the test set for the GuassianNB.\nPlot the calibration curve with n_bins=10.\n\n\n# %load solutions/02-ex01-solutions.py\nfrom sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB().fit(X_train, y_train)\n\nnb_proba = nb.predict_proba(X_test)\n\nbrier_score_loss(y_test, nb_proba[:, 1])\n\nplot_calibration_curve(y_test, nb_proba[:, 1], n_bins=10)"
  },
  {
    "objectID": "posts/2021-06-02-model-calibration.html#calibration",
    "href": "posts/2021-06-02-model-calibration.html#calibration",
    "title": "Model Calibration",
    "section": "Calibration",
    "text": "Calibration\n\nfrom sklearn.calibration import CalibratedClassifierCV\n\n\nrf = RandomForestClassifier(random_state=0)\ncal_rf = CalibratedClassifierCV(rf, method=\"isotonic\")\ncal_rf.fit(X_train, y_train)\n\nCalibratedClassifierCVCalibratedClassifierCV(base_estimator=RandomForestClassifier(random_state=0),\n                       method='isotonic')RandomForestClassifierRandomForestClassifier(random_state=0)\n\n\n\ncal_rf_proba = cal_rf.predict_proba(X_test)\n\n\ncal_rf_brier = brier_score_loss(y_test, cal_rf_proba[:, 1])\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nplot_calibration_curve(y_test, rf_proba[:, 1], ax=ax1, n_bins=10)\nax1.set_title(f\"forest no calibration: {rf_brier:0.4f}\")\nplot_calibration_curve(y_test, cal_rf_proba[:, 1], ax=ax2, n_bins=10)\nax2.set_title(f\"calibrated: {cal_rf_brier:0.4f}\");\n\n\n\n\n\nCalibrating the linear model\n\nlr = make_pipeline(StandardScaler(), LogisticRegression(random_state=42))\ncal_lr = CalibratedClassifierCV(lr, method='isotonic')\ncal_lr.fit(X_train, y_train)\n\nCalibratedClassifierCVCalibratedClassifierCV(base_estimator=Pipeline(steps=[('standardscaler',\n                                                       StandardScaler()),\n                                                      ('logisticregression',\n                                                       LogisticRegression(random_state=42))]),\n                       method='isotonic')StandardScalerStandardScaler()LogisticRegressionLogisticRegression(random_state=42)\n\n\n\ncal_lr_proba = cal_lr.predict_proba(X_test)\n\ncal_lr_brier = brier_score_loss(y_test, cal_lr_proba[:, 1])\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nplot_calibration_curve(y_test, lr_proba[:, 1], ax=ax1, n_bins=10)\nax1.set_title(f\"no calibration: {lr_brier:0.4f}\")\nplot_calibration_curve(y_test, cal_lr_proba[:, 1], ax=ax2, n_bins=10)\nax2.set_title(f\"calibrated: {cal_lr_brier:0.4f}\");"
  },
  {
    "objectID": "posts/2020-10-02-databases_sqllite_sqlalchemy_27-Copy1.html",
    "href": "posts/2020-10-02-databases_sqllite_sqlalchemy_27-Copy1.html",
    "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite",
    "section": "",
    "text": "This post includes code adapted from SQLAlchemy Example.\n\nimport sqlalchemy as db\nimport sqlite3\nimport pandas as pd\n\n\nfrom sqlalchemy import Column, Integer, String, ForeignKey, create_engine\nfrom sqlalchemy.ext.declarative import declarative_base\nfrom sqlalchemy.orm import relationship, backref, sessionmaker, joinedload\n\n# For this example we will use an in-memory sqlite DB.\n# Let's also configure it to echo everything it does to the screen.\nengine = create_engine('sqlite:///:memory:', echo=True)\n\n\n# The base class which our objects will be defined on.\nBase = declarative_base()\n\n# Our User object, mapped to the 'users' table\nclass User(Base):\n    __tablename__ = 'users'\n\n    # Every SQLAlchemy table should have a primary key named 'id'\n    id = Column(Integer, primary_key=True)\n\n    name = Column(String)\n    fullname = Column(String)\n    password = Column(String)\n\n    # Lets us print out a user object conveniently.\n    def __repr__(self):\n       return \"&lt;User(name='%s', fullname='%s', password'%s')&gt;\" % (\n                               self.name, self.fullname, self.password)\n\n\n# The Address object stores the addresses \n# of a user in the 'adressess' table.\nclass Address(Base):\n    __tablename__ = 'addresses'\n    id = Column(Integer, primary_key=True)\n    email_address = Column(String, nullable=False)\n\n    # Since we have a 1:n relationship, we need to store a foreign key \n    # to the users table.\n    user_id = Column(Integer, ForeignKey('users.id'))\n\n    # Defines the 1:n relationship between users and addresses.\n    # Also creates a backreference which is accessible from a User object.\n    user = relationship(\"User\", backref=backref('addresses'))\n\n    # Lets us print out an address object conveniently.\n    def __repr__(self):\n        return \"&lt;Address(email_address='%s')&gt;\" % self.email_address\n\n\n\n# Create all tables by issuing CREATE TABLE commands to the DB.\nBase.metadata.create_all(engine) \n\n# Creates a new session to the database by using the engine we described.\nSession = sessionmaker(bind=engine)\nsession = Session()\n\n# Let's create a user and add two e-mail addresses to that user.\nexample_user = User(name='example', fullname='example last_name_example', password='examplepassword')\nexample_user.addresses = [Address(email_address='example@gmail.com'), Address(email_address='example@yahoo.com')]\n\n# Let's add the user and its addresses we've created to the DB and commit.\nsession.add(example_user)\nsession.commit()\n\n# Now let's query the user that has the e-mail address ed@google.com\n# SQLAlchemy will construct a JOIN query automatically.\nuser_by_email = session.query(User)\\\n    .filter(Address.email_address=='example@gmail.com')\\\n    .first()\n\n2020-10-02 08:55:48,507 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(\"users\")\n2020-10-02 08:55:48,508 INFO sqlalchemy.engine.base.Engine ()\n2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine PRAGMA main.table_info(\"addresses\")\n2020-10-02 08:55:48,510 INFO sqlalchemy.engine.base.Engine ()\n2020-10-02 08:55:48,513 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n2020-10-02 08:55:48,514 INFO sqlalchemy.engine.base.Engine INSERT INTO users (name, fullname, password) VALUES (?, ?, ?)\n2020-10-02 08:55:48,515 INFO sqlalchemy.engine.base.Engine ('example', 'example last_name_example', 'examplepassword')\n2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?)\n2020-10-02 08:55:48,517 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1)\n2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine INSERT INTO addresses (email_address, user_id) VALUES (?, ?)\n2020-10-02 08:55:48,519 INFO sqlalchemy.engine.base.Engine ('example@yahoo.com', 1)\n2020-10-02 08:55:48,520 INFO sqlalchemy.engine.base.Engine COMMIT\n2020-10-02 08:55:48,522 INFO sqlalchemy.engine.base.Engine BEGIN (implicit)\n2020-10-02 08:55:48,523 INFO sqlalchemy.engine.base.Engine SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password \nFROM users, addresses \nWHERE addresses.email_address = ?\n LIMIT ? OFFSET ?\n2020-10-02 08:55:48,524 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1, 0)\n\n\n\nprint(user_by_email)\n\n&lt;User(name='example', fullname='example last_name_example', password'examplepassword')&gt;\n\n\n\n# This will cause an additional query by lazy loading from the DB.\nprint(user_by_email.addresses)\n\n2020-10-02 08:55:53,081 INFO sqlalchemy.engine.base.Engine SELECT addresses.id AS addresses_id, addresses.email_address AS addresses_email_address, addresses.user_id AS addresses_user_id \nFROM addresses \nWHERE ? = addresses.user_id\n2020-10-02 08:55:53,083 INFO sqlalchemy.engine.base.Engine (1,)\n[&lt;Address(email_address='example@gmail.com')&gt;, &lt;Address(email_address='example@yahoo.com')&gt;]\n\n\n\n# To avoid querying again when getting all addresses of a user,\n# we use the joinedload option. SQLAlchemy will load all results and hide\n# the duplicate entries from us, so we can then get for\n# the user's addressess without an additional query to the DB.\nuser_by_email = session.query(User)\\\n    .filter(Address.email_address=='example@gmail.com')\\\n    .options(joinedload(User.addresses))\\\n    .first()\n\n2020-10-02 08:56:04,305 INFO sqlalchemy.engine.base.Engine SELECT anon_1.users_id AS anon_1_users_id, anon_1.users_name AS anon_1_users_name, anon_1.users_fullname AS anon_1_users_fullname, anon_1.users_password AS anon_1_users_password, addresses_1.id AS addresses_1_id, addresses_1.email_address AS addresses_1_email_address, addresses_1.user_id AS addresses_1_user_id \nFROM (SELECT users.id AS users_id, users.name AS users_name, users.fullname AS users_fullname, users.password AS users_password \nFROM users, addresses \nWHERE addresses.email_address = ?\n LIMIT ? OFFSET ?) AS anon_1 LEFT OUTER JOIN addresses AS addresses_1 ON anon_1.users_id = addresses_1.user_id\n2020-10-02 08:56:04,306 INFO sqlalchemy.engine.base.Engine ('example@gmail.com', 1, 0)\n\n\n\nprint(user_by_email)\n\n&lt;User(name='example', fullname='example last_name_example', password'examplepassword')&gt;\n\n\n\nprint(user_by_email.addresses)\n\n[&lt;Address(email_address='example@gmail.com')&gt;, &lt;Address(email_address='example@yahoo.com')&gt;]"
  },
  {
    "objectID": "posts/2020-12-23_Healthcare_Modeling_app-Copy1.html",
    "href": "posts/2020-12-23_Healthcare_Modeling_app-Copy1.html",
    "title": "Modeling Health Care Data App",
    "section": "",
    "text": "This notebook uses SMOTE and cross-validation.\n\n\nimport sys\nimport os\n\nfrom scipy import stats\nfrom datetime import datetime, date\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\n\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport pickle\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict,cross_validate\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Import DF\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf.info()\n\n## Data Prep\n\ndf = df.drop(columns = ['id'])\n\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n\npct_list = []\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    if round(pct_missing*100) &gt;0:\n        pct_list.append([col, round(pct_missing*100)])\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\ndf = df.fillna(df.mean())\n\ndf=df.dropna()\ndf.info()\n\nFeatures = ['age','heart_disease','ever_married']\nx = df[Features]\ny = df[\"stroke\"]\n\n# Train Test split\nX_train, X_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=2)\n\n\n#### Data Preprocessing\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate\nimport xgboost as xgb\n\n\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    random_state=2021,\n    tree_method='auto'\n#    tree_method='hist'\n#    tree_method='gpu_hist'\n)\n\nkfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n\nparam_grid = { \n    'colsample_bytree':[.75,1],\n    'learning_rate':[0.01,0.05,0.1,0.3,0.5],\n    'max_depth':[1,2,3,5],\n    'subsample':[.75,1],\n    'n_estimators': list(range(50, 400, 50))\n}\n\ngrid_search = GridSearchCV(estimator=clf, scoring='roc_auc', param_grid=param_grid, n_jobs=-1, cv=kfold)\n\n%%time\ngrid_result = grid_search.fit(X_train, y_train)\n\nprint(f'Best: {grid_result.best_score_} using {grid_result.best_params_}','\\n')\n\n\n\n#Set our final hyperparameters to the tuned values\nxgbcl = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n         gamma=0.0, max_delta_step=0.0, min_child_weight=1.0,\n         missing=None, n_jobs=-1, objective='binary:logistic', random_state=42, reg_alpha=0.0,\n         reg_lambda=1.0, scale_pos_weight=1.0, tree_method='auto',\n         colsample_bytree = grid_result.best_params_['colsample_bytree'], \n         learning_rate = grid_result.best_params_['learning_rate'], \n         max_depth = grid_result.best_params_['max_depth'], \n         subsample = grid_result.best_params_['subsample'], \n         n_estimators = grid_result.best_params_['n_estimators'])\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#refit the model on k-folds to get stable avg error metrics\nscores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, \n                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n\nprint('Training 5-fold Cross Validation Results:\\n')\nprint('AUC: ', scores['test_roc_auc'].mean())\nprint('Accuracy: ', scores['test_accuracy'].mean())\nprint('Precision: ', scores['test_precision'].mean())\nprint('Recall: ', scores['test_recall'].mean())\nprint('F1: ', scores['test_f1'].mean(), '\\n')\n\n\n\nimport sklearn.metrics as metrics\n\n#Fit the final model\nxgbcl.fit(X_train, y_train)\n\n#Generate predictions against our training and test data\npred_train = xgbcl.predict(X_train)\nproba_train = xgbcl.predict_proba(X_train)\npred_test = xgbcl.predict(X_test)\nproba_test = xgbcl.predict_proba(X_test)\n\n# Print model report\nprint(\"Classification report (Test): \\n\")\nprint(metrics.classification_report(y_test, pred_test))\nprint(\"Confusion matrix (Test): \\n\")\nprint(metrics.confusion_matrix(y_test, pred_test)/len(y_test))\n\nprint ('\\nTrain Accuracy:', metrics.accuracy_score(y_train, pred_train))\nprint ('Test Accuracy:', metrics.accuracy_score(y_test, pred_test))\n\nprint ('\\nTrain AUC:', metrics.roc_auc_score(y_train, proba_train[:,1]))\nprint ('Test AUC:', metrics.roc_auc_score(y_test, proba_test[:,1]))\n\n# calculate the fpr and tpr for all thresholds of the classification\ntrain_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1])\ntest_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1])\n\ntrain_roc_auc = metrics.auc(train_fpr, train_tpr)\ntest_roc_auc = metrics.auc(test_fpr, test_tpr)\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=[7,5])\nplt.title('Receiver Operating Characteristic')\nplt.plot(train_fpr, train_tpr, 'b', label = 'Train AUC = %0.2f' % train_roc_auc)\nplt.plot(test_fpr, test_tpr, 'g', label = 'Test AUC = %0.2f' % test_roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# plot feature importance\nxgb.plot_importance(xgbcl, importance_type='gain');\n\n\n\n\n\npickle.dump(xgbcl, open('stroke_xgboost_model.pkl', 'wb'))\npickle.dump(scaler, open('scaler.pkl', 'wb'))\n\n\nmodel = pickle.load(open('stroke_xgboost_model.pkl', 'rb'))\nprint(model)\n\n\n\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n              colsample_bynode=1, colsample_bytree=0.75, gamma=0.0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.05, max_delta_step=0.0, max_depth=1,\n              min_child_weight=1.0, missing=None, monotone_constraints='()',\n              n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=42,\n              reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0,\n              subsample=0.75, tree_method='auto', validate_parameters=1,\n              verbosity=None)\n\n\n\nRandom Forest Classifier\n\n\nFeature and Target Selection\n\n# Select feature and target variables:\nX = df.drop(['stroke'], axis=1)\ny = df[['stroke']]\n\n\n#One-hot encode the data using pandas get_dummies\nX = pd.get_dummies(X)\n\n\n#rus = RandomUnderSampler(random_state=0, replacement=True)\n#X_resampled, y_resampled = rus.fit_resample(X, y)\n#print(np.vstack(np.unique([tuple(row) for row in X_resampled], axis=0)).shape)\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\ncolumns = X_train.columns\n\nsm = SMOTE(random_state=1)\nX_train_SMOTE, y_train_SMOTE = sm.fit_sample(X_train, y_train)\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n\nmodel = RandomForestClassifier(n_estimators=100, bootstrap=True,\n                               max_features='sqrt', n_jobs=3, verbose=1, class_weight=\"balanced\")\n\nmodel.fit(X_train_SMOTE, y_train_SMOTE)\n\ny_pred = model.predict(X_test)\n\nDataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train_SMOTE, y_train_SMOTE)\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.9s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    2.0s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n\n\n\nfrom sklearn.metrics import roc_auc_score\n\n# Calculate roc auc\nroc_value = roc_auc_score(y_test, y_pred)\nroc_value\n\n0.5381645695594856\n\n\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nAccuracy: 0.9531490015360983\nPrecision: 0.05491990846681922\nRecall: 0.1085972850678733\n\n\n\ny_pred_proba = model.predict_proba(X_test)[::,1]\nfpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\nauc = metrics.roc_auc_score(y_test, y_pred_proba)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()\n\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.2s finished\n\n\n\n\n\n\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\nfrom inspect import signature\n\nprecision, recall, _ = precision_recall_curve(y_test, y_pred)\n\nplt.plot(precision,recall)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\n\nText(0, 0.5, 'Precision')\n\n\n\n\n\n\n# Import numpy and matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Construct the histogram with a flattened 3d array and a range of bins\nplt.hist(y_pred_proba.ravel())\n\n# Add a title to the plot\nplt.title('Predicted Probability of Stroke')\n\n# Show the plot\nplt.show()\n\n\n\n\n\nlen(y_pred_proba)\n\n13020\n\n\n\ny_pred\n\narray([1, 0, 0, ..., 0, 0, 1])\n\n\n# Get feature importances for interpretability\n\n# Get numerical feature importances\nimportances = list(model.feature_importances_)\n\n# List of tuples with variable and importance\nfeature_importances = [(feature, round(importance, 2)) for feature, importance in zip(X, importances)]\n\n# Sort the feature importances by most important first\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n# Print out the features and importances \n[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances];\n\nVariable: age                  Importance: 0.44\nVariable: avg_glucose_level    Importance: 0.19\nVariable: bmi                  Importance: 0.15\nVariable: work_type            Importance: 0.06\nVariable: smoking_status       Importance: 0.05\nVariable: gender               Importance: 0.03\nVariable: Residence_type       Importance: 0.03\nVariable: hypertension         Importance: 0.02\nVariable: ever_married         Importance: 0.02\nVariable: heart_disease        Importance: 0.01\n\n\n\nplt.figure(1)\nplt.title('Feature Importance')\nx_values = list(range(len(importances)))\nplt.barh(x_values,  importances, align='center')\nplt.yticks(x_values, X)\nplt.xlabel('Relative Importance')\nplt.tight_layout() \n\n\n\n\n\nimport pandas as pd\nfeature_importances = pd.DataFrame(model.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance', ascending=False)\n\n\nimportances\n\n[0.030891482100094805,\n 0.4448331026265109,\n 0.017695413344245573,\n 0.009520184938617332,\n 0.01937533663501595,\n 0.06499114062861666,\n 0.026702090192497516,\n 0.18974364950033454,\n 0.14563102038830425,\n 0.050616579645762515]\n\n\n\n\nConfusion Matrix\n\nfrom sklearn.metrics import confusion_matrix\ncnf_matrix = metrics.confusion_matrix(y_test, y_pred)\ncnf_matrix\n\narray([[12386,   413],\n       [  197,    24]])\n\n\n\nsns.set(font_scale=5.0)\nconf_mat = confusion_matrix(y_test, y_pred)\ncm_normalized = conf_mat.astype('float') / conf_mat.sum(axis=1)[:, np.newaxis]\nfig, ax = plt.subplots(figsize=(30,30), dpi = 100)\nsns.heatmap(cm_normalized, annot=True, cmap=\"Blues\")\nsns.set(font_scale=1)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\n#fig.savefig('cm_augmented.png', dpi=fig.dpi, transparent=True)\nplt.show()\n\n\n\n\n\ncm_normalized\n\narray([[0.96773185, 0.03226815],\n       [0.89140271, 0.10859729]])\n\n\n\nfig, ax = plt.subplots()\n# create heatmap\nsns.heatmap(pd.DataFrame(cnf_matrix), annot=True, cmap=\"Blues\" ,fmt='g')\nax.xaxis.set_label_position(\"top\")\nplt.tight_layout()\nplt.title('Confusion matrix', y=1.5)\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nplt.tick_params(axis='both', which='major', labelsize=10, labelbottom = False, bottom=False, top = True, labeltop=True)\n\n\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n\nmodel = RandomForestClassifier(n_estimators=100, bootstrap=True,\n                               max_features='sqrt', n_jobs=3, verbose=1, class_weight=\"balanced\")\n\nmodel.fit(X_train, y_train)\n\ny_pred = model.predict(X_test)\n\nDataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train, y_train)\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.6s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n\n\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import metrics\n\n\nmodel = RandomForestClassifier(n_estimators=100, bootstrap=True,\n                               max_features='sqrt', n_jobs=3, verbose=1, class_weight=\"balanced\")\n\nmodel.fit(X_train_SMOTE, y_train_SMOTE)\n\ny_pred = model.predict(X_test)\n\nDataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  model.fit(X_train_SMOTE, y_train_SMOTE)\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.8s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    1.9s finished\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.0s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.1s finished\n\n\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_pred))\n\nAccuracy: 0.9541474654377881\nPrecision: 0.05660377358490566\nRecall: 0.1085972850678733\n\n\n\ny_pred = model.predict_proba(X_test)[:,1]\ntrain_proba = pd.DataFrame({'predicted_probability': y_pred})\ntrain_proba.info()\n\n[Parallel(n_jobs=3)]: Using backend ThreadingBackend with 3 concurrent workers.\n[Parallel(n_jobs=3)]: Done  44 tasks      | elapsed:    0.1s\n[Parallel(n_jobs=3)]: Done 100 out of 100 | elapsed:    0.2s finished\n\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13020 entries, 0 to 13019\nData columns (total 1 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   predicted_probability  13020 non-null  float64\ndtypes: float64(1)\nmemory usage: 101.8 KB\n\n\n\n##check whether y_train indexes are the same as X_train indexes \nsame_index = y_test.index == X_test.index\nsame_index.all()\n\nTrue\n\n\n\n## get them into the same pandas frame\ntable = pd.concat([y_test.reset_index(drop=True), train_proba.reset_index(drop=True)], axis=1)\ntable\n\n\n\n\n\n\n\n\nstroke\npredicted_probability\n\n\n\n\n0\n0\n0.63\n\n\n1\n0\n0.25\n\n\n2\n0\n0.00\n\n\n3\n0\n0.00\n\n\n4\n0\n0.00\n\n\n...\n...\n...\n\n\n13015\n0\n0.00\n\n\n13016\n0\n0.00\n\n\n13017\n0\n0.00\n\n\n13018\n0\n0.00\n\n\n13019\n1\n0.70\n\n\n\n\n13020 rows × 2 columns\n\n\n\n\ntable.stroke.value_counts()\n\n0    12799\n1      221\nName: stroke, dtype: int64\n\n\n\ntable.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 13020 entries, 0 to 13019\nData columns (total 2 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   stroke                 13020 non-null  int64  \n 1   predicted_probability  13020 non-null  float64\ndtypes: float64(1), int64(1)\nmemory usage: 203.6 KB\n\n\n\ntable.to_csv('../processed_csvs/healthcare_table.csv')\n\n\n\nCross-Validation Precision\n\nfrom sklearn.ensemble import RandomForestClassifier\nforest_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n\n\n#cross validation predictions for test set\ny_test_pred = cross_val_predict(forest_clf, X_test, y_test, cv=5)\n\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_test_pred))\nprint(\"Precision:\",metrics.precision_score(y_test, y_test_pred))\nprint(\"Recall:\",metrics.recall_score(y_test, y_test_pred))\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n\n\nAccuracy: 0.9827188940092166\nPrecision: 0.0\nRecall: 0.0\n\n\n\n#cross validation predictions for full dataset\ny_pred = cross_val_predict(forest_clf, X, y, cv=5)\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:862: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n\n\n\nprint(\"Accuracy:\",metrics.accuracy_score(y, y_pred))\nprint(\"Precision:\",metrics.precision_score(y, y_pred))\nprint(\"Recall:\",metrics.recall_score(y, y_pred))\n\nAccuracy: 0.9817741935483871\nPrecision: 0.0\nRecall: 0.0\n\n\n\ntest_proba = pd.DataFrame({'predicted_probability': y_pred})\ntest_proba.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 43400 entries, 0 to 43399\nData columns (total 1 columns):\n #   Column                 Non-Null Count  Dtype\n---  ------                 --------------  -----\n 0   predicted_probability  43400 non-null  int64\ndtypes: int64(1)\nmemory usage: 339.2 KB\n\n\n\n##check whether y_test indexes are the same as X_test indexes \nsame_index = y.index == X.index\nsame_index.all()\n\nTrue\n\n\n\n## get them into the same pandas frame\ntable = pd.concat([y.reset_index(drop=True), test_proba.reset_index(drop=True)], axis=1)\ntable\n\n\n\n\n\n\n\n\nstroke\npredicted_probability\n\n\n\n\n0\n0\n0\n\n\n1\n0\n0\n\n\n2\n0\n0\n\n\n3\n0\n0\n\n\n4\n0\n0\n\n\n...\n...\n...\n\n\n43395\n0\n0\n\n\n43396\n0\n0\n\n\n43397\n0\n0\n\n\n43398\n0\n0\n\n\n43399\n0\n0\n\n\n\n\n43400 rows × 2 columns\n\n\n\n\ntable.stroke.value_counts()\n\n0    42617\n1      783\nName: stroke, dtype: int64\n\n\n\ntable.to_csv('../processed_csvs/final_model_table.csv')\n\n\n\n5-Fold Cross Validation\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\n\n\nfrom sklearn.model_selection import cross_val_score\nmodels = [\n    LogisticRegression(solver=\"liblinear\", random_state=42),\n    RandomForestClassifier(n_estimators=10, random_state=42),\n    KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2),\n    GaussianNB(),\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, X, y, scoring='precision', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'precision'])\n\nsns.boxplot(x='model_name', y='precision', data=cv_df)\nsns.stripplot(x='model_name', y='precision', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.xticks(rotation=45)\n\nplt.tight_layout()\nplt.show()\n\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/model_selection/_validation.py:531: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  estimator.fit(X_train, y_train, **fit_params)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  return f(**kwargs)"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "",
    "text": "# Import required packages\n\nimport numpy as np\nimport pandas as pd\n\n%matplotlib inline\nimport seaborn as sns\n\nimport statsmodels.formula.api as smf\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.lines import Line2D\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\nimport matplotlib as mpl\nmpl.rc('axes', labelsize=14)\nmpl.rc('xtick', labelsize=12)\nmpl.rc('ytick', labelsize=12)"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#read-required-datasets",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#read-required-datasets",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Read required datasets",
    "text": "Read required datasets\n\ndf = pd.read_csv('ttb_county_clean.csv')\ndf1 = pd.read_csv('df_panel_fix.csv')"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-1",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-1",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 1",
    "text": "Figure 1\n\ndf.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n    s=df[\"specific\"]/100, label=\"Specific Purpose Transfers\", figsize=(12,8),\n    c=\"nightlights\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n    sharex=False)\n#save_fig(\"cn-spt-county-heat\")\nplt.legend()\n\n\n&lt;matplotlib.legend.Legend at 0x7f0a4e16b4e0&gt;"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#panel-regression-framework-with-year-and-province-fixed-effects",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#panel-regression-framework-with-year-and-province-fixed-effects",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Panel regression framework with year and province fixed effects",
    "text": "Panel regression framework with year and province fixed effects\n\nlin_reg = smf.ols('np.log(specific) ~ np.log(gdp) + np.log(fdi) + i + rnr + rr + C(province) + C(year)', data=df1).fit()\n\n\n#lin_reg.summary()"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-2",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-2",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 2",
    "text": "Figure 2\n\ncoef_df = pd.read_csv('coef.csv')\n\nfig, ax = plt.subplots(figsize=(16, 10))\ncoef_df.plot(x='varname', y='coef', kind='bar', \n             ax=ax, color='none', \n             yerr='err', legend=False)\nax.set_ylabel('Specific Purpose Transfers (ln)')\nax.set_xlabel('Independant Variables')\nax.scatter(x=pd.np.arange(coef_df.shape[0]), \n           marker='s', s=120, \n           y=coef_df['coef'], color='black')\nax.axhline(y=0, linestyle='--', color='blue', linewidth=4)\nax.xaxis.set_ticks_position('none')\n\n_ = ax.set_xticklabels(['GDP', 'FDI', 'Incumbent', 'Non Relevant Rival', 'Relevant Rival'], \n                       rotation=0, fontsize=20)\n\nfs = 16\nax.annotate('Controls', xy=(0.2, -0.2), xytext=(0.2, -0.3), \n            xycoords='axes fraction', \n            textcoords='axes fraction', \n            fontsize=fs, ha='center', va='bottom',\n            bbox=dict(boxstyle='square', fc='white', ec='blue'),\n            arrowprops=dict(arrowstyle='-[, widthB=5.5, lengthB=1.2', lw=2.0, color='blue'))\n\n_ = ax.annotate('Connections', xy=(0.7, -0.2), xytext=(0.7, -0.3), \n                 xycoords='axes fraction', \n                 textcoords='axes fraction', \n                 fontsize=fs, ha='center', va='bottom',\n                 bbox=dict(boxstyle='square', fc='white', ec='red'),\n                 arrowprops=dict(arrowstyle='-[, widthB=10.5, lengthB=1.2', lw=2.0, color='red'))\n\n#save_fig(\"i-coef-plot\")"
  },
  {
    "objectID": "posts/2020-08-16-pandas-stats-fiscal.html#figure-3",
    "href": "posts/2020-08-16-pandas-stats-fiscal.html#figure-3",
    "title": "Using Pandas and Statsmodels for Regression with Fiscal Data",
    "section": "Figure 3",
    "text": "Figure 3\n\nimport numpy as np\nfrom plotly import __version__\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot,iplot\nprint (__version__) # requires version &gt;= 1.9.0\n\n\n#Always run this the command before at the start of notebook\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\n\n\ntrace1 = go.Bar(\n    x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n       2004, 2005, 2006, 2007],\n    y=[188870900000.0, 185182900000.0, 237697500000.0, 347187900000.0, 296716700000.0,\n       397833100000.0, 440204800000.0, 514254300000.0, 686016600000.0, 677746300000.0, 940057900000.0,\n       1881304000000],\n    name='All Other Province Leaders',\n    marker=dict(\n        color='rgb(55, 83, 109)'\n    )\n)\ntrace2 = go.Bar(\n    x=[1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003,\n       2004, 2005, 2006, 2007],\n    y=[260376000000.0, 264934700000.0, 367350200000.0, 463861200000.0, 199068500000.0, 216582600000.0,\n  298631800000.0, 409759300000.0, 830363200000.0, 878158000000.0, 1143745000000.0, 2125891000000.0],\n    name='Incumbent Connected Province Leaders',\n    marker=dict(\n        color='rgb(26, 118, 255)'\n    )\n)\ndata = [trace1, trace2]\nlayout = go.Layout(\n    title='Specific Purpose Transfers',\n    xaxis=dict(\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    yaxis=dict(\n        title='RMB',\n        titlefont=dict(\n            size=16,\n            color='rgb(107, 107, 107)'\n        ),\n        tickfont=dict(\n            size=14,\n            color='rgb(107, 107, 107)'\n        )\n    ),\n    legend=dict(\n        x=0,\n        y=1.0,\n        bgcolor='rgba(255, 255, 255, 0)',\n        bordercolor='rgba(255, 255, 255, 0)'\n    ),\n    barmode='group',\n    bargap=0.15,\n    bargroupgap=0.1\n)\n\nfig = go.Figure(data=data, layout=layout)\n\n#iplot(fig, filename='style-bar')\niplot(fig, image='png',filename='spt-i-bar')\n\n4.1.1"
  },
  {
    "objectID": "posts/2021-02-08-Capital-Asset-Pricing-Model.html",
    "href": "posts/2021-02-08-Capital-Asset-Pricing-Model.html",
    "title": "Daily and Cumulative Returns, CAPM",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\nfrom scipy import stats\n# help(stats.linregress)\nimport pandas as pd\nimport pandas_datareader as web\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nVRTTX_stock = web.DataReader('VRTTX', 'yahoo', start, end)\nVRTTX_stock.head()\n\n\nFNCMX_stock = web.DataReader('FNCMX', 'yahoo', start, end)\nFNCMX_stock.head()\n\nFSMAX_stock = web.DataReader('FSMAX', 'yahoo', start, end)\nFSMAX_stock.head()\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-01-02\n65.269997\n65.269997\n65.269997\n65.269997\n0.0\n65.192108\n\n\n2020-01-03\n65.129997\n65.129997\n65.129997\n65.129997\n0.0\n65.052277\n\n\n2020-01-06\n65.279999\n65.279999\n65.279999\n65.279999\n0.0\n65.202103\n\n\n2020-01-07\n65.180000\n65.180000\n65.180000\n65.180000\n0.0\n65.102219\n\n\n2020-01-08\n65.410004\n65.410004\n65.410004\n65.410004\n0.0\n65.331947\nimport matplotlib.pyplot as plt\n%matplotlib inline\nstocks\n\n\n\n\n\n\n\n\nFXAIX_stock\nVRTTX_stock\nFNCMX_stock\nFSMAX_stock\n\n\nDate\n\n\n\n\n\n\n\n\n2018-01-02\n94.230003\n238.889999\n91.989998\n62.529999\n\n\n2018-01-03\n94.830002\n240.289993\n92.760002\n62.740002\n\n\n2018-01-04\n95.230003\n241.199997\n92.930000\n62.849998\n\n\n2018-01-05\n95.900002\n242.750000\n93.699997\n63.090000\n\n\n2018-01-08\n96.059998\n243.199997\n93.970001\n63.270000\n\n\n...\n...\n...\n...\n...\n\n\n2021-02-08\n135.880005\n355.709991\n175.610001\n93.940002\n\n\n2021-02-09\n135.750000\n355.730011\n175.839996\n94.400002\n\n\n2021-02-10\n135.699997\n355.579987\n175.380005\n94.309998\n\n\n2021-02-11\n135.960007\n356.440002\n176.100006\n94.790001\n\n\n2021-02-12\n136.600006\n358.170013\n176.979996\n95.250000\n\n\n\n\n785 rows × 4 columns"
  },
  {
    "objectID": "posts/2021-02-08-Capital-Asset-Pricing-Model.html#compare-cumulative-return",
    "href": "posts/2021-02-08-Capital-Asset-Pricing-Model.html#compare-cumulative-return",
    "title": "Daily and Cumulative Returns, CAPM",
    "section": "Compare Cumulative Return",
    "text": "Compare Cumulative Return\n\nFXAIX_stock['Cumulative'] = FXAIX_stock['Close']/FXAIX_stock['Close'].iloc[0]\nVRTTX_stock['Cumulative'] = VRTTX_stock['Close']/VRTTX_stock['Close'].iloc[0]\nFNCMX_stock['Cumulative'] = FNCMX_stock['Close']/FNCMX_stock['Close'].iloc[0]\nFSMAX_stock['Cumulative'] = FSMAX_stock['Close']/FSMAX_stock['Close'].iloc[0]\n\n\nFXAIX_stock['Cumulative'].plot(label='FXAIX_stock',figsize=(10,8))\nVRTTX_stock['Cumulative'].plot(label='VRTTX_stock',figsize=(10,8))\nFNCMX_stock['Cumulative'].plot(label='FNCMX_stock',figsize=(10,8))\nFSMAX_stock['Cumulative'].plot(label='FSMAX_stock',figsize=(10,8))\nplt.legend()\nplt.title('Cumulative Return')\n\nText(0.5, 1.0, 'Cumulative Return')"
  },
  {
    "objectID": "posts/2021-02-08-Capital-Asset-Pricing-Model.html#get-daily-return",
    "href": "posts/2021-02-08-Capital-Asset-Pricing-Model.html#get-daily-return",
    "title": "Daily and Cumulative Returns, CAPM",
    "section": "Get Daily Return",
    "text": "Get Daily Return\n\nFXAIX_stock['Daily Return'] = FXAIX_stock['Close'].pct_change(1)\nVRTTX_stock['Daily Return'] = VRTTX_stock['Close'].pct_change(1)\n\n\nFXAIX_stock['Daily Return']\n\nDate\n2020-01-02         NaN\n2020-01-03   -0.006992\n2020-01-06    0.003565\n2020-01-07   -0.002664\n2020-01-08    0.004898\n                ...   \n2021-02-08    0.007414\n2021-02-09   -0.000957\n2021-02-10   -0.000368\n2021-02-11    0.001916\n2021-02-12    0.004707\nName: Daily Return, Length: 282, dtype: float64\n\n\n\nplt.scatter(FXAIX_stock['Daily Return'],VRTTX_stock['Daily Return'],alpha=0.3)\n\n&lt;matplotlib.collections.PathCollection at 0x7fe6c8aef400&gt;\n\n\n\n\n\n\nVRTTX_stock['Daily Return'].hist(bins=100)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nFXAIX_stock['Daily Return'].hist(bins=100)\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nbeta,alpha,r_value,p_value,std_err = stats.linregress(FXAIX_stock['Daily Return'].iloc[1:],VRTTX_stock['Daily Return'].iloc[1:])\n\n\nbeta\n\n1.0124150273238781\n\n\n\nalpha\n\n0.00012460564758023734\n\n\n\nr_value\n\n0.9973781994620393\n\n\n\nFXAIX_stock['Daily Return'].head()\n\nDate\n2020-01-02         NaN\n2020-01-03   -0.006992\n2020-01-06    0.003565\n2020-01-07   -0.002664\n2020-01-08    0.004898\nName: Daily Return, dtype: float64\n\n\n\nimport numpy as np\n\n\nnoise = np.random.normal(0,0.001,len(FXAIX_stock['Daily Return'].iloc[1:]))\n\n\n#noise\n\n\nFXAIX_stock['Daily Return'].iloc[1:] + noise\n\nDate\n2020-01-03   -0.006673\n2020-01-06    0.002797\n2020-01-07   -0.002088\n2020-01-08    0.005773\n2020-01-09    0.007326\n                ...   \n2021-02-08    0.006484\n2021-02-09   -0.000506\n2021-02-10   -0.000078\n2021-02-11    0.001277\n2021-02-12    0.005154\nName: Daily Return, Length: 281, dtype: float64\n\n\n\nbeta,alpha,r_value,p_value,std_err = stats.linregress(FXAIX_stock['Daily Return'].iloc[1:]+noise,FXAIX_stock['Daily Return'].iloc[1:])\n\n\nbeta\n\n0.9959066537237841\n\n\n\nalpha\n\n0.00011917125339144037"
  },
  {
    "objectID": "posts/2020-10-29-pyquery_for_ebooks_python.html",
    "href": "posts/2020-10-29-pyquery_for_ebooks_python.html",
    "title": "Creating E-Books (.epub) in python using ebooklib",
    "section": "",
    "text": "This post includes code and notes from ebooklib and make-an-ebook.\n\nimport os\nimport requests\n\nfrom ebooklib import epub\nfrom pyquery import PyQuery as pq\n\n\n# coding=utf-8\n\nurl = \"http://example.com/%s.html\"\nbuild_dir = \"build/\"\n\nif not os.path.exists(build_dir):\n    os.makedirs(build_dir)\n    \nsource_urls = [url % i for i in range(1,2)]\n\nurls = [\n    (build_dir + \"%s.html\" % i, url % i) for i in range(1,2)\n]\n\nfor filename, url in urls:\n    print(\"Getting \", url)\n    response = requests.get(url)\n    with open(filename, 'wb') as f:\n        f.write(response.content)\n\nGetting  http://example.com/1.html\n\n\n\nif __name__ == '__main__':\n    book = epub.EpubBook()\n\n    # add metadata\n    book.set_identifier('sample123456')\n    book.set_title('Sample book')\n    book.set_language('en')\n\n    book.add_author('Example Author')\n\n    # intro chapter\n    c1 = epub.EpubHtml(title='Introduction', file_name='intro.xhtml', lang='en')\n    c1.content=u'&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;&lt;h1&gt;Introduction&lt;/h1&gt;&lt;p&gt;Introduction paragraph here.&lt;/p&gt;&lt;/body&gt;&lt;/html&gt;'\n\n    # about chapter\n    c2 = epub.EpubHtml(title='About this book', file_name='about.xhtml')\n    c2.content='&lt;h1&gt;About this book&lt;/h1&gt;&lt;p&gt;Text about his book.&lt;/p&gt;'\n\n    # add chapters to the book\n    book.add_item(c1)\n    book.add_item(c2)\n    \n    # create table of contents\n    # - add section\n    # - add auto created links to chapters\n\n    book.toc = (epub.Link('intro.xhtml', 'Introduction', 'intro'),\n                 (epub.Section('Languages'),\n                 (c1, c2))\n                )\n\n    # add navigation files\n    book.add_item(epub.EpubNcx())\n    book.add_item(epub.EpubNav())\n\n    # define css style\n    style = '''\n@namespace epub \"http://www.idpf.org/2007/ops\";\nbody {\n    font-family: Cambria, Liberation Serif, Bitstream Vera Serif, Georgia, Times, Times New Roman, serif;\n}\nh2 {\n     text-align: left;\n     text-transform: uppercase;\n     font-weight: 200;     \n}\nol {\n        list-style-type: none;\n}\nol &gt; li:first-child {\n        margin-top: 0.3em;\n}\nnav[epub|type~='toc'] &gt; ol &gt; li &gt; ol  {\n    list-style-type:square;\n}\nnav[epub|type~='toc'] &gt; ol &gt; li &gt; ol &gt; li {\n        margin-top: 0.3em;\n}\n'''\n\n    # add css file\n    nav_css = epub.EpubItem(uid=\"style_nav\", file_name=\"style/nav.css\", media_type=\"text/css\", content=style)\n    book.add_item(nav_css)\n\n    # create spine\n    book.spine = ['nav', c1, c2]\n\n    # create epub file\n    epub.write_epub('test.epub', book, {})"
  },
  {
    "objectID": "posts/2020-10-21-quant_data_datareader_quandl.html",
    "href": "posts/2020-10-21-quant_data_datareader_quandl.html",
    "title": "Using the Quandl API and Pandas Datareader API to call Microsoft, Apple, Zoom, Snowflake stocks and other finance data",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader.data as web\nimport datetime\n\n\nstart = datetime.datetime(2020, 1, 1)\nend = pd.to_datetime('today')\n\n\n\nAAPL_stock = web.DataReader('AAPL', 'yahoo', start, end)\nAAPL_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-09-16\n319.0\n231.110001\n245.000000\n253.929993\n36099700\n253.929993\n\n\n2020-09-17\n241.5\n215.240005\n230.759995\n227.539993\n11907500\n227.539993\n\n\n2020-09-18\n249.0\n218.589996\n235.000000\n240.000000\n7475400\n240.000000\n\n\n2020-09-21\n241.5\n218.600006\n230.000000\n228.850006\n5524900\n228.850006\n\n\n2020-09-22\n239.0\n225.149994\n238.500000\n235.160004\n3889100\n235.160004\n\n\n\n\n\n\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nAAPL_stock['Open'].plot(label='Apple')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nAAPL_stock['Volume'].plot(label='Apple')\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f3e5dc577f0&gt;\n\n\n\n\n\n\n\n\n\nimport pandas_datareader.data as web\n\nimport datetime\n\n\n\ngdp = web.DataReader(\"GDP\", \"fred\", start, end)\n\n\ngdp.head()\n\n\n\n\n\n\n\n\nGDP\n\n\nDATE\n\n\n\n\n\n2020-01-01\n21561.139\n\n\n2020-04-01\n19520.114\n\n\n2020-07-01\n21170.252\n\n\n\n\n\n\n\n\nimport quandl\n\n\n#quandl.ApiConfig.api_key = ''\n\n\nmydata = quandl.get(\"EIA/PET_RWTC_D\")\n\nmydata.head()\n\nLimitExceededError: (Status 429) (Quandl Error QELx01) You have exceeded the anonymous user limit of 50 calls per day. To make more calls today, please register for a free Quandl account and then include your API key with your requests.\n\n\n\nmydata.plot(figsize=(12,6))\n\n\n#mydata = quandl.get(\"EIA/PET_RWTC_D\", returns=\"numpy\",start_date=start,end_date=end)\n\n\nmydata = quandl.get(\"FRED/GDP\",start_date=start,end_date=end)\n\n\nmydata.head()\n\n\nmydata = quandl.get([\"NSE/OIL.1\", \"WIKI/AAPL.4\"],start_date=start,end_date=end)\n\n\nmydata.head()\n\n\n\n\n\n\n\n\nNSE/OIL - Open\nWIKI/AAPL - Close\n\n\n\n\n\n\n\n\n\n\nmydata = quandl.get(\"FRED/GDP\")\n\n\nmydata = quandl.get('WIKI/FB',start_date=start,end_date=end)\n\n\nmydata.head()\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nEx-Dividend\nSplit Ratio\nAdj. Open\nAdj. High\nAdj. Low\nAdj. Close\nAdj. Volume\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nmydata = quandl.get('WIKI/FB.1',start_date=start,end_date=end)\n\nmydata.head()\n\n\n\n\n\n\n\n\nOpen\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\nmydata = quandl.get('WIKI/FB.7',start_date=start,end_date=end)\nmydata.head()\n\n\n\n\n\n\n\n\nSplit Ratio\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n# Homes\n\n\nhouses = quandl.get('ZILLOW/M11_ZRIAH',start_date=start,end_date=end)\n\n\nhouses.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2020-01-31\n3342.0\n\n\n2020-02-29\n3358.0\n\n\n\n\n\n\n\n\nhouses.plot()\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f900d58fef0&gt;"
  },
  {
    "objectID": "posts/2021-05-31-review-sklearn.html",
    "href": "posts/2021-05-31-review-sklearn.html",
    "title": "Review of scikit-learn",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intro\n\n\nimport seaborn as sns\nimport sklearn\nsns.set_theme(context=\"notebook\", font_scale=1.2,\n              rc={\"figure.figsize\": [10, 6]})\nsklearn.set_config(display=\"diagram\")\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.30\n50661\n0.000000\n0.000000\n0.000000\n1128873\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.32\n43443\n0.000000\n0.000000\n0.000000\n1356287\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.96\n27673\n0.000000\n0.000000\n0.000000\n1518236\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.34\n26131\nNaN\nNaN\nNaN\n1646891\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.000000\n0.000000\n0.000000\n1601508\nEast China\n1499110\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n355\nZhejiang\n391292.0\n260313.0\n2003\n9705.02\n498055\n1.214286\n0.035714\n0.035714\n6217715\nEast China\n2261631\n\n\n356\n356\nZhejiang\n656175.0\n276652.0\n2004\n11648.70\n668128\n1.214286\n0.035714\n0.035714\nNaN\nEast China\n3162299\n\n\n357\n357\nZhejiang\n656175.0\nNaN\n2005\n13417.68\n772000\n1.214286\n0.035714\n0.035714\nNaN\nEast China\n2370200\n\n\n358\n358\nZhejiang\n1017303.0\n394795.0\n2006\n15718.47\n888935\n1.214286\n0.035714\n0.035714\n11537149\nEast China\n2553268\n\n\n359\n359\nZhejiang\n844647.0\n0.0\n2007\n18753.73\n1036576\n0.047619\n0.000000\n0.000000\n16494981\nEast China\n2939778\n\n\n\n\n360 rows × 13 columns\n\n\n\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['it', 'Unnamed: 0'], axis = 1), df['it']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42, stratify=y)\n\n\nTrain DummyClassifer\n\nfrom sklearn.dummy import DummyClassifier\n\ndc = DummyClassifier(strategy='prior').fit(X_train, y_train)\ndc.score(X_test, y_test)\n\n0.9811502476609797\n\n\n\n\nTrain KNN based model\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknc = make_pipeline(\n    StandardScaler(),\n    KNeighborsClassifier()\n)\nknc.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('kneighborsclassifier', KNeighborsClassifier())])StandardScalerStandardScaler()KNeighborsClassifierKNeighborsClassifier()\n\n\n\nknc.score(X_test, y_test)\n\n0.9801871216290589\n\n\n\n# %load solutions/00-ex01-solutions.py\nfrom sklearn.datasets import load_breast_cancer\n\ncancer = load_breast_cancer(as_frame=True)\n\nX, y = cancer.data, cancer.target\n\ny.value_counts()\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42, stratify=y)\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = make_pipeline(\n    StandardScaler(),\n    LogisticRegression()\n)\n\nlog_reg.fit(X_train, y_train)\n\nlog_reg.score(X_test, y_test)\n\nfrom sklearn.metrics import f1_score\ny_pred = log_reg.predict(X_test)\n\ny_pred\n\nf1_score(y_test, y_pred)\n\n0.9888888888888889"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html",
    "href": "posts/2021-05-31-missing-values.html",
    "title": "Missing values in scikit-learn",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#simpleimputer",
    "href": "posts/2021-05-31-missing-values.html#simpleimputer",
    "title": "Missing values in scikit-learn",
    "section": "SimpleImputer",
    "text": "SimpleImputer\n\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\nimport sklearn\nsklearn.set_config(display='diagram')\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['it', 'Unnamed: 0'], axis = 1), df['it']\n\n\nX = X.select_dtypes(include='number')\nX\n\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\n\n\n\n\n\ndf.isnull().sum()\n\nUnnamed: 0      0\nprovince        0\nspecific        4\ngeneral       191\nyear            0\ngdp             0\nfdi             0\nrnr            66\nrr             64\ni              73\nfr             65\nreg             0\nit              0\ndtype: int64\n\n\n\nDefault uses mean\n\nimputer = SimpleImputer()\nimputer.fit_transform(X)\n\narray([[1.47002000e+05, 3.09127538e+05, 1.99600000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.51981000e+05, 3.09127538e+05, 1.99700000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.74930000e+05, 3.09127538e+05, 1.99800000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [6.56175000e+05, 3.09127538e+05, 2.00500000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [1.01730300e+06, 3.94795000e+05, 2.00600000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [8.44647000e+05, 0.00000000e+00, 2.00700000e+03, ...,\n        4.76190480e-02, 0.00000000e+00, 0.00000000e+00]])\n\n\n\ndf.isnull().sum()\n\nUnnamed: 0      0\nprovince        0\nspecific        4\ngeneral       191\nyear            0\ngdp             0\nfdi             0\nrnr            66\nrr             64\ni              73\nfr             65\nreg             0\nit              0\ndtype: int64\n\n\n\n\nAdd indicator!\n\nimputer = SimpleImputer(add_indicator=True)\nimputer.fit_transform(X)\n\narray([[ 147002.        ,  309127.53846154,    1996.        , ...,\n              0.        ,       0.        ,       0.        ],\n       [ 151981.        ,  309127.53846154,    1997.        , ...,\n              0.        ,       0.        ,       0.        ],\n       [ 174930.        ,  309127.53846154,    1998.        , ...,\n              0.        ,       0.        ,       0.        ],\n       ...,\n       [ 656175.        ,  309127.53846154,    2005.        , ...,\n              0.        ,       0.        ,       0.        ],\n       [1017303.        ,  394795.        ,    2006.        , ...,\n              0.        ,       0.        ,       0.        ],\n       [ 844647.        ,       0.        ,    2007.        , ...,\n              0.        ,       0.        ,       0.        ]])\n\n\n\ndf.isnull().sum()\n\nUnnamed: 0      0\nprovince        0\nspecific        4\ngeneral       191\nyear            0\ngdp             0\nfdi             0\nrnr            66\nrr             64\ni              73\nfr             65\nreg             0\nit              0\ndtype: int64\n\n\n\n\nOther strategies\n\nimputer = SimpleImputer(strategy='median')\nimputer.fit_transform(X)\n\narray([[1.47002000e+05, 1.53640000e+05, 1.99600000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.51981000e+05, 1.53640000e+05, 1.99700000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.74930000e+05, 1.53640000e+05, 1.99800000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [6.56175000e+05, 1.53640000e+05, 2.00500000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [1.01730300e+06, 3.94795000e+05, 2.00600000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [8.44647000e+05, 0.00000000e+00, 2.00700000e+03, ...,\n        4.76190480e-02, 0.00000000e+00, 0.00000000e+00]])\n\n\n\nimputer = SimpleImputer(strategy='most_frequent')\nimputer.fit_transform(X)\n\narray([[1.47002000e+05, 0.00000000e+00, 1.99600000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.51981000e+05, 0.00000000e+00, 1.99700000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       [1.74930000e+05, 0.00000000e+00, 1.99800000e+03, ...,\n        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n       ...,\n       [6.56175000e+05, 0.00000000e+00, 2.00500000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [1.01730300e+06, 3.94795000e+05, 2.00600000e+03, ...,\n        1.21428571e+00, 3.57142860e-02, 3.57142860e-02],\n       [8.44647000e+05, 0.00000000e+00, 2.00700000e+03, ...,\n        4.76190480e-02, 0.00000000e+00, 0.00000000e+00]])"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#categorical-data",
    "href": "posts/2021-05-31-missing-values.html#categorical-data",
    "title": "Missing values in scikit-learn",
    "section": "Categorical data",
    "text": "Categorical data\n\nimport pandas as pd\n\n\nimputer = SimpleImputer(strategy='constant', fill_value='sk_missing')\nimputer.fit_transform(df)\n\narray([[0, 'Anhui', 147002.0, ..., '1128873', 'East China', 631930],\n       [1, 'Anhui', 151981.0, ..., '1356287', 'East China', 657860],\n       [2, 'Anhui', 174930.0, ..., '1518236', 'East China', 889463],\n       ...,\n       [357, 'Zhejiang', 656175.0, ..., 'sk_missing', 'East China',\n        2370200],\n       [358, 'Zhejiang', 1017303.0, ..., '11537149', 'East China',\n        2553268],\n       [359, 'Zhejiang', 844647.0, ..., '16494981', 'East China',\n        2939778]], dtype=object)"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#pandas-categorical",
    "href": "posts/2021-05-31-missing-values.html#pandas-categorical",
    "title": "Missing values in scikit-learn",
    "section": "pandas categorical",
    "text": "pandas categorical\n\ndf['a'] = df['a'].astype('category')\n\n\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.30\n50661\n0.000000\n0.000000\n0.000000\n1128873\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.32\n43443\n0.000000\n0.000000\n0.000000\n1356287\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.96\n27673\n0.000000\n0.000000\n0.000000\n1518236\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.34\n26131\nNaN\nNaN\nNaN\n1646891\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.000000\n0.000000\n0.000000\n1601508\nEast China\n1499110\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n355\nZhejiang\n391292.0\n260313.0\n2003\n9705.02\n498055\n1.214286\n0.035714\n0.035714\n6217715\nEast China\n2261631\n\n\n356\n356\nZhejiang\n656175.0\n276652.0\n2004\n11648.70\n668128\n1.214286\n0.035714\n0.035714\nNaN\nEast China\n3162299\n\n\n357\n357\nZhejiang\n656175.0\nNaN\n2005\n13417.68\n772000\n1.214286\n0.035714\n0.035714\nNaN\nEast China\n2370200\n\n\n358\n358\nZhejiang\n1017303.0\n394795.0\n2006\n15718.47\n888935\n1.214286\n0.035714\n0.035714\n11537149\nEast China\n2553268\n\n\n359\n359\nZhejiang\n844647.0\n0.0\n2007\n18753.73\n1036576\n0.047619\n0.000000\n0.000000\n16494981\nEast China\n2939778\n\n\n\n\n360 rows × 13 columns\n\n\n\n\ndf.dtypes\n\nUnnamed: 0      int64\nprovince       object\nspecific      float64\ngeneral       float64\nyear            int64\ngdp           float64\nfdi             int64\nrnr           float64\nrr            float64\ni             float64\nfr             object\nreg            object\nit              int64\ndtype: object\n\n\n\nimputer.fit_transform(df)\n\narray([[0, 'Anhui', 147002.0, ..., '1128873', 'East China', 631930],\n       [1, 'Anhui', 151981.0, ..., '1356287', 'East China', 657860],\n       [2, 'Anhui', 174930.0, ..., '1518236', 'East China', 889463],\n       ...,\n       [357, 'Zhejiang', 656175.0, ..., 'sk_missing', 'East China',\n        2370200],\n       [358, 'Zhejiang', 1017303.0, ..., '11537149', 'East China',\n        2553268],\n       [359, 'Zhejiang', 844647.0, ..., '16494981', 'East China',\n        2939778]], dtype=object)\n\n\n\n# %load solutions/03-ex01-solutions.py\nfrom sklearn.datasets import fetch_openml\n\ncancer = fetch_openml(data_id=15, as_frame=True)\n\nX, y = cancer.data, cancer.target\n\nX.shape\n\nX.isna().sum()\n\nimputer = SimpleImputer(add_indicator=True)\nX_imputed = imputer.fit_transform(X)\n\nX_imputed.shape\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42, stratify=y\n)\n\nlog_reg = make_pipeline(\n    SimpleImputer(add_indicator=True),\n    StandardScaler(),\n    LogisticRegression(random_state=0)\n)\n\nlog_reg.fit(X_train, y_train)\n\nlog_reg.score(X_test, y_test)\n\n0.96"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#histgradientboosting-native-support-for-missing-values",
    "href": "posts/2021-05-31-missing-values.html#histgradientboosting-native-support-for-missing-values",
    "title": "Missing values in scikit-learn",
    "section": "HistGradientBoosting Native support for missing values",
    "text": "HistGradientBoosting Native support for missing values\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\n\nhist = HistGradientBoostingClassifier(random_state=42)\nhist.fit(X_train, y_train)\n\nHistGradientBoostingClassifierHistGradientBoostingClassifier(random_state=42)\n\n\n\nhist.score(X_test, y_test)\n\n0.9485714285714286"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#grid-searching-the-imputer",
    "href": "posts/2021-05-31-missing-values.html#grid-searching-the-imputer",
    "title": "Missing values in scikit-learn",
    "section": "Grid searching the imputer",
    "text": "Grid searching the imputer\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n\niris = pd.read_csv('data/iris_w_missing.csv')\n\n\niris.head()\n\n\n\n\n\n\n\n\nsepal length (cm)\nsepal width (cm)\npetal length (cm)\npetal width (cm)\ntarget\n\n\n\n\n0\n6.4\n2.9\n4.3\n1.3\n1\n\n\n1\n5.7\n2.8\n4.1\n1.3\n1\n\n\n2\n6.8\n2.8\nNaN\n1.4\n1\n\n\n3\n6.7\n3.3\n5.7\n2.1\n2\n\n\n4\n4.8\n3.4\n1.6\n0.2\n0\n\n\n\n\n\n\n\n\nX = iris.drop('target', axis='columns')\ny = iris['target']\n\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=0, stratify=y\n)\n\n\npipe = Pipeline([\n    ('imputer', SimpleImputer(add_indicator=True)),\n    ('rf', RandomForestClassifier(random_state=42))\n])"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#scikit-learn-uses-get_params-to-find-names",
    "href": "posts/2021-05-31-missing-values.html#scikit-learn-uses-get_params-to-find-names",
    "title": "Missing values in scikit-learn",
    "section": "scikit-learn uses get_params to find names",
    "text": "scikit-learn uses get_params to find names\n\npipe.get_params()\n\n{'memory': None,\n 'steps': [('imputer', SimpleImputer(add_indicator=True)),\n  ('rf', RandomForestClassifier(random_state=42))],\n 'verbose': False,\n 'imputer': SimpleImputer(add_indicator=True),\n 'rf': RandomForestClassifier(random_state=42),\n 'imputer__add_indicator': True,\n 'imputer__copy': True,\n 'imputer__fill_value': None,\n 'imputer__missing_values': nan,\n 'imputer__strategy': 'mean',\n 'imputer__verbose': 0,\n 'rf__bootstrap': True,\n 'rf__ccp_alpha': 0.0,\n 'rf__class_weight': None,\n 'rf__criterion': 'gini',\n 'rf__max_depth': None,\n 'rf__max_features': 'auto',\n 'rf__max_leaf_nodes': None,\n 'rf__max_samples': None,\n 'rf__min_impurity_decrease': 0.0,\n 'rf__min_impurity_split': None,\n 'rf__min_samples_leaf': 1,\n 'rf__min_samples_split': 2,\n 'rf__min_weight_fraction_leaf': 0.0,\n 'rf__n_estimators': 100,\n 'rf__n_jobs': None,\n 'rf__oob_score': False,\n 'rf__random_state': 42,\n 'rf__verbose': 0,\n 'rf__warm_start': False}"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#is-it-better-to-add-the-indicator",
    "href": "posts/2021-05-31-missing-values.html#is-it-better-to-add-the-indicator",
    "title": "Missing values in scikit-learn",
    "section": "Is it better to add the indicator?",
    "text": "Is it better to add the indicator?\n\nfrom sklearn.model_selection import GridSearchCV\n\nparams = {\n    'imputer__add_indicator': [True, False]\n}\n\ngrid_search = GridSearchCV(pipe, param_grid=params, verbose=1)\n\n\ngrid_search.fit(X_train, y_train)\n\nFitting 5 folds for each of 2 candidates, totalling 10 fits\n\n\n[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n[Parallel(n_jobs=1)]: Done  10 out of  10 | elapsed:    1.2s finished\n\n\nGridSearchCVGridSearchCV(estimator=Pipeline(steps=[('imputer',\n                                        SimpleImputer(add_indicator=True)),\n                                       ('rf',\n                                        RandomForestClassifier(random_state=42))]),\n             param_grid={'imputer__add_indicator': [True, False]}, verbose=1)SimpleImputerSimpleImputer(add_indicator=True)RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\ngrid_search.best_params_\n\n{'imputer__add_indicator': True}\n\n\n\ngrid_search.best_score_\n\n0.8837944664031621\n\n\n\ngrid_search.score(X_test, y_test)\n\n0.9473684210526315"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#compare-to-make_pipeline",
    "href": "posts/2021-05-31-missing-values.html#compare-to-make_pipeline",
    "title": "Missing values in scikit-learn",
    "section": "Compare to make_pipeline",
    "text": "Compare to make_pipeline\n\nfrom sklearn.pipeline import make_pipeline\n\npipe2 = make_pipeline(SimpleImputer(add_indicator=True),\n                     RandomForestClassifier(random_state=42))\n\n\npipe2.get_params()\n\n{'memory': None,\n 'steps': [('simpleimputer', SimpleImputer(add_indicator=True)),\n  ('randomforestclassifier', RandomForestClassifier(random_state=42))],\n 'verbose': False,\n 'simpleimputer': SimpleImputer(add_indicator=True),\n 'randomforestclassifier': RandomForestClassifier(random_state=42),\n 'simpleimputer__add_indicator': True,\n 'simpleimputer__copy': True,\n 'simpleimputer__fill_value': None,\n 'simpleimputer__missing_values': nan,\n 'simpleimputer__strategy': 'mean',\n 'simpleimputer__verbose': 0,\n 'randomforestclassifier__bootstrap': True,\n 'randomforestclassifier__ccp_alpha': 0.0,\n 'randomforestclassifier__class_weight': None,\n 'randomforestclassifier__criterion': 'gini',\n 'randomforestclassifier__max_depth': None,\n 'randomforestclassifier__max_features': 'auto',\n 'randomforestclassifier__max_leaf_nodes': None,\n 'randomforestclassifier__max_samples': None,\n 'randomforestclassifier__min_impurity_decrease': 0.0,\n 'randomforestclassifier__min_impurity_split': None,\n 'randomforestclassifier__min_samples_leaf': 1,\n 'randomforestclassifier__min_samples_split': 2,\n 'randomforestclassifier__min_weight_fraction_leaf': 0.0,\n 'randomforestclassifier__n_estimators': 100,\n 'randomforestclassifier__n_jobs': None,\n 'randomforestclassifier__oob_score': False,\n 'randomforestclassifier__random_state': 42,\n 'randomforestclassifier__verbose': 0,\n 'randomforestclassifier__warm_start': False}"
  },
  {
    "objectID": "posts/2021-05-31-missing-values.html#which-imputer-to-use",
    "href": "posts/2021-05-31-missing-values.html#which-imputer-to-use",
    "title": "Missing values in scikit-learn",
    "section": "Which imputer to use?",
    "text": "Which imputer to use?\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nparams = {\n    'imputer': [\n        SimpleImputer(strategy='median', add_indicator=True),\n        SimpleImputer(strategy='mean', add_indicator=True),\n        KNNImputer(add_indicator=True),\n        IterativeImputer(estimator=RandomForestRegressor(random_state=42),\n                         random_state=42, add_indicator=True)]\n}\n\nsearch_cv = GridSearchCV(pipe, param_grid=params, verbose=1, n_jobs=-1)\n\n\nsearch_cv.fit(X_train, y_train)\n\nFitting 5 folds for each of 4 candidates, totalling 20 fits\n\n\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:    7.8s finished\n\n\nGridSearchCVGridSearchCV(estimator=Pipeline(steps=[('imputer',\n                                        SimpleImputer(add_indicator=True)),\n                                       ('rf',\n                                        RandomForestClassifier(random_state=42))]),\n             n_jobs=-1,\n             param_grid={'imputer': [SimpleImputer(add_indicator=True,\n                                                   strategy='median'),\n                                     SimpleImputer(add_indicator=True),\n                                     KNNImputer(add_indicator=True),\n                                     IterativeImputer(add_indicator=True,\n                                                      estimator=RandomForestRegressor(random_state=42),\n                                                      random_state=42)]},\n             verbose=1)SimpleImputerSimpleImputer(add_indicator=True)RandomForestClassifierRandomForestClassifier(random_state=42)\n\n\n\nsearch_cv.best_params_\n\n{'imputer': KNNImputer(add_indicator=True)}\n\n\n\nsearch_cv.best_score_\n\n0.9102766798418973\n\n\n\nsearch_cv.score(X_test, y_test)\n\n0.9736842105263158"
  },
  {
    "objectID": "posts/2020-09-28-databases_sqllite_sqlalchemy_27.html",
    "href": "posts/2020-09-28-databases_sqllite_sqlalchemy_27.html",
    "title": "Creating and accessing SQL databases with python using sqlalchemy and sqlite3",
    "section": "",
    "text": "This post includes code adapted from these sqlalchemy and sqlite gists and the sqlite3 documentation.\n\nimport sqlalchemy as db\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///music.sqlite')\n\n\nconnection = engine.connect()\nmetadata = db.MetaData()\n\nmusic = db.Table('music', metadata,\n              db.Column('Id', db.Integer()),\n              db.Column('song', db.String(255), nullable=False),\n              db.Column('album', db.String(255), nullable=False),\n              db.Column('artist', db.String(255), nullable=False)\n              )\n\nmetadata.create_all(engine) \n\n\n#Inserting one record\nquery = db.insert(music).values(Id=1, song='song3', album='album3', artist='artist3') \nResultProxy = connection.execute(query)\n\n\n#Inserting many records\nquery = db.insert(music) \nvalues_list = [{'Id':'2', 'song':'song1', 'album':'album1', 'artist':'artist1'},\n               {'Id':'3', 'song':'song2', 'album':'album2', 'artist':'artist2'}]\n\nResultProxy = connection.execute(query,values_list)\n\nresults = connection.execute(db.select([music])).fetchall()\ndf = pd.DataFrame(results)\ndf.columns = results[0].keys()\ndf.head(10)\n\n\n\n\n\n\n\n\nId\nsong\nalbum\nartist\n\n\n\n\n0\n1\nsong3\nalbum3\nartist3\n\n\n1\n2\nsong1\nalbum1\nartist1\n\n\n2\n3\nsong2\nalbum2\nartist2\n\n\n3\n2\nsong1\nalbum1\nartist1\n\n\n4\n3\nsong2\nalbum2\nartist2\n\n\n\n\n\n\n\n\nresults = connection.execute(db.select([music])).fetchall()\ndf = pd.DataFrame(results)\ndf.columns = results[0].keys()\ndf.head(4)\n\n\nquery = db.select([music]).where(db.and_(music.columns.song == 'song3', music.columns.artist == 'artist3'))\nresult = connection.execute(query).fetchall()\nresult[:3]\n\n[(1, 'song3', 'album3', 'artist3')]\n\n\n\nconn = sqlite3.connect('music.sqlite')\n\n\nc = conn.cursor()\n\n# Create table\nc.execute('''CREATE TABLE stockmarket\n             (date text, trans text, symbol text, qty real, price real)''')\n\n# Insert a row of data\nc.execute(\"INSERT INTO stockmarket VALUES ('2006-01-05','BUY','RHAT',100,35.14)\")\n\n# Save (commit) the changes\nconn.commit()\n\n# We can also close the connection if we are done with it.\n# Just be sure any changes have been committed or they will be lost.\nconn.close()\n\n\nconn = sqlite3.connect('music.sqlite')\nc = conn.cursor()\n\n\nsymbol = 'RHAT'\nc.execute(\"SELECT * FROM stockmarket WHERE symbol = '%s'\" % symbol)\n\n&lt;sqlite3.Cursor at 0x7f0098ae0180&gt;\n\n\n\nt = ('RHAT',)\nc.execute('SELECT * FROM stockmarket WHERE symbol=?', t)\nprint(c.fetchone())\n\n('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n\n\n\n# Larger example that inserts many records at a time\npurchases = [('2006-03-28', 'BUY', 'IBM', 1000, 45.00),\n             ('2006-04-05', 'BUY', 'MSFT', 1000, 72.00),\n             ('2006-04-06', 'SELL', 'IBM', 500, 53.00),\n            ]\nc.executemany('INSERT INTO stockmarket VALUES (?,?,?,?,?)', purchases)\n\n&lt;sqlite3.Cursor at 0x7f0098ae0180&gt;\n\n\n\nfor row in c.execute('SELECT * FROM stockmarket ORDER BY price'):\n        print(row)\n\n('2006-01-05', 'BUY', 'RHAT', 100.0, 35.14)\n('2006-03-28', 'BUY', 'IBM', 1000.0, 45.0)\n('2006-04-06', 'SELL', 'IBM', 500.0, 53.0)\n('2006-04-05', 'BUY', 'MSFT', 1000.0, 72.0)\n\n\n\n# Use dbeaver to examine"
  },
  {
    "objectID": "posts/2020-10-27-sql_calls_in_jupyter_Soccer_Pred.html",
    "href": "posts/2020-10-27-sql_calls_in_jupyter_Soccer_Pred.html",
    "title": "Working with sqlite databases in Jupyter for Visualizing European Soccer Match Data",
    "section": "",
    "text": "This post includes code and notes from data-analysis-using-sql.\n\nimport sqlalchemy as db\nimport sqlite3\nimport pandas as pd\nimport numpy as np\n\n\nengine = db.create_engine('sqlite:///database.sqlite')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nconnection\n\n&lt;sqlalchemy.engine.base.Connection at 0x7fb9c7c07080&gt;\n\n\n\nengine.execute(\"SELECT * FROM Country LIMIT 10\").fetchall()\n\n[(1, 'Belgium'),\n (1729, 'England'),\n (4769, 'France'),\n (7809, 'Germany'),\n (10257, 'Italy'),\n (13274, 'Netherlands'),\n (15722, 'Poland'),\n (17642, 'Portugal'),\n (19694, 'Scotland'),\n (21518, 'Spain')]\n\n\n\n%load_ext sql\n\n\n%sql sqlite:///database.sqlite\n\n\n%%sql\nSELECT *\nFROM Country\nLIMIT 10\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nname\n\n\n1\nBelgium\n\n\n1729\nEngland\n\n\n4769\nFrance\n\n\n7809\nGermany\n\n\n10257\nItaly\n\n\n13274\nNetherlands\n\n\n15722\nPoland\n\n\n17642\nPortugal\n\n\n19694\nScotland\n\n\n21518\nSpain\n\n\n\n\n\n\n%%sql\nSELECT id\n,name\nFROM Country\nWHERE name = \"England\"\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nname\n\n\n1729\nEngland\n\n\n\n\n\n\n%%sql\nSELECT * FROM League LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\ncountry_id\nname\n\n\n1\n1\nBelgium Jupiler League\n\n\n1729\n1729\nEngland Premier League\n\n\n4769\n4769\nFrance Ligue 1\n\n\n7809\n7809\nGermany 1. Bundesliga\n\n\n10257\n10257\nItaly Serie A\n\n\n13274\n13274\nNetherlands Eredivisie\n\n\n15722\n15722\nPoland Ekstraklasa\n\n\n17642\n17642\nPortugal Liga ZON Sagres\n\n\n19694\n19694\nScotland Premier League\n\n\n21518\n21518\nSpain LIGA BBVA\n\n\n\n\n\n\n%%sql\nSELECT * FROM Match LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\ncountry_id\nleague_id\nseason\nstage\ndate\nmatch_api_id\nhome_team_api_id\naway_team_api_id\nhome_team_goal\naway_team_goal\nhome_player_X1\nhome_player_X2\nhome_player_X3\nhome_player_X4\nhome_player_X5\nhome_player_X6\nhome_player_X7\nhome_player_X8\nhome_player_X9\nhome_player_X10\nhome_player_X11\naway_player_X1\naway_player_X2\naway_player_X3\naway_player_X4\naway_player_X5\naway_player_X6\naway_player_X7\naway_player_X8\naway_player_X9\naway_player_X10\naway_player_X11\nhome_player_Y1\nhome_player_Y2\nhome_player_Y3\nhome_player_Y4\nhome_player_Y5\nhome_player_Y6\nhome_player_Y7\nhome_player_Y8\nhome_player_Y9\nhome_player_Y10\nhome_player_Y11\naway_player_Y1\naway_player_Y2\naway_player_Y3\naway_player_Y4\naway_player_Y5\naway_player_Y6\naway_player_Y7\naway_player_Y8\naway_player_Y9\naway_player_Y10\naway_player_Y11\nhome_player_1\nhome_player_2\nhome_player_3\nhome_player_4\nhome_player_5\nhome_player_6\nhome_player_7\nhome_player_8\nhome_player_9\nhome_player_10\nhome_player_11\naway_player_1\naway_player_2\naway_player_3\naway_player_4\naway_player_5\naway_player_6\naway_player_7\naway_player_8\naway_player_9\naway_player_10\naway_player_11\ngoal\nshoton\nshotoff\nfoulcommit\ncard\ncross\ncorner\npossession\nB365H\nB365D\nB365A\nBWH\nBWD\nBWA\nIWH\nIWD\nIWA\nLBH\nLBD\nLBA\nPSH\nPSD\nPSA\nWHH\nWHD\nWHA\nSJH\nSJD\nSJA\nVCH\nVCD\nVCA\nGBH\nGBD\nGBA\nBSH\nBSD\nBSA\n\n\n1\n1\n1\n2008/2009\n1\n2008-08-17 00:00:00\n492473\n9987\n9993\n1\n1\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.73\n3.4\n5\n1.75\n3.35\n4.2\n1.85\n3.2\n3.5\n1.8\n3.3\n3.75\nNone\nNone\nNone\n1.7\n3.3\n4.33\n1.9\n3.3\n4\n1.65\n3.4\n4.5\n1.78\n3.25\n4\n1.73\n3.4\n4.2\n\n\n2\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492474\n10000\n9994\n0\n0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.95\n3.2\n3.6\n1.8\n3.3\n3.95\n1.9\n3.2\n3.5\n1.9\n3.2\n3.5\nNone\nNone\nNone\n1.83\n3.3\n3.6\n1.95\n3.3\n3.8\n2\n3.25\n3.25\n1.85\n3.25\n3.75\n1.91\n3.25\n3.6\n\n\n3\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492475\n9984\n8635\n0\n3\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2.38\n3.3\n2.75\n2.4\n3.3\n2.55\n2.6\n3.1\n2.3\n2.5\n3.2\n2.5\nNone\nNone\nNone\n2.5\n3.25\n2.4\n2.63\n3.3\n2.5\n2.35\n3.25\n2.65\n2.5\n3.2\n2.5\n2.3\n3.2\n2.75\n\n\n4\n1\n1\n2008/2009\n1\n2008-08-17 00:00:00\n492476\n9991\n9998\n5\n0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.44\n3.75\n7.5\n1.4\n4\n6.8\n1.4\n3.9\n6\n1.44\n3.6\n6.5\nNone\nNone\nNone\n1.44\n3.75\n6\n1.44\n4\n7.5\n1.45\n3.75\n6.5\n1.5\n3.75\n5.5\n1.44\n3.75\n6.5\n\n\n5\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492477\n7947\n9985\n1\n3\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n5\n3.5\n1.65\n5\n3.5\n1.6\n4\n3.3\n1.7\n4\n3.4\n1.72\nNone\nNone\nNone\n4.2\n3.4\n1.7\n4.5\n3.5\n1.73\n4.5\n3.4\n1.65\n4.5\n3.5\n1.65\n4.75\n3.3\n1.67\n\n\n6\n1\n1\n2008/2009\n1\n2008-09-24 00:00:00\n492478\n8203\n8342\n1\n1\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n4.75\n3.4\n1.67\n4.85\n3.4\n1.65\n3.7\n3.2\n1.8\n5\n3.25\n1.62\nNone\nNone\nNone\n4.2\n3.4\n1.7\n5.5\n3.75\n1.67\n4.35\n3.4\n1.7\n4.5\n3.4\n1.7\nNone\nNone\nNone\n\n\n7\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492479\n9999\n8571\n2\n2\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2.1\n3.2\n3.3\n2.05\n3.25\n3.15\n1.85\n3.2\n3.5\n1.83\n3.3\n3.6\nNone\nNone\nNone\n1.83\n3.3\n3.6\n1.91\n3.4\n3.6\n2.1\n3.25\n3\n1.85\n3.25\n3.75\n2.1\n3.25\n3.1\n\n\n8\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492480\n4049\n9996\n1\n2\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n3.2\n3.4\n2.2\n2.55\n3.3\n2.4\n2.4\n3.2\n2.4\n2.5\n3.2\n2.5\nNone\nNone\nNone\n2.7\n3.25\n2.25\n2.6\n3.4\n2.4\n2.8\n3.25\n2.25\n2.8\n3.2\n2.25\n2.88\n3.25\n2.2\n\n\n9\n1\n1\n2008/2009\n1\n2008-08-16 00:00:00\n492481\n10001\n9986\n1\n0\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n2.25\n3.25\n2.88\n2.3\n3.25\n2.7\n2.1\n3.1\n3\n2.25\n3.2\n2.75\nNone\nNone\nNone\n2.2\n3.25\n2.75\n2.2\n3.3\n3.1\n2.25\n3.25\n2.8\n2.2\n3.3\n2.8\n2.25\n3.2\n2.8\n\n\n10\n1\n1\n2008/2009\n10\n2008-11-01 00:00:00\n492564\n8342\n8571\n4\n1\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\nNone\n1.3\n5.25\n9.5\n1.25\n5\n10\n1.3\n4.2\n8\n1.25\n4.5\n10\nNone\nNone\nNone\n1.35\n4.2\n7\n1.27\n5\n10\n1.3\n4.35\n8.5\n1.25\n5\n10\n1.29\n4.5\n9\n\n\n\n\n\n\n%%sql\nSELECT * FROM Player LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nplayer_api_id\nplayer_name\nplayer_fifa_api_id\nbirthday\nheight\nweight\n\n\n1\n505942\nAaron Appindangoye\n218353\n1992-02-29 00:00:00\n182.88\n187\n\n\n2\n155782\nAaron Cresswell\n189615\n1989-12-15 00:00:00\n170.18\n146\n\n\n3\n162549\nAaron Doran\n186170\n1991-05-13 00:00:00\n170.18\n163\n\n\n4\n30572\nAaron Galindo\n140161\n1982-05-08 00:00:00\n182.88\n198\n\n\n5\n23780\nAaron Hughes\n17725\n1979-11-08 00:00:00\n182.88\n154\n\n\n6\n27316\nAaron Hunt\n158138\n1986-09-04 00:00:00\n182.88\n161\n\n\n7\n564793\nAaron Kuhl\n221280\n1996-01-30 00:00:00\n172.72\n146\n\n\n8\n30895\nAaron Lennon\n152747\n1987-04-16 00:00:00\n165.1\n139\n\n\n9\n528212\nAaron Lennox\n206592\n1993-02-19 00:00:00\n190.5\n181\n\n\n10\n101042\nAaron Meijers\n188621\n1987-10-28 00:00:00\n175.26\n170\n\n\n\n\n\n\n%%sql\nSELECT * FROM Player_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nplayer_fifa_api_id\nplayer_api_id\ndate\noverall_rating\npotential\npreferred_foot\nattacking_work_rate\ndefensive_work_rate\ncrossing\nfinishing\nheading_accuracy\nshort_passing\nvolleys\ndribbling\ncurve\nfree_kick_accuracy\nlong_passing\nball_control\nacceleration\nsprint_speed\nagility\nreactions\nbalance\nshot_power\njumping\nstamina\nstrength\nlong_shots\naggression\ninterceptions\npositioning\nvision\npenalties\nmarking\nstanding_tackle\nsliding_tackle\ngk_diving\ngk_handling\ngk_kicking\ngk_positioning\ngk_reflexes\n\n\n1\n218353\n505942\n2016-02-18 00:00:00\n67\n71\nright\nmedium\nmedium\n49\n44\n71\n61\n44\n51\n45\n39\n64\n49\n60\n64\n59\n47\n65\n55\n58\n54\n76\n35\n71\n70\n45\n54\n48\n65\n69\n69\n6\n11\n10\n8\n8\n\n\n2\n218353\n505942\n2015-11-19 00:00:00\n67\n71\nright\nmedium\nmedium\n49\n44\n71\n61\n44\n51\n45\n39\n64\n49\n60\n64\n59\n47\n65\n55\n58\n54\n76\n35\n71\n70\n45\n54\n48\n65\n69\n69\n6\n11\n10\n8\n8\n\n\n3\n218353\n505942\n2015-09-21 00:00:00\n62\n66\nright\nmedium\nmedium\n49\n44\n71\n61\n44\n51\n45\n39\n64\n49\n60\n64\n59\n47\n65\n55\n58\n54\n76\n35\n63\n41\n45\n54\n48\n65\n66\n69\n6\n11\n10\n8\n8\n\n\n4\n218353\n505942\n2015-03-20 00:00:00\n61\n65\nright\nmedium\nmedium\n48\n43\n70\n60\n43\n50\n44\n38\n63\n48\n60\n64\n59\n46\n65\n54\n58\n54\n76\n34\n62\n40\n44\n53\n47\n62\n63\n66\n5\n10\n9\n7\n7\n\n\n5\n218353\n505942\n2007-02-22 00:00:00\n61\n65\nright\nmedium\nmedium\n48\n43\n70\n60\n43\n50\n44\n38\n63\n48\n60\n64\n59\n46\n65\n54\n58\n54\n76\n34\n62\n40\n44\n53\n47\n62\n63\n66\n5\n10\n9\n7\n7\n\n\n6\n189615\n155782\n2016-04-21 00:00:00\n74\n76\nleft\nhigh\nmedium\n80\n53\n58\n71\n40\n73\n70\n69\n68\n71\n79\n78\n78\n67\n90\n71\n85\n79\n56\n62\n68\n67\n60\n66\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n7\n189615\n155782\n2016-04-07 00:00:00\n74\n76\nleft\nhigh\nmedium\n80\n53\n58\n71\n32\n73\n70\n69\n68\n71\n79\n78\n78\n67\n90\n71\n85\n79\n56\n60\n68\n67\n60\n66\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n8\n189615\n155782\n2016-01-07 00:00:00\n73\n75\nleft\nhigh\nmedium\n79\n52\n57\n70\n29\n71\n68\n69\n68\n70\n79\n78\n78\n67\n90\n71\n84\n79\n56\n59\n67\n66\n58\n65\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n9\n189615\n155782\n2015-12-24 00:00:00\n73\n75\nleft\nhigh\nmedium\n79\n51\n57\n70\n29\n71\n68\n69\n68\n70\n79\n78\n78\n67\n90\n71\n84\n79\n56\n58\n67\n66\n58\n65\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n10\n189615\n155782\n2015-12-17 00:00:00\n73\n75\nleft\nhigh\nmedium\n79\n51\n57\n70\n29\n71\n68\n69\n68\n70\n79\n78\n78\n67\n90\n71\n84\n79\n56\n58\n67\n66\n58\n65\n59\n76\n75\n78\n14\n7\n9\n9\n12\n\n\n\n\n\n\n%%sql\nSELECT * FROM Team LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nteam_api_id\nteam_fifa_api_id\nteam_long_name\nteam_short_name\n\n\n1\n9987\n673\nKRC Genk\nGEN\n\n\n2\n9993\n675\nBeerschot AC\nBAC\n\n\n3\n10000\n15005\nSV Zulte-Waregem\nZUL\n\n\n4\n9994\n2007\nSporting Lokeren\nLOK\n\n\n5\n9984\n1750\nKSV Cercle Brugge\nCEB\n\n\n6\n8635\n229\nRSC Anderlecht\nAND\n\n\n7\n9991\n674\nKAA Gent\nGEN\n\n\n8\n9998\n1747\nRAEC Mons\nMON\n\n\n9\n7947\nNone\nFCV Dender EH\nDEN\n\n\n10\n9985\n232\nStandard de Liège\nSTL\n\n\n\n\n\n\n%%sql\nSELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\nid\nteam_fifa_api_id\nteam_api_id\ndate\nbuildUpPlaySpeed\nbuildUpPlaySpeedClass\nbuildUpPlayDribbling\nbuildUpPlayDribblingClass\nbuildUpPlayPassing\nbuildUpPlayPassingClass\nbuildUpPlayPositioningClass\nchanceCreationPassing\nchanceCreationPassingClass\nchanceCreationCrossing\nchanceCreationCrossingClass\nchanceCreationShooting\nchanceCreationShootingClass\nchanceCreationPositioningClass\ndefencePressure\ndefencePressureClass\ndefenceAggression\ndefenceAggressionClass\ndefenceTeamWidth\ndefenceTeamWidthClass\ndefenceDefenderLineClass\n\n\n1\n434\n9930\n2010-02-22 00:00:00\n60\nBalanced\nNone\nLittle\n50\nMixed\nOrganised\n60\nNormal\n65\nNormal\n55\nNormal\nOrganised\n50\nMedium\n55\nPress\n45\nNormal\nCover\n\n\n2\n434\n9930\n2014-09-19 00:00:00\n52\nBalanced\n48\nNormal\n56\nMixed\nOrganised\n54\nNormal\n63\nNormal\n64\nNormal\nOrganised\n47\nMedium\n44\nPress\n54\nNormal\nCover\n\n\n3\n434\n9930\n2015-09-10 00:00:00\n47\nBalanced\n41\nNormal\n54\nMixed\nOrganised\n54\nNormal\n63\nNormal\n64\nNormal\nOrganised\n47\nMedium\n44\nPress\n54\nNormal\nCover\n\n\n4\n77\n8485\n2010-02-22 00:00:00\n70\nFast\nNone\nLittle\n70\nLong\nOrganised\n70\nRisky\n70\nLots\n70\nLots\nOrganised\n60\nMedium\n70\nDouble\n70\nWide\nCover\n\n\n5\n77\n8485\n2011-02-22 00:00:00\n47\nBalanced\nNone\nLittle\n52\nMixed\nOrganised\n53\nNormal\n48\nNormal\n52\nNormal\nOrganised\n47\nMedium\n47\nPress\n52\nNormal\nCover\n\n\n6\n77\n8485\n2012-02-22 00:00:00\n58\nBalanced\nNone\nLittle\n62\nMixed\nOrganised\n45\nNormal\n70\nLots\n55\nNormal\nOrganised\n40\nMedium\n40\nPress\n60\nNormal\nCover\n\n\n7\n77\n8485\n2013-09-20 00:00:00\n62\nBalanced\nNone\nLittle\n45\nMixed\nOrganised\n40\nNormal\n50\nNormal\n55\nNormal\nOrganised\n42\nMedium\n42\nPress\n60\nNormal\nCover\n\n\n8\n77\n8485\n2014-09-19 00:00:00\n58\nBalanced\n64\nNormal\n62\nMixed\nOrganised\n56\nNormal\n68\nLots\n57\nNormal\nOrganised\n41\nMedium\n42\nPress\n60\nNormal\nCover\n\n\n9\n77\n8485\n2015-09-10 00:00:00\n59\nBalanced\n64\nNormal\n53\nMixed\nOrganised\n51\nNormal\n72\nLots\n63\nNormal\nFree Form\n49\nMedium\n45\nPress\n63\nNormal\nCover\n\n\n10\n614\n8576\n2010-02-22 00:00:00\n60\nBalanced\nNone\nLittle\n40\nMixed\nOrganised\n45\nNormal\n35\nNormal\n55\nNormal\nOrganised\n30\nDeep\n70\nDouble\n30\nNarrow\nOffside Trap\n\n\n\n\n\n\n%%sql\nCREATE TABLE Team_table AS\nSELECT * FROM Team_Attributes LIMIT 10;\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n\n\n\n%%sql\nDROP TABLE IF EXISTS Team_table\n\n * sqlite:///database.sqlite\nDone.\n\n\n[]\n\n\n\nsql_query = %sql SELECT * FROM Team LIMIT 10\ndf = sql_query.DataFrame()\n\n * sqlite:///database.sqlite\nDone.\n\n\n\ndf\n\n\n\n\n\n\n\n\nid\nteam_api_id\nteam_fifa_api_id\nteam_long_name\nteam_short_name\n\n\n\n\n0\n1\n9987\n673.0\nKRC Genk\nGEN\n\n\n1\n2\n9993\n675.0\nBeerschot AC\nBAC\n\n\n2\n3\n10000\n15005.0\nSV Zulte-Waregem\nZUL\n\n\n3\n4\n9994\n2007.0\nSporting Lokeren\nLOK\n\n\n4\n5\n9984\n1750.0\nKSV Cercle Brugge\nCEB\n\n\n5\n6\n8635\n229.0\nRSC Anderlecht\nAND\n\n\n6\n7\n9991\n674.0\nKAA Gent\nGEN\n\n\n7\n8\n9998\n1747.0\nRAEC Mons\nMON\n\n\n8\n9\n7947\nNaN\nFCV Dender EH\nDEN\n\n\n9\n10\n9985\n232.0\nStandard de Liège\nSTL\n\n\n\n\n\n\n\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,7))\n\nplot = %sql SELECT team_short_name, count(*) FROM Team GROUP BY team_short_name ORDER BY team_short_name\nplot.bar();\n\n * sqlite:///database.sqlite\nDone.\n\n\n\n\n\n\nplot.pie();\n\n\n\n\n\ntype(plot)\n\nsql.run.ResultSet\n\n\n\n# #Imports\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# import sqlite3\n# import matplotlib.pyplot as plt\n\n# # Input data files are available in the \"../input/\" directory.\n# # For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# path = \"../input/\"  #Insert path here\n# database = path + 'database.sqlite'\n\nFirst we will create the connection to the DB, and see what tables we have\n\ntables = pd.read_sql(\"\"\"SELECT *\n                        FROM sqlite_master\n                        WHERE type='table';\"\"\", connection)\ntables\n\n\n\n\n\n\n\n\ntype\nname\ntbl_name\nrootpage\nsql\n\n\n\n\n0\ntable\nsqlite_sequence\nsqlite_sequence\n4\nCREATE TABLE sqlite_sequence(name,seq)\n\n\n1\ntable\nPlayer_Attributes\nPlayer_Attributes\n11\nCREATE TABLE \"Player_Attributes\" (\\n\\t`id`\\tIN...\n\n\n2\ntable\nPlayer\nPlayer\n14\nCREATE TABLE `Player` (\\n\\t`id`\\tINTEGER PRIMA...\n\n\n3\ntable\nMatch\nMatch\n18\nCREATE TABLE `Match` (\\n\\t`id`\\tINTEGER PRIMAR...\n\n\n4\ntable\nLeague\nLeague\n24\nCREATE TABLE `League` (\\n\\t`id`\\tINTEGER PRIMA...\n\n\n5\ntable\nCountry\nCountry\n26\nCREATE TABLE `Country` (\\n\\t`id`\\tINTEGER PRIM...\n\n\n6\ntable\nTeam\nTeam\n29\nCREATE TABLE \"Team\" (\\n\\t`id`\\tINTEGER PRIMARY...\n\n\n7\ntable\nTeam_Attributes\nTeam_Attributes\n2\nCREATE TABLE `Team_Attributes` (\\n\\t`id`\\tINTE...\n\n\n\n\n\n\n\n\ncountries = pd.read_sql(\"\"\"SELECT *\n                        FROM Country;\"\"\", connection)\ncountries\n\n\n\n\n\n\n\n\nid\nname\n\n\n\n\n0\n1\nBelgium\n\n\n1\n1729\nEngland\n\n\n2\n4769\nFrance\n\n\n3\n7809\nGermany\n\n\n4\n10257\nItaly\n\n\n5\n13274\nNetherlands\n\n\n6\n15722\nPoland\n\n\n7\n17642\nPortugal\n\n\n8\n19694\nScotland\n\n\n9\n21518\nSpain\n\n\n10\n24558\nSwitzerland\n\n\n\n\n\n\n\n\nleagues = pd.read_sql(\"\"\"SELECT *\n                        FROM League\n                        JOIN Country ON Country.id = League.country_id;\"\"\", connection)\nleagues\n\n\n\n\n\n\n\n\nid\ncountry_id\nname\nid\nname\n\n\n\n\n0\n1\n1\nBelgium Jupiler League\n1\nBelgium\n\n\n1\n1729\n1729\nEngland Premier League\n1729\nEngland\n\n\n2\n4769\n4769\nFrance Ligue 1\n4769\nFrance\n\n\n3\n7809\n7809\nGermany 1. Bundesliga\n7809\nGermany\n\n\n4\n10257\n10257\nItaly Serie A\n10257\nItaly\n\n\n5\n13274\n13274\nNetherlands Eredivisie\n13274\nNetherlands\n\n\n6\n15722\n15722\nPoland Ekstraklasa\n15722\nPoland\n\n\n7\n17642\n17642\nPortugal Liga ZON Sagres\n17642\nPortugal\n\n\n8\n19694\n19694\nScotland Premier League\n19694\nScotland\n\n\n9\n21518\n21518\nSpain LIGA BBVA\n21518\nSpain\n\n\n10\n24558\n24558\nSwitzerland Super League\n24558\nSwitzerland\n\n\n\n\n\n\n\n\nteams = pd.read_sql(\"\"\"SELECT *\n                        FROM Team\n                        ORDER BY team_long_name\n                        LIMIT 10;\"\"\", connection)\nteams\n\n\n\n\n\n\n\n\nid\nteam_api_id\nteam_fifa_api_id\nteam_long_name\nteam_short_name\n\n\n\n\n0\n16848\n8350\n29\n1. FC Kaiserslautern\nKAI\n\n\n1\n15624\n8722\n31\n1. FC Köln\nFCK\n\n\n2\n16239\n8165\n171\n1. FC Nürnberg\nNUR\n\n\n3\n16243\n9905\n169\n1. FSV Mainz 05\nMAI\n\n\n4\n11817\n8576\n614\nAC Ajaccio\nAJA\n\n\n5\n11074\n108893\n111989\nAC Arles-Avignon\nARL\n\n\n6\n49116\n6493\n1714\nAC Bellinzona\nBEL\n\n\n7\n26560\n10217\n650\nADO Den Haag\nHAA\n\n\n8\n9537\n8583\n57\nAJ Auxerre\nAUX\n\n\n9\n9547\n9829\n69\nAS Monaco\nMON\n\n\n\n\n\n\n\n\ndetailed_matches = pd.read_sql(\"\"\"SELECT Match.id, \n                                        Country.name AS country_name, \n                                        League.name AS league_name, \n                                        season, \n                                        stage, \n                                        date,\n                                        HT.team_long_name AS  home_team,\n                                        AT.team_long_name AS away_team,\n                                        home_team_goal, \n                                        away_team_goal                                        \n                                FROM Match\n                                JOIN Country on Country.id = Match.country_id\n                                JOIN League on League.id = Match.league_id\n                                LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id\n                                LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id\n                                WHERE country_name = 'Spain'\n                                ORDER by date\n                                LIMIT 10;\"\"\", connection)\ndetailed_matches\n\n\n\n\n\n\n\n\nid\ncountry_name\nleague_name\nseason\nstage\ndate\nhome_team\naway_team\nhome_team_goal\naway_team_goal\n\n\n\n\n0\n21518\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-30 00:00:00\nValencia CF\nRCD Mallorca\n3\n0\n\n\n1\n21525\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-30 00:00:00\nRCD Espanyol\nReal Valladolid\n1\n0\n\n\n2\n21519\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-31 00:00:00\nCA Osasuna\nVillarreal CF\n1\n1\n\n\n3\n21520\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-31 00:00:00\nRC Deportivo de La Coruña\nReal Madrid CF\n2\n1\n\n\n4\n21521\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-31 00:00:00\nCD Numancia\nFC Barcelona\n1\n0\n\n\n5\n21522\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-31 00:00:00\nRacing Santander\nSevilla FC\n1\n1\n\n\n6\n21523\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-31 00:00:00\nReal Sporting de Gijón\nGetafe CF\n1\n2\n\n\n7\n21524\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-31 00:00:00\nReal Betis Balompié\nRC Recreativo\n0\n1\n\n\n8\n21526\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-31 00:00:00\nAthletic Club de Bilbao\nUD Almería\n1\n3\n\n\n9\n21527\nSpain\nSpain LIGA BBVA\n2008/2009\n1\n2008-08-31 00:00:00\nAtlético Madrid\nMálaga CF\n4\n0\n\n\n\n\n\n\n\n\n# Star with Spain Real Madrid CF, barcelonat\n\n\n\n\nleages_by_season = pd.read_sql(\"\"\"SELECT Country.name AS country_name, \n                                        League.name AS league_name, \n                                        season,\n                                        count(distinct stage) AS number_of_stages,\n                                        count(distinct HT.team_long_name) AS number_of_teams,\n                                        avg(home_team_goal) AS avg_home_team_scors, \n                                        avg(away_team_goal) AS avg_away_team_goals, \n                                        avg(home_team_goal-away_team_goal) AS avg_goal_dif, \n                                        avg(home_team_goal+away_team_goal) AS avg_goals, \n                                        sum(home_team_goal+away_team_goal) AS total_goals                                       \n                                FROM Match\n                                JOIN Country on Country.id = Match.country_id\n                                JOIN League on League.id = Match.league_id\n                                LEFT JOIN Team AS HT on HT.team_api_id = Match.home_team_api_id\n                                LEFT JOIN Team AS AT on AT.team_api_id = Match.away_team_api_id\n                                WHERE country_name in ('Spain', 'Germany', 'France', 'Italy', 'England')\n                                GROUP BY Country.name, League.name, season\n                                HAVING count(distinct stage) &gt; 10\n                                ORDER BY Country.name, League.name, season DESC\n                                ;\"\"\", connection)\nleages_by_season\n\n\n\n\n\n\n\n\n\ncountry_name\nleague_name\nseason\nnumber_of_stages\nnumber_of_teams\navg_home_team_scors\navg_away_team_goals\navg_goal_dif\navg_goals\ntotal_goals\n\n\n\n\n0\nEngland\nEngland Premier League\n2015/2016\n38\n20\n1.492105\n1.207895\n0.284211\n2.700000\n1026\n\n\n1\nEngland\nEngland Premier League\n2014/2015\n38\n20\n1.473684\n1.092105\n0.381579\n2.565789\n975\n\n\n2\nEngland\nEngland Premier League\n2013/2014\n38\n20\n1.573684\n1.194737\n0.378947\n2.768421\n1052\n\n\n3\nEngland\nEngland Premier League\n2012/2013\n38\n20\n1.557895\n1.239474\n0.318421\n2.797368\n1063\n\n\n4\nEngland\nEngland Premier League\n2011/2012\n38\n20\n1.589474\n1.215789\n0.373684\n2.805263\n1066\n\n\n5\nEngland\nEngland Premier League\n2010/2011\n38\n20\n1.623684\n1.173684\n0.450000\n2.797368\n1063\n\n\n6\nEngland\nEngland Premier League\n2009/2010\n38\n20\n1.697368\n1.073684\n0.623684\n2.771053\n1053\n\n\n7\nEngland\nEngland Premier League\n2008/2009\n38\n20\n1.400000\n1.078947\n0.321053\n2.478947\n942\n\n\n8\nFrance\nFrance Ligue 1\n2015/2016\n38\n20\n1.436842\n1.089474\n0.347368\n2.526316\n960\n\n\n9\nFrance\nFrance Ligue 1\n2014/2015\n38\n20\n1.410526\n1.081579\n0.328947\n2.492105\n947\n\n\n10\nFrance\nFrance Ligue 1\n2013/2014\n38\n20\n1.415789\n1.039474\n0.376316\n2.455263\n933\n\n\n11\nFrance\nFrance Ligue 1\n2012/2013\n38\n20\n1.468421\n1.076316\n0.392105\n2.544737\n967\n\n\n12\nFrance\nFrance Ligue 1\n2011/2012\n38\n20\n1.473684\n1.042105\n0.431579\n2.515789\n956\n\n\n13\nFrance\nFrance Ligue 1\n2010/2011\n38\n20\n1.342105\n1.000000\n0.342105\n2.342105\n890\n\n\n14\nFrance\nFrance Ligue 1\n2009/2010\n38\n20\n1.389474\n1.021053\n0.368421\n2.410526\n916\n\n\n15\nFrance\nFrance Ligue 1\n2008/2009\n38\n20\n1.286842\n0.971053\n0.315789\n2.257895\n858\n\n\n16\nGermany\nGermany 1. Bundesliga\n2015/2016\n34\n18\n1.565359\n1.264706\n0.300654\n2.830065\n866\n\n\n17\nGermany\nGermany 1. Bundesliga\n2014/2015\n34\n18\n1.588235\n1.166667\n0.421569\n2.754902\n843\n\n\n18\nGermany\nGermany 1. Bundesliga\n2013/2014\n34\n18\n1.748366\n1.411765\n0.336601\n3.160131\n967\n\n\n19\nGermany\nGermany 1. Bundesliga\n2012/2013\n34\n18\n1.591503\n1.343137\n0.248366\n2.934641\n898\n\n\n20\nGermany\nGermany 1. Bundesliga\n2011/2012\n34\n18\n1.660131\n1.199346\n0.460784\n2.859477\n875\n\n\n21\nGermany\nGermany 1. Bundesliga\n2010/2011\n34\n18\n1.647059\n1.274510\n0.372549\n2.921569\n894\n\n\n22\nGermany\nGermany 1. Bundesliga\n2009/2010\n34\n18\n1.513072\n1.316993\n0.196078\n2.830065\n866\n\n\n23\nGermany\nGermany 1. Bundesliga\n2008/2009\n34\n18\n1.699346\n1.222222\n0.477124\n2.921569\n894\n\n\n24\nItaly\nItaly Serie A\n2015/2016\n38\n20\n1.471053\n1.105263\n0.365789\n2.576316\n979\n\n\n25\nItaly\nItaly Serie A\n2014/2015\n38\n20\n1.498681\n1.187335\n0.311346\n2.686016\n1018\n\n\n26\nItaly\nItaly Serie A\n2013/2014\n38\n20\n1.536842\n1.186842\n0.350000\n2.723684\n1035\n\n\n27\nItaly\nItaly Serie A\n2012/2013\n38\n20\n1.494737\n1.144737\n0.350000\n2.639474\n1003\n\n\n28\nItaly\nItaly Serie A\n2011/2012\n38\n20\n1.511173\n1.072626\n0.438547\n2.583799\n925\n\n\n29\nItaly\nItaly Serie A\n2010/2011\n38\n20\n1.431579\n1.081579\n0.350000\n2.513158\n955\n\n\n30\nItaly\nItaly Serie A\n2009/2010\n38\n20\n1.542105\n1.068421\n0.473684\n2.610526\n992\n\n\n31\nItaly\nItaly Serie A\n2008/2009\n38\n20\n1.521053\n1.078947\n0.442105\n2.600000\n988\n\n\n32\nSpain\nSpain LIGA BBVA\n2015/2016\n38\n20\n1.618421\n1.126316\n0.492105\n2.744737\n1043\n\n\n33\nSpain\nSpain LIGA BBVA\n2014/2015\n38\n20\n1.536842\n1.118421\n0.418421\n2.655263\n1009\n\n\n34\nSpain\nSpain LIGA BBVA\n2013/2014\n38\n20\n1.631579\n1.118421\n0.513158\n2.750000\n1045\n\n\n35\nSpain\nSpain LIGA BBVA\n2012/2013\n38\n20\n1.686842\n1.184211\n0.502632\n2.871053\n1091\n\n\n36\nSpain\nSpain LIGA BBVA\n2011/2012\n38\n20\n1.678947\n1.084211\n0.594737\n2.763158\n1050\n\n\n37\nSpain\nSpain LIGA BBVA\n2010/2011\n38\n20\n1.636842\n1.105263\n0.531579\n2.742105\n1042\n\n\n38\nSpain\nSpain LIGA BBVA\n2009/2010\n38\n20\n1.600000\n1.113158\n0.486842\n2.713158\n1031\n\n\n39\nSpain\nSpain LIGA BBVA\n2008/2009\n38\n20\n1.660526\n1.236842\n0.423684\n2.897368\n1101\n\n\n\n\n\n\n\n\ndf = pd.DataFrame(index=np.sort(leages_by_season['season'].unique()), columns=leages_by_season['country_name'].unique())\n\ndf.loc[:,'Germany'] = list(leages_by_season.loc[leages_by_season['country_name']=='Germany','avg_goals'])\ndf.loc[:,'Spain']   = list(leages_by_season.loc[leages_by_season['country_name']=='Spain','avg_goals'])\ndf.loc[:,'France']   = list(leages_by_season.loc[leages_by_season['country_name']=='France','avg_goals'])\ndf.loc[:,'Italy']   = list(leages_by_season.loc[leages_by_season['country_name']=='Italy','avg_goals'])\ndf.loc[:,'England']   = list(leages_by_season.loc[leages_by_season['country_name']=='England','avg_goals'])\n\ndf.plot(figsize=(12,5),title='Average Goals per Game Over Time')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb9c3770780&gt;\n\n\n\n\n\n\ndf = pd.DataFrame(index=np.sort(leages_by_season['season'].unique()), columns=leages_by_season['country_name'].unique())\n\ndf.loc[:,'Germany'] = list(leages_by_season.loc[leages_by_season['country_name']=='Germany','avg_goal_dif'])\ndf.loc[:,'Spain']   = list(leages_by_season.loc[leages_by_season['country_name']=='Spain','avg_goal_dif'])\ndf.loc[:,'France']   = list(leages_by_season.loc[leages_by_season['country_name']=='France','avg_goal_dif'])\ndf.loc[:,'Italy']   = list(leages_by_season.loc[leages_by_season['country_name']=='Italy','avg_goal_dif'])\ndf.loc[:,'England']   = list(leages_by_season.loc[leages_by_season['country_name']=='England','avg_goal_dif'])\n\ndf.plot(figsize=(12,5),title='Average Goals Difference Home vs Out')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb9c39687f0&gt;\n\n\n\n\n\n\n\n\nplayers_height = pd.read_sql(\"\"\"SELECT CASE\n                                        WHEN ROUND(height)&lt;165 then 165\n                                        WHEN ROUND(height)&gt;195 then 195\n                                        ELSE ROUND(height)\n                                        END AS calc_height, \n                                        COUNT(height) AS distribution, \n                                        (avg(PA_Grouped.avg_overall_rating)) AS avg_overall_rating,\n                                        (avg(PA_Grouped.avg_potential)) AS avg_potential,\n                                        AVG(weight) AS avg_weight \n                            FROM PLAYER\n                            LEFT JOIN (SELECT Player_Attributes.player_api_id, \n                                        avg(Player_Attributes.overall_rating) AS avg_overall_rating,\n                                        avg(Player_Attributes.potential) AS avg_potential  \n                                        FROM Player_Attributes\n                                        GROUP BY Player_Attributes.player_api_id) \n                                        AS PA_Grouped ON PLAYER.player_api_id = PA_Grouped.player_api_id\n                            GROUP BY calc_height\n                            ORDER BY calc_height\n                                ;\"\"\", connection)\nplayers_height\n\n\n\n\n\n\n\n\ncalc_height\ndistribution\navg_overall_rating\navg_potential\navg_weight\n\n\n\n\n0\n165.0\n74\n67.365543\n73.327754\n139.459459\n\n\n1\n168.0\n118\n67.500518\n73.124182\n144.127119\n\n\n2\n170.0\n403\n67.726903\n73.379056\n147.799007\n\n\n3\n173.0\n530\n66.980272\n72.848746\n152.824528\n\n\n4\n175.0\n1188\n66.805204\n72.258774\n156.111953\n\n\n5\n178.0\n1489\n66.367212\n71.943339\n160.665547\n\n\n6\n180.0\n1388\n66.419053\n71.846394\n165.261527\n\n\n7\n183.0\n1954\n66.634380\n71.754555\n170.167861\n\n\n8\n185.0\n1278\n66.928964\n71.833475\n174.636933\n\n\n9\n188.0\n1305\n67.094253\n72.151949\n179.278161\n\n\n10\n191.0\n652\n66.997649\n71.846159\n184.791411\n\n\n11\n193.0\n470\n67.485141\n72.459225\n188.795745\n\n\n12\n195.0\n211\n67.425619\n72.615373\n196.464455\n\n\n\n\n\n\n\n\nplayers_height.calc_height\n\n0     165.0\n1     168.0\n2     170.0\n3     173.0\n4     175.0\n5     178.0\n6     180.0\n7     183.0\n8     185.0\n9     188.0\n10    191.0\n11    193.0\n12    195.0\nName: calc_height, dtype: float64\n\n\n\n# players_height.plot(x=['calc_height'],y=['avg_overall_rating'],figsize=(12,5),title='Potential vs Height')"
  },
  {
    "objectID": "posts/2020-11-13-Timeseries.html",
    "href": "posts/2020-11-13-Timeseries.html",
    "title": "Timeseries",
    "section": "",
    "text": "try:\n    %tensorflow_version 2.x\n    COLAB = True\n    print(\"Note: using Google CoLab\")\nexcept:\n    print(\"Note: not using Google CoLab\")\n    COLAB = False\n\nNote: using Google CoLab\n\n\n\n# \n\nx = [\n    [32],\n    [41],\n    [39],\n    [20],\n    [15]\n]\n\ny = [\n    1,\n    -1,\n    0,\n    -1,\n    1\n]\n\nprint(x)\nprint(y)\n\n[[32], [41], [39], [20], [15]]\n[1, -1, 0, -1, 1]\n\n\n\nfrom IPython.display import display, HTML\nimport pandas as pd\nimport numpy as np\n\nx = np.array(x)\nprint(x[:,0])\n\n\ndf = pd.DataFrame({'x':x[:,0], 'y':y})\ndisplay(df)\n\n[32 41 39 20 15]\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n32\n1\n\n\n1\n41\n-1\n\n\n2\n39\n0\n\n\n3\n20\n-1\n\n\n4\n15\n1\n\n\n\n\n\n\n\n\nx = [\n    [32,1383],\n    [41,2928],\n    [39,8823],\n    [20,1252],\n    [15,1532]\n]\n\ny = [\n    1,\n    -1,\n    0,\n    -1,\n    1\n]\n\nprint(x)\nprint(y)\n\n[[32, 1383], [41, 2928], [39, 8823], [20, 1252], [15, 1532]]\n[1, -1, 0, -1, 1]\n\n\n\nfrom IPython.display import display, HTML\nimport pandas as pd\nimport numpy as np\n\nx = np.array(x)\nprint(x[:,0])\n\n\ndf = pd.DataFrame({'price':x[:,0], 'volume':x[:,1], 'y':y})\ndisplay(df)\n\n[32 41 39 20 15]\n\n\n\n\n\n\n\n\n\nprice\nvolume\ny\n\n\n\n\n0\n32\n1383\n1\n\n\n1\n41\n2928\n-1\n\n\n2\n39\n8823\n0\n\n\n3\n20\n1252\n-1\n\n\n4\n15\n1532\n1\n\n\n\n\n\n\n\n\nx = [\n    [[32,1383],[41,2928],[39,8823],[20,1252],[15,1532]],\n    [[35,8272],[32,1383],[41,2928],[39,8823],[20,1252]],\n    [[37,2738],[35,8272],[32,1383],[41,2928],[39,8823]],\n    [[34,2845],[37,2738],[35,8272],[32,1383],[41,2928]],\n    [[32,2345],[34,2845],[37,2738],[35,8272],[32,1383]],\n]\n\ny = [\n    1,\n    -1,\n    0,\n    -1,\n    1\n]\n\nprint(x)\nprint(y)\n\n[[[32, 1383], [41, 2928], [39, 8823], [20, 1252], [15, 1532]], [[35, 8272], [32, 1383], [41, 2928], [39, 8823], [20, 1252]], [[37, 2738], [35, 8272], [32, 1383], [41, 2928], [39, 8823]], [[34, 2845], [37, 2738], [35, 8272], [32, 1383], [41, 2928]], [[32, 2345], [34, 2845], [37, 2738], [35, 8272], [32, 1383]]]\n[1, -1, 0, -1, 1]\n\n\nEven if there is only one feature (price), the 3rd dimension must be used:\n\nx = [\n    [[32],[41],[39],[20],[15]],\n    [[35],[32],[41],[39],[20]],\n    [[37],[35],[32],[41],[39]],\n    [[34],[37],[35],[32],[41]],\n    [[32],[34],[37],[35],[32]],\n]\n\ny = [\n    1,\n    -1,\n    0,\n    -1,\n    1\n]\n\nprint(x)\nprint(y)\n\n[[[32], [41], [39], [20], [15]], [[35], [32], [41], [39], [20]], [[37], [35], [32], [41], [39]], [[34], [37], [35], [32], [41]], [[32], [34], [37], [35], [32]]]\n[1, -1, 0, -1, 1]"
  },
  {
    "objectID": "posts/2020-11-13-Timeseries.html#credit-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "href": "posts/2020-11-13-Timeseries.html#credit-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "title": "Timeseries",
    "section": "",
    "text": "try:\n    %tensorflow_version 2.x\n    COLAB = True\n    print(\"Note: using Google CoLab\")\nexcept:\n    print(\"Note: not using Google CoLab\")\n    COLAB = False\n\nNote: using Google CoLab\n\n\n\n# \n\nx = [\n    [32],\n    [41],\n    [39],\n    [20],\n    [15]\n]\n\ny = [\n    1,\n    -1,\n    0,\n    -1,\n    1\n]\n\nprint(x)\nprint(y)\n\n[[32], [41], [39], [20], [15]]\n[1, -1, 0, -1, 1]\n\n\n\nfrom IPython.display import display, HTML\nimport pandas as pd\nimport numpy as np\n\nx = np.array(x)\nprint(x[:,0])\n\n\ndf = pd.DataFrame({'x':x[:,0], 'y':y})\ndisplay(df)\n\n[32 41 39 20 15]\n\n\n\n\n\n\n\n\n\nx\ny\n\n\n\n\n0\n32\n1\n\n\n1\n41\n-1\n\n\n2\n39\n0\n\n\n3\n20\n-1\n\n\n4\n15\n1\n\n\n\n\n\n\n\n\nx = [\n    [32,1383],\n    [41,2928],\n    [39,8823],\n    [20,1252],\n    [15,1532]\n]\n\ny = [\n    1,\n    -1,\n    0,\n    -1,\n    1\n]\n\nprint(x)\nprint(y)\n\n[[32, 1383], [41, 2928], [39, 8823], [20, 1252], [15, 1532]]\n[1, -1, 0, -1, 1]\n\n\n\nfrom IPython.display import display, HTML\nimport pandas as pd\nimport numpy as np\n\nx = np.array(x)\nprint(x[:,0])\n\n\ndf = pd.DataFrame({'price':x[:,0], 'volume':x[:,1], 'y':y})\ndisplay(df)\n\n[32 41 39 20 15]\n\n\n\n\n\n\n\n\n\nprice\nvolume\ny\n\n\n\n\n0\n32\n1383\n1\n\n\n1\n41\n2928\n-1\n\n\n2\n39\n8823\n0\n\n\n3\n20\n1252\n-1\n\n\n4\n15\n1532\n1\n\n\n\n\n\n\n\n\nx = [\n    [[32,1383],[41,2928],[39,8823],[20,1252],[15,1532]],\n    [[35,8272],[32,1383],[41,2928],[39,8823],[20,1252]],\n    [[37,2738],[35,8272],[32,1383],[41,2928],[39,8823]],\n    [[34,2845],[37,2738],[35,8272],[32,1383],[41,2928]],\n    [[32,2345],[34,2845],[37,2738],[35,8272],[32,1383]],\n]\n\ny = [\n    1,\n    -1,\n    0,\n    -1,\n    1\n]\n\nprint(x)\nprint(y)\n\n[[[32, 1383], [41, 2928], [39, 8823], [20, 1252], [15, 1532]], [[35, 8272], [32, 1383], [41, 2928], [39, 8823], [20, 1252]], [[37, 2738], [35, 8272], [32, 1383], [41, 2928], [39, 8823]], [[34, 2845], [37, 2738], [35, 8272], [32, 1383], [41, 2928]], [[32, 2345], [34, 2845], [37, 2738], [35, 8272], [32, 1383]]]\n[1, -1, 0, -1, 1]\n\n\nEven if there is only one feature (price), the 3rd dimension must be used:\n\nx = [\n    [[32],[41],[39],[20],[15]],\n    [[35],[32],[41],[39],[20]],\n    [[37],[35],[32],[41],[39]],\n    [[34],[37],[35],[32],[41]],\n    [[32],[34],[37],[35],[32]],\n]\n\ny = [\n    1,\n    -1,\n    0,\n    -1,\n    1\n]\n\nprint(x)\nprint(y)\n\n[[[32], [41], [39], [20], [15]], [[35], [32], [41], [39], [20]], [[37], [35], [32], [41], [39]], [[34], [37], [35], [32], [41]], [[32], [34], [37], [35], [32]]]\n[1, -1, 0, -1, 1]"
  },
  {
    "objectID": "posts/2020-12-10_EDA_Health_Data.html",
    "href": "posts/2020-12-10_EDA_Health_Data.html",
    "title": "EDA for Health Data (Pipeline Step 2)",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n30669\nMale\n3.0\n0\n0\nNo\nchildren\nRural\n95.12\n18.0\nNaN\n0\n\n\n1\n30468\nMale\n58.0\n1\n0\nYes\nPrivate\nUrban\n87.96\n39.2\nnever smoked\n0\n\n\n2\n16523\nFemale\n8.0\n0\n0\nNo\nPrivate\nUrban\n110.89\n17.6\nNaN\n0\n\n\n3\n56543\nFemale\n70.0\n0\n0\nYes\nPrivate\nRural\n69.04\n35.9\nformerly smoked\n0\n\n\n4\n46136\nMale\n14.0\n0\n0\nNo\nNever_worked\nRural\n161.28\n19.1\nNaN\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n43395\n56196\nFemale\n10.0\n0\n0\nNo\nchildren\nUrban\n58.64\n20.4\nnever smoked\n0\n\n\n43396\n5450\nFemale\n56.0\n0\n0\nYes\nGovt_job\nUrban\n213.61\n55.4\nformerly smoked\n0\n\n\n43397\n28375\nFemale\n82.0\n1\n0\nYes\nPrivate\nUrban\n91.94\n28.9\nformerly smoked\n0\n\n\n43398\n27973\nMale\n40.0\n0\n0\nYes\nPrivate\nUrban\n99.16\n33.2\nnever smoked\n0\n\n\n43399\n36271\nFemale\n82.0\n0\n0\nYes\nPrivate\nUrban\n79.48\n20.6\nnever smoked\n0\n\n\n\n\n43400 rows × 12 columns\n#df = df.drop(columns = ['id'])\nimport matplotlib.pyplot as plt\n\nfrom pandas_profiling import ProfileReport\n\nprofile = ProfileReport(df, title='Pandas Profiling Report')\nprofile.to_widgets()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n30669\nMale\n3.0\n0\n0\nNo\nchildren\nRural\n95.12\n18.0\nNaN\n0\n\n\n1\n30468\nMale\n58.0\n1\n0\nYes\nPrivate\nUrban\n87.96\n39.2\nnever smoked\n0\n\n\n2\n16523\nFemale\n8.0\n0\n0\nNo\nPrivate\nUrban\n110.89\n17.6\nNaN\n0\n\n\n3\n56543\nFemale\n70.0\n0\n0\nYes\nPrivate\nRural\n69.04\n35.9\nformerly smoked\n0\n\n\n4\n46136\nMale\n14.0\n0\n0\nNo\nNever_worked\nRural\n161.28\n19.1\nNaN\n0\n\n\n5\n32257\nFemale\n47.0\n0\n0\nYes\nPrivate\nUrban\n210.95\n50.1\nNaN\n0\n\n\n6\n52800\nFemale\n52.0\n0\n0\nYes\nPrivate\nUrban\n77.59\n17.7\nformerly smoked\n0\n\n\n7\n41413\nFemale\n75.0\n0\n1\nYes\nSelf-employed\nRural\n243.53\n27.0\nnever smoked\n0\n\n\n8\n15266\nFemale\n32.0\n0\n0\nYes\nPrivate\nRural\n77.67\n32.3\nsmokes\n0\n\n\n9\n28674\nFemale\n74.0\n1\n0\nYes\nSelf-employed\nUrban\n205.84\n54.6\nnever smoked\n0\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n43390\n10096\nFemale\n69.0\n0\n0\nYes\nSelf-employed\nUrban\n229.85\n31.2\nnever smoked\n0\n\n\n43391\n30077\nMale\n6.0\n0\n0\nNo\nchildren\nUrban\n77.48\n19.1\nNaN\n0\n\n\n43392\n45266\nFemale\n18.0\n0\n0\nNo\nPrivate\nUrban\n131.96\n22.8\nNaN\n0\n\n\n43393\n69344\nMale\n39.0\n0\n0\nYes\nPrivate\nRural\n132.22\n31.6\nnever smoked\n0\n\n\n43394\n52380\nMale\n47.0\n0\n0\nNo\nGovt_job\nUrban\n68.52\n25.2\nformerly smoked\n0\n\n\n43395\n56196\nFemale\n10.0\n0\n0\nNo\nchildren\nUrban\n58.64\n20.4\nnever smoked\n0\n\n\n43396\n5450\nFemale\n56.0\n0\n0\nYes\nGovt_job\nUrban\n213.61\n55.4\nformerly smoked\n0\n\n\n43397\n28375\nFemale\n82.0\n1\n0\nYes\nPrivate\nUrban\n91.94\n28.9\nformerly smoked\n0\n\n\n43398\n27973\nMale\n40.0\n0\n0\nYes\nPrivate\nUrban\n99.16\n33.2\nnever smoked\n0\n\n\n43399\n36271\nFemale\n82.0\n0\n0\nYes\nPrivate\nUrban\n79.48\n20.6\nnever smoked\n0\nprofile.to_notebook_iframe()\nprofile.to_file(output_file=\"pandas_profiling.html\")"
  },
  {
    "objectID": "posts/2020-12-10_EDA_Health_Data.html#find-and-plot-missingness-in-the-data",
    "href": "posts/2020-12-10_EDA_Health_Data.html#find-and-plot-missingness-in-the-data",
    "title": "EDA for Health Data (Pipeline Step 2)",
    "section": "Find and plot missingness in the data",
    "text": "Find and plot missingness in the data\n\npct_list = []\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    if round(pct_missing*100) &gt;0:\n        pct_list.append([col, round(pct_missing*100)])\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\nid - 0%\ngender - 0%\nage - 0%\nhypertension - 0%\nheart_disease - 0%\never_married - 0%\nwork_type - 0%\nResidence_type - 0%\navg_glucose_level - 0%\nbmi - 3%\nsmoking_status - 31%\nstroke - 0%\n\n\n\ncols = df.columns \ncolours = ['darkblue', 'red'] \nsns.heatmap(df[cols].isnull(), cmap=sns.color_palette(colours))\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\ndf.groupby([\"age\", 'heart_disease'])['stroke'].agg(['sum']).round(0)\n\n\n\n\n\n\n\n\n\nsum\n\n\nage\nheart_disease\n\n\n\n\n\n0.08\n0\n0\n\n\n0.16\n0\n0\n\n\n0.24\n0\n0\n\n\n0.32\n0\n0\n\n\n0.40\n0\n0\n\n\n...\n...\n...\n\n\n80.00\n1\n13\n\n\n81.00\n0\n32\n\n\n1\n11\n\n\n82.00\n0\n24\n\n\n1\n12\n\n\n\n\n163 rows × 1 columns\n\n\n\n\ndf.dtypes\n\nid                     int64\ngender                object\nage                  float64\nhypertension           int64\nheart_disease          int64\never_married          object\nwork_type             object\nResidence_type        object\navg_glucose_level    float64\nbmi                  float64\nsmoking_status        object\nstroke                 int64\ndtype: object\n\n\n\n# Discretize with respective equal-width bin\ndf['age_binned'] = pd.cut(df['age'], np.arange(0, 91, 5))\ndf['avg_glucose_level_binned'] = pd.cut(df['avg_glucose_level'], np.arange(0, 301, 10))\ndf['bmi_binned'] = pd.cut(df['bmi'], np.arange(0, 101, 5))\n\n\nimport seaborn as sns\n\n\n# Create the correlation heatmap\nheatmap = sns.heatmap(df[['age', 'avg_glucose_level', 'bmi']].corr(), vmin=-1, vmax=1, annot=True)\n# Create the title\nheatmap.set_title('Correlation Heatmap');\n\n\n\n\n\ndef get_stacked_bar_chart(column):\n    # Get the count of records by column and stroke    \n    df_pct = df.groupby([column, 'stroke'])['age'].count()\n    # Create proper DataFrame's format\n    df_pct = df_pct.unstack()    \n    return df_pct.plot.bar(stacked=True, figsize=(6,6), width=1);\n\n\ndef get_100_percent_stacked_bar_chart(column, width = 0.5):\n    # Get the count of records by column and stroke\n    df_breakdown = df.groupby([column, 'stroke'])['age'].count()\n    # Get the count of records by gender\n    df_total = df.groupby([column])['age'].count()\n    # Get the percentage for 100% stacked bar chart\n    df_pct = df_breakdown / df_total * 100\n    # Create proper DataFrame's format\n    df_pct = df_pct.unstack()\n    return df_pct.plot.bar(stacked=True, figsize=(6,6), width=width);\n\n\n# Age related to risk\nget_stacked_bar_chart('age_binned')\n\n&lt;AxesSubplot:xlabel='age_binned'&gt;\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('age_binned', width = 0.9)\n\n&lt;AxesSubplot:xlabel='age_binned'&gt;\n\n\n\n\n\n\nget_stacked_bar_chart('bmi_binned')\nget_100_percent_stacked_bar_chart('bmi_binned', width = 0.9)\n\n&lt;AxesSubplot:xlabel='bmi_binned'&gt;\n\n\n\n\n\n\n\n\n\nget_stacked_bar_chart('avg_glucose_level_binned')\nget_100_percent_stacked_bar_chart('avg_glucose_level_binned', width = 0.9)\n\n&lt;AxesSubplot:xlabel='avg_glucose_level_binned'&gt;\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('hypertension')\nget_100_percent_stacked_bar_chart('heart_disease')\n\n&lt;AxesSubplot:xlabel='heart_disease'&gt;\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('gender')\nget_100_percent_stacked_bar_chart('Residence_type')\n\n&lt;AxesSubplot:xlabel='Residence_type'&gt;\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('work_type')\ndf.groupby(['work_type'])[['age']].agg(['count', 'mean'])\n\n\n\n\n\n\n\n\nage\n\n\n\ncount\nmean\n\n\nwork_type\n\n\n\n\n\n\nGovt_job\n5440\n49.097610\n\n\nNever_worked\n177\n17.757062\n\n\nPrivate\n24834\n45.015060\n\n\nSelf-employed\n6793\n59.307817\n\n\nchildren\n6156\n6.699253\n\n\n\n\n\n\n\n\n\n\n\nget_100_percent_stacked_bar_chart('ever_married')\ndf.groupby(['ever_married'])[['age']].agg(['count', 'mean'])\n\n\n\n\n\n\n\n\nage\n\n\n\ncount\nmean\n\n\never_married\n\n\n\n\n\n\nNo\n15462\n21.238236\n\n\nYes\n27938\n53.828871\n\n\n\n\n\n\n\n\n\n\n\ng = sns.catplot(x=\"Residence_type\", hue=\"smoking_status\", col=\"work_type\",\n                data=df, kind=\"count\",\n                height=4, aspect=.7)\n\n\n\n\n\nimport missingno\nmissingno.matrix(df, figsize = (30,5))\n\n&lt;AxesSubplot:&gt;\n\n\n\n\n\n\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(1,4,figsize=(25,7))\n\nfig.suptitle(\"Countplot for the dataset\", fontsize=35)\n\nsns.countplot(x=\"gender\", data=df,ax=ax1)\nsns.countplot(x=\"stroke\", data=df,ax=ax2)\nsns.countplot(x=\"ever_married\", data=df,ax=ax3)\nsns.countplot(x=\"hypertension\", data=df,ax=ax4)\n\n&lt;AxesSubplot:xlabel='hypertension', ylabel='count'&gt;\n\n\n\n\n\n\nsns.displot(x=\"age\", data=df, kind=\"kde\", hue=\"gender\", col=\"smoking_status\", row=\"Residence_type\")\n\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/seaborn/distributions.py:305: UserWarning: Dataset has 0 variance; skipping density estimate.\n  warnings.warn(msg, UserWarning)\n\n\n\n\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1,3,figsize=(20,7))\nfig.suptitle(\"Boxplot for Dataset\", fontsize=35)\n\nsns.boxplot(x=\"stroke\", y=\"avg_glucose_level\", data=df,ax=ax1)\nsns.boxplot(x=\"stroke\", y=\"bmi\", data=df,ax=ax2)\nsns.boxplot(x=\"stroke\", y=\"age\", data=df,ax=ax3)\n\n&lt;AxesSubplot:xlabel='stroke', ylabel='age'&gt;\n\n\n\n\n\n\n# Compute a correlation matrix and convert to long-form\ncorr_mat = df.corr(\"kendall\").stack().reset_index(name=\"correlation\")\n\n# Draw each cell as a scatter point with varying size and color\ng = sns.relplot(\n    data=corr_mat,\n    x=\"level_0\", y=\"level_1\", hue=\"correlation\", size=\"correlation\",\n    palette=\"vlag\", hue_norm=(-1, 1), edgecolor=\".7\",\n    height=5, sizes=(50, 250), size_norm=(-.2, .8),\n)\n\n# Tweak the figure to finalize\ng.set(xlabel=\"\", ylabel=\"\", aspect=\"equal\")\ng.despine(left=True, bottom=True)\ng.ax.margins(0.25)\nfor label in g.ax.get_xticklabels():\n    label.set_rotation(90)\nfor artist in g.legend.legendHandles:\n    artist.set_edgecolor(\".1\")\n\n\n\n\n\nstrokes_temp_df=df\nstrokes_temp_df[['stroke','hypertension']] = df[['stroke','hypertension']].astype('int')\ncorr = strokes_temp_df.corr()\ncorr.style.background_gradient()\ncorr.style.background_gradient().set_precision(2)\n\n\n\n\n\n\nid\nage\nhypertension\nheart_disease\navg_glucose_level\nbmi\nstroke\n\n\n\n\nid\n1.00\n0.01\n0.01\n0.01\n0.02\n0.02\n0.00\n\n\nage\n0.01\n1.00\n0.27\n0.25\n0.24\n0.36\n0.16\n\n\nhypertension\n0.01\n0.27\n1.00\n0.12\n0.16\n0.16\n0.08\n\n\nheart_disease\n0.01\n0.25\n0.12\n1.00\n0.15\n0.06\n0.11\n\n\navg_glucose_level\n0.02\n0.24\n0.16\n0.15\n1.00\n0.19\n0.08\n\n\nbmi\n0.02\n0.36\n0.16\n0.06\n0.19\n1.00\n0.02\n\n\nstroke\n0.00\n0.16\n0.08\n0.11\n0.08\n0.02\n1.00"
  },
  {
    "objectID": "posts/2020-11-07-spacy_example.html",
    "href": "posts/2020-11-07-spacy_example.html",
    "title": "Spacy in Python for Natural Language Processing (NLP) Example",
    "section": "",
    "text": "import urllib.request\nimport csv\nimport codecs\nimport numpy as np\n\n\n\nurl = \"https://data.heatonresearch.com/data/t81-558/datasets/sonnet_18.txt\"\nwith urllib.request.urlopen(url) as urlstream:\n    for line in codecs.iterdecode(urlstream, 'utf-8'):\n        print(line.rstrip())\n\nSonnet 18 original text\nWilliam Shakespeare\n\nShall I compare thee to a summer's day?\nThou art more lovely and more temperate:\nRough winds do shake the darling buds of May,\nAnd summer's lease hath all too short a date:\nSometime too hot the eye of heaven shines,\nAnd often is his gold complexion dimm'd;\nAnd every fair from fair sometime declines,\nBy chance or nature's changing course untrimm'd;\nBut thy eternal summer shall not fade\nNor lose possession of that fair thou owest;\nNor shall Death brag thou wander'st in his shade,\nWhen in eternal lines to time thou growest:\nSo long as men can breathe or eyes can see,\nSo long lives this and this gives life to thee.\n\n\n\nimport spacy\n\nnlp = spacy.load('en')\ndoc = nlp(line.rstrip())\nfor token in doc:\n    print(token.text)\n\nSo\nlong\nlives\nthis\nand\nthis\ngives\nlife\nto\nthee\n.\n\n\n\nimport spacy\n\nnlp = spacy.load('en')\ndoc = nlp(u\"Apple is looking at buying a U.K. startup for $1 billion\")\nfor token in doc:\n    print(token.text)\n\nApple\nis\nlooking\nat\nbuying\na\nU.K.\nstartup\nfor\n$\n1\nbillion\n\n\nYou can also obtain the part of speech for each word. Common parts of speech include nouns, verbs, pronouns, and adjectives.\n\nfor word in doc:  \n    print(word.text,  word.pos_)\n\nApple PROPN\nis AUX\nlooking VERB\nat ADP\nbuying VERB\na DET\nU.K. PROPN\nstartup NOUN\nfor ADP\n$ SYM\n1 NUM\nbillion NUM\n\n\nSpacy includes functions to check if parts of a sentence appear to be numbers, acronyms, or other entities.\n\nfor word in doc:\n    print(f\"{word} is like number? {word.like_num}\")\n\nApple is like number? False\nis is like number? False\nlooking is like number? False\nat is like number? False\nbuying is like number? False\na is like number? False\nU.K. is like number? False\nstartup is like number? False\nfor is like number? False\n$ is like number? False\n1 is like number? True\nbillion is like number? True\n\n\n\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load('en')\ndoc = nlp(u\"This is a sentance\")\ndisplacy.serve(doc, style=\"dep\")\n\n/home/gao/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n  \"__main__\", mod_spec)\n\n\n\n\n    \n        displaCy\n    \n\n    \n\n\n\n    This\n    DET\n\n\n\n    is\n    AUX\n\n\n\n    a\n    DET\n\n\n\n    sentance\n    NOUN\n\n\n\n    \n    \n        nsubj\n    \n    \n\n\n\n    \n    \n        det\n    \n    \n\n\n\n    \n    \n        attr\n    \n    \n\n\n\n\n\n\n\n\nUsing the 'dep' visualizer\nServing on http://0.0.0.0:5000 ...\n\nShutting down server on port 5000.\n\n\nNote, you will have to manually stop the above cell\n\nprint(doc)\n\nThis is a sentance\n\n\nThe following code shows how to reduce words to their stems. Here the sentence words are reduced to their most basic form. For example, “striped” to “stripe.”\n\nimport spacy\n\n# Initialize spacy 'en' model, keeping only tagger \n# component needed for lemmatization\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\nsentence = \"The striped bats are hanging on their feet for best\"\n\n# Parse the sentence using the loaded 'en' model object `nlp`\ndoc = nlp(sentence)\n\n# Extract the lemma for each token and join\n\n\" \".join([token.lemma_ for token in doc])\n\n'the stripe bat be hang on -PRON- foot for good'\n\n\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nprint(STOP_WORDS)\n\n{'moreover', 'does', 'becomes', 'though', 'done', 'often', 'all', 'next', 'sometime', 'show', 'your', 'forty', 'am', 'on', 'however', 'empty', '’m', 'again', 'have', 'up', 'six', 'any', 'ours', 'may', 'mine', 'not', 'upon', 'top', 'twenty', 'please', 'latter', 'noone', 'this', 'make', 'former', 'wherein', 'hereupon', 'nevertheless', \"'ll\", 'less', 'nowhere', 'side', 'via', 'whatever', '’s', 'becoming', 'onto', 'by', 'being', 'n‘t', 'should', 'themselves', 'almost', 'rather', 'nor', 'once', 'hence', 'few', 'unless', 'along', 'off', 'everyone', 'put', 'fifty', 'one', 'hereby', 'neither', 'anyhow', 'whom', '‘ve', 'it', 'give', 'seemed', '‘s', 'or', 'first', 'is', \"'ve\", 'everything', 'per', 'front', 'whose', 'whoever', 'three', '’re', 'just', 'could', 'beyond', 'none', 'below', 'you', 'thereupon', 'wherever', 'full', 'a', 'whereupon', 'go', 'then', 'although', 'has', 'yet', 'we', 'call', 'something', 'ten', 'using', 'anything', 'until', 'two', 'but', '‘d', 'now', 'amongst', 'serious', 'if', 'already', 'some', 'me', 'their', 'latterly', 'part', 'further', 'between', 'down', 'get', 'namely', 'more', 'nothing', 'do', 'back', 'anywhere', 'hers', 'become', 'there', 'always', 'eight', 'anyway', 'sixty', '’ll', 'around', 'alone', 'who', 'move', 'over', 'well', 'yourself', 'in', \"'d\", 'else', 'about', 'name', 'without', 'therefore', 'thence', 'anyone', '‘m', 'least', 'had', \"'m\", 'see', 'last', 'beside', 'i', 'cannot', 're', 'she', 'therein', 'made', 'must', 'own', 'they', 'became', 'are', 'other', 'at', 'someone', 'never', 'while', 'here', 'when', 'meanwhile', 'each', 'ever', 'his', 'five', 'thru', 'somewhere', 'itself', 'what', 'only', 'than', 'very', 'under', 'many', 'whole', '’d', 'say', 'together', 'most', 'seeming', 'ca', 'where', '‘ll', 'eleven', 'among', 'our', 'otherwise', 'of', 'out', 'myself', 'keep', 'her', 'might', 'really', 'why', 'an', 'against', 'him', 'thereby', 'were', 'twelve', 'towards', \"n't\", 'can', 'so', 'also', 'whither', 'hundred', 'seems', 'thereafter', 'whereby', 'behind', 'whether', 'ourselves', 'formerly', 'either', 'afterwards', 'its', 'various', 'whereafter', 'mostly', 'doing', 'those', 'to', 'nobody', 'perhaps', 'with', 'too', 'these', 'seem', 'toward', 'third', 'into', 'be', 'bottom', 'the', 'enough', 'amount', 'four', 'regarding', 'which', 'even', 'before', 'them', 'same', 'after', 'that', 'will', 'would', 'hereafter', 'elsewhere', 'through', 'how', 'whence', '‘re', 'above', 'take', 'indeed', 'whereas', 'from', 'himself', 'did', 'quite', 'herein', 'he', 'yours', 'was', 'because', 'herself', 'us', 'thus', 'during', 'everywhere', 'been', \"'re\", 'another', 'no', 'several', 'much', 'due', 'throughout', 'within', 'still', 'except', 'n’t', 'as', 'my', 'whenever', 'fifteen', 'besides', 'sometimes', 'used', 'nine', \"'s\", 'across', 'somehow', 'yourselves', 'both', 'others', 'for', 'every', 'such', 'and', 'since', 'beforehand', '’ve'}"
  },
  {
    "objectID": "posts/2020-11-07-spacy_example.html#notebook-and-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "href": "posts/2020-11-07-spacy_example.html#notebook-and-code-from-httpsgithub.comjeffheatont81_558_deep_learning",
    "title": "Spacy in Python for Natural Language Processing (NLP) Example",
    "section": "",
    "text": "import urllib.request\nimport csv\nimport codecs\nimport numpy as np\n\n\n\nurl = \"https://data.heatonresearch.com/data/t81-558/datasets/sonnet_18.txt\"\nwith urllib.request.urlopen(url) as urlstream:\n    for line in codecs.iterdecode(urlstream, 'utf-8'):\n        print(line.rstrip())\n\nSonnet 18 original text\nWilliam Shakespeare\n\nShall I compare thee to a summer's day?\nThou art more lovely and more temperate:\nRough winds do shake the darling buds of May,\nAnd summer's lease hath all too short a date:\nSometime too hot the eye of heaven shines,\nAnd often is his gold complexion dimm'd;\nAnd every fair from fair sometime declines,\nBy chance or nature's changing course untrimm'd;\nBut thy eternal summer shall not fade\nNor lose possession of that fair thou owest;\nNor shall Death brag thou wander'st in his shade,\nWhen in eternal lines to time thou growest:\nSo long as men can breathe or eyes can see,\nSo long lives this and this gives life to thee.\n\n\n\nimport spacy\n\nnlp = spacy.load('en')\ndoc = nlp(line.rstrip())\nfor token in doc:\n    print(token.text)\n\nSo\nlong\nlives\nthis\nand\nthis\ngives\nlife\nto\nthee\n.\n\n\n\nimport spacy\n\nnlp = spacy.load('en')\ndoc = nlp(u\"Apple is looking at buying a U.K. startup for $1 billion\")\nfor token in doc:\n    print(token.text)\n\nApple\nis\nlooking\nat\nbuying\na\nU.K.\nstartup\nfor\n$\n1\nbillion\n\n\nYou can also obtain the part of speech for each word. Common parts of speech include nouns, verbs, pronouns, and adjectives.\n\nfor word in doc:  \n    print(word.text,  word.pos_)\n\nApple PROPN\nis AUX\nlooking VERB\nat ADP\nbuying VERB\na DET\nU.K. PROPN\nstartup NOUN\nfor ADP\n$ SYM\n1 NUM\nbillion NUM\n\n\nSpacy includes functions to check if parts of a sentence appear to be numbers, acronyms, or other entities.\n\nfor word in doc:\n    print(f\"{word} is like number? {word.like_num}\")\n\nApple is like number? False\nis is like number? False\nlooking is like number? False\nat is like number? False\nbuying is like number? False\na is like number? False\nU.K. is like number? False\nstartup is like number? False\nfor is like number? False\n$ is like number? False\n1 is like number? True\nbillion is like number? True\n\n\n\nimport spacy\nfrom spacy import displacy\n\nnlp = spacy.load('en')\ndoc = nlp(u\"This is a sentance\")\ndisplacy.serve(doc, style=\"dep\")\n\n/home/gao/anaconda3/lib/python3.7/runpy.py:193: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n  \"__main__\", mod_spec)\n\n\n\n\n    \n        displaCy\n    \n\n    \n\n\n\n    This\n    DET\n\n\n\n    is\n    AUX\n\n\n\n    a\n    DET\n\n\n\n    sentance\n    NOUN\n\n\n\n    \n    \n        nsubj\n    \n    \n\n\n\n    \n    \n        det\n    \n    \n\n\n\n    \n    \n        attr\n    \n    \n\n\n\n\n\n\n\n\nUsing the 'dep' visualizer\nServing on http://0.0.0.0:5000 ...\n\nShutting down server on port 5000.\n\n\nNote, you will have to manually stop the above cell\n\nprint(doc)\n\nThis is a sentance\n\n\nThe following code shows how to reduce words to their stems. Here the sentence words are reduced to their most basic form. For example, “striped” to “stripe.”\n\nimport spacy\n\n# Initialize spacy 'en' model, keeping only tagger \n# component needed for lemmatization\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\nsentence = \"The striped bats are hanging on their feet for best\"\n\n# Parse the sentence using the loaded 'en' model object `nlp`\ndoc = nlp(sentence)\n\n# Extract the lemma for each token and join\n\n\" \".join([token.lemma_ for token in doc])\n\n'the stripe bat be hang on -PRON- foot for good'\n\n\n\nfrom spacy.lang.en.stop_words import STOP_WORDS\n\nprint(STOP_WORDS)\n\n{'moreover', 'does', 'becomes', 'though', 'done', 'often', 'all', 'next', 'sometime', 'show', 'your', 'forty', 'am', 'on', 'however', 'empty', '’m', 'again', 'have', 'up', 'six', 'any', 'ours', 'may', 'mine', 'not', 'upon', 'top', 'twenty', 'please', 'latter', 'noone', 'this', 'make', 'former', 'wherein', 'hereupon', 'nevertheless', \"'ll\", 'less', 'nowhere', 'side', 'via', 'whatever', '’s', 'becoming', 'onto', 'by', 'being', 'n‘t', 'should', 'themselves', 'almost', 'rather', 'nor', 'once', 'hence', 'few', 'unless', 'along', 'off', 'everyone', 'put', 'fifty', 'one', 'hereby', 'neither', 'anyhow', 'whom', '‘ve', 'it', 'give', 'seemed', '‘s', 'or', 'first', 'is', \"'ve\", 'everything', 'per', 'front', 'whose', 'whoever', 'three', '’re', 'just', 'could', 'beyond', 'none', 'below', 'you', 'thereupon', 'wherever', 'full', 'a', 'whereupon', 'go', 'then', 'although', 'has', 'yet', 'we', 'call', 'something', 'ten', 'using', 'anything', 'until', 'two', 'but', '‘d', 'now', 'amongst', 'serious', 'if', 'already', 'some', 'me', 'their', 'latterly', 'part', 'further', 'between', 'down', 'get', 'namely', 'more', 'nothing', 'do', 'back', 'anywhere', 'hers', 'become', 'there', 'always', 'eight', 'anyway', 'sixty', '’ll', 'around', 'alone', 'who', 'move', 'over', 'well', 'yourself', 'in', \"'d\", 'else', 'about', 'name', 'without', 'therefore', 'thence', 'anyone', '‘m', 'least', 'had', \"'m\", 'see', 'last', 'beside', 'i', 'cannot', 're', 'she', 'therein', 'made', 'must', 'own', 'they', 'became', 'are', 'other', 'at', 'someone', 'never', 'while', 'here', 'when', 'meanwhile', 'each', 'ever', 'his', 'five', 'thru', 'somewhere', 'itself', 'what', 'only', 'than', 'very', 'under', 'many', 'whole', '’d', 'say', 'together', 'most', 'seeming', 'ca', 'where', '‘ll', 'eleven', 'among', 'our', 'otherwise', 'of', 'out', 'myself', 'keep', 'her', 'might', 'really', 'why', 'an', 'against', 'him', 'thereby', 'were', 'twelve', 'towards', \"n't\", 'can', 'so', 'also', 'whither', 'hundred', 'seems', 'thereafter', 'whereby', 'behind', 'whether', 'ourselves', 'formerly', 'either', 'afterwards', 'its', 'various', 'whereafter', 'mostly', 'doing', 'those', 'to', 'nobody', 'perhaps', 'with', 'too', 'these', 'seem', 'toward', 'third', 'into', 'be', 'bottom', 'the', 'enough', 'amount', 'four', 'regarding', 'which', 'even', 'before', 'them', 'same', 'after', 'that', 'will', 'would', 'hereafter', 'elsewhere', 'through', 'how', 'whence', '‘re', 'above', 'take', 'indeed', 'whereas', 'from', 'himself', 'did', 'quite', 'herein', 'he', 'yours', 'was', 'because', 'herself', 'us', 'thus', 'during', 'everywhere', 'been', \"'re\", 'another', 'no', 'several', 'much', 'due', 'throughout', 'within', 'still', 'except', 'n’t', 'as', 'my', 'whenever', 'fifteen', 'besides', 'sometimes', 'used', 'nine', \"'s\", 'across', 'somehow', 'yourselves', 'both', 'others', 'for', 'every', 'such', 'and', 'since', 'beforehand', '’ve'}"
  },
  {
    "objectID": "posts/2021-06-19-health_data.html",
    "href": "posts/2021-06-19-health_data.html",
    "title": "Forecasting running data",
    "section": "",
    "text": "#os.getcwd()\n#os.listdir()\n\n\nfrom datetime import date\nimport os \ntoday = date.today()\n\n\nfor file in os.listdir():\n    if file.endswith('.ipynb'):\n        cd=today\n        os.rename(file, f'{today}-{file}')\n\n\nimport shutil\n\nshutil.copy(\n    os.path.join('2021-06-19-health_data.ipynb'),\n    os.path.join('../git-repos/Kearney_Data_Science/_notebooks')\n)\n\n'../git-repos/Kearney_Data_Science/_notebooks/2021-06-19-health_data.ipynb'\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\nengine = db.create_engine('sqlite:///../../Downloads/fitbit.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\n\nsql = \"\"\"\nselect DATE(date_time) as day\n, sum(distance_miles) as distance\nfrom distance_v\ngroup by DATE(date_time)\n\"\"\"\n\ncnxn = connection\n\ndf = pd.read_sql(sql, cnxn)\n\ndf\n\n\n\n\n\n\n\n\nday\ndistance\n\n\n\n\n0\n2020-12-02\n11.238989\n\n\n1\n2020-12-03\n7.615898\n\n\n2\n2020-12-04\n11.392033\n\n\n3\n2020-12-05\n9.929077\n\n\n4\n2020-12-06\n10.442889\n\n\n...\n...\n...\n\n\n186\n2021-06-08\n0.935723\n\n\n187\n2021-06-09\n4.844334\n\n\n188\n2021-06-10\n8.554417\n\n\n189\n2021-06-11\n6.167171\n\n\n190\n2021-06-12\n5.006263\n\n\n\n\n191 rows × 2 columns\n\n\n\n\ndf['ds'] = df.day\ndf['y'] = df.distance\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 191 entries, 0 to 190\nData columns (total 4 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   day       191 non-null    object \n 1   distance  191 non-null    float64\n 2   ds        191 non-null    object \n 3   y         191 non-null    float64\ndtypes: float64(2), object(2)\nmemory usage: 6.1+ KB\n\n\n\nimport statsmodels.api as sm\nimport pandas as pd\nfrom prophet import Prophet\n\n\nimport pandas as pd\npd.set_option('compute.use_numexpr', False)\n\nm = Prophet()\nm.fit(df)\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n&lt;prophet.forecaster.Prophet at 0x7f0100bb6c70&gt;\n\n\n\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()\n\n\n\n\n\n\n\n\nds\n\n\n\n\n551\n2022-06-08\n\n\n552\n2022-06-09\n\n\n553\n2022-06-10\n\n\n554\n2022-06-11\n\n\n555\n2022-06-12\n\n\n\n\n\n\n\n\nforecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()\n\n\n\n\n\n\n\n\nds\nyhat\nyhat_lower\nyhat_upper\n\n\n\n\n551\n2022-06-08\n9.954126\n5.626830\n14.090458\n\n\n552\n2022-06-09\n11.067850\n6.415623\n15.269029\n\n\n553\n2022-06-10\n9.524963\n5.164365\n13.816646\n\n\n554\n2022-06-11\n10.393233\n5.943450\n14.634128\n\n\n555\n2022-06-12\n11.320983\n6.836854\n15.647303\n\n\n\n\n\n\n\n\nfig1 = m.plot(forecast)\n\n\n\n\n\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n# Python\nfig1 = m.plot(forecast)\n\n\n\n\n\n# Python\nfig2 = m.plot_components(forecast)\n\n\n\n\n\n# Python\nfrom prophet.plot import plot_plotly, plot_components_plotly\n\nplot_plotly(m, forecast)\n\n\n                                                \n\n\n\n# Python\nplot_components_plotly(m, forecast)\n\n\n                                                \n\n\n\n# Model fit\nm = Prophet() #Instanticate from Prophet class. \nm.fit(df) # Fit the Prophet model.\n\n# Predict\nfuture = m.make_future_dataframe(periods=365) # Make future date data frame for the next 365 days (it gives daily because it follows the frequency in input dataframe by default).\nforecast = m.predict(future) # Predict future value.\n\n# Plot results\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\nforecast # Displaying various results in table format.\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nds\ntrend\nyhat_lower\nyhat_upper\ntrend_lower\ntrend_upper\nadditive_terms\nadditive_terms_lower\nadditive_terms_upper\nweekly\nweekly_lower\nweekly_upper\nmultiplicative_terms\nmultiplicative_terms_lower\nmultiplicative_terms_upper\nyhat\n\n\n\n\n0\n2020-12-02\n8.826908\n4.690634\n11.818085\n8.826908\n8.826908\n-0.380921\n-0.380921\n-0.380921\n-0.380921\n-0.380921\n-0.380921\n0.0\n0.0\n0.0\n8.445987\n\n\n1\n2020-12-03\n8.805304\n5.826039\n12.902548\n8.805304\n8.805304\n0.727109\n0.727109\n0.727109\n0.727109\n0.727109\n0.727109\n0.0\n0.0\n0.0\n9.532413\n\n\n2\n2020-12-04\n8.783700\n4.373529\n11.382860\n8.783700\n8.783700\n-0.821473\n-0.821473\n-0.821473\n-0.821473\n-0.821473\n-0.821473\n0.0\n0.0\n0.0\n7.962227\n\n\n3\n2020-12-05\n8.762096\n5.593958\n12.411443\n8.762096\n8.762096\n0.041102\n0.041102\n0.041102\n0.041102\n0.041102\n0.041102\n0.0\n0.0\n0.0\n8.803198\n\n\n4\n2020-12-06\n8.740492\n6.342656\n13.346898\n8.740492\n8.740492\n0.963158\n0.963158\n0.963158\n0.963158\n0.963158\n0.963158\n0.0\n0.0\n0.0\n9.703650\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n551\n2022-06-08\n10.335047\n5.742947\n14.393370\n7.579529\n13.222686\n-0.380921\n-0.380921\n-0.380921\n-0.380921\n-0.380921\n-0.380921\n0.0\n0.0\n0.0\n9.954126\n\n\n552\n2022-06-09\n10.340742\n6.769705\n15.861835\n7.575976\n13.239863\n0.727109\n0.727109\n0.727109\n0.727109\n0.727109\n0.727109\n0.0\n0.0\n0.0\n11.067850\n\n\n553\n2022-06-10\n10.346436\n4.792225\n14.159937\n7.573519\n13.261637\n-0.821473\n-0.821473\n-0.821473\n-0.821473\n-0.821473\n-0.821473\n0.0\n0.0\n0.0\n9.524963\n\n\n554\n2022-06-11\n10.352131\n5.946817\n14.833565\n7.570061\n13.285497\n0.041102\n0.041102\n0.041102\n0.041102\n0.041102\n0.041102\n0.0\n0.0\n0.0\n10.393233\n\n\n555\n2022-06-12\n10.357825\n7.099368\n15.701414\n7.563656\n13.302798\n0.963158\n0.963158\n0.963158\n0.963158\n0.963158\n0.963158\n0.0\n0.0\n0.0\n11.320983\n\n\n\n\n556 rows × 16 columns\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\n# Load test data: log-transformed daily page views for the Wikipedia page for Peyton Manning.\n\ndf['cap'] = 10 # Saturating maximum\ndf['floor'] = 7 # Saturating minimum\n\n# Model setup\nm = Prophet(growth='logistic')\nm.add_country_holidays(country_name='US') # Adding US holiday regressor\nm.fit(df) \n\n# Future data generation\nfuture = m.make_future_dataframe(periods=365*5)\nfuture['cap'] = 10 # Saturating maximum\nfuture['floor'] = 7 # Saturating minimum\n\n# Future forecast\nforecast = m.predict(future) \n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n\n# Visualize\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\n\n\n\n\n\n\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\ndef is_nfl_season(ds):\n    date = pd.to_datetime(ds)\n    return (date.month &gt; 8 or date.month &lt; 2)\n\ndf['on_season'] = df['ds'].apply(is_nfl_season) #on_season dummy.\ndf['off_season'] = ~df['ds'].apply(is_nfl_season) #off_season dummy.\n\n# set user-defined seasonality and fit\nm = Prophet(weekly_seasonality=False)\nm.add_seasonality(name='weekly_on_season', period=7, fourier_order=3, condition_name='on_season')\nm.add_seasonality(name='weekly_off_season', period=7, fourier_order=3, condition_name='off_season')\nm.fit(df)\n\n# Make the same columns to future data.\nfuture = m.make_future_dataframe(periods=365*5) # Make future date data frame for the next 365 days (it gives daily because it follows the frequency in input dataframe by default).\nfuture['on_season'] = future['ds'].apply(is_nfl_season)\nfuture['off_season'] = ~future['ds'].apply(is_nfl_season)\n\n# Predict future value.\nforecast = m.predict(future)\n\n\nINFO:prophet:Disabling yearly seasonality. Run prophet with yearly_seasonality=True to override this.\nINFO:prophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n\n\n\n# Plot results\nfig1 = m.plot(forecast) # Plot the fit to past data and future forcast.\nfig2 = m.plot_components(forecast) # Plot breakdown of components.\nplt.show()\n\n\n\n\n\n\n\n\n# After getting forecast dataframe using user-defined seasonality \"on-season\"/\"off-season\" above...\n\nfrom statsmodels.graphics.tsaplots import plot_pacf, plot_acf\n\ndf['ds'] = pd.to_datetime(df['ds'],format='%Y-%m-%d')\ndf_res = df.merge(forecast,how=\"inner\",on=\"ds\")\ndf_res['residual'] = df_res['y'] - df_res['yhat']\nplot_acf(df_res['residual'])\nplot_pacf(df_res['residual'])\nplt.show()"
  },
  {
    "objectID": "posts/2020-11-03-MCMC.html",
    "href": "posts/2020-11-03-MCMC.html",
    "title": "MCMC algorithms, sampling problems/sampling techniques, Bayesian data analysis",
    "section": "",
    "text": "This post includes code and notes from : https://www.tweag.io/blog/2019-10-25-mcmc-intro1/\n\nhttps://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\nhttps://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\n\n\n%matplotlib notebook\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10, 6]\nnp.random.seed(42)\n\n\nstate_space = (\"sunny\", \"cloudy\", \"rainy\")\n\n\ntransition_matrix = np.array(((0.6, 0.3, 0.1),\n                              (0.3, 0.4, 0.3),\n                              (0.2, 0.3, 0.5)))\n\n\nn_steps = 20000\nstates = [0]\nfor i in range(n_steps):\n    states.append(np.random.choice((0, 1, 2), p=transition_matrix[states[-1]]))\nstates = np.array(states)\n\n\ndef despine(ax, spines=('top', 'left', 'right')):\n    for spine in spines:\n        ax.spines[spine].set_visible(False)\n\nfig, ax = plt.subplots()\nwidth = 1000\noffsets = range(1, n_steps, 5)\nfor i, label in enumerate(state_space):\n    ax.plot(offsets, [np.sum(states[:offset] == i) / offset\n            for offset in offsets], label=label)\nax.set_xlabel(\"number of steps\")\nax.set_ylabel(\"likelihood\")\nax.legend(frameon=False)\ndespine(ax, ('top', 'right'))\nplt.show()\n\n\n\n\n\ndef log_prob(x):\n     return -0.5 * np.sum(x ** 2)\n\n\ndef proposal(x, stepsize):\n    return np.random.uniform(low=x - 0.5 * stepsize,\n                             high=x + 0.5 * stepsize,\n                             size=x.shape)\n\n\ndef p_acc_MH(x_new, x_old, log_prob):\n    return min(1, np.exp(log_prob(x_new) - log_prob(x_old)))\n\n\ndef sample_MH(x_old, log_prob, stepsize):\n    x_new = proposal(x_old, stepsize)\n    # here we determine whether we accept the new state or not:\n    # we draw a random number uniformly from [0,1] and compare\n    # it with the acceptance probability\n    accept = np.random.random() &lt; p_acc_MH(x_new, x_old, log_prob)\n    if accept:\n        return accept, x_new\n    else:\n        return accept, x_old\n\n\ndef build_MH_chain(init, stepsize, n_total, log_prob):\n\n    n_accepted = 0\n    chain = [init]\n\n    for _ in range(n_total):\n        accept, state = sample_MH(chain[-1], log_prob, stepsize)\n        chain.append(state)\n        n_accepted += accept \n\n    acceptance_rate = n_accepted / float(n_total)\n\n    return chain, acceptance_rate\n\n\nchain, acceptance_rate = build_MH_chain(np.array([2.0]), 3.0, 10000, log_prob)\n\n\nchain = [state for state, in chain]\n\n\nprint(\"Acceptance rate: {:.3f}\".format(acceptance_rate))\nlast_states = \", \".join(\"{:.5f}\".format(state)\n                        for state in chain[-10:])\nprint(\"Last ten states of chain: \" + last_states)\n\nAcceptance rate: 0.720\nLast ten states of chain: 1.05847, 1.59966, 0.14389, -1.13281, 0.24131, -0.77448, -0.59703, 0.67707, 1.47065, 1.27361\n\n\n\ndef plot_samples(chain, log_prob, ax, orientation='vertical', normalize=True,\n                 xlims=(-5, 5), legend=True):\n    \n    from scipy.integrate import quad\n\n    ax.hist(chain, bins=50, density=True, label=\"MCMC samples\",\n           orientation=orientation)\n    # we numerically calculate the normalization constant of our PDF\n    if normalize:\n        Z, _ = quad(lambda x: np.exp(log_prob(x)), -np.inf, np.inf)\n    else:\n        Z = 1.0\n    xses = np.linspace(xlims[0], xlims[1], 1000)\n    yses = [np.exp(log_prob(x)) / Z for x in xses]\n    if orientation == 'horizontal':\n        (yses, xses) = (xses, yses)\n    ax.plot(xses, yses, label=\"true distribution\")\n    if legend:\n        ax.legend(frameon=False)\n\nfig, ax = plt.subplots()\nplot_samples(chain[500:], log_prob, ax)\ndespine(ax)\nax.set_yticks(())\nplt.show()\n\n\n\n\n\ndef sample_and_display(init_state, stepsize, n_total, n_burnin, log_prob):\n    chain, acceptance_rate = build_MH_chain(init_state, stepsize, n_total, log_prob)\n    print(\"Acceptance rate: {:.3f}\".format(acceptance_rate))\n    fig, ax = plt.subplots()\n    plot_samples([state for state, in chain[n_burnin:]], log_prob, ax)\n    despine(ax)\n    ax.set_yticks(())\n    plt.show()\n\nsample_and_display(np.array([2.0]), 30, 10000, 500, log_prob)\n\nAcceptance rate: 0.104\n\n\n\n\n\n\nsample_and_display(np.array([2.0]), 0.1, 10000, 500, log_prob)\n\nAcceptance rate: 0.985\n\n\n\n\n\n\nsample_and_display(np.array([2.0]), 0.1, 500000, 25000, log_prob)\n\nAcceptance rate: 0.990\n\n\n\n\n\n\n%matplotlib notebook\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10, 6]\n\nnp.random.seed(42)\n\ndef log_gaussian(x, mu, sigma):\n    # The np.sum() is for compatibility with sample_MH\n    return - 0.5 * np.sum((x - mu) ** 2) / sigma ** 2 \\\n           - np.log(np.sqrt(2 * np.pi * sigma ** 2))\n\n\nclass BivariateNormal(object):\n    n_variates = 2\n    \n    def __init__(self, mu1, mu2, sigma1, sigma2):\n        self.mu1, self.mu2 = mu1, mu2\n        self.sigma1, self.sigma2 = sigma1, sigma2\n        \n    def log_p_x(self, x):\n        return log_gaussian(x, self.mu1, self.sigma1)\n        \n    def log_p_y(self, x):\n        return log_gaussian(x, self.mu2, self.sigma2)\n    \n    def log_prob(self, x):        \n        cov_matrix = np.array([[self.sigma1 ** 2, 0],\n                               [0, self.sigma2 ** 2]])\n        inv_cov_matrix = np.linalg.inv(cov_matrix)\n        kernel = -0.5 * (x - self.mu1) @ inv_cov_matrix @ (x - self.mu2).T\n        normalization = np.log(np.sqrt((2 * np.pi) ** self.n_variates * np.linalg.det(cov_matrix)))\n        \n        return kernel - normalization               \n\n    \nbivariate_normal = BivariateNormal(mu1=0.0, mu2=0.0, sigma1=1.0, sigma2=0.15)\n\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfig, ax = plt.subplots()\nxses = np.linspace(-2, 2, 200)\nyses = np.linspace(-0.5, 0.5, 200)\nlog_density_values = [[bivariate_normal.log_prob(np.array((x, y))) for x in xses] for y in yses]\ndx = (xses[1] - xses[0]) / 2\ndy = (yses[1] - yses[0]) / 2\nextent = [xses[0] - dx, xses[-1] + dx, yses[0] - dy, yses[-1] + dy]\nim = ax.imshow(np.exp(log_density_values), extent=extent)\nax.set_xlabel('x')\nax.set_ylabel('y')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes('right', size='5%', pad=0.05)\ncb = fig.colorbar(im, cax=cax)\ncb.set_label('probability density')\nplt.show()\n\n\n\n\n\ndef sample_gibbs(old_state, bivariate_dist, stepsizes):\n    \"\"\"Draws a single sample using the systematic Gibbs sampling\n    transition kernel\n    \n    Arguments:\n    - old_state: the old (two-dimensional) state of a Markov chain\n                 (a list containing two floats)\n    - bivariate_dist: an object representing a bivariate distribution\n                      (in our case, an instance of BivariateNormal)\n    - stepsizes: a list of step sizes\n    \n    \"\"\"\n    x_old, y_old = old_state\n    \n    # for compatibility with sample_MH, change floats to one-dimensional\n    # numpy arrays of length one\n    x_old = np.array([x_old])\n    y_old = np.array([y_old])\n    \n    # draw new x conditioned on y\n    p_x_y = bivariate_dist.log_p_x\n    accept_x, x_new = sample_MH(x_old, p_x_y, stepsizes[0])\n    \n    # draw new y conditioned on x\n    p_y_x = bivariate_dist.log_p_y\n    accept_y, y_new = sample_MH(y_old, p_y_x, stepsizes[1])\n    \n    # Don't forget to turn the one-dimensional numpy arrays x_new, y_new\n    # of length one back into floats\n    \n    return (accept_x, accept_y), (x_new[0], y_new[0])\n\n\ndef build_gibbs_chain(init, stepsizes, n_total, bivariate_dist):\n    \"\"\"Builds a Markov chain by performing repeated transitions using\n    the systematic Gibbs sampling transition kernel\n    \n    Arguments:\n    - init: an initial (two-dimensional) state for the Markov chain\n            (a list containing two floats)\n    - stepsizes: a list of step sizes of type float\n    - n_total: the total length of the Markov chain\n    - bivariate_dist: an object representing a bivariate distribution\n                      (in our case, an instance of BivariateNormal)\n    \n    \"\"\"\n    init_x, init_k = init\n    chain = [init]\n    acceptances = []\n    \n    for _ in range(n_total):\n        accept, new_state = sample_gibbs(chain[-1], bivariate_dist, stepsizes)\n        chain.append(new_state)        \n        acceptances.append(accept)\n    \n    acceptance_rates = np.mean(acceptances, 0)\n    print(\"Acceptance rates: x: {:.3f}, y: {:.3f}\".format(acceptance_rates[0],\n                                                          acceptance_rates[1]))\n    \n    return chain \n\nstepsizes = (6.5, 1.0)\ninitial_state = [2.0, -1.0]\nchain = build_gibbs_chain(initial_state, stepsizes, 100000, bivariate_normal)\nchain = np.array(chain)\n\nAcceptance rates: x: 0.462, y: 0.456\n\n\n\ndef plot_samples_2D(chain, path_length, burnin, ax, xlims=(-3, 3), ylims=(-0.5, 0.5)):\n    chain = np.array(chain)\n    bins = [np.linspace(xlims[0], xlims[1], 100), np.linspace(ylims[0], ylims[1], 100)]\n    ax.hist2d(*chain[burnin:].T, bins=bins)\n    ax.plot(*chain[:path_length].T, marker='o', c='w', lw=0.4, \n            markersize=1, alpha=0.75)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_xlim(xlims[0], xlims[1])\n    ax.set_ylim(ylims[0], ylims[1])\n    \ndef plot_bivariate_samples(chain, burnin, pdf):\n    fig = plt.figure(figsize=(12,7))\n   \n    ax_c = plt.subplot2grid((4, 4), (1, 0), rowspan=1, colspan=3)\n    plot_samples_2D(chain, 100, burnin, ax_c)\n    \n    ax_t = plt.subplot2grid((4, 4), (0, 0), rowspan=1, colspan=3, sharex=ax_c)\n    plot_samples(chain[:,0], pdf.log_p_x, ax_t, normalize=False)\n    plt.setp(ax_t.get_xticklabels(), visible=False)\n    ax_t.set_yticks(())\n    for spine in ('top', 'left', 'right'):\n        ax_t.spines[spine].set_visible(False)\n\n    ax_r = plt.subplot2grid((4, 4), (1, 3), rowspan=1, colspan=1, sharey=ax_c)\n    plot_samples(chain[:,1], pdf.log_p_y, ax_r, orientation='horizontal',\n                 normalize=False, legend=False)\n    plt.setp(ax_r.get_yticklabels(), visible=False)\n    ax_r.set_xticks(())\n    for spine in ('top', 'bottom', 'right'):\n        ax_r.spines[spine].set_visible(False)\n\n    plt.show()\n    \nplot_bivariate_samples(chain, burnin=200, pdf=bivariate_normal)\n\n\n\n\n\nmix_params = dict(mu1=1.0, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7)\n\n\nfig, ax = plt.subplots()\nxspace = np.linspace(-0.5, 3, 200)\n\n# densities of both components\nfirst_component = [np.exp(log_gaussian(x, mix_params['mu1'], mix_params['sigma1']))\n                   for x in xspace]\nsecond_component = [np.exp(log_gaussian(x, mix_params['mu2'], mix_params['sigma2']))\n                    for x in xspace]\n\n# apply component weights\nfirst_component = mix_params['w1'] * np.array(first_component)\nsecond_component = mix_params['w2'] * np.array(second_component)\n\nax.plot(xspace, first_component, color='black')\nax.fill_between(xspace, first_component, alpha=0.6, label=\"1st component\")\nax.plot(xspace, second_component, color='black')\nax.fill_between(xspace, second_component, alpha=0.6, label=\"2nd component\")\nax.set_xlabel('x')\nax.set_yticks(())\nax.legend(frameon=False)\nfor spine in ('top', 'left', 'right'):\n    ax.spines[spine].set_visible(False)\nplt.show()\n\n\n\n\n\nclass GaussianMixture(object):\n    \n    def __init__(self, mu1, mu2, sigma1, sigma2, w1, w2):\n        self.mu1, self.mu2 = mu1, mu2\n        self.sigma1, self.sigma2 = sigma1, sigma2\n        self.w1, self.w2 = w1, w2\n        \n    def log_prob(self, x):\n        return np.logaddexp(np.log(self.w1) + log_gaussian(x, self.mu1, self.sigma1),\n                            np.log(self.w2) + log_gaussian(x, self.mu2, self.sigma2))\n    \n    def log_p_x_k(self, x, k):\n        # logarithm of p(x|k)\n        mu = (self.mu1, self.mu2)[k]\n        sigma = (self.sigma1, self.sigma2)[k]\n    \n        return log_gaussian(x, mu, sigma)\n    \n    def p_k_x(self, k, x):\n        # p(k|x) using Bayes' theorem\n        mu = (self.mu1, self.mu2)[k]\n        sigma = (self.sigma1, self.sigma2)[k]\n        weight = (self.w1, self.w2)[k]\n        log_normalization = self.log_prob(x)\n\n        return np.exp(log_gaussian(x, mu, sigma) + np.log(weight) - log_normalization)\n\n\ndef sample_gibbs(old_state, mixture, stepsize):\n    \"\"\"Draws a single sample using the systematic Gibbs sampling\n    transition kernel\n    \n    Arguments:\n    - old_state: the old (two-dimensional) state of a Markov chain\n                 (a list containing a float and an integer representing \n                 the initial mixture component)\n    - mixture: an object representing a mixture of densities\n               (in our case, an instance of GaussianMixture)\n    - stepsize: a step size of type float \n    \n    \"\"\"\n    x_old, k_old = old_state\n    \n    # for compatibility with sample_MH, change floats to one-dimensional\n    # numpy arrays of length one\n    x_old = np.array([x_old])\n    \n    # draw new x conditioned on k\n    x_pdf = lambda x: mixture.log_p_x_k(x, k_old)\n    accept, x_new = sample_MH(x_old, x_pdf, stepsize)\n    \n    # ... turn the one-dimensional numpy arrays of length one back\n    # into floats\n    x_new = x_new[0]\n    \n    # draw new k conditioned on x \n    k_probabilities = (mixture.p_k_x(0, x_new), mixture.p_k_x(1, x_new))\n    jump_probability = k_probabilities[1 - k_old]\n    k_new = np.random.choice((0,1), p=k_probabilities)\n    \n    return accept, jump_probability, (x_new, k_new)\n\n\ndef build_gibbs_chain(init, stepsize, n_total, mixture):\n    \"\"\"Builds a Markov chain by performing repeated transitions using\n    the systematic Gibbs sampling transition kernel\n    \n    Arguments:\n    - init: an initial (two-dimensional) state of a Markov chain\n            (a list containing a one-dimensional numpy array\n            of length one and an integer representing the initial\n            mixture component)\n    - stepsize: a step size of type float\n    - n_total: the total length of the Markov chain\n    - mixture: an object representing a mixture of densities\n               (in our case, an instance of GaussianMixture)\n    \n    \"\"\"\n    init_x, init_k = init\n    chain = [init]\n    acceptances = []\n    jump_probabilities = []\n    \n    for _ in range(n_total):\n        accept, jump_probability, new_state = sample_gibbs(chain[-1], mixture, stepsize)\n        chain.append(new_state)\n        jump_probabilities.append(jump_probability)\n        acceptances.append(accept)\n    \n    acceptance_rates = np.mean(acceptances)\n    print(\"Acceptance rate: x: {:.3f}\".format(acceptance_rates))\n    print(\"Average probability to change mode: {}\".format(np.mean(jump_probabilities)))\n    \n    return chain\n\nmixture = GaussianMixture(**mix_params)\nstepsize = 1.0\ninitial_state = [2.0, 1]\nchain = build_gibbs_chain(initial_state, stepsize, 10000, mixture)\nburnin = 1000\nx_states = [state[0] for state in chain[burnin:]]\n\nAcceptance rate: x: 0.631\nAverage probability to change mode: 0.08629295966662387\n\n\n\nfig, ax = plt.subplots()\nplot_samples(x_states, mixture.log_prob, ax, normalize=False, xlims=(-1,2.5))\nfor spine in ('top', 'left', 'right'):\n    ax.spines[spine].set_visible(False)\nax.set_yticks(())\nax.set_xlabel('x')\nplt.show()\n\n\n\n\n\nmixture = GaussianMixture(mu1=-1.0, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7)\nstepsize = 1.0\ninitial_state = [2.0, 1]\nchain = build_gibbs_chain(initial_state, stepsize, 100000, mixture)\nburnin = 10000\nx_states = [state[0] for state in chain[burnin:]]\n\nAcceptance rate: x: 0.558\nAverage probability to change mode: 6.139534006013391e-06\n\n\n\nfig, ax = plt.subplots()\nplot_samples(x_states, mixture.log_prob, ax, normalize=False, xlims=(-2,2.5))\nfor spine in ('top', 'left', 'right'):\n    ax.spines[spine].set_visible(False)\nax.set_yticks(())\nax.set_xlabel('x')\nplt.show()"
  },
  {
    "objectID": "posts/2020-11-03-MCMC.html#metropolis-hastings",
    "href": "posts/2020-11-03-MCMC.html#metropolis-hastings",
    "title": "MCMC algorithms, sampling problems/sampling techniques, Bayesian data analysis",
    "section": "",
    "text": "This post includes code and notes from : https://www.tweag.io/blog/2019-10-25-mcmc-intro1/\n\nhttps://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo\nhttps://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm\n\n\n%matplotlib notebook\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10, 6]\nnp.random.seed(42)\n\n\nstate_space = (\"sunny\", \"cloudy\", \"rainy\")\n\n\ntransition_matrix = np.array(((0.6, 0.3, 0.1),\n                              (0.3, 0.4, 0.3),\n                              (0.2, 0.3, 0.5)))\n\n\nn_steps = 20000\nstates = [0]\nfor i in range(n_steps):\n    states.append(np.random.choice((0, 1, 2), p=transition_matrix[states[-1]]))\nstates = np.array(states)\n\n\ndef despine(ax, spines=('top', 'left', 'right')):\n    for spine in spines:\n        ax.spines[spine].set_visible(False)\n\nfig, ax = plt.subplots()\nwidth = 1000\noffsets = range(1, n_steps, 5)\nfor i, label in enumerate(state_space):\n    ax.plot(offsets, [np.sum(states[:offset] == i) / offset\n            for offset in offsets], label=label)\nax.set_xlabel(\"number of steps\")\nax.set_ylabel(\"likelihood\")\nax.legend(frameon=False)\ndespine(ax, ('top', 'right'))\nplt.show()\n\n\n\n\n\ndef log_prob(x):\n     return -0.5 * np.sum(x ** 2)\n\n\ndef proposal(x, stepsize):\n    return np.random.uniform(low=x - 0.5 * stepsize,\n                             high=x + 0.5 * stepsize,\n                             size=x.shape)\n\n\ndef p_acc_MH(x_new, x_old, log_prob):\n    return min(1, np.exp(log_prob(x_new) - log_prob(x_old)))\n\n\ndef sample_MH(x_old, log_prob, stepsize):\n    x_new = proposal(x_old, stepsize)\n    # here we determine whether we accept the new state or not:\n    # we draw a random number uniformly from [0,1] and compare\n    # it with the acceptance probability\n    accept = np.random.random() &lt; p_acc_MH(x_new, x_old, log_prob)\n    if accept:\n        return accept, x_new\n    else:\n        return accept, x_old\n\n\ndef build_MH_chain(init, stepsize, n_total, log_prob):\n\n    n_accepted = 0\n    chain = [init]\n\n    for _ in range(n_total):\n        accept, state = sample_MH(chain[-1], log_prob, stepsize)\n        chain.append(state)\n        n_accepted += accept \n\n    acceptance_rate = n_accepted / float(n_total)\n\n    return chain, acceptance_rate\n\n\nchain, acceptance_rate = build_MH_chain(np.array([2.0]), 3.0, 10000, log_prob)\n\n\nchain = [state for state, in chain]\n\n\nprint(\"Acceptance rate: {:.3f}\".format(acceptance_rate))\nlast_states = \", \".join(\"{:.5f}\".format(state)\n                        for state in chain[-10:])\nprint(\"Last ten states of chain: \" + last_states)\n\nAcceptance rate: 0.720\nLast ten states of chain: 1.05847, 1.59966, 0.14389, -1.13281, 0.24131, -0.77448, -0.59703, 0.67707, 1.47065, 1.27361\n\n\n\ndef plot_samples(chain, log_prob, ax, orientation='vertical', normalize=True,\n                 xlims=(-5, 5), legend=True):\n    \n    from scipy.integrate import quad\n\n    ax.hist(chain, bins=50, density=True, label=\"MCMC samples\",\n           orientation=orientation)\n    # we numerically calculate the normalization constant of our PDF\n    if normalize:\n        Z, _ = quad(lambda x: np.exp(log_prob(x)), -np.inf, np.inf)\n    else:\n        Z = 1.0\n    xses = np.linspace(xlims[0], xlims[1], 1000)\n    yses = [np.exp(log_prob(x)) / Z for x in xses]\n    if orientation == 'horizontal':\n        (yses, xses) = (xses, yses)\n    ax.plot(xses, yses, label=\"true distribution\")\n    if legend:\n        ax.legend(frameon=False)\n\nfig, ax = plt.subplots()\nplot_samples(chain[500:], log_prob, ax)\ndespine(ax)\nax.set_yticks(())\nplt.show()\n\n\n\n\n\ndef sample_and_display(init_state, stepsize, n_total, n_burnin, log_prob):\n    chain, acceptance_rate = build_MH_chain(init_state, stepsize, n_total, log_prob)\n    print(\"Acceptance rate: {:.3f}\".format(acceptance_rate))\n    fig, ax = plt.subplots()\n    plot_samples([state for state, in chain[n_burnin:]], log_prob, ax)\n    despine(ax)\n    ax.set_yticks(())\n    plt.show()\n\nsample_and_display(np.array([2.0]), 30, 10000, 500, log_prob)\n\nAcceptance rate: 0.104\n\n\n\n\n\n\nsample_and_display(np.array([2.0]), 0.1, 10000, 500, log_prob)\n\nAcceptance rate: 0.985\n\n\n\n\n\n\nsample_and_display(np.array([2.0]), 0.1, 500000, 25000, log_prob)\n\nAcceptance rate: 0.990\n\n\n\n\n\n\n%matplotlib notebook\n%matplotlib inline\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.rcParams['figure.figsize'] = [10, 6]\n\nnp.random.seed(42)\n\ndef log_gaussian(x, mu, sigma):\n    # The np.sum() is for compatibility with sample_MH\n    return - 0.5 * np.sum((x - mu) ** 2) / sigma ** 2 \\\n           - np.log(np.sqrt(2 * np.pi * sigma ** 2))\n\n\nclass BivariateNormal(object):\n    n_variates = 2\n    \n    def __init__(self, mu1, mu2, sigma1, sigma2):\n        self.mu1, self.mu2 = mu1, mu2\n        self.sigma1, self.sigma2 = sigma1, sigma2\n        \n    def log_p_x(self, x):\n        return log_gaussian(x, self.mu1, self.sigma1)\n        \n    def log_p_y(self, x):\n        return log_gaussian(x, self.mu2, self.sigma2)\n    \n    def log_prob(self, x):        \n        cov_matrix = np.array([[self.sigma1 ** 2, 0],\n                               [0, self.sigma2 ** 2]])\n        inv_cov_matrix = np.linalg.inv(cov_matrix)\n        kernel = -0.5 * (x - self.mu1) @ inv_cov_matrix @ (x - self.mu2).T\n        normalization = np.log(np.sqrt((2 * np.pi) ** self.n_variates * np.linalg.det(cov_matrix)))\n        \n        return kernel - normalization               \n\n    \nbivariate_normal = BivariateNormal(mu1=0.0, mu2=0.0, sigma1=1.0, sigma2=0.15)\n\n\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\n\nfig, ax = plt.subplots()\nxses = np.linspace(-2, 2, 200)\nyses = np.linspace(-0.5, 0.5, 200)\nlog_density_values = [[bivariate_normal.log_prob(np.array((x, y))) for x in xses] for y in yses]\ndx = (xses[1] - xses[0]) / 2\ndy = (yses[1] - yses[0]) / 2\nextent = [xses[0] - dx, xses[-1] + dx, yses[0] - dy, yses[-1] + dy]\nim = ax.imshow(np.exp(log_density_values), extent=extent)\nax.set_xlabel('x')\nax.set_ylabel('y')\ndivider = make_axes_locatable(ax)\ncax = divider.append_axes('right', size='5%', pad=0.05)\ncb = fig.colorbar(im, cax=cax)\ncb.set_label('probability density')\nplt.show()\n\n\n\n\n\ndef sample_gibbs(old_state, bivariate_dist, stepsizes):\n    \"\"\"Draws a single sample using the systematic Gibbs sampling\n    transition kernel\n    \n    Arguments:\n    - old_state: the old (two-dimensional) state of a Markov chain\n                 (a list containing two floats)\n    - bivariate_dist: an object representing a bivariate distribution\n                      (in our case, an instance of BivariateNormal)\n    - stepsizes: a list of step sizes\n    \n    \"\"\"\n    x_old, y_old = old_state\n    \n    # for compatibility with sample_MH, change floats to one-dimensional\n    # numpy arrays of length one\n    x_old = np.array([x_old])\n    y_old = np.array([y_old])\n    \n    # draw new x conditioned on y\n    p_x_y = bivariate_dist.log_p_x\n    accept_x, x_new = sample_MH(x_old, p_x_y, stepsizes[0])\n    \n    # draw new y conditioned on x\n    p_y_x = bivariate_dist.log_p_y\n    accept_y, y_new = sample_MH(y_old, p_y_x, stepsizes[1])\n    \n    # Don't forget to turn the one-dimensional numpy arrays x_new, y_new\n    # of length one back into floats\n    \n    return (accept_x, accept_y), (x_new[0], y_new[0])\n\n\ndef build_gibbs_chain(init, stepsizes, n_total, bivariate_dist):\n    \"\"\"Builds a Markov chain by performing repeated transitions using\n    the systematic Gibbs sampling transition kernel\n    \n    Arguments:\n    - init: an initial (two-dimensional) state for the Markov chain\n            (a list containing two floats)\n    - stepsizes: a list of step sizes of type float\n    - n_total: the total length of the Markov chain\n    - bivariate_dist: an object representing a bivariate distribution\n                      (in our case, an instance of BivariateNormal)\n    \n    \"\"\"\n    init_x, init_k = init\n    chain = [init]\n    acceptances = []\n    \n    for _ in range(n_total):\n        accept, new_state = sample_gibbs(chain[-1], bivariate_dist, stepsizes)\n        chain.append(new_state)        \n        acceptances.append(accept)\n    \n    acceptance_rates = np.mean(acceptances, 0)\n    print(\"Acceptance rates: x: {:.3f}, y: {:.3f}\".format(acceptance_rates[0],\n                                                          acceptance_rates[1]))\n    \n    return chain \n\nstepsizes = (6.5, 1.0)\ninitial_state = [2.0, -1.0]\nchain = build_gibbs_chain(initial_state, stepsizes, 100000, bivariate_normal)\nchain = np.array(chain)\n\nAcceptance rates: x: 0.462, y: 0.456\n\n\n\ndef plot_samples_2D(chain, path_length, burnin, ax, xlims=(-3, 3), ylims=(-0.5, 0.5)):\n    chain = np.array(chain)\n    bins = [np.linspace(xlims[0], xlims[1], 100), np.linspace(ylims[0], ylims[1], 100)]\n    ax.hist2d(*chain[burnin:].T, bins=bins)\n    ax.plot(*chain[:path_length].T, marker='o', c='w', lw=0.4, \n            markersize=1, alpha=0.75)\n    ax.set_xlabel('x')\n    ax.set_ylabel('y')\n    ax.set_xlim(xlims[0], xlims[1])\n    ax.set_ylim(ylims[0], ylims[1])\n    \ndef plot_bivariate_samples(chain, burnin, pdf):\n    fig = plt.figure(figsize=(12,7))\n   \n    ax_c = plt.subplot2grid((4, 4), (1, 0), rowspan=1, colspan=3)\n    plot_samples_2D(chain, 100, burnin, ax_c)\n    \n    ax_t = plt.subplot2grid((4, 4), (0, 0), rowspan=1, colspan=3, sharex=ax_c)\n    plot_samples(chain[:,0], pdf.log_p_x, ax_t, normalize=False)\n    plt.setp(ax_t.get_xticklabels(), visible=False)\n    ax_t.set_yticks(())\n    for spine in ('top', 'left', 'right'):\n        ax_t.spines[spine].set_visible(False)\n\n    ax_r = plt.subplot2grid((4, 4), (1, 3), rowspan=1, colspan=1, sharey=ax_c)\n    plot_samples(chain[:,1], pdf.log_p_y, ax_r, orientation='horizontal',\n                 normalize=False, legend=False)\n    plt.setp(ax_r.get_yticklabels(), visible=False)\n    ax_r.set_xticks(())\n    for spine in ('top', 'bottom', 'right'):\n        ax_r.spines[spine].set_visible(False)\n\n    plt.show()\n    \nplot_bivariate_samples(chain, burnin=200, pdf=bivariate_normal)\n\n\n\n\n\nmix_params = dict(mu1=1.0, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7)\n\n\nfig, ax = plt.subplots()\nxspace = np.linspace(-0.5, 3, 200)\n\n# densities of both components\nfirst_component = [np.exp(log_gaussian(x, mix_params['mu1'], mix_params['sigma1']))\n                   for x in xspace]\nsecond_component = [np.exp(log_gaussian(x, mix_params['mu2'], mix_params['sigma2']))\n                    for x in xspace]\n\n# apply component weights\nfirst_component = mix_params['w1'] * np.array(first_component)\nsecond_component = mix_params['w2'] * np.array(second_component)\n\nax.plot(xspace, first_component, color='black')\nax.fill_between(xspace, first_component, alpha=0.6, label=\"1st component\")\nax.plot(xspace, second_component, color='black')\nax.fill_between(xspace, second_component, alpha=0.6, label=\"2nd component\")\nax.set_xlabel('x')\nax.set_yticks(())\nax.legend(frameon=False)\nfor spine in ('top', 'left', 'right'):\n    ax.spines[spine].set_visible(False)\nplt.show()\n\n\n\n\n\nclass GaussianMixture(object):\n    \n    def __init__(self, mu1, mu2, sigma1, sigma2, w1, w2):\n        self.mu1, self.mu2 = mu1, mu2\n        self.sigma1, self.sigma2 = sigma1, sigma2\n        self.w1, self.w2 = w1, w2\n        \n    def log_prob(self, x):\n        return np.logaddexp(np.log(self.w1) + log_gaussian(x, self.mu1, self.sigma1),\n                            np.log(self.w2) + log_gaussian(x, self.mu2, self.sigma2))\n    \n    def log_p_x_k(self, x, k):\n        # logarithm of p(x|k)\n        mu = (self.mu1, self.mu2)[k]\n        sigma = (self.sigma1, self.sigma2)[k]\n    \n        return log_gaussian(x, mu, sigma)\n    \n    def p_k_x(self, k, x):\n        # p(k|x) using Bayes' theorem\n        mu = (self.mu1, self.mu2)[k]\n        sigma = (self.sigma1, self.sigma2)[k]\n        weight = (self.w1, self.w2)[k]\n        log_normalization = self.log_prob(x)\n\n        return np.exp(log_gaussian(x, mu, sigma) + np.log(weight) - log_normalization)\n\n\ndef sample_gibbs(old_state, mixture, stepsize):\n    \"\"\"Draws a single sample using the systematic Gibbs sampling\n    transition kernel\n    \n    Arguments:\n    - old_state: the old (two-dimensional) state of a Markov chain\n                 (a list containing a float and an integer representing \n                 the initial mixture component)\n    - mixture: an object representing a mixture of densities\n               (in our case, an instance of GaussianMixture)\n    - stepsize: a step size of type float \n    \n    \"\"\"\n    x_old, k_old = old_state\n    \n    # for compatibility with sample_MH, change floats to one-dimensional\n    # numpy arrays of length one\n    x_old = np.array([x_old])\n    \n    # draw new x conditioned on k\n    x_pdf = lambda x: mixture.log_p_x_k(x, k_old)\n    accept, x_new = sample_MH(x_old, x_pdf, stepsize)\n    \n    # ... turn the one-dimensional numpy arrays of length one back\n    # into floats\n    x_new = x_new[0]\n    \n    # draw new k conditioned on x \n    k_probabilities = (mixture.p_k_x(0, x_new), mixture.p_k_x(1, x_new))\n    jump_probability = k_probabilities[1 - k_old]\n    k_new = np.random.choice((0,1), p=k_probabilities)\n    \n    return accept, jump_probability, (x_new, k_new)\n\n\ndef build_gibbs_chain(init, stepsize, n_total, mixture):\n    \"\"\"Builds a Markov chain by performing repeated transitions using\n    the systematic Gibbs sampling transition kernel\n    \n    Arguments:\n    - init: an initial (two-dimensional) state of a Markov chain\n            (a list containing a one-dimensional numpy array\n            of length one and an integer representing the initial\n            mixture component)\n    - stepsize: a step size of type float\n    - n_total: the total length of the Markov chain\n    - mixture: an object representing a mixture of densities\n               (in our case, an instance of GaussianMixture)\n    \n    \"\"\"\n    init_x, init_k = init\n    chain = [init]\n    acceptances = []\n    jump_probabilities = []\n    \n    for _ in range(n_total):\n        accept, jump_probability, new_state = sample_gibbs(chain[-1], mixture, stepsize)\n        chain.append(new_state)\n        jump_probabilities.append(jump_probability)\n        acceptances.append(accept)\n    \n    acceptance_rates = np.mean(acceptances)\n    print(\"Acceptance rate: x: {:.3f}\".format(acceptance_rates))\n    print(\"Average probability to change mode: {}\".format(np.mean(jump_probabilities)))\n    \n    return chain\n\nmixture = GaussianMixture(**mix_params)\nstepsize = 1.0\ninitial_state = [2.0, 1]\nchain = build_gibbs_chain(initial_state, stepsize, 10000, mixture)\nburnin = 1000\nx_states = [state[0] for state in chain[burnin:]]\n\nAcceptance rate: x: 0.631\nAverage probability to change mode: 0.08629295966662387\n\n\n\nfig, ax = plt.subplots()\nplot_samples(x_states, mixture.log_prob, ax, normalize=False, xlims=(-1,2.5))\nfor spine in ('top', 'left', 'right'):\n    ax.spines[spine].set_visible(False)\nax.set_yticks(())\nax.set_xlabel('x')\nplt.show()\n\n\n\n\n\nmixture = GaussianMixture(mu1=-1.0, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7)\nstepsize = 1.0\ninitial_state = [2.0, 1]\nchain = build_gibbs_chain(initial_state, stepsize, 100000, mixture)\nburnin = 10000\nx_states = [state[0] for state in chain[burnin:]]\n\nAcceptance rate: x: 0.558\nAverage probability to change mode: 6.139534006013391e-06\n\n\n\nfig, ax = plt.subplots()\nplot_samples(x_states, mixture.log_prob, ax, normalize=False, xlims=(-2,2.5))\nfor spine in ('top', 'left', 'right'):\n    ax.spines[spine].set_visible(False)\nax.set_yticks(())\nax.set_xlabel('x')\nplt.show()"
  },
  {
    "objectID": "posts/2020-11-03-MCMC.html#hamiltonian-monte-carlo",
    "href": "posts/2020-11-03-MCMC.html#hamiltonian-monte-carlo",
    "title": "MCMC algorithms, sampling problems/sampling techniques, Bayesian data analysis",
    "section": "Hamiltonian Monte Carlo",
    "text": "Hamiltonian Monte Carlo\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nnp.random.seed(42)\n\nxspace = np.linspace(-2, 2, 100)\nunnormalized_probs = np.exp(-0.5 * xspace ** 2)\nenergies = 0.5 * xspace ** 2\n\nfig, ax = plt.subplots(dpi=80)\nax.plot(xspace, unnormalized_probs, label=r\"$p(x)$\")\nax.plot(xspace, energies, label=r\"$E(x)=-\\log\\ p(x)$\")\nprop = dict(arrowstyle=\"-|&gt;,head_width=0.4,head_length=0.8\",\n            shrinkA=0,shrinkB=0)\nx_index1 = 75\nax.scatter((xspace[x_index1],), (energies[x_index1],), color=\"k\")\na_start1 = np.array((xspace[x_index1], energies[x_index1]))\na_end1 = np.array((xspace[x_index1] - xspace[x_index1], energies[x_index1]))\nax.annotate(\"\",a_end1, a_start1, arrowprops=prop)\ntext_pos1 = (a_start1[0] + 0.5 * (a_end1[0] - a_start1[0]), a_end1[1] + 0.075)\nax.text(*text_pos1, r\"force\", horizontalalignment=\"center\")\n\nx_index2 = 38\nax.scatter((xspace[x_index2],), (energies[x_index2],), color=\"k\")\na_start2 = np.array((xspace[x_index2], energies[x_index2]))\na_end2 = np.array((xspace[x_index2] - xspace[x_index2], energies[x_index2]))\nax.annotate(\"\",a_end2, a_start2, arrowprops=prop)\ntext_pos2 = (a_start2[0] + 0.5 * (a_end2[0] - a_start2[0]), a_end2[1] + 0.075, )\nax.text(*text_pos2, r\"force\", horizontalalignment=\"center\")\n\nax.set_xlabel(\"x\")\nax.set_yticks(())\nfor spine in ('top', 'right', 'left'):\n    ax.spines[spine].set_visible(False)\nax.legend(frameon=False)\nplt.show()\n\n\n\n\n\ndef leapfrog(x, v, gradient, timestep, trajectory_length):\n    v -= 0.5 * timestep * gradient(x)\n    for _ in range(trajectory_length - 1):\n        x += timestep * v\n        v -= timestep * gradient(x)\n    x += timestep * v\n    v -= 0.5 * timestep * gradient(x)\n\n    return x, v\n\n\ndef sample_HMC(x_old, log_prob, log_prob_gradient, timestep, trajectory_length):\n    # switch to physics mode!\n    def E(x): return -log_prob(x)\n    def gradient(x): return -log_prob_gradient(x)\n    def K(v): return 0.5 * np.sum(v ** 2)\n    def H(x, v): return K(v) + E(x)\n\n    # Metropolis acceptance probability, implemented in \"logarithmic space\"\n    # for numerical stability:\n    def log_p_acc(x_new, v_new, x_old, v_old):\n        return min(0, -(H(x_new, v_new) - H(x_old, v_old)))\n\n    # give a random kick to particle by drawing its momentum from p(v)\n    v_old = np.random.normal(size=x_old.shape)\n\n    # approximately calculate position x_new and momentum v_new after\n    # time trajectory_length  * timestep\n    x_new, v_new = leapfrog(x_old.copy(), v_old.copy(), gradient, \n                            timestep, trajectory_length)\n\n    # accept / reject based on Metropolis criterion\n    accept = np.log(np.random.random()) &lt; log_p_acc(x_new, v_new, x_old, v_old)\n\n    # we consider only the position x (meaning, we marginalize out v)\n    if accept:\n        return accept, x_new\n    else:\n        return accept, x_old\n\n\ndef build_HMC_chain(init, timestep, trajectory_length, n_total, log_prob, gradient):\n    n_accepted = 0\n    chain = [init]\n\n    for _ in range(n_total):\n        accept, state = sample_HMC(chain[-1].copy(), log_prob, gradient,\n                                   timestep, trajectory_length)\n        chain.append(state)\n        n_accepted += accept\n\n    acceptance_rate = n_accepted / float(n_total)\n\n    return chain, acceptance_rate\n\n\ndef log_prob(x): return -0.5 * np.sum(x ** 2)\n\n\ndef log_prob_gradient(x): return -x\n\n\nchain, acceptance_rate = build_HMC_chain(np.array([5.0, 1.0]), 1.5, 10, 10000,\n                                         log_prob, log_prob_gradient)\nprint(\"Acceptance rate: {:.3f}\".format(acceptance_rate))\n\nAcceptance rate: 0.622\n\n\n\nfig, ax = plt.subplots(dpi=80)\nplot_samples_2D(chain, 100, 200, ax, xlims=(-5.5, 5.5), ylims=(-5.5, 5.5))\nplt.show()\n\n\n\n\n\nchain, acceptance_rate = build_MH_chain(np.array([5.0, 1.0]), 2.6, 10000, log_prob)\nprint(\"Acceptance rate: {:.3f}\".format(acceptance_rate))\n\nAcceptance rate: 0.623\n\n\n\nfig, ax = plt.subplots(dpi=80)\nplot_samples_2D(chain, 100, 200, ax, xlims=(-5.5, 5.5), ylims=(-5.5, 5.5))\nplt.show()"
  },
  {
    "objectID": "posts/2020-11-03-MCMC.html#replica-exchange",
    "href": "posts/2020-11-03-MCMC.html#replica-exchange",
    "title": "MCMC algorithms, sampling problems/sampling techniques, Bayesian data analysis",
    "section": "Replica Exchange",
    "text": "Replica Exchange\n\nmix_params = dict(mu1=-1.5, mu2=2.0, sigma1=0.5, sigma2=0.2, w1=0.3, w2=0.7)\nmixture = GaussianMixture(**mix_params)\ntemperatures = [0.1, 0.4, 0.6, 0.8, 1.0]\n\n\nfrom scipy.integrate import quad\n\ndef plot_tempered_distributions(log_prob, temperatures, axes, xlim=(-4, 4)):\n    xspace = np.linspace(*xlim, 1000)\n    for i, (temp, ax) in enumerate(zip(temperatures, axes)):\n        pdf = lambda x: np.exp(temp * log_prob(x))\n        Z = quad(pdf, -1000, 1000)[0]\n        ax.plot(xspace, np.array(list(map(pdf, xspace))) / Z)\n        ax.text(0.8, 0.3, r'$\\beta={}$'.format(temp), transform=ax.transAxes)\n        ax.text(0.05, 0.3, 'replica {}'.format(len(temperatures) - i - 1), \n                transform=ax.transAxes)\n        ax.set_yticks(())\n    plt.show()\n    \nfig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True,\n                        figsize=(8, 7))\nplot_tempered_distributions(mixture.log_prob, temperatures, axes)\nplt.show()\n\n\n\n\n\ndef handle_left_border(leftmost_old_state, leftmost_temperature, \n                       leftmost_stepsize, log_prob, new_multistate):\n    accepted, state = sample_MH(leftmost_old_state, \n                                lambda x: leftmost_temperature * log_prob(x), \n                                leftmost_stepsize)\n    new_multistate = [state] + new_multistate\n    return new_multistate, accepted\n\n\ndef handle_right_border(rightmost_old_state, rightmost_temperature, \n                        rightmost_stepsize, log_prob, new_multistate):\n    accepted, state = sample_MH(rightmost_old_state, \n                                lambda x: rightmost_temperature * log_prob(x),\n                                rightmost_stepsize)\n    new_multistate = new_multistate + [state]\n    return new_multistate, accepted\n    \n    \ndef build_RE_chain(init, stepsizes, n_total, temperatures, swap_interval, log_prob):\n\n    from itertools import cycle\n\n    n_replicas = len(temperatures)\n\n    # a bunch of arrays in which we will store how many\n    # Metropolis-Hastings / swap moves were accepted\n    # and how many there were performed in total\n\n    accepted_MH_moves = np.zeros(n_replicas)\n    total_MH_moves = np.zeros(n_replicas)\n    accepted_swap_moves = np.zeros(n_replicas - 1)\n    total_swap_moves = np.zeros(n_replicas - 1)\n\n    cycler = cycle((True, False))\n    chain = [init]\n    for k in range(n_total):\n        new_multistate = []\n        if k &gt; 0 and k % swap_interval == 0:\n            # perform RE swap\n            # First, determine the swap partners\n            if next(cycler):\n                # swap (0,1), (2,3), ...\n                partners = [(j-1, j) for j in range(1, n_replicas, 2)]\n            else:\n                # swap (1,2), (3,4), ...\n                partners = [(j-1, j) for j in range(2, len(temperatures), 2)]\n            # Now, for each pair of replicas, attempt an exchange\n            for (i,j) in partners:\n                bi, bj = temperatures[i], temperatures[j]\n                lpi, lpj = log_prob(chain[-1][i]), log_prob(chain[-1][j])\n                log_p_acc = min(0, bi * lpj - bi * lpi + bj * lpi - bj * lpj)\n                if np.log(np.random.uniform()) &lt; log_p_acc:\n                    new_multistate += [chain[-1][j], chain[-1][i]]\n                    accepted_swap_moves[i] += 1\n                else:\n                    new_multistate += [chain[-1][i], chain[-1][j]]\n                total_swap_moves[i] += 1\n            # We might have border cases: if left- / rightmost replicas don't participate\n            # in swaps, have them draw a sample\n            if partners[0][0] != 0:\n                new_multistate, accepted = handle_left_border(chain[-1][0], temperatures[0],\n                                                              stepsizes[0], log_prob,\n                                                              new_multistate)\n                accepted_MH_moves[0] += accepted\n                total_MH_moves[0] += 1\n            if partners[-1][1] != len(temperatures) - 1:\n                new_multistate, accepted = handle_right_border(chain[-1][-1], temperatures[-1],\n                                                               stepsizes[-1], log_prob,\n                                                               new_multistate)\n                accepted_MH_moves[-1] += accepted\n                total_MH_moves[-1] += 1\n        else:\n            # perform sampling in single chains\n            for j, temp in enumerate(temperatures):\n                accepted, state = sample_MH(chain[-1][j], lambda x: temp * log_prob(x), stepsizes[j])\n                accepted_MH_moves[j] += accepted\n                total_MH_moves[j] += 1\n                new_multistate.append(state)\n        chain.append(new_multistate)\n\n    # calculate acceptance rates\n    MH_acceptance_rates = accepted_MH_moves / total_MH_moves\n    # safe division in case of zero total swap moves\n    swap_acceptance_rates = np.divide(accepted_swap_moves, total_swap_moves,\n                                      out=np.zeros(n_replicas - 1), where=total_swap_moves != 0)\n\n    return MH_acceptance_rates, swap_acceptance_rates, np.array(chain)\n\n\nstepsizes = [2.75, 2.5, 2.0, 1.75, 1.6]\n\n\ndef print_MH_acceptance_rates(mh_acceptance_rates):\n    print(\"MH acceptance rates: \" + \"\".join([\"{}: {:.3f} \".format(i, x)\n                                             for i, x in enumerate(mh_acceptance_rates)]))\n\n\nmh_acc_rates, swap_acc_rates, chains = build_RE_chain(np.random.uniform(low=-3, high=3,\n                                                                        size=len(temperatures)),\n                                                      stepsizes, 10000, temperatures, 500000000,\n                                                      mixture.log_prob)\nprint_MH_acceptance_rates(mh_acc_rates)\n\nMH acceptance rates: 0: 0.790 1: 0.551 2: 0.415 3: 0.552 4: 0.705 \n\n\n\ndef plot_RE_samples(chains, axes, bins=np.linspace(-4, 4, 50)):\n    for i, (chain, ax) in enumerate(zip(chains, axes)):\n        ax.hist(chain, bins, density=True, label=\"MCMC samples\")\n        \nfig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True,\n                         figsize=(8, 7))\nplot_RE_samples(chains[100:].T, axes)\nplot_tempered_distributions(mixture.log_prob, temperatures, axes)\nplt.show()\n\n\n\n\n\ninit = np.random.uniform(low=-4, high=4, size=len(temperatures))\nmh_acc_rates, swap_acc_rates, chains = build_RE_chain(init, stepsizes,\n                                                      10000, temperatures, 5,\n                                                      mixture.log_prob)\nprint_MH_acceptance_rates(mh_acc_rates)\nswap_rate_string = \"\".join([\"{}&lt;-&gt;{}: {:.3f}, \".format(i, i+1, x)\n                            for i, x in enumerate(swap_acc_rates)])[:-2]\nprint(\"Swap acceptance rates:\", swap_rate_string)\n\nMH acceptance rates: 0: 0.797 1: 0.552 2: 0.539 3: 0.499 4: 0.466 \nSwap acceptance rates: 0&lt;-&gt;1: 0.585, 1&lt;-&gt;2: 0.846, 2&lt;-&gt;3: 0.829, 3&lt;-&gt;4: 0.876\n\n\n\nfig, axes = plt.subplots(len(temperatures), 1, sharex=True, sharey=True,\n                        figsize=(8, 7))\nplot_RE_samples(chains[100:].T, axes)\nplot_tempered_distributions(mixture.log_prob, temperatures, axes)\nplt.show()\n\n\n\n\n\n# Detect swaps. This method works only under the assumption that\n# when performing local MCMC moves, starting from two different \n# initial states, you cannot end up with the same state\nswaps = {}\n# for each pair of chains...\nfor i in range(len(chains) - 1):\n    # shift one chain by one state to the left.\n    # Where states from both chains match up, a successful exchange\n    # was performed\n    matches = np.where(chains[i, :-1] == chains[i+1, 1:])[0]\n    if len(matches) &gt; 0:\n        swaps[i] = matches\n\n\n# Reconstruct trajectories of single states through the temperature\n# ladder\ndef reconstruct_trajectory(start_index, chains):\n    res = []\n    current_ens = start_index\n    for i in range(len(chains)):\n        res.append(current_ens)\n        if i in swaps:\n            if current_ens in swaps[i]:\n                current_ens += 1\n            elif current_ens in swaps[i] + 1:\n                current_ens -= 1\n\n    return np.array(res)\n\n\ndef plot_state_trajectories(trajectories, ax, max_samples=300):\n    for trajectory in trajectories:\n        ax.plot(-trajectory[:max_samples] - 1, lw=2)\n    ax.set_xlabel(\"# of MCMC samples\")\n    ax.set_ylabel(r\"inverse temperature $\\beta$\")\n    # make order of temperatures appear as above - whatever it takes...\n    ax.set_yticks(range(-len(temperatures), 0))\n    ax.set_yticklabels(temperatures[::-1])\n    \n\n# which states to follow\nstart_state_indices = (4, 0)\n\nfig, ax = plt.subplots(figsize=(8, 6))\ntrajectories = np.array([reconstruct_trajectory(i, chains) \n                         for i in start_state_indices])\nplot_state_trajectories(trajectories, ax)\nplt.show()"
  },
  {
    "objectID": "posts/2020-09-21-StockMarketPortfolioAnaylsis2.html",
    "href": "posts/2020-09-21-StockMarketPortfolioAnaylsis2.html",
    "title": "Stock Market and Optimal Portfolio Anaylsis scipy and quandl",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport quandl\n%matplotlib inline\n\n\nstart = pd.to_datetime('2010-01-01')\nend = pd.to_datetime('today')\n\n\n# Grabbing a bunch of tech stocks for our portfolio\nCOST = quandl.get('WIKI/COST.11',\n                  start_date = start,\n                  end_date = end)\nNLSN = quandl.get('WIKI/NLSN.11',\n                   start_date = start,\n                   end_date = end)\nNKE = quandl.get('WIKI/NKE.11',\n                 start_date = start,\n                 end_date = end)\nDIS = quandl.get('WIKI/DIS.11',\n                  start_date = start,\n                  end_date = end)\n\n\n\nstocks = pd.concat([COST, NLSN, NKE, DIS],\n                   axis = 1)\nstocks.columns = ['COST','NLSN','NKE','DIS']\n\n\nstocks\n\n\n\n\n\n\n\n\nCOST\nNLSN\nNKE\nDIS\n\n\nDate\n\n\n\n\n\n\n\n\n2010-01-04\n49.085078\nNaN\n14.751122\n28.960651\n\n\n2010-01-05\n48.936361\nNaN\n14.809811\n28.888407\n\n\n2010-01-06\n49.572542\nNaN\n14.719521\n28.734890\n\n\n2010-01-07\n49.332941\nNaN\n14.863985\n28.743920\n\n\n2010-01-08\n48.977671\nNaN\n14.834641\n28.789072\n\n\n...\n...\n...\n...\n...\n\n\n2018-03-21\n186.070000\n32.44\n66.350000\n101.820000\n\n\n2018-03-22\n182.640000\n31.82\n64.420000\n100.600000\n\n\n2018-03-23\n180.840000\n31.51\n64.630000\n98.540000\n\n\n2018-03-26\n187.220000\n32.03\n65.900000\n100.650000\n\n\n2018-03-27\n183.150000\n32.09\n66.170000\n99.360000\n\n\n\n\n2071 rows × 4 columns\n\n\n\n\nmean_daily_ret = stocks.pct_change(1).mean()\nmean_daily_ret\n\nCOST    0.000699\nNLSN    0.000312\nNKE     0.000833\nDIS     0.000683\ndtype: float64\n\n\n\nstocks.pct_change(1).corr()\n\n\n\n\n\n\n\n\nCOST\nNLSN\nNKE\nDIS\n\n\n\n\nCOST\n1.000000\n0.265003\n0.370978\n0.415377\n\n\nNLSN\n0.265003\n1.000000\n0.312192\n0.392808\n\n\nNKE\n0.370978\n0.312192\n1.000000\n0.446150\n\n\nDIS\n0.415377\n0.392808\n0.446150\n1.000000\n\n\n\n\n\n\n\n\nstocks.head()\n\n\n\n\n\n\n\n\nCOST\nNLSN\nNKE\nDIS\n\n\nDate\n\n\n\n\n\n\n\n\n2010-01-04\n49.085078\nNaN\n14.751122\n28.960651\n\n\n2010-01-05\n48.936361\nNaN\n14.809811\n28.888407\n\n\n2010-01-06\n49.572542\nNaN\n14.719521\n28.734890\n\n\n2010-01-07\n49.332941\nNaN\n14.863985\n28.743920\n\n\n2010-01-08\n48.977671\nNaN\n14.834641\n28.789072\n\n\n\n\n\n\n\n\nstock_normed = stocks/stocks.iloc[0]\nstock_normed.plot()\n\n&lt;AxesSubplot:xlabel='Date'&gt;\n\n\n\n\n\n\nstock_daily_ret = stocks.pct_change(1)\nstock_daily_ret.head()\n\n\n\n\n\n\n\n\nCOST\nNLSN\nNKE\nDIS\n\n\nDate\n\n\n\n\n\n\n\n\n2010-01-04\nNaN\nNaN\nNaN\nNaN\n\n\n2010-01-05\n-0.003030\nNaN\n0.003979\n-0.002495\n\n\n2010-01-06\n0.013000\nNaN\n-0.006097\n-0.005314\n\n\n2010-01-07\n-0.004833\nNaN\n0.009814\n0.000314\n\n\n2010-01-08\n-0.007201\nNaN\n-0.001974\n0.001571\n\n\n\n\n\n\n\n\nlog_ret = np.log(stocks / stocks.shift(1))\nlog_ret.head()\n\n\n\n\n\n\n\n\nCOST\nNLSN\nNKE\nDIS\n\n\nDate\n\n\n\n\n\n\n\n\n2010-01-04\nNaN\nNaN\nNaN\nNaN\n\n\n2010-01-05\n-0.003034\nNaN\n0.003971\n-0.002498\n\n\n2010-01-06\n0.012916\nNaN\n-0.006115\n-0.005328\n\n\n2010-01-07\n-0.004845\nNaN\n0.009767\n0.000314\n\n\n2010-01-08\n-0.007228\nNaN\n-0.001976\n0.001570\n\n\n\n\n\n\n\n\nlog_ret.hist(bins = 100,\n             figsize = (12, 6));\nplt.tight_layout()\n\n\n\n\n\nlog_ret.describe().transpose()\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nCOST\n2068.0\n0.000633\n0.011172\n-0.083110\n-0.005293\n0.000413\n0.006618\n0.060996\n\n\nNLSN\n1801.0\n0.000198\n0.015121\n-0.185056\n-0.007131\n0.000000\n0.008051\n0.095201\n\n\nNKE\n2070.0\n0.000725\n0.014682\n-0.098743\n-0.006602\n0.000656\n0.008155\n0.115342\n\n\nDIS\n2070.0\n0.000596\n0.013220\n-0.096190\n-0.005710\n0.000776\n0.007453\n0.073531\n\n\n\n\n\n\n\n\nlog_ret.mean() * 252\n\nCOST    0.159439\nNLSN    0.049979\nNKE     0.182719\nDIS     0.150081\ndtype: float64\n\n\n\n# Compute pairwise covariance of columns\nlog_ret.cov()\n\n\n\n\n\n\n\n\nCOST\nNLSN\nNKE\nDIS\n\n\n\n\nCOST\n0.000125\n0.000045\n0.000061\n0.000061\n\n\nNLSN\n0.000045\n0.000229\n0.000070\n0.000077\n\n\nNKE\n0.000061\n0.000070\n0.000216\n0.000087\n\n\nDIS\n0.000061\n0.000077\n0.000087\n0.000175\n\n\n\n\n\n\n\n\n# Set seed (optional)\nnp.random.seed(101)\n\n# Stock Columns\nprint('Stocks')\nprint(stocks.columns)\nprint('\\n')\n\n# Create Random Weights\nprint('Creating Random Weights')\nweights = np.array(np.random.random(4))\nprint(weights)\nprint('\\n')\n\n# Rebalance Weights\nprint('Rebalance to sum to 1.0')\nweights = weights / np.sum(weights)\nprint(weights)\nprint('\\n')\n\n# Expected Return\nprint('Expected Portfolio Return')\nexp_ret = np.sum(log_ret.mean() * weights) *252\nprint(exp_ret)\nprint('\\n')\n\n# Expected Variance\nprint('Expected Volatility')\nexp_vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\nprint(exp_vol)\nprint('\\n')\n\n# Sharpe Ratio\nSR = exp_ret/exp_vol\nprint('Sharpe Ratio')\nprint(SR)\n\nStocks\nIndex(['COST', 'NLSN', 'NKE', 'DIS'], dtype='object')\n\n\nCreating Random Weights\n[0.51639863 0.57066759 0.02847423 0.17152166]\n\n\nRebalance to sum to 1.0\n[0.40122278 0.44338777 0.02212343 0.13326603]\n\n\nExpected Portfolio Return\n0.11017373023155777\n\n\nExpected Volatility\n0.16110487214223854\n\n\nSharpe Ratio\n0.6838634286260817\n\n\n\nnum_ports = 15000\n\nall_weights = np.zeros((num_ports, len(stocks.columns)))\nret_arr = np.zeros(num_ports)\nvol_arr = np.zeros(num_ports)\nsharpe_arr = np.zeros(num_ports)\n\nfor ind in range(num_ports):\n\n    # Create Random Weights\n    weights = np.array(np.random.random(4))\n\n    # Rebalance Weights\n    weights = weights / np.sum(weights)\n    \n    # Save Weights\n    all_weights[ind,:] = weights\n\n    # Expected Return\n    ret_arr[ind] = np.sum((log_ret.mean() * weights) *252)\n\n    # Expected Variance\n    vol_arr[ind] = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n\n    # Sharpe Ratio\n    sharpe_arr[ind] = ret_arr[ind] / vol_arr[ind]\n\n\nsharpe_arr.max()\n\n1.042687299617254\n\n\n\nsharpe_arr.argmax()\n\n10619\n\n\n\nall_weights[10619,:]\n\narray([5.06395348e-01, 4.67772019e-04, 2.64242193e-01, 2.28894687e-01])\n\n\n\nmax_sr_ret = ret_arr[1419]\nmax_sr_vol = vol_arr[1419]\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n# Add red dot for max SR\nplt.scatter(max_sr_vol,\n            max_sr_ret,\n            c = 'red',\n            s = 50,\n            edgecolors = 'black')\n\n&lt;matplotlib.collections.PathCollection at 0x7f703b26fd00&gt;\n\n\n\n\n\n\ndef get_ret_vol_sr(weights):\n    \"\"\"\n    Takes in weights, returns array or return,volatility, sharpe ratio\n    \"\"\"\n    weights = np.array(weights)\n    ret = np.sum(log_ret.mean() * weights) * 252\n    vol = np.sqrt(np.dot(weights.T, np.dot(log_ret.cov() * 252, weights)))\n    sr = ret/vol\n    return np.array([ret, vol, sr])\n\nfrom scipy.optimize import minimize\nimport numpy as np\n\ndef neg_sharpe(weights):\n    return  get_ret_vol_sr(weights)[2] * -1\n\n# Contraints\ndef check_sum(weights):\n    '''\n    Returns 0 if sum of weights is 1.0\n    '''\n    return np.sum(weights) - 1\n\n# By convention of minimize function it should be a function that returns zero for conditions\ncons = ({'type' : 'eq', 'fun': check_sum})\n\n# 0-1 bounds for each weight\nbounds = ((0, 1), (0, 1), (0, 1), (0, 1))\n\n# Initial Guess (equal distribution)\ninit_guess = [0.25, 0.25, 0.25, 0.25]\n\n# Sequential Least Squares \nopt_results = minimize(neg_sharpe,\n                       init_guess,\n                       method = 'SLSQP',\n                       bounds = bounds,\n                       constraints = cons)\n\nopt_results\n\n     fun: -1.0442236428192482\n     jac: array([-1.85623765e-04,  3.00063133e-01,  3.43203545e-04,  1.72853470e-05])\n message: 'Optimization terminated successfully'\n    nfev: 20\n     nit: 4\n    njev: 4\n  status: 0\n success: True\n       x: array([0.53438392, 0.        , 0.27969302, 0.18592306])\n\n\n\nopt_results.x\n\nget_ret_vol_sr(opt_results.x)\n\narray([0.16421049, 0.15725605, 1.04422364])\n\n\n\nfrontier_y = np.linspace(0, 0.3, 100)\n\n\ndef minimize_volatility(weights):\n    return  get_ret_vol_sr(weights)[1] \n\nfrontier_volatility = []\n\nfor possible_return in frontier_y:\n    # function for return\n    cons = ({'type':'eq','fun': check_sum},\n            {'type':'eq','fun': lambda w: get_ret_vol_sr(w)[0] - possible_return})\n    \n    result = minimize(minimize_volatility,\n                      init_guess,\n                      method = 'SLSQP',\n                      bounds = bounds,\n                      constraints = cons)\n    \n    frontier_volatility.append(result['fun'])\n\n\nplt.figure(figsize = (12, 8))\nplt.scatter(vol_arr,\n            ret_arr,\n            c = sharpe_arr,\n            cmap = 'plasma')\nplt.colorbar(label = 'Sharpe Ratio')\nplt.xlabel('Volatility')\nplt.ylabel('Return')\n\n\n\n# Add frontier line\nplt.plot(frontier_volatility,\n         frontier_y,\n         'g--',\n         linewidth = 3)"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "",
    "text": "#Fetal cardiotocography example\n#Code from https://github.com/pycaret/pycaret/\n#Dataset link: https://www.kaggle.com/akshat0007/fetalhr\n## Importing necessary libraries \n\nimport pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n## Reading the dataset using pandas\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/CTG.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nFileName\nDate\nSegFile\nb\ne\nLBE\nLB\nAC\nFM\nUC\n...\nC\nD\nE\nAD\nDE\nLD\nFS\nSUSP\nCLASS\nNSP\n\n\n\n\n0\nVariab10.txt\n12/1/1996\nCTG0001.txt\n240.0\n357.0\n120.0\n120.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n9.0\n2.0\n\n\n1\nFmcs_1.txt\n5/3/1996\nCTG0002.txt\n5.0\n632.0\n132.0\n132.0\n4.0\n0.0\n4.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n2\nFmcs_1.txt\n5/3/1996\nCTG0003.txt\n177.0\n779.0\n133.0\n133.0\n2.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n3\nFmcs_1.txt\n5/3/1996\nCTG0004.txt\n411.0\n1192.0\n134.0\n134.0\n2.0\n0.0\n6.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n4\nFmcs_1.txt\n5/3/1996\nCTG0005.txt\n533.0\n1147.0\n132.0\n132.0\n4.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n2124\nS8001045.dsp\n6/6/1998\nCTG2127.txt\n1576.0\n3049.0\n140.0\n140.0\n1.0\n0.0\n9.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n5.0\n2.0\n\n\n2125\nS8001045.dsp\n6/6/1998\nCTG2128.txt\n2796.0\n3415.0\n142.0\n142.0\n1.0\n1.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n2126\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2127\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2128\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n564.0\n23.0\n...\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n2129 rows × 40 columns\n## Having a look of our data\ndf.head()\n\n\n\n\n\n\n\n\nFileName\nDate\nSegFile\nb\ne\nLBE\nLB\nAC\nFM\nUC\n...\nC\nD\nE\nAD\nDE\nLD\nFS\nSUSP\nCLASS\nNSP\n\n\n\n\n0\nVariab10.txt\n12/1/1996\nCTG0001.txt\n240.0\n357.0\n120.0\n120.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n9.0\n2.0\n\n\n1\nFmcs_1.txt\n5/3/1996\nCTG0002.txt\n5.0\n632.0\n132.0\n132.0\n4.0\n0.0\n4.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n2\nFmcs_1.txt\n5/3/1996\nCTG0003.txt\n177.0\n779.0\n133.0\n133.0\n2.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n3\nFmcs_1.txt\n5/3/1996\nCTG0004.txt\n411.0\n1192.0\n134.0\n134.0\n2.0\n0.0\n6.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n6.0\n1.0\n\n\n4\nFmcs_1.txt\n5/3/1996\nCTG0005.txt\n533.0\n1147.0\n132.0\n132.0\n4.0\n0.0\n5.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2.0\n1.0\n\n\n\n\n5 rows × 40 columns"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#performing-some-basic-preprocessing-techniques",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#performing-some-basic-preprocessing-techniques",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Performing some basic preprocessing techniques",
    "text": "Performing some basic preprocessing techniques\n\n## This will print the number of columns and rows\nprint(df.shape)\n\n(2129, 35)\n\n\n\n## Checking for the null values\ndf.isnull().sum()\n\nLBE         3\nLB          3\nAC          3\nFM          2\nUC          2\nASTV        2\nMSTV        2\nALTV        2\nMLTV        2\nDL          1\nDS          1\nDP          1\nDR          1\nWidth       3\nMin         3\nMax         3\nNmax        3\nNzeros      3\nMode        3\nMean        3\nMedian      3\nVariance    3\nTendency    3\nA           3\nB           3\nC           3\nD           3\nE           3\nAD          3\nDE          3\nLD          3\nFS          3\nSUSP        3\nCLASS       3\nNSP         3\ndtype: int64\n\n\n\n## Dropping the the rows containing null values\ndf=df.dropna()\n\n\ndf.isnull().sum()\n\nLBE         0\nLB          0\nAC          0\nFM          0\nUC          0\nASTV        0\nMSTV        0\nALTV        0\nMLTV        0\nDL          0\nDS          0\nDP          0\nDR          0\nWidth       0\nMin         0\nMax         0\nNmax        0\nNzeros      0\nMode        0\nMean        0\nMedian      0\nVariance    0\nTendency    0\nA           0\nB           0\nC           0\nD           0\nE           0\nAD          0\nDE          0\nLD          0\nFS          0\nSUSP        0\nCLASS       0\nNSP         0\ndtype: int64\n\n\n\n## Checking the data type of the columns\ndf.dtypes\n\nLBE         float64\nLB          float64\nAC          float64\nFM          float64\nUC          float64\nASTV        float64\nMSTV        float64\nALTV        float64\nMLTV        float64\nDL          float64\nDS          float64\nDP          float64\nDR          float64\nWidth       float64\nMin         float64\nMax         float64\nNmax        float64\nNzeros      float64\nMode        float64\nMean        float64\nMedian      float64\nVariance    float64\nTendency    float64\nA           float64\nB           float64\nC           float64\nD           float64\nE           float64\nAD          float64\nDE          float64\nLD          float64\nFS          float64\nSUSP        float64\nCLASS       float64\nNSP         float64\ndtype: object"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#importing-the-pycaret-library",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#importing-the-pycaret-library",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Importing the pycaret library",
    "text": "Importing the pycaret library\n\n# This command will basically import all the modules from pycaret that are necessary for classification tasks\nfrom pycaret.classification import *\n\n\n# Setting up the classifier\n# Pass the complete dataset as data and the featured to be predicted as target\nclf=setup(data=df,target='NSP')\n\n\n\n\n\n\nDescription\nValue\n\n\n\n\n0\nsession_id\n7481\n\n\n1\nTarget\nNSP\n\n\n2\nTarget Type\nMulticlass\n\n\n3\nLabel Encoded\nNone\n\n\n4\nOriginal Data\n(2126, 35)\n\n\n5\nMissing Values\nFalse\n\n\n6\nNumeric Features\n23\n\n\n7\nCategorical Features\n11\n\n\n8\nOrdinal Features\nFalse\n\n\n9\nHigh Cardinality Features\nFalse\n\n\n10\nHigh Cardinality Method\nNone\n\n\n11\nTransformed Train Set\n(1488, 33)\n\n\n12\nTransformed Test Set\n(638, 33)\n\n\n13\nShuffle Train-Test\nTrue\n\n\n14\nStratify Train-Test\nFalse\n\n\n15\nFold Generator\nStratifiedKFold\n\n\n16\nFold Number\n10\n\n\n17\nCPU Jobs\n-1\n\n\n18\nUse GPU\nFalse\n\n\n19\nLog Experiment\nFalse\n\n\n20\nExperiment Name\nclf-default-name\n\n\n21\nUSI\n3ae4\n\n\n22\nImputation Type\nsimple\n\n\n23\nIterative Imputation Iteration\nNone\n\n\n24\nNumeric Imputer\nmean\n\n\n25\nIterative Imputation Numeric Model\nNone\n\n\n26\nCategorical Imputer\nconstant\n\n\n27\nIterative Imputation Categorical Model\nNone\n\n\n28\nUnknown Categoricals Handling\nleast_frequent\n\n\n29\nNormalize\nFalse\n\n\n30\nNormalize Method\nNone\n\n\n31\nTransformation\nFalse\n\n\n32\nTransformation Method\nNone\n\n\n33\nPCA\nFalse\n\n\n34\nPCA Method\nNone\n\n\n35\nPCA Components\nNone\n\n\n36\nIgnore Low Variance\nFalse\n\n\n37\nCombine Rare Levels\nFalse\n\n\n38\nRare Level Threshold\nNone\n\n\n39\nNumeric Binning\nFalse\n\n\n40\nRemove Outliers\nFalse\n\n\n41\nOutliers Threshold\nNone\n\n\n42\nRemove Multicollinearity\nFalse\n\n\n43\nMulticollinearity Threshold\nNone\n\n\n44\nClustering\nFalse\n\n\n45\nClustering Iteration\nNone\n\n\n46\nPolynomial Features\nFalse\n\n\n47\nPolynomial Degree\nNone\n\n\n48\nTrignometry Features\nFalse\n\n\n49\nPolynomial Threshold\nNone\n\n\n50\nGroup Features\nFalse\n\n\n51\nFeature Selection\nFalse\n\n\n52\nFeature Selection Method\nclassic\n\n\n53\nFeatures Selection Threshold\nNone\n\n\n54\nFeature Interaction\nFalse\n\n\n55\nFeature Ratio\nFalse\n\n\n56\nInteraction Threshold\nNone\n\n\n57\nFix Imbalance\nFalse\n\n\n58\nFix Imbalance Method\nSMOTE\n\n\n\n\n\n\n# This model will be used to compare all the model along with the cross validation\ncompare_models()\n\n\n\n\n\n\nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\net\nExtra Trees Classifier\n0.9906\n0.9950\n0.9786\n0.9908\n0.9905\n0.9733\n0.9737\n0.0740\n\n\nrf\nRandom Forest Classifier\n0.9899\n0.9979\n0.9770\n0.9901\n0.9898\n0.9714\n0.9718\n0.0940\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.9899\n0.9983\n0.9783\n0.9901\n0.9898\n0.9714\n0.9718\n0.0780\n\n\ngbc\nGradient Boosting Classifier\n0.9893\n0.9978\n0.9767\n0.9894\n0.9891\n0.9695\n0.9699\n0.2870\n\n\ndt\nDecision Tree Classifier\n0.9872\n0.9812\n0.9758\n0.9874\n0.9872\n0.9641\n0.9643\n0.0090\n\n\nridge\nRidge Classifier\n0.9845\n0.0000\n0.9624\n0.9848\n0.9842\n0.9556\n0.9565\n0.0070\n\n\nlda\nLinear Discriminant Analysis\n0.9845\n0.9965\n0.9624\n0.9848\n0.9842\n0.9556\n0.9565\n0.0090\n\n\nlr\nLogistic Regression\n0.9839\n0.9969\n0.9690\n0.9841\n0.9836\n0.9542\n0.9548\n0.4540\n\n\nnb\nNaive Bayes\n0.9664\n0.9902\n0.9696\n0.9702\n0.9674\n0.9096\n0.9115\n0.0070\n\n\nsvm\nSVM - Linear Kernel\n0.9153\n0.0000\n0.7730\n0.9222\n0.9085\n0.7477\n0.7579\n0.0140\n\n\nada\nAda Boost Classifier\n0.9100\n0.9843\n0.9211\n0.9629\n0.9106\n0.8425\n0.8624\n0.0460\n\n\nknn\nK Neighbors Classifier\n0.9079\n0.9264\n0.7654\n0.9066\n0.9028\n0.7173\n0.7250\n0.0150\n\n\nqda\nQuadratic Discriminant Analysis\n0.7137\n0.8002\n0.7495\n0.8723\n0.7475\n0.4461\n0.5074\n0.0070\n\n\n\n\n\nExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='gini', max_depth=None, max_features='auto',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n                     oob_score=False, random_state=7481, verbose=0,\n                     warm_start=False)\n\n\n\nThe AUC score is 0.000 because it is not supported for the muli-classification tasks\n\n\nAlso, from the above it is understood that Extreme Gradient Boosting(popularly known as XGBoost) model really performed well. So, we will proceed with Extreme Gradient Boosting model."
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#creating-the-extreme-gradient-boostingxgboost-model",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#creating-the-extreme-gradient-boostingxgboost-model",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Creating the Extreme Gradient Boosting(XGBoost) model",
    "text": "Creating the Extreme Gradient Boosting(XGBoost) model\n\nxgboost_classifier=create_model('rf')\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9866\n0.9992\n0.9683\n0.9868\n0.9863\n0.9616\n0.9624\n\n\n1\n0.9866\n0.9990\n0.9813\n0.9866\n0.9866\n0.9625\n0.9625\n\n\n2\n0.9933\n1.0000\n0.9841\n0.9933\n0.9932\n0.9810\n0.9812\n\n\n3\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n4\n0.9933\n1.0000\n0.9833\n0.9938\n0.9933\n0.9813\n0.9814\n\n\n5\n0.9866\n0.9981\n0.9667\n0.9868\n0.9863\n0.9617\n0.9625\n\n\n6\n0.9866\n0.9861\n0.9667\n0.9868\n0.9863\n0.9617\n0.9625\n\n\n7\n0.9799\n0.9974\n0.9389\n0.9797\n0.9795\n0.9424\n0.9431\n\n\n8\n0.9932\n0.9994\n0.9972\n0.9936\n0.9933\n0.9810\n0.9812\n\n\n9\n0.9932\n0.9993\n0.9833\n0.9933\n0.9932\n0.9805\n0.9807\n\n\nMean\n0.9899\n0.9979\n0.9770\n0.9901\n0.9898\n0.9714\n0.9718\n\n\nSD\n0.0054\n0.0040\n0.0168\n0.0054\n0.0055\n0.0155\n0.0152\n\n\n\n\n\n\n## Let's now check the model hyperparameters\nprint(xgboost_classifier)\n\nRandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='auto',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, n_estimators=100,\n                       n_jobs=-1, oob_score=False, random_state=7481, verbose=0,\n                       warm_start=False)"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#tuning-the-hyperparametes-for-better-performance",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#tuning-the-hyperparametes-for-better-performance",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Tuning the hyperparametes for better performance",
    "text": "Tuning the hyperparametes for better performance\n\n# Whenenver we compare different models or build a model, the model uses deault\n#hyperparameter values. Hence, we need to tune our model to get better performance\n\ntuned_xgboost_classifier=tune_model(xgboost_classifier)\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9866\n0.9990\n0.9683\n0.9868\n0.9863\n0.9616\n0.9624\n\n\n1\n0.9933\n0.9988\n0.9972\n0.9936\n0.9934\n0.9815\n0.9817\n\n\n2\n0.9933\n0.9999\n0.9841\n0.9933\n0.9932\n0.9810\n0.9812\n\n\n3\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n4\n0.9933\n0.9992\n0.9833\n0.9938\n0.9933\n0.9813\n0.9814\n\n\n5\n0.9866\n0.9927\n0.9667\n0.9868\n0.9863\n0.9617\n0.9625\n\n\n6\n0.9866\n0.9897\n0.9667\n0.9868\n0.9863\n0.9617\n0.9625\n\n\n7\n0.9866\n0.9995\n0.9556\n0.9866\n0.9864\n0.9621\n0.9623\n\n\n8\n0.9932\n1.0000\n0.9972\n0.9936\n0.9933\n0.9810\n0.9812\n\n\n9\n0.9865\n0.9997\n0.9805\n0.9865\n0.9865\n0.9615\n0.9615\n\n\nMean\n0.9906\n0.9979\n0.9799\n0.9908\n0.9905\n0.9733\n0.9737\n\n\nSD\n0.0045\n0.0034\n0.0145\n0.0045\n0.0046\n0.0128\n0.0126\n\n\n\n\n\n\nWe can clearly conclude that our tuned model has performed better than our original model with default hyperparameters. The mean accuracy increased from 0.9899 to 0.9906\n\n\npycaret library really makes the process of tuning hyperparameters easy\n\n\nWe just need to pass the model in the following command\n\n\ntune_model(model_name)"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#plotting-classification-plots",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#plotting-classification-plots",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Plotting classification plots",
    "text": "Plotting classification plots"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#classification-report",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#classification-report",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Classification Report",
    "text": "Classification Report\n\nplot_model(tuned_xgboost_classifier,plot='class_report')"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#plotting-the-confusion-matrix",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#plotting-the-confusion-matrix",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Plotting the confusion matrix",
    "text": "Plotting the confusion matrix\n\nplot_model(tuned_xgboost_classifier,plot='confusion_matrix')"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#saving-the-model-for-future-predictions",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#saving-the-model-for-future-predictions",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Saving the model for future predictions",
    "text": "Saving the model for future predictions\n\n## This can be used to save our trained model for future use.\nsave_model(tuned_xgboost_classifier,\"XGBOOST CLASSIFIER\")\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n(Pipeline(memory=None,\n          steps=[('dtypes',\n                  DataTypes_Auto_infer(categorical_features=[],\n                                       display_types=True, features_todrop=[],\n                                       id_columns=[],\n                                       ml_usecase='classification',\n                                       numerical_features=[], target='NSP',\n                                       time_features=[])),\n                 ('imputer',\n                  Simple_Imputer(categorical_strategy='not_available',\n                                 fill_value_categorical=None,\n                                 fill_value_numerical=None,\n                                 numeric_strategy...\n                  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                         class_weight='balanced_subsample',\n                                         criterion='entropy', max_depth=11,\n                                         max_features='sqrt',\n                                         max_leaf_nodes=None, max_samples=None,\n                                         min_impurity_decrease=0.002,\n                                         min_impurity_split=None,\n                                         min_samples_leaf=4, min_samples_split=9,\n                                         min_weight_fraction_leaf=0.0,\n                                         n_estimators=130, n_jobs=-1,\n                                         oob_score=False, random_state=7481,\n                                         verbose=0, warm_start=False)]],\n          verbose=False),\n 'XGBOOST CLASSIFIER.pkl')"
  },
  {
    "objectID": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#loading-the-saved-model",
    "href": "posts/2021-05-30-Classification using Health Data with PyCaret 1.html#loading-the-saved-model",
    "title": "Classification example 1 using Health Data with PyCaret",
    "section": "Loading the saved model",
    "text": "Loading the saved model\n\n## This can be used to load our model. We don't need to train our model again and again.\nsaved_model=load_model('XGBOOST CLASSIFIER')\n\nTransformation Pipeline and Model Successfully Loaded"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html",
    "href": "posts/2021-04-02-poisson-regression.html",
    "title": "Poisson regression",
    "section": "",
    "text": "#code from https://github.com/thomasjpfan/ml-workshop-advanced\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nsns.set_theme(font_scale=1.5, rc={'figure.figsize': [12, 8]})\nsklearn.set_config(display='diagram')"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#load-london-bike-data",
    "href": "posts/2021-04-02-poisson-regression.html#load-london-bike-data",
    "title": "Poisson regression",
    "section": "Load London Bike Data",
    "text": "Load London Bike Data\n\nfrom pathlib import Path\n\ndata_path = Path(\"data\")\nbikes_path = data_path / \"london_bikes.csv\"\n\n\n“timestamp” - timestamp field for grouping the data\n“cnt” - the count of a new bike shares\n“t1” - real temperature in C\n“t2” - temperature in C “feels like”\n“hum” - humidity in percentage\n“windspeed” - wind speed in km/h\n“weathercode” - category of the weather\n“isholiday” - boolean field - 1 holiday / 0 non holiday\n“isweekend” - boolean field - 1 if the day is weekend\n“season” - category field meteorological seasons: 0-spring ; 1-summer; 2-fall; 3-winter.\n\n\nbikes = pd.read_csv(bikes_path, parse_dates=['timestamp'])\n\n\nbikes.head()\n\n\n\n\n\n\n\n\ntimestamp\ncnt\nt1\nt2\nhum\nwind_speed\nweather_code\nis_holiday\nis_weekend\nseason\n\n\n\n\n0\n2015-01-04 00:00:00\n182\n3.0\n2.0\n93.0\n6.0\n3.0\n0.0\n1.0\n3.0\n\n\n1\n2015-01-04 01:00:00\n138\n3.0\n2.5\n93.0\n5.0\n1.0\n0.0\n1.0\n3.0\n\n\n2\n2015-01-04 02:00:00\n134\n2.5\n2.5\n96.5\n0.0\n1.0\n0.0\n1.0\n3.0\n\n\n3\n2015-01-04 03:00:00\n72\n2.0\n2.0\n100.0\n0.0\n1.0\n0.0\n1.0\n3.0\n\n\n4\n2015-01-04 04:00:00\n47\n2.0\n0.0\n93.0\n6.5\n1.0\n0.0\n1.0\n3.0\n\n\n\n\n\n\n\n\nbikes['timestamp'].head()\n\n0   2015-01-04 00:00:00\n1   2015-01-04 01:00:00\n2   2015-01-04 02:00:00\n3   2015-01-04 03:00:00\n4   2015-01-04 04:00:00\nName: timestamp, dtype: datetime64[ns]\n\n\n\nbikes['hr'] = bikes['timestamp'].dt.hour\n\n\nbikes['weather_code'].unique()\n\narray([ 3.,  1.,  4.,  7.,  2., 26., 10.])\n\n\n\nbikes['season'].unique()\n\narray([3., 0., 1., 2.])\n\n\n\nX = bikes[['t1', 't2', 'hum', 'wind_speed', 'weather_code', 'is_holiday', 'is_weekend', 'season', 'hr']]\ny = bikes['cnt']\n\n\nSplit data\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\n\n\nColumn Transformer\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnumerical_featurse = ['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend', 'hr']\ncat_features = ['weather_code', 'season']\n\nct = ColumnTransformer([\n    ('numerical', 'passthrough', numerical_featurse),\n    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n    \n])\n\n\n\nPipeline\n\nfrom sklearn.linear_model import PoissonRegressor\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\n\npois_reg = Pipeline([\n    ('prep', ct),\n    ('scaler', StandardScaler()),\n    ('reg', PoissonRegressor())\n])\n\npois_reg\n\nPipelinePipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('numerical', 'passthrough',\n                                                  ['t1', 't2', 'hum',\n                                                   'wind_speed', 'is_holiday',\n                                                   'is_weekend', 'hr']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['weather_code',\n                                                   'season'])])),\n                ('scaler', StandardScaler()), ('reg', PoissonRegressor())])prep: ColumnTransformerColumnTransformer(transformers=[('numerical', 'passthrough',\n                                 ['t1', 't2', 'hum', 'wind_speed', 'is_holiday',\n                                  'is_weekend', 'hr']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['weather_code', 'season'])])numerical['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend', 'hr']passthroughpassthroughcategorical['weather_code', 'season']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)StandardScalerStandardScaler()PoissonRegressorPoissonRegressor()\n\n\n\npois_reg.fit(X_train, y_train);\n\n\npois_reg.score(X_test, y_test)\n\n0.331281167666172\n\n\n\n\nThis seems low what can we change?\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\n\nnumerical_featurse = ['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend']\ncat_features = ['weather_code', 'season', 'hr']\n\nct = ColumnTransformer([\n    ('numerical', 'passthrough', numerical_featurse),\n    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n    \n])\n\npois_reg = Pipeline([\n    ('prep', ct),\n    ('scalar', StandardScaler()),\n    ('reg', PoissonRegressor())\n])\n\npois_reg.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('numerical', 'passthrough',\n                                                  ['t1', 't2', 'hum',\n                                                   'wind_speed', 'is_holiday',\n                                                   'is_weekend']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['weather_code', 'season',\n                                                   'hr'])])),\n                ('scalar', StandardScaler()), ('reg', PoissonRegressor())])prep: ColumnTransformerColumnTransformer(transformers=[('numerical', 'passthrough',\n                                 ['t1', 't2', 'hum', 'wind_speed', 'is_holiday',\n                                  'is_weekend']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['weather_code', 'season', 'hr'])])numerical['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend']passthroughpassthroughcategorical['weather_code', 'season', 'hr']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)StandardScalerStandardScaler()PoissonRegressorPoissonRegressor()\n\n\n\npois_reg.score(X_test, y_test)\n\n0.8156604730601431\n\n\n\n\nWhat about ridge?\n\nfrom sklearn.linear_model import Ridge\n\nct = ColumnTransformer([\n    ('numerical', 'passthrough', numerical_featurse),\n    ('categorical', OneHotEncoder(sparse=False, handle_unknown='ignore'), cat_features)\n    \n])\n\nridge = Pipeline([\n    ('prep', ct),\n    ('scalar', StandardScaler()),\n    ('reg', Ridge(random_state=42))\n])\n\nridge.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('prep',\n                 ColumnTransformer(transformers=[('numerical', 'passthrough',\n                                                  ['t1', 't2', 'hum',\n                                                   'wind_speed', 'is_holiday',\n                                                   'is_weekend']),\n                                                 ('categorical',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False),\n                                                  ['weather_code', 'season',\n                                                   'hr'])])),\n                ('scalar', StandardScaler()), ('reg', Ridge(random_state=42))])prep: ColumnTransformerColumnTransformer(transformers=[('numerical', 'passthrough',\n                                 ['t1', 't2', 'hum', 'wind_speed', 'is_holiday',\n                                  'is_weekend']),\n                                ('categorical',\n                                 OneHotEncoder(handle_unknown='ignore',\n                                               sparse=False),\n                                 ['weather_code', 'season', 'hr'])])numerical['t1', 't2', 'hum', 'wind_speed', 'is_holiday', 'is_weekend']passthroughpassthroughcategorical['weather_code', 'season', 'hr']OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)StandardScalerStandardScaler()RidgeRidge(random_state=42)\n\n\n\nridge.score(X_test, y_test)\n\n0.7264287681524049\n\n\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import mean_poisson_deviance\n\ndef compute_metrics(y_true, y_pred, sample_weight=None):\n    \n    mask = y_pred &gt; 0\n    if (~mask).any():\n        n_masked, n_samples = (~mask).sum(), mask.shape[0]\n        print(f\"WARNING: Estimator yields invalid, non-positive predictions \"\n              f\" for {n_masked} samples out of {n_samples}. These predictions \"\n              f\"are ignored when computing the Poisson deviance.\")\n        \n        y_true = y_true[mask]\n        y_pred = y_pred[mask]\n        if sample_weight is not None:\n            sample_weight = sample_weight[mask]\n   \n    return {\n        'mse': mean_squared_error(y_true, y_pred, sample_weight=sample_weight),\n        'mean poisson deviance': mean_poisson_deviance(y_true, y_pred, sample_weight=sample_weight)\n    }\n\n\n\nCompute metrics\n\nridge_pred = ridge.predict(X_test)\ncompute_metrics(y_test, ridge_pred)\n\nWARNING: Estimator yields invalid, non-positive predictions  for 403 samples out of 4354. These predictions are ignored when computing the Poisson deviance.\n\n\n{'mse': 347993.3878355729, 'mean poisson deviance': 254.31140084177204}\n\n\n\npoisson_pred = pois_reg.predict(X_test)\ncompute_metrics(y_test, poisson_pred)\n\n{'mse': 273873.6171595357, 'mean poisson deviance': 181.15334073868877}"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#plotting-the-prediction-distrubutions",
    "href": "posts/2021-04-02-poisson-regression.html#plotting-the-prediction-distrubutions",
    "title": "Poisson regression",
    "section": "Plotting the prediction distrubutions",
    "text": "Plotting the prediction distrubutions\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\nax1.hist(y_test, bins=30, alpha=0.5)\nax1.set_title(\"Test data\")\nax2.hist(poisson_pred, bins=30, alpha=0.5)\nax2.set_title(\"Poisson predictions\")\nax3.hist(ridge_pred, bins=30, alpha=0.5)\nax3.set_title(\"Ridge predictions\")\n\nText(0.5, 1.0, 'Ridge predictions')\n\n\n\n\n\n\n# %load solutions/03-ex01-solutions.py\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nhist = HistGradientBoostingRegressor(random_state=42)\nhist.fit(X_train, y_train)\nhist_pred = hist.predict(X_test)\n\ncompute_metrics(y_test, hist_pred)\n\nhist_poisson = HistGradientBoostingRegressor(loss='poisson', random_state=42)\nhist_poisson.fit(X_train, y_train)\n\nhist_poisson_pred = hist_poisson.predict(X_test)\n\ncompute_metrics(y_test, hist_poisson_pred)\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 6), sharey=True)\nax1.hist(y_test, bins=30, alpha=0.5)\nax1.set_title(\"Test data\")\nax2.hist(hist_pred, bins=30, alpha=0.5)\nax2.set_title(\"Default Hist\")\nax3.hist(hist_poisson_pred, bins=30, alpha=0.5)\nax3.set_title(\"Poisson Hist\");\n\nWARNING: Estimator yields invalid, non-positive predictions  for 22 samples out of 4354. These predictions are ignored when computing the Poisson deviance."
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#calibration",
    "href": "posts/2021-04-02-poisson-regression.html#calibration",
    "title": "Poisson regression",
    "section": "Calibration",
    "text": "Calibration\n\nfrom sklearn.utils import gen_even_slices\n\ndef _calibration_curve_weighted(y_true, y_pred, n_bins=10, sample_weight=None):\n    y_true = np.asarray(y_true)\n    y_pred = np.asarray(y_pred)\n    \n    idx_sort = np.argsort(y_pred)\n    y_pred_bin = np.zeros(n_bins)\n    y_true_bin = np.zeros(n_bins)\n    \n    if sample_weight is not None:\n        sample_weight = np.asarray(sample_weight)\n    \n    for i, sl in enumerate(gen_even_slices(len(y_true), n_bins)):\n        if sample_weight is None:\n            y_pred_bin[i] = np.average(y_pred[idx_sort][sl])\n            y_true_bin[i] = np.average(y_true[idx_sort][sl])\n        else:\n            weights = sample_weight[idx_sort][sl]\n            y_pred_bin[i] = np.average(y_pred[idx_sort][sl], weights=weights)\n            y_true_bin[i] = np.average(y_true[idx_sort][sl], weights=weights)\n    return y_pred_bin, y_true_bin\n\ndef plot_calibration_curve_weights(y_true, y_pred, n_bins=10, ax=None, title=\"\", sample_weight=None):\n    if ax is None:\n        fig, ax = plt.subplots()\n    y_pred_bin, y_true_bin = _calibration_curve_weighted(y_test, y_pred, sample_weight=sample_weight)\n    \n    bin_centers = np.arange(1, len(y_pred_bin) + 1)\n    ax.plot(bin_centers, y_pred_bin, marker='x', linestyle=\"--\", label=\"predictions\")\n    ax.plot(bin_centers, y_true_bin, marker='o', linestyle=\"--\", label=\"observations\")\n    ax.set(xlabel=\"Bin number\", xticks=bin_centers, title=title)\n    ax.legend()\n    return ax\n\n\nfig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 8))\nplot_calibration_curve_weights(y_test, ridge_pred, ax=ax1, title=\"Ridge\")\nplot_calibration_curve_weights(y_test, poisson_pred, ax=ax2, title=\"Poisson Regression\")\nplot_calibration_curve_weights(y_test, hist_poisson_pred, ax=ax3, title=\"Hist Poisson\");"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#claims-dataset",
    "href": "posts/2021-04-02-poisson-regression.html#claims-dataset",
    "title": "Poisson regression",
    "section": "Claims dataset",
    "text": "Claims dataset\n\nclaims_path = data_path / \"claims.csv\"\nclaims = pd.read_csv(claims_path)\n\n\nClaimNb: number of claims on the given policy;\nExposure: total exposure in yearly units;\nArea: area code (categorical, ordinal);\nVehPower: power of the car (categorical, ordinal);\nVehAge: age of the car in years;\nDrivAge: age of the (most common) driver in years;\nBonusMalus: bonus-malus level between 50 and 230 (with reference level 100);\nVehBrand: car brand (categorical, nominal);\nVehGas: diesel or regular fuel car (binary);\nDensity: density of inhabitants per km2 in the city of the living place of the driver;\nRegion: regions in France (prior to 2016)\n\n\nclaims.head()\n\n\n\n\n\n\n\n\nClaimNb\nExposure\nArea\nVehPower\nVehAge\nDrivAge\nBonusMalus\nVehBrand\nVehGas\nDensity\nRegion\n\n\n\n\n0\n0.0\n1.00\nD\n4.0\n11.0\n42.0\n64.0\nB2\nRegular\n856.0\nR24\n\n\n1\n0.0\n0.18\nE\n10.0\n12.0\n35.0\n100.0\nB1\nRegular\n4762.0\nR93\n\n\n2\n0.0\n0.08\nE\n6.0\n4.0\n53.0\n50.0\nB1\nDiesel\n3317.0\nR93\n\n\n3\n0.0\n0.36\nA\n5.0\n2.0\n44.0\n50.0\nB2\nDiesel\n35.0\nR52\n\n\n4\n0.0\n0.60\nC\n4.0\n0.0\n32.0\n85.0\nB12\nDiesel\n200.0\nR73\n\n\n\n\n\n\n\n\nexposure = claims['Exposure']\n\n\ny = claims[\"ClaimNb\"] / exposure\n\n\nX = claims.drop([\"Exposure\", \"ClaimNb\"], axis=\"columns\")\n\n\nX.head()\n\n\n\n\n\n\n\n\nArea\nVehPower\nVehAge\nDrivAge\nBonusMalus\nVehBrand\nVehGas\nDensity\nRegion\n\n\n\n\n0\nD\n4.0\n11.0\n42.0\n64.0\nB2\nRegular\n856.0\nR24\n\n\n1\nE\n10.0\n12.0\n35.0\n100.0\nB1\nRegular\n4762.0\nR93\n\n\n2\nE\n6.0\n4.0\n53.0\n50.0\nB1\nDiesel\n3317.0\nR93\n\n\n3\nA\n5.0\n2.0\n44.0\n50.0\nB2\nDiesel\n35.0\nR52\n\n\n4\nC\n4.0\n0.0\n32.0\n85.0\nB12\nDiesel\n200.0\nR73\n\n\n\n\n\n\n\n\nSplit data\n\nX_train, X_test, y_train, y_test, exposure_train, exposure_test = train_test_split(\n    X, y, exposure, random_state=42)\n\n\n\nTrain simple dummy regresor\n\nfrom sklearn.dummy import DummyRegressor\n\ndummy = DummyRegressor()\ndummy.fit(X_train, y_train, sample_weight=exposure_train)\n\nDummyRegressorDummyRegressor()\n\n\n\ndummy_pred = dummy.predict(X_test)\ncompute_metrics(y_test, dummy_pred, sample_weight=exposure_test)\n\n{'mse': 0.5307356741415867, 'mean poisson deviance': 0.6308647534621802}\n\n\n\ndummy_pred[:10]\n\narray([0.10038206, 0.10038206, 0.10038206, 0.10038206, 0.10038206,\n       0.10038206, 0.10038206, 0.10038206, 0.10038206, 0.10038206])"
  },
  {
    "objectID": "posts/2021-04-02-poisson-regression.html#ridge",
    "href": "posts/2021-04-02-poisson-regression.html#ridge",
    "title": "Poisson regression",
    "section": "Ridge",
    "text": "Ridge\n\nX['Density'].hist(bins=20);\n\n\n\n\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import FunctionTransformer, OneHotEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.compose import ColumnTransformer\n\nlinear_model_preprocessor = ColumnTransformer(\n    [\n        (\"passthrough_numeric\", \"passthrough\",\n            [\"BonusMalus\"]),\n        (\"binned_numeric\", KBinsDiscretizer(n_bins=10),\n            [\"VehAge\", \"DrivAge\"]),\n        (\"log_scaled_numeric\", FunctionTransformer(np.log, validate=False),\n            [\"Density\"]),\n        (\"onehot_categorical\", OneHotEncoder(handle_unknown='ignore'),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n    ],\n)\n\n\nlinear_model_preprocessor.fit_transform(X_train)\n\n&lt;254254x75 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 2288282 stored elements in Compressed Sparse Row format&gt;\n\n\n\nfrom sklearn.linear_model import Ridge\nfrom sklearn.preprocessing import MaxAbsScaler\n\nridge = Pipeline([\n    (\"preprocessor\", linear_model_preprocessor),\n    (\"scaler\", MaxAbsScaler()),\n    (\"reg\", Ridge(alpha=1e-6))])\nridge.fit(X_train, y_train, reg__sample_weight=exposure_train)\n\nPipelinePipeline(steps=[('preprocessor',\n                 ColumnTransformer(transformers=[('passthrough_numeric',\n                                                  'passthrough',\n                                                  ['BonusMalus']),\n                                                 ('binned_numeric',\n                                                  KBinsDiscretizer(n_bins=10),\n                                                  ['VehAge', 'DrivAge']),\n                                                 ('log_scaled_numeric',\n                                                  FunctionTransformer(func=),\n                                                  ['Density']),\n                                                 ('onehot_categorical',\n                                                  OneHotEncoder(handle_unknown='ignore'),\n                                                  ['VehBrand', 'VehPower',\n                                                   'VehGas', 'Region',\n                                                   'Area'])])),\n                ('scaler', MaxAbsScaler()), ('reg', Ridge(alpha=1e-06))])preprocessor: ColumnTransformerColumnTransformer(transformers=[('passthrough_numeric', 'passthrough',\n                                 ['BonusMalus']),\n                                ('binned_numeric', KBinsDiscretizer(n_bins=10),\n                                 ['VehAge', 'DrivAge']),\n                                ('log_scaled_numeric',\n                                 FunctionTransformer(func=),\n                                 ['Density']),\n                                ('onehot_categorical',\n                                 OneHotEncoder(handle_unknown='ignore'),\n                                 ['VehBrand', 'VehPower', 'VehGas', 'Region',\n                                  'Area'])])passthrough_numeric['BonusMalus']passthroughpassthroughbinned_numeric['VehAge', 'DrivAge']KBinsDiscretizerKBinsDiscretizer(n_bins=10)log_scaled_numeric['Density']FunctionTransformerFunctionTransformer(func=)onehot_categorical['VehBrand', 'VehPower', 'VehGas', 'Region', 'Area']OneHotEncoderOneHotEncoder(handle_unknown='ignore')MaxAbsScalerMaxAbsScaler()RidgeRidge(alpha=1e-06)\n\n\n\nridge_pred = ridge.predict(X_test)\ncompute_metrics(y_test, ridge_pred, sample_weight=exposure_test)\n\nWARNING: Estimator yields invalid, non-positive predictions  for 866 samples out of 84752. These predictions are ignored when computing the Poisson deviance.\n\n\n{'mse': 0.5306782259705353, 'mean poisson deviance': 0.6051468688331222}\n\n\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_calibration_curve_weights(y_test, ridge_pred, ax=ax, title=\"Ridge\", sample_weight=exposure_test);\n\n\n\n\n\n# %load solutions/03-ex02-solutions.py\npoission_reg = Pipeline([\n    (\"preprocessor\", linear_model_preprocessor),\n    (\"scaler\", MaxAbsScaler()),\n    (\"reg\", PoissonRegressor(alpha=1e-12))])\n\npoission_reg.fit(X_train, y_train, reg__sample_weight=exposure_train)\n\npoisson_pred = poission_reg.predict(X_test)\ncompute_metrics(y_test, poisson_pred, sample_weight=exposure_test)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_calibration_curve_weights(y_test, poisson_pred, ax=ax, title=\"Poisson\", sample_weight=exposure_test)\n\n&lt;AxesSubplot:title={'center':'Poisson'}, xlabel='Bin number'&gt;\n\n\n\n\n\n\n# %load solutions/03-ex03-solutions.py\nfrom sklearn.preprocessing import OrdinalEncoder\n\ntree_preprocessor = ColumnTransformer(\n    [\n        (\"categorical\", OrdinalEncoder(),\n            [\"VehBrand\", \"VehPower\", \"VehGas\", \"Region\", \"Area\"]),\n        (\"numeric\", \"passthrough\",\n            [\"VehAge\", \"DrivAge\", \"BonusMalus\", \"Density\"]),\n    ]\n)\n\nhist_poisson = Pipeline([\n    (\"preprocessor\", tree_preprocessor),\n    (\"reg\", HistGradientBoostingRegressor(loss=\"poisson\", random_state=0))\n])\nhist_poisson.fit(X_train, y_train, reg__sample_weight=exposure_train)\n\nhist_poisson_pred = hist_poisson.predict(X_test)\ncompute_metrics(y_test, hist_poisson_pred, sample_weight=exposure_test)\n\nfig, ax = plt.subplots(figsize=(8, 8))\nplot_calibration_curve_weights(y_test, hist_poisson_pred, ax=ax, title=\"Hist Poisson\", sample_weight=exposure_test);"
  },
  {
    "objectID": "posts/2021-05-29-pycaret-reg.html",
    "href": "posts/2021-05-29-pycaret-reg.html",
    "title": "Regression using Health Data with PyCaret",
    "section": "",
    "text": "Code from https://github.com/pycaret/pycaret/\n\n# check version\nfrom pycaret.utils import version\nversion()\n\n'2.3.1'\n\n\n\n1. Loading Dataset\n\nfrom pycaret.datasets import get_data\ndata = get_data('insurance')\n\n\n\n\n\n\n\n\nage\nsex\nbmi\nchildren\nsmoker\nregion\ncharges\n\n\n\n\n0\n19\nfemale\n27.900\n0\nyes\nsouthwest\n16884.92400\n\n\n1\n18\nmale\n33.770\n1\nno\nsoutheast\n1725.55230\n\n\n2\n28\nmale\n33.000\n3\nno\nsoutheast\n4449.46200\n\n\n3\n33\nmale\n22.705\n0\nno\nnorthwest\n21984.47061\n\n\n4\n32\nmale\n28.880\n0\nno\nnorthwest\n3866.85520\n\n\n\n\n\n\n\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n30669\nMale\n3.0\n0\n0\nNo\nchildren\nRural\n95.12\n18.0\nNaN\n0\n\n\n1\n30468\nMale\n58.0\n1\n0\nYes\nPrivate\nUrban\n87.96\n39.2\nnever smoked\n0\n\n\n2\n16523\nFemale\n8.0\n0\n0\nNo\nPrivate\nUrban\n110.89\n17.6\nNaN\n0\n\n\n3\n56543\nFemale\n70.0\n0\n0\nYes\nPrivate\nRural\n69.04\n35.9\nformerly smoked\n0\n\n\n4\n46136\nMale\n14.0\n0\n0\nNo\nNever_worked\nRural\n161.28\n19.1\nNaN\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n43395\n56196\nFemale\n10.0\n0\n0\nNo\nchildren\nUrban\n58.64\n20.4\nnever smoked\n0\n\n\n43396\n5450\nFemale\n56.0\n0\n0\nYes\nGovt_job\nUrban\n213.61\n55.4\nformerly smoked\n0\n\n\n43397\n28375\nFemale\n82.0\n1\n0\nYes\nPrivate\nUrban\n91.94\n28.9\nformerly smoked\n0\n\n\n43398\n27973\nMale\n40.0\n0\n0\nYes\nPrivate\nUrban\n99.16\n33.2\nnever smoked\n0\n\n\n43399\n36271\nFemale\n82.0\n0\n0\nYes\nPrivate\nUrban\n79.48\n20.6\nnever smoked\n0\n\n\n\n\n43400 rows × 12 columns\n\n\n\n\n\n2. Initialize Setup\n\nfrom pycaret.regression import *\nreg1 = setup(df, target = 'avg_glucose_level', session_id=153, log_experiment=True, experiment_name='health1')\n\n\n\n\n\n\nDescription\nValue\n\n\n\n\n0\nsession_id\n153\n\n\n1\nTarget\navg_glucose_level\n\n\n2\nOriginal Data\n(43400, 12)\n\n\n3\nMissing Values\nTrue\n\n\n4\nNumeric Features\n3\n\n\n5\nCategorical Features\n8\n\n\n6\nOrdinal Features\nFalse\n\n\n7\nHigh Cardinality Features\nFalse\n\n\n8\nHigh Cardinality Method\nNone\n\n\n9\nTransformed Train Set\n(30379, 19)\n\n\n10\nTransformed Test Set\n(13021, 19)\n\n\n11\nShuffle Train-Test\nTrue\n\n\n12\nStratify Train-Test\nFalse\n\n\n13\nFold Generator\nKFold\n\n\n14\nFold Number\n10\n\n\n15\nCPU Jobs\n-1\n\n\n16\nUse GPU\nFalse\n\n\n17\nLog Experiment\nTrue\n\n\n18\nExperiment Name\nhealth1\n\n\n19\nUSI\n2590\n\n\n20\nImputation Type\nsimple\n\n\n21\nIterative Imputation Iteration\nNone\n\n\n22\nNumeric Imputer\nmean\n\n\n23\nIterative Imputation Numeric Model\nNone\n\n\n24\nCategorical Imputer\nconstant\n\n\n25\nIterative Imputation Categorical Model\nNone\n\n\n26\nUnknown Categoricals Handling\nleast_frequent\n\n\n27\nNormalize\nFalse\n\n\n28\nNormalize Method\nNone\n\n\n29\nTransformation\nFalse\n\n\n30\nTransformation Method\nNone\n\n\n31\nPCA\nFalse\n\n\n32\nPCA Method\nNone\n\n\n33\nPCA Components\nNone\n\n\n34\nIgnore Low Variance\nFalse\n\n\n35\nCombine Rare Levels\nFalse\n\n\n36\nRare Level Threshold\nNone\n\n\n37\nNumeric Binning\nFalse\n\n\n38\nRemove Outliers\nFalse\n\n\n39\nOutliers Threshold\nNone\n\n\n40\nRemove Multicollinearity\nFalse\n\n\n41\nMulticollinearity Threshold\nNone\n\n\n42\nClustering\nFalse\n\n\n43\nClustering Iteration\nNone\n\n\n44\nPolynomial Features\nFalse\n\n\n45\nPolynomial Degree\nNone\n\n\n46\nTrignometry Features\nFalse\n\n\n47\nPolynomial Threshold\nNone\n\n\n48\nGroup Features\nFalse\n\n\n49\nFeature Selection\nFalse\n\n\n50\nFeature Selection Method\nclassic\n\n\n51\nFeatures Selection Threshold\nNone\n\n\n52\nFeature Interaction\nFalse\n\n\n53\nFeature Ratio\nFalse\n\n\n54\nInteraction Threshold\nNone\n\n\n55\nTransform Target\nFalse\n\n\n56\nTransform Target Method\nbox-cox\n\n\n\n\n\n\n\n3. Compare Baseline\n\nbest_model = compare_models(fold=5)\n\n\n\n\n\n\nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\ngbr\nGradient Boosting Regressor\n29.9120\n1671.5862\n40.8836\n0.1111\n0.3360\n0.2897\n0.7480\n\n\nlightgbm\nLight Gradient Boosting Machine\n29.8101\n1680.0991\n40.9877\n0.1065\n0.3369\n0.2889\n0.1120\n\n\nridge\nRidge Regression\n30.3617\n1705.3918\n41.2947\n0.0931\n0.3394\n0.2936\n0.0400\n\n\nlar\nLeast Angle Regression\n30.3618\n1705.3961\n41.2948\n0.0931\n0.3394\n0.2936\n0.0300\n\n\nbr\nBayesian Ridge\n30.3665\n1705.4186\n41.2951\n0.0931\n0.3394\n0.2936\n0.0400\n\n\nlr\nLinear Regression\n30.3618\n1705.3961\n41.2948\n0.0931\n0.3394\n0.2936\n0.4600\n\n\nen\nElastic Net\n30.7817\n1744.7248\n41.7681\n0.0722\n0.3428\n0.2971\n0.0340\n\n\nlasso\nLasso Regression\n30.8121\n1747.2784\n41.7986\n0.0709\n0.3431\n0.2974\n0.2980\n\n\nomp\nOrthogonal Matching Pursuit\n30.8859\n1772.4020\n42.0980\n0.0575\n0.3440\n0.2968\n0.0300\n\n\nrf\nRandom Forest Regressor\n31.0628\n1783.7249\n42.2324\n0.0515\n0.3507\n0.3061\n2.4020\n\n\nllar\nLasso Least Angle Regression\n31.0430\n1880.7945\n43.3662\n-0.0001\n0.3516\n0.2960\n0.0300\n\n\net\nExtra Trees Regressor\n32.0922\n1941.3104\n44.0589\n-0.0324\n0.3645\n0.3153\n1.6780\n\n\nada\nAdaBoost Regressor\n36.2249\n1942.8968\n44.0775\n-0.0335\n0.3835\n0.3864\n0.2040\n\n\nhuber\nHuber Regressor\n30.5788\n1947.3049\n44.1187\n-0.0359\n0.3591\n0.2734\n0.2000\n\n\nknn\nK Neighbors Regressor\n32.9997\n2098.5319\n45.8082\n-0.1160\n0.3778\n0.3161\n0.0760\n\n\ndt\nDecision Tree Regressor\n40.5230\n3465.8436\n58.8670\n-0.8435\n0.4733\n0.3972\n0.0820\n\n\npar\nPassive Aggressive Regressor\n61.6977\n7002.6674\n77.1516\n-2.6714\n0.7598\n0.6310\n0.0500\n\n\n\n\n\n\n\n4. Create Model\n\ngbr = create_model('gbr')\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n29.4483\n1605.5183\n40.0689\n0.1065\n0.3327\n0.2893\n\n\n1\n30.1403\n1696.7591\n41.1917\n0.1159\n0.3351\n0.2869\n\n\n2\n29.9468\n1650.7967\n40.6300\n0.1151\n0.3359\n0.2924\n\n\n3\n30.9916\n1797.2590\n42.3941\n0.1164\n0.3429\n0.2943\n\n\n4\n29.7740\n1654.5201\n40.6758\n0.1206\n0.3339\n0.2886\n\n\n5\n29.4301\n1644.3258\n40.5503\n0.1098\n0.3356\n0.2880\n\n\n6\n29.8422\n1649.4613\n40.6136\n0.1063\n0.3369\n0.2940\n\n\n7\n30.1508\n1683.2950\n41.0280\n0.1142\n0.3346\n0.2861\n\n\n8\n29.8594\n1681.1659\n41.0020\n0.1008\n0.3383\n0.2920\n\n\n9\n29.4870\n1635.5078\n40.4414\n0.1104\n0.3323\n0.2853\n\n\nMean\n29.9070\n1669.8609\n40.8596\n0.1116\n0.3358\n0.2897\n\n\nSD\n0.4393\n49.2434\n0.5964\n0.0056\n0.0029\n0.0031\n\n\n\n\n\n\nimport numpy as np\ngbrs = [create_model('gbr', learning_rate=i) for i in np.arange(0.1,1,0.1)]\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n30.0877\n1704.1490\n41.2813\n0.0517\n0.3429\n0.2952\n\n\n1\n31.1447\n1869.6863\n43.2399\n0.0258\n0.3573\n0.2956\n\n\n2\n30.5247\n1759.6697\n41.9484\n0.0568\n0.3475\n0.2990\n\n\n3\n31.8691\n1938.1533\n44.0245\n0.0471\n0.3581\n0.3021\n\n\n4\n30.6442\n1808.0313\n42.5210\n0.0390\n0.3494\n0.2982\n\n\n5\n30.0122\n1742.8764\n41.7478\n0.0564\n0.3456\n0.2943\n\n\n6\n30.5092\n1785.0937\n42.2504\n0.0328\n0.3503\n0.3013\n\n\n7\n30.6449\n1770.6888\n42.0796\n0.0682\n0.3457\n0.2922\n\n\n8\n30.6724\n1813.1471\n42.5811\n0.0302\n0.3533\n0.2999\n\n\n9\n30.3654\n1773.0299\n42.1074\n0.0356\n0.3480\n0.2961\n\n\nMean\n30.6475\n1796.4526\n42.3781\n0.0444\n0.3498\n0.2974\n\n\nSD\n0.5070\n63.1980\n0.7402\n0.0131\n0.0048\n0.0030\n\n\n\n\n\n\nprint(len(gbrs))\n\n9\n\n\n\n\n5. Tune Hyperparameters\n\ntuned_gbr = tune_model(gbr, n_iter=50, optimize = 'RMSE')\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n29.4002\n1615.6882\n40.1956\n0.1009\n0.3328\n0.2882\n\n\n1\n30.0942\n1699.4660\n41.2246\n0.1145\n0.3346\n0.2859\n\n\n2\n29.9120\n1656.1440\n40.6957\n0.1123\n0.3358\n0.2914\n\n\n3\n30.9406\n1808.0163\n42.5208\n0.1111\n0.3430\n0.2928\n\n\n4\n29.8471\n1665.1412\n40.8061\n0.1149\n0.3346\n0.2890\n\n\n5\n29.4329\n1643.2520\n40.5370\n0.1104\n0.3351\n0.2880\n\n\n6\n29.6530\n1636.3363\n40.4517\n0.1134\n0.3355\n0.2918\n\n\n7\n30.1129\n1686.1293\n41.0625\n0.1127\n0.3343\n0.2850\n\n\n8\n29.8222\n1683.4829\n41.0303\n0.0996\n0.3380\n0.2913\n\n\n9\n29.3902\n1638.9886\n40.4844\n0.1085\n0.3319\n0.2840\n\n\nMean\n29.8605\n1673.2645\n40.9009\n0.1098\n0.3356\n0.2887\n\n\nSD\n0.4414\n51.2132\n0.6187\n0.0051\n0.0029\n0.0029\n\n\n\n\n\n\ntuned_gbr\n\nGradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.01, loss='ls', max_depth=8,\n                          max_features='sqrt', max_leaf_nodes=None,\n                          min_impurity_decrease=0.2, min_impurity_split=None,\n                          min_samples_leaf=5, min_samples_split=10,\n                          min_weight_fraction_leaf=0.0, n_estimators=280,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=153, subsample=0.65, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n\n\n\n6. Ensemble Model\n\ndt = create_model('dt')\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n38.8967\n3256.2671\n57.0637\n-0.8121\n0.4602\n0.3865\n\n\n1\n40.8905\n3506.9767\n59.2197\n-0.8272\n0.4737\n0.3927\n\n\n2\n39.8432\n3339.3332\n57.7870\n-0.7899\n0.4682\n0.3920\n\n\n3\n41.0738\n3547.0195\n59.5569\n-0.7439\n0.4741\n0.3947\n\n\n4\n41.0748\n3545.6724\n59.5455\n-0.8846\n0.4780\n0.4037\n\n\n5\n40.8333\n3518.6152\n59.3179\n-0.9049\n0.4776\n0.4027\n\n\n6\n40.5197\n3506.7571\n59.2179\n-0.9000\n0.4754\n0.4060\n\n\n7\n40.0449\n3418.0876\n58.4644\n-0.7987\n0.4696\n0.3896\n\n\n8\n40.7124\n3463.5019\n58.8515\n-0.8525\n0.4754\n0.4031\n\n\n9\n39.4837\n3352.3464\n57.8995\n-0.8234\n0.4671\n0.3819\n\n\nMean\n40.3373\n3445.4577\n58.6924\n-0.8337\n0.4719\n0.3953\n\n\nSD\n0.7033\n94.8957\n0.8124\n0.0492\n0.0053\n0.0078\n\n\n\n\n\n\nbagged_dt = ensemble_model(dt, n_estimators=50)\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n30.4101\n1705.5270\n41.2980\n0.0509\n0.3458\n0.3031\n\n\n1\n31.1895\n1815.9909\n42.6144\n0.0538\n0.3495\n0.2995\n\n\n2\n31.1459\n1782.9179\n42.2246\n0.0443\n0.3522\n0.3090\n\n\n3\n32.1774\n1926.5104\n43.8920\n0.0528\n0.3581\n0.3104\n\n\n4\n31.4072\n1807.7021\n42.5171\n0.0392\n0.3534\n0.3106\n\n\n5\n30.8286\n1787.9650\n42.2843\n0.0320\n0.3533\n0.3073\n\n\n6\n31.0847\n1802.8904\n42.4605\n0.0232\n0.3557\n0.3122\n\n\n7\n31.2760\n1797.0518\n42.3916\n0.0544\n0.3507\n0.3029\n\n\n8\n31.1355\n1795.7085\n42.3758\n0.0396\n0.3535\n0.3108\n\n\n9\n30.7127\n1760.7991\n41.9619\n0.0423\n0.3486\n0.3020\n\n\nMean\n31.1368\n1798.3063\n42.4020\n0.0432\n0.3521\n0.3068\n\n\nSD\n0.4454\n52.1778\n0.6116\n0.0097\n0.0034\n0.0043\n\n\n\n\n\n\nboosted_dt = ensemble_model(dt, method = 'Boosting')\n\n\n\n\n\n\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\n32.0837\n2316.4857\n48.1299\n-0.2891\n0.3905\n0.3044\n\n\n1\n32.8474\n2379.9324\n48.7846\n-0.2400\n0.3920\n0.3000\n\n\n2\n32.5372\n2325.1765\n48.2201\n-0.2463\n0.3920\n0.3091\n\n\n3\n33.3153\n2448.9699\n49.4871\n-0.2040\n0.3948\n0.3026\n\n\n4\n32.6835\n2316.9897\n48.1351\n-0.2315\n0.3894\n0.3049\n\n\n5\n32.3224\n2299.7394\n47.9556\n-0.2450\n0.3899\n0.3068\n\n\n6\n32.6263\n2307.7303\n48.0388\n-0.2503\n0.3927\n0.3125\n\n\n7\n32.4531\n2287.3343\n47.8261\n-0.2036\n0.3883\n0.3019\n\n\n8\n32.1537\n2270.1737\n47.6463\n-0.2142\n0.3867\n0.3068\n\n\n9\n31.9558\n2247.6878\n47.4098\n-0.2225\n0.3851\n0.2979\n\n\nMean\n32.4978\n2320.0220\n48.1633\n-0.2347\n0.3901\n0.3047\n\n\nSD\n0.3820\n54.4179\n0.5613\n0.0244\n0.0028\n0.0041\n\n\n\n\n\n\n\n9. Analyze Model\n\nplot_model(dt)\n\n\n\n\n\nplot_model(dt, plot = 'error')\n\n\n\n\n\nplot_model(dt, plot = 'feature')\n\n\n\n\n\nevaluate_model(dt)\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\nccp_alpha\n0.0\n\n\ncriterion\nmse\n\n\nmax_depth\nNone\n\n\nmax_features\nNone\n\n\nmax_leaf_nodes\nNone\n\n\nmin_impurity_decrease\n0.0\n\n\nmin_impurity_split\nNone\n\n\nmin_samples_leaf\n1\n\n\nmin_samples_split\n2\n\n\nmin_weight_fraction_leaf\n0.0\n\n\npresort\ndeprecated\n\n\nrandom_state\n153\n\n\nsplitter\nbest\n\n\n\n\n\n\n\n\n\n10. Interpret Model\n\ninterpret_model(dt)\n\n\n\n\n\ninterpret_model(dt, plot = 'correlation')\n\n\n\n\n\ninterpret_model(dt, plot = 'reason', observation = 12)\n\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written.\n\n \n\n\n\n\n11. AutoML()\n\nbest = automl(optimize = 'MAE')\nbest\n\nLGBMRegressor(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n              importance_type='split', learning_rate=0.1, max_depth=-1,\n              min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n              n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,\n              random_state=153, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n              subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n\n\n\n\n12. Predict Model\n\npred_holdouts = predict_model(dt)\npred_holdouts.head()\n\n\n\n\n\n\nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nDecision Tree Regressor\n39.4954\n3317.2600\n57.5957\n-0.8368\n0.4665\n0.3924\n\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nbmi\ngender_Male\ngender_Other\nhypertension_1\nheart_disease_0\never_married_Yes\nwork_type_Govt_job\nwork_type_Never_worked\n...\nwork_type_Self-employed\nwork_type_children\nResidence_type_Urban\nsmoking_status_formerly smoked\nsmoking_status_never smoked\nsmoking_status_not_available\nsmoking_status_smokes\nstroke_0\navg_glucose_level\nLabel\n\n\n\n\n0\n22489.0\n55.0\n41.099998\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n99.110001\n109.269997\n\n\n1\n26145.0\n17.0\n16.200001\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n83.470001\n73.949997\n\n\n2\n34373.0\n0.4\n20.200001\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n76.430000\n92.430000\n\n\n3\n60599.0\n69.0\n32.500000\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n80.370003\n86.099998\n\n\n4\n54268.0\n35.0\n27.200001\n1.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n1.0\n151.589996\n175.320007\n\n\n\n\n5 rows × 21 columns\n\n\n\n\nnew_data = df.copy()\nnew_data.drop(['avg_glucose_level'], axis=1, inplace=True)\npredict_new = predict_model(best, data=new_data)\npredict_new.head()\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\nbmi\nsmoking_status\nstroke\nLabel\n\n\n\n\n0\n30669\nMale\n3.0\n0\n0\nNo\nchildren\nRural\n18.0\nNaN\n0\n91.930029\n\n\n1\n30468\nMale\n58.0\n1\n0\nYes\nPrivate\nUrban\n39.2\nnever smoked\n0\n143.044226\n\n\n2\n16523\nFemale\n8.0\n0\n0\nNo\nPrivate\nUrban\n17.6\nNaN\n0\n91.227274\n\n\n3\n56543\nFemale\n70.0\n0\n0\nYes\nPrivate\nRural\n35.9\nformerly smoked\n0\n123.386541\n\n\n4\n46136\nMale\n14.0\n0\n0\nNo\nNever_worked\nRural\n19.1\nNaN\n0\n97.715823\n\n\n\n\n\n\n\n\n\n13. Save / Load Model\n\nsave_model(best, model_name='best-model')\n\nTransformation Pipeline and Model Succesfully Saved\n\n\n(Pipeline(memory=None,\n          steps=[('dtypes',\n                  DataTypes_Auto_infer(categorical_features=[],\n                                       display_types=True, features_todrop=[],\n                                       id_columns=[], ml_usecase='regression',\n                                       numerical_features=[],\n                                       target='avg_glucose_level',\n                                       time_features=[])),\n                 ('imputer',\n                  Simple_Imputer(categorical_strategy='not_available',\n                                 fill_value_categorical=None,\n                                 fill_value_numerical=None,\n                                 numeri...\n                  LGBMRegressor(boosting_type='gbdt', class_weight=None,\n                                colsample_bytree=1.0, importance_type='split',\n                                learning_rate=0.1, max_depth=-1,\n                                min_child_samples=20, min_child_weight=0.001,\n                                min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n                                num_leaves=31, objective=None, random_state=153,\n                                reg_alpha=0.0, reg_lambda=0.0, silent=True,\n                                subsample=1.0, subsample_for_bin=200000,\n                                subsample_freq=0)]],\n          verbose=False),\n 'best-model.pkl')\n\n\n\nloaded_bestmodel = load_model('best-model')\nprint(loaded_bestmodel)\n\nTransformation Pipeline and Model Successfully Loaded\nPipeline(memory=None,\n         steps=[('dtypes',\n                 DataTypes_Auto_infer(categorical_features=[],\n                                      display_types=True, features_todrop=[],\n                                      id_columns=[], ml_usecase='regression',\n                                      numerical_features=[],\n                                      target='avg_glucose_level',\n                                      time_features=[])),\n                ('imputer',\n                 Simple_Imputer(categorical_strategy='not_available',\n                                fill_value_categorical=None,\n                                fill_value_numerical=None,\n                                numeri...\n                 LGBMRegressor(boosting_type='gbdt', class_weight=None,\n                               colsample_bytree=1.0, importance_type='split',\n                               learning_rate=0.1, max_depth=-1,\n                               min_child_samples=20, min_child_weight=0.001,\n                               min_split_gain=0.0, n_estimators=100, n_jobs=-1,\n                               num_leaves=31, objective=None, random_state=153,\n                               reg_alpha=0.0, reg_lambda=0.0, silent=True,\n                               subsample=1.0, subsample_for_bin=200000,\n                               subsample_freq=0)]],\n         verbose=False)\n\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\nloaded_bestmodel[0]\n\nDataTypes_Auto_inferDataTypes_Auto_infer(categorical_features=[], display_types=True,\n                     features_todrop=[], id_columns=[], ml_usecase='regression',\n                     numerical_features=[], target='avg_glucose_level',\n                     time_features=[])\n\n\n\nfrom sklearn import set_config\nset_config(display='text')\n\n\n\n14. Deploy Model\n\n#deploy_model(best, model_name = 'best-aws', authentication = {'bucket' : 'pycaret-test'})\n\n\n\n15. Get Config / Set Config\n\nX_train = get_config('X_train')\nX_train.head()\n\n\n\n\n\n\n\n\nid\nage\nbmi\ngender_Male\ngender_Other\nhypertension_1\nheart_disease_0\never_married_Yes\nwork_type_Govt_job\nwork_type_Never_worked\nwork_type_Private\nwork_type_Self-employed\nwork_type_children\nResidence_type_Urban\nsmoking_status_formerly smoked\nsmoking_status_never smoked\nsmoking_status_not_available\nsmoking_status_smokes\nstroke_0\n\n\n\n\n41231\n51687.0\n55.0\n25.799999\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n21949\n11886.0\n38.0\n19.500000\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n12409\n18294.0\n4.0\n18.000000\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n\n\n9632\n35206.0\n37.0\n29.000000\n0.0\n0.0\n0.0\n1.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n15044\n69458.0\n41.0\n38.299999\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\n\nget_config('seed')\n\n153\n\n\n\nfrom pycaret.regression import set_config\nset_config('seed', 999)\n\n\nget_config('seed')\n\n999\n\n\n\n\n16. MLFlow UI\n\n!mlflow ui\n\n[2021-05-29 11:34:03 -0500] [17806] [INFO] Starting gunicorn 20.0.4\n[2021-05-29 11:34:03 -0500] [17806] [INFO] Listening at: http://127.0.0.1:5000 (17806)\n[2021-05-29 11:34:03 -0500] [17806] [INFO] Using worker: sync\n[2021-05-29 11:34:03 -0500] [17808] [INFO] Booting worker with pid: 17808"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html",
    "href": "posts/2021-06-04-imbalanced-data.html",
    "title": "Imbalanced data",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-advanced\nimport sklearn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nplt.rcParams['font.size'] = 16\nplt.rcParams['figure.figsize'] = [12, 8]\n\nsklearn.set_config(display='diagram')"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#load-mammography-data",
    "href": "posts/2021-06-04-imbalanced-data.html#load-mammography-data",
    "title": "Imbalanced data",
    "section": "Load Mammography Data",
    "text": "Load Mammography Data\n\n# %load solutions/classifier_example.py\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split\n\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\nX, y = df.drop(['stroke', 'id'], axis = 1), df['stroke']\n\n\nX = X.select_dtypes(include='number')\nX\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report\n\nrf = RandomForestClassifier(random_state=42)\nrf.fit(X_train, y_train)\n\nrf.score(X_test, y_test)\n\ny_pred = rf.predict(X_test)\n\nprint(classification_report(y_test, y_pred))\n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      7139\n           1       0.17      0.01      0.01       129\n\n    accuracy                           0.98      7268\n   macro avg       0.57      0.50      0.50      7268\nweighted avg       0.97      0.98      0.97      7268\n\n\n\n\nfrom sklearn.datasets import fetch_openml\n\n\nnp.bincount(y)\n\narray([28524,   548])"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#split-data-into-train-test-split",
    "href": "posts/2021-06-04-imbalanced-data.html#split-data-into-train-test-split",
    "title": "Imbalanced data",
    "section": "Split data into train test split",
    "text": "Split data into train test split\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, stratify=y, random_state=0)\n\n\nBase models\n\nLinear model\n\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\n\n\nbase_log_reg = LogisticRegression(random_state=42)\ncv_results = cross_validate(base_log_reg,\n                            X_train, y_train, scoring=['roc_auc', 'average_precision'])\n\n\ncv_results\n\n{'fit_time': array([0.18456721, 0.18922424, 0.21849942, 0.16009235, 0.14718604]),\n 'score_time': array([0.02156591, 0.01044083, 0.00906014, 0.00876856, 0.00877857]),\n 'test_roc_auc': array([0.85926447, 0.82782335, 0.83645313, 0.82829213, 0.82486117]),\n 'test_average_precision': array([0.08657556, 0.08525184, 0.10518971, 0.0796069 , 0.08259916])}\n\n\n\nlog_reg_base_auc = cv_results['test_roc_auc'].mean()\nlog_reg_base_auc\n\n0.8353388498771366\n\n\n\nlog_reg_base_ap = cv_results['test_average_precision'].mean()\nlog_reg_base_ap\n\n0.08784463474925555\n\n\n\ndef compute_metrics(estimator):\n    cv_results = cross_validate(estimator,\n                                X_train, y_train, scoring=['roc_auc', 'average_precision'])\n    return {\n        \"auc\": cv_results[\"test_roc_auc\"].mean(),\n        \"average_precision\": cv_results[\"test_average_precision\"].mean(),\n    }\n\n\nbase_log_reg_metrics = compute_metrics(base_log_reg)\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\n\nRandom Forest\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nbase_rf = RandomForestClassifier(random_state=42, n_jobs=-1)\n\n\nbase_rf_metrics = compute_metrics(base_rf)\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\n\n\nImbalance-learn sampler\n\nUnder sampler\n\nnp.bincount(y_train)\n\narray([21393,   411])\n\n\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nunder_sampler = RandomUnderSampler(random_state=42)\n\n\nX_train_subsample, y_train_subsample = under_sampler.fit_resample(X_train, y_train)\n\n\nX_train.shape\n\n(21804, 5)\n\n\n\nX_train_subsample.shape\n\n(822, 5)\n\n\n\nnp.bincount(y_train_subsample)\n\narray([411, 411])\n\n\n\n\nOversampling\n\nfrom imblearn.over_sampling import RandomOverSampler\n\n\nover_sampler = RandomOverSampler(random_state=42)\n\n\nX_train_subsample, y_train_subsample = over_sampler.fit_resample(X_train, y_train)\n\n\nX_train_subsample.shape\n\n(42786, 5)\n\n\n\nnp.bincount(y_train_subsample)\n\narray([21393, 21393])"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#pipelines-with-imblean",
    "href": "posts/2021-06-04-imbalanced-data.html#pipelines-with-imblean",
    "title": "Imbalanced data",
    "section": "Pipelines with imblean",
    "text": "Pipelines with imblean\n\nLinear model with under sampling\n\nfrom imblearn.pipeline import make_pipeline as make_imb_pipeline\n\n\nunder_log_reg = make_imb_pipeline(\n    RandomUnderSampler(random_state=42), LogisticRegression(random_state=42))\n\n\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\ncompute_metrics(under_log_reg)\n\n{'auc': 0.8347615373366913, 'average_precision': 0.08779628352835236}\n\n\n\n\nRandom Forest with under sampling\n\nunder_rf = make_imb_pipeline(\n    RandomUnderSampler(random_state=42), RandomForestClassifier(random_state=42))\n\n\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\ncompute_metrics(under_rf)\n\n{'auc': 0.7930356061088476, 'average_precision': 0.0634524323793531}\n\n\n\n\nLinear model with over sampling\n\nover_log_reg = make_imb_pipeline(\n    RandomOverSampler(), LogisticRegression(random_state=42))\n\n\nbase_log_reg_metrics\n\n{'auc': 0.8353388498771366, 'average_precision': 0.08784463474925555}\n\n\n\ncompute_metrics(over_log_reg)\n\n{'auc': 0.835413689744715, 'average_precision': 0.08676060348173371}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#exercise-1",
    "href": "posts/2021-06-04-imbalanced-data.html#exercise-1",
    "title": "Imbalanced data",
    "section": "Exercise 1",
    "text": "Exercise 1\n\nUse make_imb_pipeline with RandomOverSampler to create a pipline with random forset called over_rf.\nCompute our metrics using compute_metrics.\n\n\n# %load solutions/02-ex01-solutions.py\nover_rf = make_imb_pipeline(\n    RandomOverSampler(),\n    RandomForestClassifier(random_state=42, n_jobs=-1)\n)\n\nbase_rf_metrics\n\ncompute_metrics(over_rf)\n\n{'auc': 0.713025487676646, 'average_precision': 0.04324731383558929}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#plotting-curves-for-logistic-regression",
    "href": "posts/2021-06-04-imbalanced-data.html#plotting-curves-for-logistic-regression",
    "title": "Imbalanced data",
    "section": "Plotting curves for logistic regression",
    "text": "Plotting curves for logistic regression\n\nbase_log_reg.fit(X_train, y_train)\nunder_log_reg.fit(X_train, y_train)\nover_log_reg.fit(X_train, y_train);\n\n\nbase_log_reg.score(X_test, y_test)\n\n0.9811502476609797\n\n\n\nPlotting\n\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.metrics import plot_roc_curve\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_log_reg, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_log_reg, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_log_reg, X_test, y_test, ax=ax1, name=\"oversampling\")\n\nplot_precision_recall_curve(base_log_reg, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_log_reg, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_log_reg, X_test, y_test, ax=ax2, name=\"oversampling\");\n\n\n\n\n\n# %load solutions/02-ex02-solutions.py\nbase_rf.fit(X_train, y_train)\nunder_rf.fit(X_train, y_train)\nover_rf.fit(X_train, y_train);\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#class-weights",
    "href": "posts/2021-06-04-imbalanced-data.html#class-weights",
    "title": "Imbalanced data",
    "section": "Class-Weights",
    "text": "Class-Weights\n\nLinear model with class weights\n\nclass_weight_log_reg = LogisticRegression(class_weight='balanced', random_state=42)\nclass_weight_log_reg.fit(X_train, y_train)\n\nLogisticRegressionLogisticRegression(class_weight='balanced', random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_log_reg, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(class_weight_log_reg, X_test, y_test, ax=ax1, name=\"class-weighted\")\n\nplot_precision_recall_curve(base_log_reg, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(class_weight_log_reg, X_test, y_test, ax=ax2, name=\"class-weighted\")\n\n&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4551550&gt;\n\n\n\n\n\n\n\nRandom forest with class weights\n\nclass_weight_rf = RandomForestClassifier(class_weight='balanced', random_state=42)\nclass_weight_rf.fit(X_train, y_train)\n\nRandomForestClassifierRandomForestClassifier(class_weight='balanced', random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(class_weight_rf, X_test, y_test, ax=ax1, name=\"class-weighted\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(class_weight_rf, X_test, y_test, ax=ax2, name=\"class-weighted\")\n\n&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef439b910&gt;"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#ensemble-resampling",
    "href": "posts/2021-06-04-imbalanced-data.html#ensemble-resampling",
    "title": "Imbalanced data",
    "section": "Ensemble Resampling",
    "text": "Ensemble Resampling\n\nfrom imblearn.ensemble import BalancedRandomForestClassifier\n\n\nbalanced_rf = BalancedRandomForestClassifier(random_state=0)\nbalanced_rf.fit(X_train, y_train)\n\nBalancedRandomForestClassifierBalancedRandomForestClassifier(random_state=0)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\nplot_roc_curve(balanced_rf, X_test, y_test, ax=ax1, name=\"balanced bagging\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");\nplot_precision_recall_curve(balanced_rf, X_test, y_test, ax=ax2, name=\"balanced bagging\")\n\n&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4021fa0&gt;"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#smote",
    "href": "posts/2021-06-04-imbalanced-data.html#smote",
    "title": "Imbalanced data",
    "section": "SMOTE",
    "text": "SMOTE\n\nfrom imblearn.over_sampling import SMOTE\n\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\nX_train_smote.shape\n\n(42786, 5)\n\n\n\nnp.bincount(y_train_smote)\n\narray([21393, 21393])\n\n\n\nfig, axes = plt.subplots(1, 2, figsize=(16, 8))\nsorting = np.argsort(y_train)\n\naxes[0].set_title(\"Original\")\naxes[0].scatter(X_train.iloc[sorting, 3], X_train.iloc[sorting, 4], c=plt.cm.tab10(y_train.iloc[sorting]), alpha=.3, s=2)\n\naxes[1].set_title(\"SMOTE\")\naxes[1].scatter(X_train_smote.iloc[:, 3], X_train_smote.iloc[:, 4], c=plt.cm.tab10(y_train_smote), alpha=.1, s=2)\n\n&lt;matplotlib.collections.PathCollection at 0x7f4ec7f8efa0&gt;\n\n\n\n\n\n\ncv_results\n\n{'fit_time': array([0.18456721, 0.18922424, 0.21849942, 0.16009235, 0.14718604]),\n 'score_time': array([0.02156591, 0.01044083, 0.00906014, 0.00876856, 0.00877857]),\n 'test_roc_auc': array([0.85926447, 0.82782335, 0.83645313, 0.82829213, 0.82486117]),\n 'test_average_precision': array([0.08657556, 0.08525184, 0.10518971, 0.0796069 , 0.08259916])}\n\n\n\nsmote_log_reg = make_imb_pipeline(\n    SMOTE(random_state=42), LogisticRegression(random_state=42))\ncompute_metrics(smote_log_reg)\n\n{'auc': 0.7872750489175167, 'average_precision': 0.06044681160292857}\n\n\n\nbase_rf_metrics\n\n{'auc': 0.7132260944197908, 'average_precision': 0.04824294956894796}\n\n\n\nsmote_rf = make_imb_pipeline(SMOTE(random_state=42), RandomForestClassifier(random_state=42, n_jobs=-1))\ncompute_metrics(smote_rf)\n\n{'auc': 0.7160674179833459, 'average_precision': 0.03946048645139655}"
  },
  {
    "objectID": "posts/2021-06-04-imbalanced-data.html#plotting-all-the-version-of-random-forest",
    "href": "posts/2021-06-04-imbalanced-data.html#plotting-all-the-version-of-random-forest",
    "title": "Imbalanced data",
    "section": "Plotting all the version of random forest",
    "text": "Plotting all the version of random forest\n\nsmote_rf.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('smote', SMOTE(random_state=42)),\n                ('randomforestclassifier',\n                 RandomForestClassifier(n_jobs=-1, random_state=42))])SMOTESMOTE(random_state=42)RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=42)\n\n\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_rf, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(under_rf, X_test, y_test, ax=ax1, name=\"undersampling\")\nplot_roc_curve(over_rf, X_test, y_test, ax=ax1, name=\"oversampling\")\nplot_roc_curve(balanced_rf, X_test, y_test, ax=ax1, name=\"balanced bagging\")\nplot_roc_curve(smote_rf, X_test, y_test, ax=ax1, name=\"smote\")\n\nplot_precision_recall_curve(base_rf, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(under_rf, X_test, y_test, ax=ax2, name=\"undersampling\")\nplot_precision_recall_curve(over_rf, X_test, y_test, ax=ax2, name=\"oversampling\");\nplot_precision_recall_curve(balanced_rf, X_test, y_test, ax=ax2, name=\"balanced bagging\")\nplot_precision_recall_curve(smote_rf, X_test, y_test, ax=ax2, name=\"smote\")\n\n&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4280fa0&gt;\n\n\n\n\n\n\n# %load solutions/02-ex03-solutions.py\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nbase_hist = HistGradientBoostingClassifier(random_state=42)\nbase_hist.fit(X_train, y_train)\n\nsmote_hist = make_imb_pipeline(\n    SMOTE(), HistGradientBoostingClassifier(random_state=42))\nsmote_hist.fit(X_train, y_train)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 8))\nplot_roc_curve(base_hist, X_test, y_test, ax=ax1, name=\"original\")\nplot_roc_curve(smote_hist, X_test, y_test, ax=ax1, name=\"smote\")\n\nplot_precision_recall_curve(base_hist, X_test, y_test, ax=ax2, name=\"original\")\nplot_precision_recall_curve(smote_hist, X_test, y_test, ax=ax2, name=\"smote\")\n\n&lt;sklearn.metrics._plot.precision_recall_curve.PrecisionRecallDisplay at 0x7f4ef4577e50&gt;"
  },
  {
    "objectID": "posts/2020-09-23-AnalyzingUSInflation.html",
    "href": "posts/2020-09-23-AnalyzingUSInflation.html",
    "title": "Analyzing US Inflation From 1959 - 2009 with statsmodels",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\n%matplotlib inline\n\ndf = sm.datasets.macrodata.load_pandas().data\n\nprint(sm.datasets.macrodata.NOTE)\n\n::\n    Number of Observations - 203\n\n    Number of Variables - 14\n\n    Variable name definitions::\n\n        year      - 1959q1 - 2009q3\n        quarter   - 1-4\n        realgdp   - Real gross domestic product (Bil. of chained 2005 US$,\n                    seasonally adjusted annual rate)\n        realcons  - Real personal consumption expenditures (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realinv   - Real gross private domestic investment (Bil. of chained\n                    2005 US$, seasonally adjusted annual rate)\n        realgovt  - Real federal consumption expenditures & gross investment\n                    (Bil. of chained 2005 US$, seasonally adjusted annual rate)\n        realdpi   - Real private disposable income (Bil. of chained 2005\n                    US$, seasonally adjusted annual rate)\n        cpi       - End of the quarter consumer price index for all urban\n                    consumers: all items (1982-84 = 100, seasonally adjusted).\n        m1        - End of the quarter M1 nominal money stock (Seasonally\n                    adjusted)\n        tbilrate  - Quarterly monthly average of the monthly 3-month\n                    treasury bill: secondary market rate\n        unemp     - Seasonally adjusted unemployment rate (%)\n        pop       - End of the quarter total population: all ages incl. armed\n                    forces over seas\n        infl      - Inflation rate (ln(cpi_{t}/cpi_{t-1}) * 400)\n        realint   - Real interest rate (tbilrate - infl)\n\n\n\n\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n0\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n2\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n3\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n4\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\nindex = pd.Index(sm.tsa.datetools.dates_from_range('1959Q1', '2009Q3'))\ndf.index = index\ndf.head()\n\n\n\n\n\n\n\n\nyear\nquarter\nrealgdp\nrealcons\nrealinv\nrealgovt\nrealdpi\ncpi\nm1\ntbilrate\nunemp\npop\ninfl\nrealint\n\n\n\n\n1959-03-31\n1959.0\n1.0\n2710.349\n1707.4\n286.898\n470.045\n1886.9\n28.98\n139.7\n2.82\n5.8\n177.146\n0.00\n0.00\n\n\n1959-06-30\n1959.0\n2.0\n2778.801\n1733.7\n310.859\n481.301\n1919.7\n29.15\n141.7\n3.08\n5.1\n177.830\n2.34\n0.74\n\n\n1959-09-30\n1959.0\n3.0\n2775.488\n1751.8\n289.226\n491.260\n1916.4\n29.35\n140.5\n3.82\n5.3\n178.657\n2.74\n1.09\n\n\n1959-12-31\n1959.0\n4.0\n2785.204\n1753.7\n299.356\n484.052\n1931.3\n29.37\n140.0\n4.33\n5.6\n179.386\n0.27\n4.06\n\n\n1960-03-31\n1960.0\n1.0\n2847.699\n1770.5\n331.722\n462.199\n1955.5\n29.54\n139.6\n3.50\n5.2\n180.007\n2.31\n1.19\n\n\n\n\n\n\n\n\ndf['infl'].plot()\nplt.ylabel(\"infl\")\n\nText(0, 0.5, 'infl')\n\n\n\n\n\n\\(\\min_{\\\\{ \\tau_{t}\\\\} }\\sum_{t}^{T}\\zeta_{t}^{2}+\\lambda\\sum_{t=1}^{T}\\left[\\left(\\tau_{t}-\\tau_{t-1}\\right)-\\left(\\tau_{t-1}-\\tau_{t-2}\\right)\\right]^{2}\\)\n\n# unpacking\ninfl_cycle, infl_trend = sm.tsa.filters.hpfilter(df.infl)\n\ninfl_cycle\n\n1959-03-31    -1.206811\n1959-06-30     1.141499\n1959-09-30     1.550564\n1959-12-31    -0.909577\n1960-03-31     1.140149\n                ...    \n2008-09-30    -5.064733\n2008-12-31   -10.550048\n2009-03-31    -0.681429\n2009-06-30     1.883255\n2009-09-30     2.206560\nName: infl, Length: 203, dtype: float64\n\n\n\ntype(infl_cycle)\n\npandas.core.series.Series\n\n\n\ndf[\"trend\"] = infl_trend\n\ndf[['trend','infl']].plot(figsize = (12, 8))\n\ndf[['trend','infl']][\"2000-03-31\":].plot(figsize = (12, 8))\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fec38616be0&gt;"
  },
  {
    "objectID": "posts/2021-02-01-newsplease.html",
    "href": "posts/2021-02-01-newsplease.html",
    "title": "Using NewsPlease an open source, news crawler that extracts structured information",
    "section": "",
    "text": "https://github.com/fhamborg/news-please\n\nfrom newsplease import NewsPlease\narticle = NewsPlease.from_url('https://www.bbc.com/vietnamese/vert-earth-55751040')\nprint(article.title)\n\nNhững loài vật chịu được độ lạnh lẽo khủng khiếp nhất hành tinh\n\n\n\narticle.authors\n\n['Https', 'Www.Facebook.Com Bbcnews']\n\n\n\narticle.date_publish\n\ndatetime.datetime(2021, 1, 25, 0, 0)\n\n\n\narticle.description\n\n'Một số loài động vật kỳ lạ đã có chiến lược thông minh và tiến hóa để sinh tồn được trong những điều kiện khí hậu khủng khiếp nhất.'\n\n\n\narticle.filename\n\n'https%3A%2F%2Fwww.bbc.com%2Fvietnamese%2Fvert-earth-55751040.json'\n\n\n\narticle.language\n\n'vi'\n\n\n\narticle.maintext\n\n'Những loài vật chịu được độ lạnh lẽo khủng khiếp nhất hành tinh\\nDale Shaw\\nBBC Earth\\nNguồn hình ảnh, Getty Images\\nLoài động vật nào có thể sống sót trong điều kiện nhiệt độ thấp nhất?\\nChúng ta đang nói tới những động vật sống ở xứ hàn đới vĩ đại nhất trên hành tinh.\\nĐúng vậy, những loài ở xứ lạnh, hay nói cách khác, là những sinh vật có thể thích nghi và phát triển trong thời tiết lạnh (từ chionophile trong tiếng Anh có nghĩa là \"những kẻ mê tuyết\").\\nNhưng loài nào có thể phát triển trong điều kiện nhiệt độ thấp nhất? Sau đây là một trong số những loài hay ho nhất trên Trái Đất làm được vậy…\\nNguồn hình ảnh, Getty Images Chụp lại hình ảnh, Chim cánh cụt hoàng đế sống sót nhờ khả năng hoạt động tập hợp sức mạnh tập thể - chúng ôm nhau để giữ ấm\\nChim cánh cụt Hoàng đế\\nGió lạnh ở Nam Cực làm nhiệt độ giảm xuống đến -60 độ C khiến ta run cầm cập, đây là thách thức với chim cánh cụt hoàng đế.\\nĐây là loài chim cánh cụt lớn nhất trên hành tinh - và vì cao lớn như vậy nên chúng dễ bị hứng những đợt gió lạnh buốt hơn so với các loài kích cỡ nhỏ hơn sống ở đó.\\nNhưng chúng vẫn có khả năng sống sót nhờ vào hoạt động đồng đội, ôm nhau giữ ấm trong suốt những tháng lạnh nhất ở Nam Cực.\\nNhững chú chim cánh cụt đứng giữa nhóm ôm nhau là những con ấm nhất, và khi đã ấm áp rồi, chúng sẽ luân phiên chui ra ngoài để các chú chim bên ngoài có thể thay phiên đi vào giữa sưởi ấm.\\nNguồn hình ảnh, Getty Images Chụp lại hình ảnh, Sóc Bắc Cực tránh cái lạnh khắc nghiệt nhờ đào hang sâu vào lòng đất\\nSóc đất Bắc Cực\\nNếu tình hình hiện thời của thế giới khiến bạn muốn đi ngủ đông cho rồi, thì bạn có thể xem xét loài sóc đất Bắc Cực để chọn làm hình tượng noi theo.\\nSống ở vùng lãnh nguyên Bắc Cực ở Bắc Mỹ, nơi nhiệt độ có thể xuống dưới -63 độ C, chúng có thể thoát khỏi mùa đông tồi tệ nhất nhờ vào việc đào hang dưới lòng đất và ngủ một mạch tám tháng trong năm.\\nKhi ngủ đông, nhiệt độ trong não các chú sóc này có thể giảm xuống chỉ vừa trên mức đóng băng, trong khi nhiệt độ cơ thể có thể giảm tới mức -2,9 độ C và nhịp tim giảm xuống chỉ còn một nhịp mỗi phút.\\nKhi thời gian ngủ đông kết thúc, các chú sóc này cần khoảng ba giờ để làm ấm cơ thể trở lại.\\nNguồn hình ảnh, Getty Images Chụp lại hình ảnh, Hải cẩu Weddel dùng răng để đào hố trong khối băng đại dương để thở\\nHải cẩu Weddell\\nNếu bạn muốn tìm một loài động vật có vú yêu thích cái lạnh trong từng giây phút, tôi giới thiệu bạn với hải cẩu Weddel.\\nChúng là loài sống ở miền nam xa xôi nhất so với bất kỳ giống hải cẩu nào khác, và dành hầu hết thời gian sống bên dưới lớp băng Nam Cực, nơi chúng có thể săn mồi và tránh bị cá voi sát thủ bắt.\\nLặn sâu xuống độ sâu hơn 2.000 feet, chúng có thể ở dưới nước đến 45 phút và nếu không thể đập vỡ băng để ngớp khí oxy cần thiết, chúng sẽ dùng răng để đào cho mình một lỗ lấy khí.\\nNước biển Nam Cực thực ra ấm hơn so với không khí trên bề mặt (vốn có thể giảm xuống -70 độ C), vì vậy đặc biệt trong những trận bão mùa đông dữ dội, hải cẩu sẽ giữ ấm bằng cách lặn xuống biển.\\nNguồn hình ảnh, Getty Images Chụp lại hình ảnh, Cá mập khổng lồ Greenland có thể chịu được nước biển lạnh buốt như những nơi thế này\\nSống ở vùng nước sâu trong khu vực Bắc Đại Tây Dương và Bắc Cực, những con cá mập khổng lồ này không chỉ sống sót trong làn nước lạnh căm, mà còn có tuổi thọ dài nhất so với bất cứ loài có xương sống nào trên hành tinh.\\nChúng có tuổi thọ trung bình từ 300 đến 500 năm tuổi.\\nNhịp trao đổi chất cực kỳ chậm của chúng giúp kiểm soát nhiệt độ và kéo dài tuổi thọ, và chúng cũng có thể là loài cá mập lớn nhất, có thể đạt tới 6,4m chiều dài và nặng khoảng 1,000kg.\\nDù vậy, Usain Bolt không có gì phải lo lắng, vì chúng chỉ có thể tăng tốc độ bơi đến 1,6 dặm/giờ, và do vậy, không có gì ngạc nhiên khi chúng thường săn con mồi đã đi ngủ.\\nNguồn hình ảnh, Getty Images Chụp lại hình ảnh, Lớp lông dày mịn giúp chim sheathbill tuyết giữ nhiệt tốt\\nChim sheathbill tuyết\\nLoài chim trông giống bồ câu, trắng phau và dũng cảm này là loài chim bản địa duy nhất sống trên bề mặt Nam Cực và là loài duy nhất sinh sản tại đây.\\nVới lớp lông dày giúp giữ ấm, chúng hầu như sống trên mặt đất, cố gắng lượm lặt thức ăn thừa rơi vãi của các loài chim khác. Điều này thật hay.\\nChúng cũng là loài chim duy nhất ở Nam Cực không có màng chân.\\nVậy làm sao chúng có thể giữ ngón chân ấm áp, trong khi lại dành quá nhiều thời gian trên bề mặt đất lạnh cóng?\\nKhông có giải pháp sinh học kỳ diệu nào ở đây cả, chúng chỉ dành nhiều thời gian nhảy lò cò từ chân này đổi sang chân kia.\\nNguồn hình ảnh, Getty Images Chụp lại hình ảnh, Bí kíp để loài bò xạ hương có thể sống sót là nhờ lớp lông rậm\\nBò xạ hương\\nMột trong những sinh vật sống trên lãnh nguyên xuất hiện ở khu vực từ Siberia đến Greenland là loài bò xạ hương. Loài này có cái tên nổi bật, được đặt theo mùi hăng khó chịu thoát ra trong mùa động đực.\\nNhững con thú kỳ vĩ này đã sống sót giữa môi trường khắc nghiệt nhất trong hàng ngàn năm qua nhờ vào lớp lông dày rậm.\\nLớp lông rậm rạp và dày nổi bật được làm từ nhiều lớp, với phần bên ngoài - được gọi là lớp lông bảo vệ - che phủ cho lớp lông thứ hai bên dưới, ngắn hơn, đem lại khả năng giữ ấm tăng cường trong những tháng lạnh nhất.\\nNguồn hình ảnh, Getty Images Chụp lại hình ảnh, Nhờ sống bên dưới lớp bỏ cây, những chú bọ cánh cứng can trường này có thể sống sót qua tháng lạnh nhất trong mùa đông\\nBọ cánh cứng dẹt vỏ cây đỏ\\nLoài bọ cánh cứng dài khoảng nửa inch này có khu vực sinh sống từ North Carolina đến Vòng Cực Bắc.\\nChúng sống bên dưới lớp vỏ cây và cơ thể được cấu tạo có chủ đích để sinh tồn trong điều kiện khắc nghiệt nhất giữa mùa đông.\\nKhí hậu ở vùng Bắc Cực vào tháng Tám có tác dụng như khí hậu nghỉ dưỡng đối với loài này.\\nTrong các phòng thí nghiệm, chúng có thể chống chọi được nhiệt độ lạnh cóng đến mức -150 độ C. Mức này đúng là cực lạnh!'\n\n\n\narticle.url\n\n'https://www.bbc.com/vietnamese/vert-earth-55751040'"
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-Linear-Regresion.html",
    "href": "posts/2020-10-15-dask-xgboost-Linear-Regresion.html",
    "title": "Linear Regression using Dask Data Frames",
    "section": "",
    "text": "This post includes code from Scalable-Data-Analysis-in-Python-with-Dask and coiled-examples.\nimport numpy as np\nimport dask.array as da\nimport pandas as pd\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n# engine = db.create_engine(\"sqlite:///fiscal.db\")\n# connection = engine.connect()\n# metadata = db.MetaData()\n#engine.execute(\"SELECT * FROM fiscal_data LIMIT 1\").fetchall()\n# sql = \"\"\"\n# SELECT year\n# , region\n# , province\n# , gdp\n# , fdi\n# , it\n# , specific\n# FROM fiscal_table\n# \"\"\"\n#\n# cnxn = connection\n#df = pd.read_sql(sql, cnxn)\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\ndf = df[['year', 'province', 'gdp', 'fdi', 'it', 'specific']]\ndf\n\n/tmp/ipykernel_191754/2939603070.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n\n\n  df = pd.read_csv(url, error_bad_lines=False)\n\n\n\n\n\n\n\n\n\nyear\nprovince\ngdp\nfdi\nit\nspecific\n\n\n\n\n0\n1996\nAnhui\n2093.30\n50661\n631930\n147002.0\n\n\n1\n1997\nAnhui\n2347.32\n43443\n657860\n151981.0\n\n\n2\n1998\nAnhui\n2542.96\n27673\n889463\n174930.0\n\n\n3\n1999\nAnhui\n2712.34\n26131\n1227364\n285324.0\n\n\n4\n2000\nAnhui\n2902.09\n31847\n1499110\n195580.0\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nZhejiang\n9705.02\n498055\n2261631\n391292.0\n\n\n356\n2004\nZhejiang\n11648.70\n668128\n3162299\n656175.0\n\n\n357\n2005\nZhejiang\n13417.68\n772000\n2370200\n656175.0\n\n\n358\n2006\nZhejiang\n15718.47\n888935\n2553268\n1017303.0\n\n\n359\n2007\nZhejiang\n18753.73\n1036576\n2939778\n844647.0\n\n\n\n\n360 rows × 6 columns\nfrom dask.distributed import Client\n\nclient = Client(processes=False, threads_per_worker=2,\n                n_workers=3, memory_limit='4GB')\nclient\n\n/home/gao/anaconda3/lib/python3.9/site-packages/distributed/node.py:180: UserWarning: Port 8787 is already in use.\nPerhaps you already have a cluster running?\nHosting the HTTP server on port 44929 instead\n  warnings.warn(\n\n\n\n     \n    \n        Client\n        Client-d0860491-426d-11ed-ad0a-6d51532fdfa7\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://192.168.0.198:44929/status\n\n\n\n\n\nCluster Info\n\n\n\n\n\n\nLocalCluster\nd9a33090\n\n\n\nDashboard: http://192.168.0.198:44929/status\nWorkers: 3\n\n\nTotal threads: 6\nTotal memory: 11.18 GiB\n\n\nStatus: running\nUsing processes: False\n\n\n\n\n\nScheduler Info\n\n\n\n\n\n\nScheduler\nScheduler-ce00786c-ba29-4c87-b06e-00f2f12d928e\n\n\n\nComm: inproc://192.168.0.198/191754/19\nWorkers: 3\n\n\nDashboard: http://192.168.0.198:44929/status\nTotal threads: 6\n\n\nStarted: Just now\nTotal memory: 11.18 GiB\n\n\n\n\n\n\nWorkers\n\n\n\n\n\nWorker: 0\n\n\n\nComm: inproc://192.168.0.198/191754/23\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:45683/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-dl06a7xq\n\n\n\n\n\n\n\n\n\n\nWorker: 1\n\n\n\nComm: inproc://192.168.0.198/191754/22\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:44107/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-p8bgcjht\n\n\n\n\n\n\n\n\n\n\nWorker: 2\n\n\n\nComm: inproc://192.168.0.198/191754/24\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:34225/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-v50r3s5i"
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-Linear-Regresion.html#selecting-features-and-target",
    "href": "posts/2020-10-15-dask-xgboost-Linear-Regresion.html#selecting-features-and-target",
    "title": "Linear Regression using Dask Data Frames",
    "section": "Selecting Features and Target",
    "text": "Selecting Features and Target\n\nfeat_list = [\"year\", \"fdi\"]\ncat_feat_list = [\"province\"]\ntarget = [\"gdp\"]\n\n\nddf[\"year\"] = ddf[\"year\"].astype(int)\nddf[\"fdi\"] = ddf[\"fdi\"].astype(float)\nddf[\"gdp\"] = ddf[\"gdp\"].astype(float)\nddf[\"it\"] = ddf[\"it\"].astype(float)\n\n\n#OHE\nfrom dask_ml.preprocessing import OneHotEncoder\n\n\nddf = ddf.categorize(cat_feat_list)\n\n\nohe = OneHotEncoder(sparse=False)\n\n\nohe_ddf = ohe.fit_transform(ddf[cat_feat_list])\n\n\nfeat_list = feat_list + ohe_ddf.columns.tolist()\nfeat_list = [f for f in feat_list if f not in cat_feat_list]\n\n\nddf_processed = (dd.concat([ddf,ohe_ddf], axis=1) [feat_list + target])\n\n\nddf_processed.compute()\n\n\n\n\n\n\n\n\nyear\nfdi\nprovince_Anhui\nprovince_Beijing\nprovince_Chongqing\nprovince_Fujian\nprovince_Gansu\nprovince_Guangdong\nprovince_Guangxi\nprovince_Guizhou\n...\nprovince_Shandong\nprovince_Shanghai\nprovince_Shanxi\nprovince_Sichuan\nprovince_Tianjin\nprovince_Tibet\nprovince_Xinjiang\nprovince_Yunnan\nprovince_Zhejiang\ngdp\n\n\n\n\n0\n1996\n50661.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2093.30\n\n\n1\n1997\n43443.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2347.32\n\n\n2\n1998\n27673.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2542.96\n\n\n3\n1999\n26131.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2712.34\n\n\n4\n2000\n31847.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n2902.09\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\n498055.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n9705.02\n\n\n356\n2004\n668128.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n11648.70\n\n\n357\n2005\n772000.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n13417.68\n\n\n358\n2006\n888935.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n15718.47\n\n\n359\n2007\n1036576.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n18753.73\n\n\n\n\n360 rows × 33 columns\n\n\n\n\nfeat_list\n\n['year',\n 'fdi',\n 'province_Anhui',\n 'province_Beijing',\n 'province_Chongqing',\n 'province_Fujian',\n 'province_Gansu',\n 'province_Guangdong',\n 'province_Guangxi',\n 'province_Guizhou',\n 'province_Hainan',\n 'province_Hebei',\n 'province_Heilongjiang',\n 'province_Henan',\n 'province_Hubei',\n 'province_Hunan',\n 'province_Jiangsu',\n 'province_Jiangxi',\n 'province_Jilin',\n 'province_Liaoning',\n 'province_Ningxia',\n 'province_Qinghai',\n 'province_Shaanxi',\n 'province_Shandong',\n 'province_Shanghai',\n 'province_Shanxi',\n 'province_Sichuan',\n 'province_Tianjin',\n 'province_Tibet',\n 'province_Xinjiang',\n 'province_Yunnan',\n 'province_Zhejiang']"
  },
  {
    "objectID": "posts/2020-10-15-dask-xgboost-Linear-Regresion.html#dask-linear-regression",
    "href": "posts/2020-10-15-dask-xgboost-Linear-Regresion.html#dask-linear-regression",
    "title": "Linear Regression using Dask Data Frames",
    "section": "Dask Linear Regression",
    "text": "Dask Linear Regression\n\nX=ddf_processed[feat_list].persist()\ny=ddf_processed[target].persist()\n\n\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n\nX\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\nyear\nfdi\nprovince_Anhui\nprovince_Beijing\nprovince_Chongqing\nprovince_Fujian\nprovince_Gansu\nprovince_Guangdong\nprovince_Guangxi\nprovince_Guizhou\nprovince_Hainan\nprovince_Hebei\nprovince_Heilongjiang\nprovince_Henan\nprovince_Hubei\nprovince_Hunan\nprovince_Jiangsu\nprovince_Jiangxi\nprovince_Jilin\nprovince_Liaoning\nprovince_Ningxia\nprovince_Qinghai\nprovince_Shaanxi\nprovince_Shandong\nprovince_Shanghai\nprovince_Shanxi\nprovince_Sichuan\nprovince_Tianjin\nprovince_Tibet\nprovince_Xinjiang\nprovince_Yunnan\nprovince_Zhejiang\n\n\nnpartitions=5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n0\nint64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\nfloat64\n\n\n72\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n288\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n359\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n\n\n\nDask Name: getitem, 5 tasks\n\n\n\nX.compute()\n\n\n\n\n\n\n\n\nyear\nfdi\nprovince_Anhui\nprovince_Beijing\nprovince_Chongqing\nprovince_Fujian\nprovince_Gansu\nprovince_Guangdong\nprovince_Guangxi\nprovince_Guizhou\n...\nprovince_Shaanxi\nprovince_Shandong\nprovince_Shanghai\nprovince_Shanxi\nprovince_Sichuan\nprovince_Tianjin\nprovince_Tibet\nprovince_Xinjiang\nprovince_Yunnan\nprovince_Zhejiang\n\n\n\n\n0\n1996\n50661.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n1\n1997\n43443.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n2\n1998\n27673.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n3\n1999\n26131.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4\n2000\n31847.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\n498055.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n356\n2004\n668128.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n357\n2005\n772000.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n358\n2006\n888935.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n359\n2007\n1036576.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n360 rows × 32 columns\n\n\n\n\ny\n\nDask DataFrame Structure:\n\n\n\n\n\n\n\ngdp\n\n\nnpartitions=5\n\n\n\n\n\n0\nfloat64\n\n\n72\n...\n\n\n...\n...\n\n\n288\n...\n\n\n359\n...\n\n\n\n\n\nDask Name: getitem, 5 tasks\n\n\n\nLinReg = LinearRegression()\n\n\nLinReg.fit(X, y)\n\nLinearRegression()\n\n\n\nRidgeReg = Ridge()\nRidgeReg.fit(X, y)\n\nRidge()\n\n\n\nLinReg.predict(X)[:5]\n\narray([[1830.87851075],\n       [2076.99855131],\n       [2220.28956049],\n       [2534.65768128],\n       [2936.29581023]])\n\n\n\nRidgeReg.predict(X)[:5]\n\narray([[1743.12844255],\n       [1993.63521868],\n       [2142.78090348],\n       [2460.56148157],\n       [2864.36624124]])\n\n\n\nclient.restart()\n\n\n     \n    \n        Client\n        Client-d0860491-426d-11ed-ad0a-6d51532fdfa7\n        \n\n\n\nConnection method: Cluster object\nCluster type: distributed.LocalCluster\n\n\nDashboard: http://192.168.0.198:44929/status\n\n\n\n\n\nCluster Info\n\n\n\n\n\n\nLocalCluster\nd9a33090\n\n\n\nDashboard: http://192.168.0.198:44929/status\nWorkers: 3\n\n\nTotal threads: 6\nTotal memory: 11.18 GiB\n\n\nStatus: running\nUsing processes: False\n\n\n\n\n\nScheduler Info\n\n\n\n\n\n\nScheduler\nScheduler-ce00786c-ba29-4c87-b06e-00f2f12d928e\n\n\n\nComm: inproc://192.168.0.198/191754/19\nWorkers: 3\n\n\nDashboard: http://192.168.0.198:44929/status\nTotal threads: 6\n\n\nStarted: Just now\nTotal memory: 11.18 GiB\n\n\n\n\n\n\nWorkers\n\n\n\n\n\nWorker: 0\n\n\n\nComm: inproc://192.168.0.198/191754/23\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:45683/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-dl06a7xq\n\n\n\n\n\n\n\n\n\n\nWorker: 1\n\n\n\nComm: inproc://192.168.0.198/191754/22\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:44107/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-p8bgcjht\n\n\n\n\n\n\n\n\n\n\nWorker: 2\n\n\n\nComm: inproc://192.168.0.198/191754/24\nTotal threads: 2\n\n\nDashboard: http://192.168.0.198:34225/status\nMemory: 3.73 GiB\n\n\nNanny: None\n\n\n\nLocal directory: /home/gao/git-repos/Kearney_Data_Science/_notebooks/dask-worker-space/worker-v50r3s5i"
  },
  {
    "objectID": "posts/2021-05-30-Classification example 2 using Health Data with PyCaret.html",
    "href": "posts/2021-05-30-Classification example 2 using Health Data with PyCaret.html",
    "title": "Classification example 2 using Health Data with PyCaret",
    "section": "",
    "text": "#Code from https://github.com/pycaret/pycaret/\n\n\n# check version\nfrom pycaret.utils import version\nversion()\n\n'2.3.1'\n\n\n\n1. Data Repository\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/strokes_training.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n\n\n\n\n\n\n\nid\ngender\nage\nhypertension\nheart_disease\never_married\nwork_type\nResidence_type\navg_glucose_level\nbmi\nsmoking_status\nstroke\n\n\n\n\n0\n30669\nMale\n3.0\n0\n0\nNo\nchildren\nRural\n95.12\n18.0\nNaN\n0\n\n\n1\n30468\nMale\n58.0\n1\n0\nYes\nPrivate\nUrban\n87.96\n39.2\nnever smoked\n0\n\n\n2\n16523\nFemale\n8.0\n0\n0\nNo\nPrivate\nUrban\n110.89\n17.6\nNaN\n0\n\n\n3\n56543\nFemale\n70.0\n0\n0\nYes\nPrivate\nRural\n69.04\n35.9\nformerly smoked\n0\n\n\n4\n46136\nMale\n14.0\n0\n0\nNo\nNever_worked\nRural\n161.28\n19.1\nNaN\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n43395\n56196\nFemale\n10.0\n0\n0\nNo\nchildren\nUrban\n58.64\n20.4\nnever smoked\n0\n\n\n43396\n5450\nFemale\n56.0\n0\n0\nYes\nGovt_job\nUrban\n213.61\n55.4\nformerly smoked\n0\n\n\n43397\n28375\nFemale\n82.0\n1\n0\nYes\nPrivate\nUrban\n91.94\n28.9\nformerly smoked\n0\n\n\n43398\n27973\nMale\n40.0\n0\n0\nYes\nPrivate\nUrban\n99.16\n33.2\nnever smoked\n0\n\n\n43399\n36271\nFemale\n82.0\n0\n0\nYes\nPrivate\nUrban\n79.48\n20.6\nnever smoked\n0\n\n\n\n\n43400 rows × 12 columns\n\n\n\n\ndata=df\n\n\n\n2. Initialize Setup\n\nfrom pycaret.classification import *\nclf1 = setup(df, target = 'stroke', session_id=123, log_experiment=True, experiment_name='health2')\n\n\n\n\n\n\nDescription\nValue\n\n\n\n\n0\nsession_id\n123\n\n\n1\nTarget\nstroke\n\n\n2\nTarget Type\nBinary\n\n\n3\nLabel Encoded\n0: 0, 1: 1\n\n\n4\nOriginal Data\n(43400, 12)\n\n\n5\nMissing Values\nTrue\n\n\n6\nNumeric Features\n4\n\n\n7\nCategorical Features\n7\n\n\n8\nOrdinal Features\nFalse\n\n\n9\nHigh Cardinality Features\nFalse\n\n\n10\nHigh Cardinality Method\nNone\n\n\n11\nTransformed Train Set\n(30379, 19)\n\n\n12\nTransformed Test Set\n(13021, 19)\n\n\n13\nShuffle Train-Test\nTrue\n\n\n14\nStratify Train-Test\nFalse\n\n\n15\nFold Generator\nStratifiedKFold\n\n\n16\nFold Number\n10\n\n\n17\nCPU Jobs\n-1\n\n\n18\nUse GPU\nFalse\n\n\n19\nLog Experiment\nTrue\n\n\n20\nExperiment Name\nhealth2\n\n\n21\nUSI\neaf8\n\n\n22\nImputation Type\nsimple\n\n\n23\nIterative Imputation Iteration\nNone\n\n\n24\nNumeric Imputer\nmean\n\n\n25\nIterative Imputation Numeric Model\nNone\n\n\n26\nCategorical Imputer\nconstant\n\n\n27\nIterative Imputation Categorical Model\nNone\n\n\n28\nUnknown Categoricals Handling\nleast_frequent\n\n\n29\nNormalize\nFalse\n\n\n30\nNormalize Method\nNone\n\n\n31\nTransformation\nFalse\n\n\n32\nTransformation Method\nNone\n\n\n33\nPCA\nFalse\n\n\n34\nPCA Method\nNone\n\n\n35\nPCA Components\nNone\n\n\n36\nIgnore Low Variance\nFalse\n\n\n37\nCombine Rare Levels\nFalse\n\n\n38\nRare Level Threshold\nNone\n\n\n39\nNumeric Binning\nFalse\n\n\n40\nRemove Outliers\nFalse\n\n\n41\nOutliers Threshold\nNone\n\n\n42\nRemove Multicollinearity\nFalse\n\n\n43\nMulticollinearity Threshold\nNone\n\n\n44\nClustering\nFalse\n\n\n45\nClustering Iteration\nNone\n\n\n46\nPolynomial Features\nFalse\n\n\n47\nPolynomial Degree\nNone\n\n\n48\nTrignometry Features\nFalse\n\n\n49\nPolynomial Threshold\nNone\n\n\n50\nGroup Features\nFalse\n\n\n51\nFeature Selection\nFalse\n\n\n52\nFeature Selection Method\nclassic\n\n\n53\nFeatures Selection Threshold\nNone\n\n\n54\nFeature Interaction\nFalse\n\n\n55\nFeature Ratio\nFalse\n\n\n56\nInteraction Threshold\nNone\n\n\n57\nFix Imbalance\nFalse\n\n\n58\nFix Imbalance Method\nSMOTE\n\n\n\n\n\n\n\n3. Compare Baseline\n\nbest_model = compare_models()\n\n\n\n\n\n\nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\nridge\nRidge Classifier\n0.9822\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0210\n\n\nrf\nRandom Forest Classifier\n0.9822\n0.7962\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.7070\n\n\nlr\nLogistic Regression\n0.9821\n0.6887\n0.0000\n0.0000\n0.0000\n-0.0002\n-0.0007\n0.4090\n\n\nada\nAda Boost Classifier\n0.9821\n0.8441\n0.0000\n0.0000\n0.0000\n-0.0001\n-0.0003\n0.3870\n\n\nknn\nK Neighbors Classifier\n0.9819\n0.5417\n0.0000\n0.0000\n0.0000\n-0.0005\n-0.0015\n0.0940\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.9818\n0.8354\n0.0111\n0.2450\n0.0210\n0.0195\n0.0470\n0.1320\n\n\net\nExtra Trees Classifier\n0.9817\n0.7437\n0.0037\n0.1500\n0.0072\n0.0060\n0.0202\n0.4670\n\n\ngbc\nGradient Boosting Classifier\n0.9816\n0.8480\n0.0000\n0.0000\n0.0000\n-0.0012\n-0.0027\n1.2880\n\n\nlda\nLinear Discriminant Analysis\n0.9795\n0.8395\n0.0259\n0.1347\n0.0433\n0.0376\n0.0515\n0.0560\n\n\nsvm\nSVM - Linear Kernel\n0.9794\n0.0000\n0.0111\n0.0534\n0.0178\n0.0127\n0.0167\n0.2770\n\n\nnb\nNaive Bayes\n0.9657\n0.8367\n0.1181\n0.1022\n0.1092\n0.0919\n0.0923\n0.0230\n\n\ndt\nDecision Tree Classifier\n0.9621\n0.5334\n0.0889\n0.0675\n0.0766\n0.0576\n0.0583\n0.0520\n\n\nqda\nQuadratic Discriminant Analysis\n0.6119\n0.5457\n0.4771\n0.0222\n0.0414\n0.0085\n0.0266\n0.0570\n\n\n\n\n\n\n\n4. Create Model\n\nlr = create_model('lr')\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9819\n0.6751\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n1\n0.9822\n0.6980\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n2\n0.9822\n0.6669\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n3\n0.9822\n0.6529\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n4\n0.9822\n0.7013\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n5\n0.9819\n0.6957\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n6\n0.9822\n0.7857\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n7\n0.9819\n0.6158\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n8\n0.9819\n0.6596\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n9\n0.9822\n0.7355\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nMean\n0.9821\n0.6887\n0.0000\n0.0000\n0.0000\n-0.0002\n-0.0007\n\n\nSD\n0.0002\n0.0447\n0.0000\n0.0000\n0.0000\n0.0003\n0.0011\n\n\n\n\n\n\ndt = create_model('dt')\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9618\n0.5351\n0.0926\n0.0694\n0.0794\n0.0603\n0.0609\n\n\n1\n0.9608\n0.5073\n0.0370\n0.0290\n0.0325\n0.0128\n0.0129\n\n\n2\n0.9631\n0.5539\n0.1296\n0.0972\n0.1111\n0.0927\n0.0937\n\n\n3\n0.9625\n0.5172\n0.0556\n0.0455\n0.0500\n0.0311\n0.0312\n\n\n4\n0.9638\n0.5997\n0.2222\n0.1500\n0.1791\n0.1613\n0.1646\n\n\n5\n0.9628\n0.5265\n0.0741\n0.0597\n0.0661\n0.0474\n0.0477\n\n\n6\n0.9645\n0.5455\n0.1111\n0.0909\n0.1000\n0.0821\n0.0825\n\n\n7\n0.9628\n0.5265\n0.0741\n0.0597\n0.0661\n0.0474\n0.0477\n\n\n8\n0.9562\n0.4958\n0.0182\n0.0125\n0.0148\n-0.0068\n-0.0069\n\n\n9\n0.9631\n0.5266\n0.0741\n0.0606\n0.0667\n0.0480\n0.0483\n\n\nMean\n0.9621\n0.5334\n0.0889\n0.0675\n0.0766\n0.0576\n0.0583\n\n\nSD\n0.0022\n0.0273\n0.0542\n0.0366\n0.0436\n0.0444\n0.0453\n\n\n\n\n\n\nrf = create_model('rf', fold = 5)\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9822\n0.8053\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n1\n0.9822\n0.7562\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n2\n0.9822\n0.8108\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n3\n0.9821\n0.7705\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n4\n0.9822\n0.8142\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nMean\n0.9822\n0.7914\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nSD\n0.0001\n0.0235\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\n\nmodels()\n\n\n\n\n\n\n\n\nName\nReference\nTurbo\n\n\nID\n\n\n\n\n\n\n\nlr\nLogistic Regression\nsklearn.linear_model._logistic.LogisticRegression\nTrue\n\n\nknn\nK Neighbors Classifier\nsklearn.neighbors._classification.KNeighborsCl...\nTrue\n\n\nnb\nNaive Bayes\nsklearn.naive_bayes.GaussianNB\nTrue\n\n\ndt\nDecision Tree Classifier\nsklearn.tree._classes.DecisionTreeClassifier\nTrue\n\n\nsvm\nSVM - Linear Kernel\nsklearn.linear_model._stochastic_gradient.SGDC...\nTrue\n\n\nrbfsvm\nSVM - Radial Kernel\nsklearn.svm._classes.SVC\nFalse\n\n\ngpc\nGaussian Process Classifier\nsklearn.gaussian_process._gpc.GaussianProcessC...\nFalse\n\n\nmlp\nMLP Classifier\nsklearn.neural_network._multilayer_perceptron....\nFalse\n\n\nridge\nRidge Classifier\nsklearn.linear_model._ridge.RidgeClassifier\nTrue\n\n\nrf\nRandom Forest Classifier\nsklearn.ensemble._forest.RandomForestClassifier\nTrue\n\n\nqda\nQuadratic Discriminant Analysis\nsklearn.discriminant_analysis.QuadraticDiscrim...\nTrue\n\n\nada\nAda Boost Classifier\nsklearn.ensemble._weight_boosting.AdaBoostClas...\nTrue\n\n\ngbc\nGradient Boosting Classifier\nsklearn.ensemble._gb.GradientBoostingClassifier\nTrue\n\n\nlda\nLinear Discriminant Analysis\nsklearn.discriminant_analysis.LinearDiscrimina...\nTrue\n\n\net\nExtra Trees Classifier\nsklearn.ensemble._forest.ExtraTreesClassifier\nTrue\n\n\nlightgbm\nLight Gradient Boosting Machine\nlightgbm.sklearn.LGBMClassifier\nTrue\n\n\n\n\n\n\n\n\nmodels(type='ensemble').index.tolist()\n\n['rf', 'ada', 'gbc', 'et', 'lightgbm']\n\n\n\n#ensembled_models = compare_models(whitelist = models(type='ensemble').index.tolist(), fold = 3)\n\n\n\n5. Tune Hyperparameters\n\ntuned_lr = tune_model(lr)\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9819\n0.6751\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n1\n0.9822\n0.8103\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n2\n0.9822\n0.6669\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n3\n0.9822\n0.6529\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n4\n0.9822\n0.7013\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n5\n0.9819\n0.6957\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n6\n0.9822\n0.6268\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n7\n0.9819\n0.6158\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n8\n0.9819\n0.6596\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n9\n0.9822\n0.7355\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nMean\n0.9821\n0.6840\n0.0000\n0.0000\n0.0000\n-0.0002\n-0.0007\n\n\nSD\n0.0002\n0.0537\n0.0000\n0.0000\n0.0000\n0.0003\n0.0011\n\n\n\n\n\n\ntuned_rf = tune_model(rf)\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n1\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n2\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n3\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n4\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n5\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n6\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n7\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n8\n0.9819\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n9\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nMean\n0.9822\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nSD\n0.0001\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\n\n\n6. Ensemble Model\n\nbagged_dt = ensemble_model(dt)\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9819\n0.6107\n0.0185\n0.3333\n0.0351\n0.0333\n0.0751\n\n\n1\n0.9819\n0.6763\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n2\n0.9812\n0.6402\n0.0000\n0.0000\n0.0000\n-0.0019\n-0.0042\n\n\n3\n0.9816\n0.5961\n0.0000\n0.0000\n0.0000\n-0.0013\n-0.0035\n\n\n4\n0.9819\n0.6676\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n5\n0.9822\n0.6122\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n6\n0.9822\n0.6289\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n7\n0.9809\n0.6649\n0.0000\n0.0000\n0.0000\n-0.0025\n-0.0049\n\n\n8\n0.9819\n0.6254\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n9\n0.9816\n0.6504\n0.0000\n0.0000\n0.0000\n-0.0013\n-0.0035\n\n\nMean\n0.9817\n0.6373\n0.0019\n0.0333\n0.0035\n0.0025\n0.0054\n\n\nSD\n0.0004\n0.0258\n0.0056\n0.1000\n0.0105\n0.0103\n0.0233\n\n\n\n\n\n\nboosted_dt = ensemble_model(dt, method = 'Boosting')\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9615\n0.5349\n0.0926\n0.0685\n0.0787\n0.0595\n0.0602\n\n\n1\n0.9608\n0.5073\n0.0370\n0.0290\n0.0325\n0.0128\n0.0129\n\n\n2\n0.9598\n0.5341\n0.0926\n0.0641\n0.0758\n0.0559\n0.0569\n\n\n3\n0.9602\n0.5160\n0.0556\n0.0411\n0.0472\n0.0274\n0.0277\n\n\n4\n0.9641\n0.5908\n0.2037\n0.1429\n0.1679\n0.1502\n0.1527\n\n\n5\n0.9658\n0.5462\n0.1111\n0.0968\n0.1034\n0.0861\n0.0863\n\n\n6\n0.9638\n0.5452\n0.1111\n0.0882\n0.0984\n0.0801\n0.0807\n\n\n7\n0.9598\n0.5250\n0.0741\n0.0526\n0.0615\n0.0416\n0.0423\n\n\n8\n0.9575\n0.5054\n0.0364\n0.0256\n0.0301\n0.0090\n0.0092\n\n\n9\n0.9648\n0.5275\n0.0741\n0.0656\n0.0696\n0.0517\n0.0518\n\n\nMean\n0.9618\n0.5332\n0.0888\n0.0674\n0.0765\n0.0574\n0.0581\n\n\nSD\n0.0025\n0.0234\n0.0460\n0.0334\n0.0385\n0.0393\n0.0399\n\n\n\n\n\n\n\n7. Blend Models\n\nblender = blend_models(estimator_list = [boosted_dt, bagged_dt, tuned_rf], method = 'soft')\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9819\n0.6065\n0.0185\n0.3333\n0.0351\n0.0333\n0.0751\n\n\n1\n0.9809\n0.6705\n0.0000\n0.0000\n0.0000\n-0.0025\n-0.0049\n\n\n2\n0.9816\n0.6368\n0.0185\n0.2500\n0.0345\n0.0321\n0.0638\n\n\n3\n0.9806\n0.5950\n0.0000\n0.0000\n0.0000\n-0.0030\n-0.0055\n\n\n4\n0.9826\n0.6973\n0.0185\n1.0000\n0.0364\n0.0357\n0.1349\n\n\n5\n0.9819\n0.6485\n0.0000\n0.0000\n0.0000\n-0.0006\n-0.0024\n\n\n6\n0.9816\n0.6384\n0.0000\n0.0000\n0.0000\n-0.0013\n-0.0035\n\n\n7\n0.9816\n0.6687\n0.0000\n0.0000\n0.0000\n-0.0013\n-0.0035\n\n\n8\n0.9809\n0.6266\n0.0000\n0.0000\n0.0000\n-0.0019\n-0.0043\n\n\n9\n0.9809\n0.6580\n0.0000\n0.0000\n0.0000\n-0.0025\n-0.0049\n\n\nMean\n0.9814\n0.6446\n0.0056\n0.1583\n0.0106\n0.0088\n0.0245\n\n\nSD\n0.0006\n0.0293\n0.0085\n0.3038\n0.0162\n0.0163\n0.0469\n\n\n\n\n\n\n\n8. Stack Models\n\nstacker = stack_models(estimator_list = [boosted_dt,bagged_dt,tuned_rf], meta_model=rf)\n\n\n\n\n\n\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\n0.9822\n0.7220\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n1\n0.9822\n0.8276\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n2\n0.9822\n0.7600\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n3\n0.9822\n0.8048\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n4\n0.9822\n0.8862\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n5\n0.9822\n0.8140\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n6\n0.9822\n0.7350\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n7\n0.9822\n0.8074\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n8\n0.9819\n0.7859\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n9\n0.9822\n0.7983\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nMean\n0.9822\n0.7941\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\nSD\n0.0001\n0.0450\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n\n\n\n\n\n\n\n9. Analyze Model\n\nplot_model(rf)\n\n\n\n\n\nplot_model(rf, plot = 'confusion_matrix')\n\n\n\n\n\nplot_model(rf, plot = 'boundary')\n\n\n\n\n\nplot_model(rf, plot = 'feature')\n\n\n\n\n\nplot_model(rf, plot = 'pr')\n\n\n\n\n\nplot_model(rf, plot = 'class_report')\n\n\n\n\n\nevaluate_model(rf)\n\n\n\n\n\n\n\n\nParameters\n\n\n\n\nbootstrap\nTrue\n\n\nccp_alpha\n0.0\n\n\nclass_weight\nNone\n\n\ncriterion\ngini\n\n\nmax_depth\nNone\n\n\nmax_features\nauto\n\n\nmax_leaf_nodes\nNone\n\n\nmax_samples\nNone\n\n\nmin_impurity_decrease\n0.0\n\n\nmin_impurity_split\nNone\n\n\nmin_samples_leaf\n1\n\n\nmin_samples_split\n2\n\n\nmin_weight_fraction_leaf\n0.0\n\n\nn_estimators\n100\n\n\nn_jobs\n-1\n\n\noob_score\nFalse\n\n\nrandom_state\n123\n\n\nverbose\n0\n\n\nwarm_start\nFalse\n\n\n\n\n\n\n\n\n\n10. Interpret Model\n\ncatboost = create_model('rf', cross_validation=False)\n\n\ninterpret_model(catboost)\n\n\ninterpret_model(catboost, plot = 'correlation')\n\n\ninterpret_model(catboost, plot = 'reason', observation = 12)\n\n\n\n11. AutoML()\n\nbest = automl(optimize = 'Recall')\nbest\n\n\n\n12. Predict Model\n\npred_holdouts = predict_model(lr)\npred_holdouts.head()\n\n\nnew_data = data.copy()\nnew_data.drop(['Purchase'], axis=1, inplace=True)\npredict_new = predict_model(best, data=new_data)\npredict_new.head()\n\n\n\n13. Save / Load Model\n\nsave_model(best, model_name='best-model')\n\n\nloaded_bestmodel = load_model('best-model')\nprint(loaded_bestmodel)\n\n\nfrom sklearn import set_config\nset_config(display='diagram')\nloaded_bestmodel[0]\n\n\nfrom sklearn import set_config\nset_config(display='text')\n\n\n\n14. Deploy Model\n\ndeploy_model(best, model_name = 'best-aws', authentication = {'bucket' : 'pycaret-test'})\n\n\n\n15. Get Config / Set Config\n\nX_train = get_config('X_train')\nX_train.head()\n\n\nget_config('seed')\n\n\nfrom pycaret.classification import set_config\nset_config('seed', 999)\n\n\nget_config('seed')\n\n\n\n16. MLFlow UI\n\n# !mlflow ui"
  },
  {
    "objectID": "posts/2020-11-17-diab_prediction_with_h2o.html",
    "href": "posts/2020-11-17-diab_prediction_with_h2o.html",
    "title": "Predicting Onset/Diagnosis of Chronic Conditions, Diabetes",
    "section": "",
    "text": "toc: true\nbadges: true\ncomments: true\nsticky_rank: 1\ncategories: [Big Data , h2o]\n\n\nNational Institute of Diabetes and Digestive and Kidney Diseases, https://www.niddk.nih.gov/\n\n\n\nh2o.ai\n\n\nCredit: code from https://www.kaggle.com/sudalairajkumar/getting-started-with-h2o\n::: {.cell _cell_guid=‘b1076dfc-b9ad-4769-8c92-a6c4dae69d19’ _uuid=‘8f2839f25d086af736a60e9eeb907d3b93b6e0e5’ cell_id=‘00013-4da94486-1569-4f92-817d-3c1f9fdc46c0’ execution_count=3}\nimport h2o\nimport time\nimport seaborn\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\n\n%matplotlib inline\n:::\n::: {.cell _cell_guid=‘79c7e3d0-c299-4dcb-8224-4455121ee9b0’ _uuid=‘d629ff2d2480ee46fbb7e2d37f6b5fab8052498a’ cell_id=‘00015-bb260fb8-2e8c-4e94-bcc9-757d8359acb7’ outputId=‘b1a9ccbe-b005-4e8f-eb2a-254351f0fcb5’ scrolled=‘true’ execution_count=4}\nh2o.init()\n\nChecking whether there is an H2O instance running at http://localhost:54321 ..... not found.\nAttempting to start a local H2O server...\n  Java Version: openjdk version \"11.0.11\" 2021-04-20; OpenJDK Runtime Environment (build 11.0.11+9-Ubuntu-0ubuntu2.20.04); OpenJDK 64-Bit Server VM (build 11.0.11+9-Ubuntu-0ubuntu2.20.04, mixed mode, sharing)\n  Starting server from /home/gao/anaconda3/lib/python3.9/site-packages/h2o/backend/bin/h2o.jar\n  Ice root: /tmp/tmp_bd4si0c\n  JVM stdout: /tmp/tmp_bd4si0c/h2o_gao_started_from_python.out\n  JVM stderr: /tmp/tmp_bd4si0c/h2o_gao_started_from_python.err\n  Server is running at http://127.0.0.1:54321\nConnecting to H2O server at http://127.0.0.1:54321 ... successful.\n\n\n\n\n      \n\n  \n\n\n\nH2O_cluster_uptime:\n02 secs\n\n\nH2O_cluster_timezone:\nAmerica/Los_Angeles\n\n\nH2O_data_parsing_timezone:\nUTC\n\n\nH2O_cluster_version:\n3.38.0.1\n\n\nH2O_cluster_version_age:\n12 days\n\n\nH2O_cluster_name:\nH2O_from_python_gao_qldgcm\n\n\nH2O_cluster_total_nodes:\n1\n\n\nH2O_cluster_free_memory:\n3.854 Gb\n\n\nH2O_cluster_total_cores:\n8\n\n\nH2O_cluster_allowed_cores:\n8\n\n\nH2O_cluster_status:\nlocked, healthy\n\n\nH2O_connection_url:\nhttp://127.0.0.1:54321\n\n\nH2O_connection_proxy:\n{\"http\": null, \"https\": null}\n\n\nH2O_internal_security:\nFalse\n\n\nPython_version:\n3.9.12 final\n\n\n\n\n\n\n:::\n::: {.cell _uuid=‘c46a4904ace52737d39237a685f0d10ab5665248’ cell_id=‘00017-7f297b90-62ea-4782-9552-065ba779192b’ outputId=‘93dd3c83-a2ea-4c47-b012-004babb307a2’ execution_count=5}\ndiabetes_df = h2o.import_file(\"https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv\", destination_frame=\"diabetes_df\")\n\nParse progress: |████████████████████████████████████████████████████████████████| (done) 100%\n\n:::\n\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/diabetes.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\n/tmp/ipykernel_194458/4254946008.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n\n\n  df = pd.read_csv(url, error_bad_lines=False)\n\n\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\n0\n6\n148\n72\n35\n0\n33.6\n0.627\n50\n1\n\n\n1\n1\n85\n66\n29\n0\n26.6\n0.351\n31\n0\n\n\n2\n8\n183\n64\n0\n0\n23.3\n0.672\n32\n1\n\n\n3\n1\n89\n66\n23\n94\n28.1\n0.167\n21\n0\n\n\n4\n0\n137\n40\n35\n168\n43.1\n2.288\n33\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n763\n10\n101\n76\n48\n180\n32.9\n0.171\n63\n0\n\n\n764\n2\n122\n70\n27\n0\n36.8\n0.340\n27\n0\n\n\n765\n5\n121\n72\n23\n112\n26.2\n0.245\n30\n0\n\n\n766\n1\n126\n60\n0\n0\n30.1\n0.349\n47\n1\n\n\n767\n1\n93\n70\n31\n0\n30.4\n0.315\n23\n0\n\n\n\n\n768 rows × 9 columns\n\n\n\n\ndiabetes_df.describe()\n\nRows:768\nCols:9\n\n\n\n\n\n\n\nPregnancies\nGlucose\nBloodPressure\nSkinThickness\nInsulin\nBMI\nDiabetesPedigreeFunction\nAge\nOutcome\n\n\n\n\ntype\nint\nint\nint\nint\nint\nreal\nreal\nint\nint\n\n\nmins\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.078\n21.0\n0.0\n\n\nmean\n3.845052083333336\n120.89453125\n69.10546874999994\n20.536458333333357\n79.79947916666666\n31.99257812500003\n0.4718763020833334\n33.240885416666615\n0.3489583333333333\n\n\nmaxs\n17.0\n199.0\n122.0\n99.0\n846.0\n67.1\n2.42\n81.0\n1.0\n\n\nsigma\n3.36957806269887\n31.972618195136224\n19.355807170644777\n15.952217567727642\n115.24400235133803\n7.884160320375441\n0.331328595012775\n11.760231540678689\n0.47695137724279896\n\n\nzeros\n111\n5\n35\n227\n374\n11\n0\n0\n500\n\n\nmissing\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n0\n6.0\n148.0\n72.0\n35.0\n0.0\n33.6\n0.627\n50.0\n1.0\n\n\n1\n1.0\n85.0\n66.0\n29.0\n0.0\n26.6\n0.351\n31.0\n0.0\n\n\n2\n8.0\n183.0\n64.0\n0.0\n0.0\n23.3\n0.672\n32.0\n1.0\n\n\n3\n1.0\n89.0\n66.0\n23.0\n94.0\n28.1\n0.167\n21.0\n0.0\n\n\n4\n0.0\n137.0\n40.0\n35.0\n168.0\n43.1\n2.288\n33.0\n1.0\n\n\n5\n5.0\n116.0\n74.0\n0.0\n0.0\n25.6\n0.201\n30.0\n0.0\n\n\n6\n3.0\n78.0\n50.0\n32.0\n88.0\n31.0\n0.248\n26.0\n1.0\n\n\n7\n10.0\n115.0\n0.0\n0.0\n0.0\n35.3\n0.134\n29.0\n0.0\n\n\n8\n2.0\n197.0\n70.0\n45.0\n543.0\n30.5\n0.158\n53.0\n1.0\n\n\n9\n8.0\n125.0\n96.0\n0.0\n0.0\n0.0\n0.232\n54.0\n1.0\n\n\n\n[768 rows x 9 columns]\n\n\n::: {.cell _uuid=‘5a717975571bd32f2d6db1f96ab1834eb77ef7a7’ cell_id=‘00023-efc57e13-af89-4378-a7fb-d4bba14a2e58’ outputId=‘81bd770a-0c77-4465-ec66-472f7485a35e’ execution_count=8}\nfor col in diabetes_df.columns:\n    diabetes_df[col].hist()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n:::\n::: {.cell _uuid=‘49b983c53e5fb7df93be42c2ea06a1660830fae3’ cell_id=‘00025-9c1a807d-869e-426a-b641-545fc25cc007’ outputId=‘510fcd74-ddea-4414-915a-058096c96ac2’ execution_count=9}\nplt.figure(figsize=(10,10))\ncorr = diabetes_df.cor().as_data_frame()\ncorr.index = diabetes_df.columns\nsns.heatmap(corr, annot = True, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.show()\n\n\n\n:::\n::: {.cell _uuid=‘32b93240ba733b29f4e6469b5e12910eac2582c8’ cell_id=‘00027-f9e84e29-9104-4296-9391-ff8d86737b87’ outputId=‘441633ac-d48c-41ca-b9ac-288978894639’ execution_count=10}\ntrain, valid, test = diabetes_df.split_frame(ratios=[0.6,0.2], seed=1234)\nresponse = \"Outcome\"\ntrain[response] = train[response].asfactor()\nvalid[response] = valid[response].asfactor()\ntest[response] = test[response].asfactor()\nprint(\"Number of rows in train, valid and test set : \", train.shape[0], valid.shape[0], test.shape[0])\n\nNumber of rows in train, valid and test set :  465 148 155\n\n:::\n::: {.cell _uuid=‘7d235577e43920222b380aa79b4604da85357fef’ cell_id=‘00031-03cfd9dd-61bd-44dd-9676-9dee27cac5d4’ outputId=‘8170ed6e-d4ef-4bea-d631-2bdab17544f9’ execution_count=11}\npredictors = diabetes_df.columns[:-1]\ngbm = H2OGradientBoostingEstimator()\ngbm.train(x=predictors, y=response, training_frame=train)\n\ngbm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n\n\n\nModel Details\n=============\nH2OGradientBoostingEstimator : Gradient Boosting Machine\nModel Key: GBM_model_python_1664728158045_1\n\n\n      \n\n  \n\nModel Summary:\n\n\n\nnumber_of_trees\nnumber_of_internal_trees\nmodel_size_in_bytes\nmin_depth\nmax_depth\nmean_depth\nmin_leaves\nmax_leaves\nmean_leaves\n\n\n\n\n\n50.0\n50.0\n11549.0\n5.0\n5.0\n5.0\n6.0\n23.0\n13.68\n\n\n\n\nModelMetricsBinomial: gbm\n** Reported on train data. **\n\nMSE: 0.05394537414652564\nRMSE: 0.2322614349101582\nLogLoss: 0.21164045617145613\nMean Per-Class Error: 0.05721884192287545\nAUC: 0.9899262602248459\nAUCPR: 0.9845164166436653\nGini: 0.9798525204496917\n\n\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.46220528851885034\n\n\n\n0\n1\nError\nRate\n\n\n\n\n0\n290.0\n9.0\n0.0301\n(9.0/299.0)\n\n\n1\n14.0\n152.0\n0.0843\n(14.0/166.0)\n\n\nTotal\n304.0\n161.0\n0.0495\n(23.0/465.0)\n\n\n\n\n\n\n\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n\n\nmetric\nthreshold\nvalue\nidx\n\n\n\n\nmax f1\n0.4622053\n0.9296636\n150.0\n\n\nmax f2\n0.3238256\n0.9448357\n176.0\n\n\nmax f0point5\n0.5480218\n0.9673025\n132.0\n\n\nmax accuracy\n0.5069764\n0.9505376\n142.0\n\n\nmax precision\n0.9838497\n1.0\n0.0\n\n\nmax recall\n0.1733829\n1.0\n227.0\n\n\nmax specificity\n0.9838497\n1.0\n0.0\n\n\nmax absolute_mcc\n0.5069764\n0.8920664\n142.0\n\n\nmax min_per_class_accuracy\n0.3721344\n0.9397993\n163.0\n\n\nmax mean_per_class_accuracy\n0.3704585\n0.9458033\n164.0\n\n\nmax tns\n0.9838497\n299.0\n0.0\n\n\nmax fns\n0.9838497\n165.0\n0.0\n\n\nmax fps\n0.0187128\n299.0\n399.0\n\n\nmax tps\n0.1733829\n166.0\n227.0\n\n\nmax tnr\n0.9838497\n1.0\n0.0\n\n\nmax fnr\n0.9838497\n0.9939759\n0.0\n\n\nmax fpr\n0.0187128\n1.0\n399.0\n\n\nmax tpr\n0.1733829\n1.0\n227.0\n\n\n\n\n\n\n\n\nGains/Lift Table: Avg response rate: 35.70 %, avg score: 35.71 %\n\n\ngroup\ncumulative_data_fraction\nlower_threshold\nlift\ncumulative_lift\nresponse_rate\nscore\ncumulative_response_rate\ncumulative_score\ncapture_rate\ncumulative_capture_rate\ngain\ncumulative_gain\nkolmogorov_smirnov\n\n\n\n\n1\n0.0107527\n0.9647603\n2.8012048\n2.8012048\n1.0\n0.9701344\n1.0\n0.9701344\n0.0301205\n0.0301205\n180.1204819\n180.1204819\n0.0301205\n\n\n2\n0.0215054\n0.9598415\n2.8012048\n2.8012048\n1.0\n0.9634412\n1.0\n0.9667878\n0.0301205\n0.0602410\n180.1204819\n180.1204819\n0.0602410\n\n\n3\n0.0301075\n0.9481976\n2.8012048\n2.8012048\n1.0\n0.9528239\n1.0\n0.9627981\n0.0240964\n0.0843373\n180.1204819\n180.1204819\n0.0843373\n\n\n4\n0.0408602\n0.9440162\n2.8012048\n2.8012048\n1.0\n0.9462455\n1.0\n0.9584422\n0.0301205\n0.1144578\n180.1204819\n180.1204819\n0.1144578\n\n\n5\n0.0516129\n0.9402434\n2.8012048\n2.8012048\n1.0\n0.9422679\n1.0\n0.9550725\n0.0301205\n0.1445783\n180.1204819\n180.1204819\n0.1445783\n\n\n6\n0.1010753\n0.9109362\n2.8012048\n2.8012048\n1.0\n0.9246690\n1.0\n0.9401942\n0.1385542\n0.2831325\n180.1204819\n180.1204819\n0.2831325\n\n\n7\n0.1505376\n0.8685528\n2.8012048\n2.8012048\n1.0\n0.8898148\n1.0\n0.9236410\n0.1385542\n0.4216867\n180.1204819\n180.1204819\n0.4216867\n\n\n8\n0.2\n0.7970538\n2.8012048\n2.8012048\n1.0\n0.8346415\n1.0\n0.9016303\n0.1385542\n0.5602410\n180.1204819\n180.1204819\n0.5602410\n\n\n9\n0.3010753\n0.5563266\n2.8012048\n2.8012048\n1.0\n0.6801791\n1.0\n0.8272860\n0.2831325\n0.8433735\n180.1204819\n180.1204819\n0.8433735\n\n\n10\n0.4\n0.3260873\n1.1570194\n2.3945783\n0.4130435\n0.4418483\n0.8548387\n0.7319627\n0.1144578\n0.9578313\n15.7019382\n139.4578313\n0.8675303\n\n\n11\n0.5010753\n0.1895177\n0.3576006\n1.9836858\n0.1276596\n0.2570744\n0.7081545\n0.6361698\n0.0361446\n0.9939759\n-64.2399385\n98.3685816\n0.7665512\n\n\n12\n0.6\n0.1197063\n0.0608958\n1.6666667\n0.0217391\n0.1481433\n0.5949821\n0.5557067\n0.0060241\n1.0\n-93.9104243\n66.6666667\n0.6220736\n\n\n13\n0.6989247\n0.0837243\n0.0\n1.4307692\n0.0\n0.1011523\n0.5107692\n0.4913697\n0.0\n1.0\n-100.0\n43.0769231\n0.4682274\n\n\n14\n0.8\n0.0531339\n0.0\n1.25\n0.0\n0.0675576\n0.4462366\n0.4378236\n0.0\n1.0\n-100.0\n25.0\n0.3110368\n\n\n15\n0.8989247\n0.0342035\n0.0\n1.1124402\n0.0\n0.0424480\n0.3971292\n0.3943133\n0.0\n1.0\n-100.0\n11.2440191\n0.1571906\n\n\n16\n1.0\n0.0187128\n0.0\n1.0\n0.0\n0.0264826\n0.3569892\n0.3571348\n0.0\n1.0\n-100.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nScoring History:\n\n\n\ntimestamp\nduration\nnumber_of_trees\ntraining_rmse\ntraining_logloss\ntraining_auc\ntraining_pr_auc\ntraining_lift\ntraining_classification_error\n\n\n\n\n\n2022-10-02 09:29:28\n0.058 sec\n0.0\n0.4791116\n0.6516662\n0.5\n0.3569892\n1.0\n0.6430108\n\n\n\n2022-10-02 09:29:28\n0.288 sec\n1.0\n0.4566016\n0.6062100\n0.8991115\n0.8419908\n2.8012048\n0.1784946\n\n\n\n2022-10-02 09:29:28\n0.344 sec\n2.0\n0.4371737\n0.5688548\n0.9180501\n0.8681803\n2.8012048\n0.1548387\n\n\n\n2022-10-02 09:29:28\n0.371 sec\n3.0\n0.4205751\n0.5378978\n0.9199944\n0.8702967\n2.8012048\n0.1569892\n\n\n\n2022-10-02 09:29:28\n0.399 sec\n4.0\n0.4059872\n0.5111693\n0.9259379\n0.8819960\n2.8012048\n0.1483871\n\n\n\n2022-10-02 09:29:28\n0.428 sec\n5.0\n0.3925857\n0.4867222\n0.9342386\n0.8943880\n2.8012048\n0.1419355\n\n\n\n2022-10-02 09:29:29\n0.481 sec\n6.0\n0.3804756\n0.4647923\n0.9379357\n0.8998426\n2.8012048\n0.1419355\n\n\n\n2022-10-02 09:29:29\n0.501 sec\n7.0\n0.3704564\n0.4465408\n0.9392050\n0.9018515\n2.8012048\n0.1397849\n\n\n\n2022-10-02 09:29:29\n0.519 sec\n8.0\n0.3620314\n0.4313833\n0.9422070\n0.9066073\n2.8012048\n0.1333333\n\n\n\n2022-10-02 09:29:29\n0.541 sec\n9.0\n0.3553615\n0.4188818\n0.9418342\n0.9072374\n2.8012048\n0.1333333\n\n\n---\n---\n---\n---\n---\n---\n---\n---\n---\n---\n\n\n\n2022-10-02 09:29:29\n1.198 sec\n41.0\n0.2536526\n0.2398467\n0.9822299\n0.9723231\n2.8012048\n0.0731183\n\n\n\n2022-10-02 09:29:29\n1.216 sec\n42.0\n0.2505088\n0.2361929\n0.9835597\n0.9746489\n2.8012048\n0.0623656\n\n\n\n2022-10-02 09:29:29\n1.236 sec\n43.0\n0.2462930\n0.2304267\n0.9856147\n0.9778374\n2.8012048\n0.0602151\n\n\n\n2022-10-02 09:29:29\n1.258 sec\n44.0\n0.2449578\n0.2283342\n0.9861385\n0.9784748\n2.8012048\n0.0580645\n\n\n\n2022-10-02 09:29:29\n1.278 sec\n45.0\n0.2429230\n0.2253850\n0.9866523\n0.9794384\n2.8012048\n0.0537634\n\n\n\n2022-10-02 09:29:29\n1.297 sec\n46.0\n0.2413702\n0.2234642\n0.9872064\n0.9803476\n2.8012048\n0.0537634\n\n\n\n2022-10-02 09:29:29\n1.317 sec\n47.0\n0.2406076\n0.2224398\n0.9873675\n0.9806372\n2.8012048\n0.0537634\n\n\n\n2022-10-02 09:29:29\n1.346 sec\n48.0\n0.2389076\n0.2197529\n0.9876093\n0.9809663\n2.8012048\n0.0537634\n\n\n\n2022-10-02 09:29:29\n1.364 sec\n49.0\n0.2375624\n0.2179044\n0.9879720\n0.9815194\n2.8012048\n0.0516129\n\n\n\n2022-10-02 09:29:29\n1.389 sec\n50.0\n0.2322614\n0.2116405\n0.9899263\n0.9845164\n2.8012048\n0.0494624\n\n\n\n\n[51 rows x 10 columns]\n\n\n\n\nVariable Importances:\n\n\nvariable\nrelative_importance\nscaled_importance\npercentage\n\n\n\n\nGlucose\n160.8897247\n1.0\n0.3861147\n\n\nBMI\n107.1570282\n0.6660278\n0.2571631\n\n\nDiabetesPedigreeFunction\n49.3825264\n0.3069340\n0.1185117\n\n\nAge\n28.7894745\n0.1789392\n0.0690910\n\n\nBloodPressure\n24.5161362\n0.1523785\n0.0588356\n\n\nPregnancies\n22.0554619\n0.1370843\n0.0529303\n\n\nInsulin\n12.9495811\n0.0804873\n0.0310773\n\n\nSkinThickness\n10.9490166\n0.0680529\n0.0262762\n\n\n\n\n\n\n\n[tips]\nUse `model.explain()` to inspect the model.\n--\nUse `h2o.display.toggle_user_tips()` to switch on/off this section.\n\n:::\n::: {.cell _uuid=‘72471d6633f6c4fb5bca62990143f3a7bb2d49ed’ cell_id=‘00033-06796915-498e-4437-872e-7c3bc3450c85’ outputId=‘a667b6e1-cf75-4443-e1d3-62c19f26524b’ execution_count=12}\nprint(gbm)\n\nModel Details\n=============\nH2OGradientBoostingEstimator : Gradient Boosting Machine\nModel Key: GBM_model_python_1664728158045_1\n\n\nModel Summary: \n    number_of_trees    number_of_internal_trees    model_size_in_bytes    min_depth    max_depth    mean_depth    min_leaves    max_leaves    mean_leaves\n--  -----------------  --------------------------  ---------------------  -----------  -----------  ------------  ------------  ------------  -------------\n    50                 50                          11549                  5            5            5             6             23            13.68\n\nModelMetricsBinomial: gbm\n** Reported on train data. **\n\nMSE: 0.05394537414652564\nRMSE: 0.2322614349101582\nLogLoss: 0.21164045617145613\nMean Per-Class Error: 0.05721884192287545\nAUC: 0.9899262602248459\nAUCPR: 0.9845164166436653\nGini: 0.9798525204496917\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.46220528851885034\n       0    1    Error    Rate\n-----  ---  ---  -------  ------------\n0      290  9    0.0301   (9.0/299.0)\n1      14   152  0.0843   (14.0/166.0)\nTotal  304  161  0.0495   (23.0/465.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.462205     0.929664  150\nmax f2                       0.323826     0.944836  176\nmax f0point5                 0.548022     0.967302  132\nmax accuracy                 0.506976     0.950538  142\nmax precision                0.98385      1         0\nmax recall                   0.173383     1         227\nmax specificity              0.98385      1         0\nmax absolute_mcc             0.506976     0.892066  142\nmax min_per_class_accuracy   0.372134     0.939799  163\nmax mean_per_class_accuracy  0.370458     0.945803  164\nmax tns                      0.98385      299       0\nmax fns                      0.98385      165       0\nmax fps                      0.0187128    299       399\nmax tps                      0.173383     166       227\nmax tnr                      0.98385      1         0\nmax fnr                      0.98385      0.993976  0\nmax fpr                      0.0187128    1         399\nmax tpr                      0.173383     1         227\n\nGains/Lift Table: Avg response rate: 35.70 %, avg score: 35.71 %\ngroup    cumulative_data_fraction    lower_threshold    lift       cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  ---------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0107527                   0.96476            2.8012     2.8012             1                0.970134   1                           0.970134            0.0301205       0.0301205                  180.12    180.12             0.0301205\n2        0.0215054                   0.959842           2.8012     2.8012             1                0.963441   1                           0.966788            0.0301205       0.060241                   180.12    180.12             0.060241\n3        0.0301075                   0.948198           2.8012     2.8012             1                0.952824   1                           0.962798            0.0240964       0.0843373                  180.12    180.12             0.0843373\n4        0.0408602                   0.944016           2.8012     2.8012             1                0.946246   1                           0.958442            0.0301205       0.114458                   180.12    180.12             0.114458\n5        0.0516129                   0.940243           2.8012     2.8012             1                0.942268   1                           0.955073            0.0301205       0.144578                   180.12    180.12             0.144578\n6        0.101075                    0.910936           2.8012     2.8012             1                0.924669   1                           0.940194            0.138554        0.283133                   180.12    180.12             0.283133\n7        0.150538                    0.868553           2.8012     2.8012             1                0.889815   1                           0.923641            0.138554        0.421687                   180.12    180.12             0.421687\n8        0.2                         0.797054           2.8012     2.8012             1                0.834641   1                           0.90163             0.138554        0.560241                   180.12    180.12             0.560241\n9        0.301075                    0.556327           2.8012     2.8012             1                0.680179   1                           0.827286            0.283133        0.843373                   180.12    180.12             0.843373\n10       0.4                         0.326087           1.15702    2.39458            0.413043         0.441848   0.854839                    0.731963            0.114458        0.957831                   15.7019   139.458            0.86753\n11       0.501075                    0.189518           0.357601   1.98369            0.12766          0.257074   0.708155                    0.63617             0.0361446       0.993976                   -64.2399  98.3686            0.766551\n12       0.6                         0.119706           0.0608958  1.66667            0.0217391        0.148143   0.594982                    0.555707            0.0060241       1                          -93.9104  66.6667            0.622074\n13       0.698925                    0.0837243          0          1.43077            0                0.101152   0.510769                    0.49137             0               1                          -100      43.0769            0.468227\n14       0.8                         0.0531339          0          1.25               0                0.0675576  0.446237                    0.437824            0               1                          -100      25                 0.311037\n15       0.898925                    0.0342035          0          1.11244            0                0.042448   0.397129                    0.394313            0               1                          -100      11.244             0.157191\n16       1                           0.0187128          0          1                  0                0.0264826  0.356989                    0.357135            0               1                          -100      0                  0\n\nScoring History: \n     timestamp            duration    number_of_trees    training_rmse        training_logloss     training_auc        training_pr_auc     training_lift      training_classification_error\n---  -------------------  ----------  -----------------  -------------------  -------------------  ------------------  ------------------  -----------------  -------------------------------\n     2022-10-02 09:29:28  0.058 sec   0.0                0.47911159933314496  0.6516662199722991   0.5                 0.3569892473118279  1.0                0.6430107526881721\n     2022-10-02 09:29:28  0.288 sec   1.0                0.45660159380365134  0.6062100495400597   0.8991114961518314  0.8419907834296682  2.801204819277108  0.17849462365591398\n     2022-10-02 09:29:28  0.344 sec   2.0                0.4371736639749466   0.5688548438276857   0.9180501269291211  0.8681802926527425  2.801204819277108  0.15483870967741936\n     2022-10-02 09:29:28  0.371 sec   3.0                0.42057507402726807  0.5378978469420505   0.9199943587057259  0.8702966684660556  2.801204819277108  0.15698924731182795\n     2022-10-02 09:29:28  0.399 sec   4.0                0.4059871978973146   0.5111693324278693   0.9259378651730669  0.8819959741925466  2.801204819277108  0.14838709677419354\n     2022-10-02 09:29:28  0.428 sec   5.0                0.3925857075479894   0.48672224871642344  0.9342386267477938  0.8943880350850508  2.801204819277108  0.14193548387096774\n     2022-10-02 09:29:29  0.481 sec   6.0                0.3804756017059107   0.464792336706405    0.9379356892452754  0.8998425852336472  2.801204819277108  0.14193548387096774\n     2022-10-02 09:29:29  0.501 sec   7.0                0.3704563513552021   0.4465407775765197   0.9392049804569448  0.9018515274641279  2.801204819277108  0.13978494623655913\n     2022-10-02 09:29:29  0.519 sec   8.0                0.36203137164241256  0.4313832769476179   0.9422069549099408  0.9066072793734568  2.801204819277108  0.13333333333333333\n     2022-10-02 09:29:29  0.541 sec   9.0                0.35536148198301     0.41888182687672004  0.94183422653826    0.9072373862813187  2.801204819277108  0.13333333333333333\n---  ---                  ---         ---                ---                  ---                  ---                 ---                 ---                ---\n     2022-10-02 09:29:29  1.198 sec   41.0               0.2536525547928615   0.23984672921662842  0.9822299230366283  0.9723231039983983  2.801204819277108  0.07311827956989247\n     2022-10-02 09:29:29  1.216 sec   42.0               0.25050881290992116  0.23619288477522027  0.9835596566869484  0.9746488666990857  2.801204819277108  0.06236559139784946\n     2022-10-02 09:29:29  1.236 sec   43.0               0.24629295679558694  0.23042674158974302  0.9856146996010798  0.9778374268870283  2.801204819277108  0.060215053763440864\n     2022-10-02 09:29:29  1.258 sec   44.0               0.24495780183266505  0.2283341613278266   0.9861385340693879  0.9784747542482269  2.801204819277108  0.05806451612903226\n     2022-10-02 09:29:29  1.278 sec   45.0               0.24292302732873203  0.22538499036354145  0.9866522947979208  0.9794384262974829  2.801204819277108  0.053763440860215055\n     2022-10-02 09:29:29  1.297 sec   46.0               0.24137021574348858  0.22346419481040292  0.9872063504855544  0.9803476010988198  2.801204819277108  0.053763440860215055\n     2022-10-02 09:29:29  1.317 sec   47.0               0.24060757430879762  0.222439845640081    0.9873675303219567  0.9806371756309132  2.801204819277108  0.053763440860215055\n     2022-10-02 09:29:29  1.346 sec   48.0               0.23890756755670148  0.21975294491043756  0.9876093000765604  0.9809663011168502  2.801204819277108  0.053763440860215055\n     2022-10-02 09:29:29  1.364 sec   49.0               0.23756239789012898  0.21790437816782626  0.987971954708466   0.9815193925814205  2.801204819277108  0.05161290322580645\n     2022-10-02 09:29:29  1.389 sec   50.0               0.2322614349101582   0.21164045617145613  0.9899262602248459  0.9845164166436653  2.801204819277108  0.04946236559139785\n[51 rows x 10 columns]\n\n\nVariable Importances: \nvariable                  relative_importance    scaled_importance    percentage\n------------------------  ---------------------  -------------------  ------------\nGlucose                   160.89                 1                    0.386115\nBMI                       107.157                0.666028             0.257163\nDiabetesPedigreeFunction  49.3825                0.306934             0.118512\nAge                       28.7895                0.178939             0.069091\nBloodPressure             24.5161                0.152379             0.0588356\nPregnancies               22.0555                0.137084             0.0529303\nInsulin                   12.9496                0.0804873            0.0310773\nSkinThickness             10.949                 0.0680529            0.0262762\n\n:::\n::: {.cell _uuid=‘ff63ee07d7de276598c8c4445f37bb775541ca20’ cell_id=‘00035-4736dcf7-7b42-470a-beb4-61a1f958ab27’ outputId=‘7701f0b9-7ac7-4de8-c45b-8bc41bb4fca0’ execution_count=13}\nperf = gbm.model_performance(valid)\nprint(perf)\n\nModelMetricsBinomial: gbm\n** Reported on test data. **\n\nMSE: 0.18115330538099383\nRMSE: 0.4256210819273334\nLogLoss: 0.5366686568411096\nMean Per-Class Error: 0.25621588841722254\nAUC: 0.804932282191227\nAUCPR: 0.6470306042290592\nGini: 0.6098645643824541\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.1458393490707833\n       0    1    Error    Rate\n-----  ---  ---  -------  ------------\n0      53   44   0.4536   (44.0/97.0)\n1      3    48   0.0588   (3.0/51.0)\nTotal  56   92   0.3176   (47.0/148.0)\n\nMaximum Metrics: Maximum metrics at their respective thresholds\nmetric                       threshold    value     idx\n---------------------------  -----------  --------  -----\nmax f1                       0.145839     0.671329  91\nmax f2                       0.112531     0.819936  106\nmax f0point5                 0.668426     0.642458  31\nmax accuracy                 0.668426     0.75      31\nmax precision                0.980739     1         0\nmax recall                   0.112531     1         106\nmax specificity              0.980739     1         0\nmax absolute_mcc             0.145839     0.477769  91\nmax min_per_class_accuracy   0.328581     0.686275  64\nmax mean_per_class_accuracy  0.145839     0.743784  91\nmax tns                      0.980739     97        0\nmax fns                      0.980739     50        0\nmax fps                      0.0199636    97        147\nmax tps                      0.112531     51        106\nmax tnr                      0.980739     1         0\nmax fnr                      0.980739     0.980392  0\nmax fpr                      0.0199636    1         147\nmax tpr                      0.112531     1         106\n\nGains/Lift Table: Avg response rate: 34.46 %, avg score: 36.45 %\ngroup    cumulative_data_fraction    lower_threshold    lift      cumulative_lift    response_rate    score      cumulative_response_rate    cumulative_score    capture_rate    cumulative_capture_rate    gain      cumulative_gain    kolmogorov_smirnov\n-------  --------------------------  -----------------  --------  -----------------  ---------------  ---------  --------------------------  ------------------  --------------  -------------------------  --------  -----------------  --------------------\n1        0.0135135                   0.973098           1.45098   1.45098            0.5              0.977228   0.5                         0.977228            0.0196078       0.0196078                  45.098    45.098             0.00929856\n2        0.0202703                   0.966872           2.90196   1.93464            1                0.9724     0.666667                    0.975618            0.0196078       0.0392157                  190.196   93.4641            0.0289064\n3        0.0337838                   0.952356           2.90196   2.32157            1                0.963776   0.8                         0.970881            0.0392157       0.0784314                  190.196   132.157            0.0681221\n4        0.0405405                   0.927994           2.90196   2.4183             1                0.93987    0.833333                    0.965713            0.0196078       0.0980392                  190.196   141.83             0.0877299\n5        0.0540541                   0.923162           1.45098   2.17647            0.5              0.925415   0.75                        0.955638            0.0196078       0.117647                   45.098    117.647            0.0970285\n6        0.101351                    0.892244           2.07283   2.1281             0.714286         0.906349   0.733333                    0.932637            0.0980392       0.215686                   107.283   112.81             0.174449\n7        0.155405                    0.836001           2.17647   2.14493            0.75             0.861825   0.73913                     0.908006            0.117647        0.333333                   117.647   114.493            0.271478\n8        0.202703                    0.722515           1.65826   2.03137            0.571429         0.773092   0.7                         0.876527            0.0784314       0.411765                   65.8263   103.137            0.318981\n9        0.304054                    0.517149           1.54771   1.87015            0.533333         0.605657   0.644444                    0.786237            0.156863        0.568627                   54.7712   87.0153            0.403679\n10       0.398649                    0.409838           1.03641   1.67232            0.357143         0.461534   0.576271                    0.709188            0.0980392       0.666667                   3.64146   67.2316            0.408935\n11       0.5                         0.255816           0.96732   1.52941            0.333333         0.314072   0.527027                    0.629097            0.0980392       0.764706                   -3.26797  52.9412            0.403881\n12       0.601351                    0.146848           1.35425   1.49989            0.466667         0.196076   0.516854                    0.556116            0.137255        0.901961                   35.4248   49.989             0.458662\n13       0.695946                    0.115818           0.621849  1.38054            0.214286         0.134532   0.475728                    0.498813            0.0588235       0.960784                   -37.8151  38.0544            0.404083\n14       0.797297                    0.0623975          0.386928  1.25424            0.133333         0.0911935  0.432203                    0.446997            0.0392157       1                          -61.3072  25.4237            0.309278\n15       0.898649                    0.0398863          0         1.11278            0                0.0496852  0.383459                    0.402188            0               1                          -100      11.2782            0.154639\n16       1                           0.0199636          0         1                  0                0.0302071  0.344595                    0.364487            0               1                          -100      0                  0\n\n:::\n::: {.cell _uuid=‘d82fc0aa8c47d88e787698cab0d1c0f24964dcea’ cell_id=‘00037-ccce92d1-a97d-4d62-aa0f-4b63c82ce597’ outputId=‘236a9835-4af6-4bb8-b4be-e44cc45732dc’ scrolled=‘true’ execution_count=14}\ngbm_tune = H2OGradientBoostingEstimator(\n    ntrees = 3000,\n    learn_rate = 0.01,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    col_sample_rate = 0.7,\n    sample_rate = 0.7,\n    seed = 1234\n)      \ngbm_tune.train(x=predictors, y=response, training_frame=train, validation_frame=valid)\n\ngbm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n\n\n/home/gao/anaconda3/lib/python3.9/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: early stopping is enabled but neither score_tree_interval or score_each_iteration are defined. Early stopping will not be reproducible!\n  warnings.warn(mesg[\"message\"], RuntimeWarning)\n\n\n\nModel Details\n=============\nH2OGradientBoostingEstimator : Gradient Boosting Machine\nModel Key: GBM_model_python_1664728158045_54\n\n\n      \n\n  \n\nModel Summary:\n\n\n\nnumber_of_trees\nnumber_of_internal_trees\nmodel_size_in_bytes\nmin_depth\nmax_depth\nmean_depth\nmin_leaves\nmax_leaves\nmean_leaves\n\n\n\n\n\n46.0\n46.0\n13030.0\n5.0\n5.0\n5.0\n13.0\n22.0\n17.891304\n\n\n\n\nModelMetricsBinomial: gbm\n** Reported on train data. **\n\nMSE: 0.16518968941442272\nRMSE: 0.40643534469140685\nLogLoss: 0.5115661709706083\nMean Per-Class Error: 0.1314522303259862\nAUC: 0.9354776967401377\nAUCPR: 0.8897929742456694\nGini: 0.8709553934802754\n\n\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.33018826601891127\n\n\n\n0\n1\nError\nRate\n\n\n\n\n0\n233.0\n66.0\n0.2207\n(66.0/299.0)\n\n\n1\n7.0\n159.0\n0.0422\n(7.0/166.0)\n\n\nTotal\n240.0\n225.0\n0.157\n(73.0/465.0)\n\n\n\n\n\n\n\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n\n\nmetric\nthreshold\nvalue\nidx\n\n\n\n\nmax f1\n0.3301883\n0.8132992\n207.0\n\n\nmax f2\n0.3301883\n0.8942632\n207.0\n\n\nmax f0point5\n0.4209194\n0.8262108\n126.0\n\n\nmax accuracy\n0.4017699\n0.8559140\n137.0\n\n\nmax precision\n0.5813679\n1.0\n0.0\n\n\nmax recall\n0.2750071\n1.0\n287.0\n\n\nmax specificity\n0.5813679\n1.0\n0.0\n\n\nmax absolute_mcc\n0.3301883\n0.7066698\n207.0\n\n\nmax min_per_class_accuracy\n0.3669302\n0.8461538\n172.0\n\n\nmax mean_per_class_accuracy\n0.3301883\n0.8685478\n207.0\n\n\nmax tns\n0.5813679\n299.0\n0.0\n\n\nmax fns\n0.5813679\n165.0\n0.0\n\n\nmax fps\n0.2260367\n299.0\n399.0\n\n\nmax tps\n0.2750071\n166.0\n287.0\n\n\nmax tnr\n0.5813679\n1.0\n0.0\n\n\nmax fnr\n0.5813679\n0.9939759\n0.0\n\n\nmax fpr\n0.2260367\n1.0\n399.0\n\n\nmax tpr\n0.2750071\n1.0\n287.0\n\n\n\n\n\n\n\n\nGains/Lift Table: Avg response rate: 35.70 %, avg score: 35.61 %\n\n\ngroup\ncumulative_data_fraction\nlower_threshold\nlift\ncumulative_lift\nresponse_rate\nscore\ncumulative_response_rate\ncumulative_score\ncapture_rate\ncumulative_capture_rate\ngain\ncumulative_gain\nkolmogorov_smirnov\n\n\n\n\n1\n0.0107527\n0.5689699\n2.8012048\n2.8012048\n1.0\n0.5725391\n1.0\n0.5725391\n0.0301205\n0.0301205\n180.1204819\n180.1204819\n0.0301205\n\n\n2\n0.0215054\n0.5654823\n2.8012048\n2.8012048\n1.0\n0.5675106\n1.0\n0.5700249\n0.0301205\n0.0602410\n180.1204819\n180.1204819\n0.0602410\n\n\n3\n0.0301075\n0.5603805\n2.8012048\n2.8012048\n1.0\n0.5632164\n1.0\n0.5680796\n0.0240964\n0.0843373\n180.1204819\n180.1204819\n0.0843373\n\n\n4\n0.0408602\n0.5558952\n2.8012048\n2.8012048\n1.0\n0.5578060\n1.0\n0.5653760\n0.0301205\n0.1144578\n180.1204819\n180.1204819\n0.1144578\n\n\n5\n0.0516129\n0.5504172\n2.8012048\n2.8012048\n1.0\n0.5534003\n1.0\n0.5628811\n0.0301205\n0.1445783\n180.1204819\n180.1204819\n0.1445783\n\n\n6\n0.1010753\n0.5233717\n2.8012048\n2.8012048\n1.0\n0.5362640\n1.0\n0.5498557\n0.1385542\n0.2831325\n180.1204819\n180.1204819\n0.2831325\n\n\n7\n0.1505376\n0.5023571\n2.1922472\n2.6011188\n0.7826087\n0.5126311\n0.9285714\n0.5376247\n0.1084337\n0.3915663\n119.2247250\n160.1118761\n0.3748439\n\n\n8\n0.2\n0.4727929\n2.5576218\n2.5903614\n0.9130435\n0.4887618\n0.9247312\n0.5255404\n0.1265060\n0.5180723\n155.7621792\n159.0361446\n0.4946609\n\n\n9\n0.3010753\n0.4052011\n1.9668034\n2.3810241\n0.7021277\n0.4403890\n0.85\n0.4969538\n0.1987952\n0.7168675\n96.6803384\n138.1024096\n0.6466334\n\n\n10\n0.4\n0.3669266\n1.3397067\n2.1234940\n0.4782609\n0.3860622\n0.7580645\n0.4695290\n0.1325301\n0.8493976\n33.9706653\n112.3493976\n0.6988959\n\n\n11\n0.5010753\n0.3269576\n1.1324019\n1.9235741\n0.4042553\n0.3430889\n0.6866953\n0.4440239\n0.1144578\n0.9638554\n13.2401948\n92.3574125\n0.7197083\n\n\n12\n0.6\n0.2946335\n0.1826873\n1.6365462\n0.0652174\n0.3076579\n0.5842294\n0.4215407\n0.0180723\n0.9819277\n-81.7312729\n63.6546185\n0.5939678\n\n\n13\n0.6989247\n0.2733746\n0.1826873\n1.4307692\n0.0652174\n0.2833528\n0.5107692\n0.4019818\n0.0180723\n1.0\n-81.7312729\n43.0769231\n0.4682274\n\n\n14\n0.8\n0.2585034\n0.0\n1.25\n0.0\n0.2657935\n0.4462366\n0.3847752\n0.0\n1.0\n-100.0\n25.0\n0.3110368\n\n\n15\n0.8989247\n0.2386350\n0.0\n1.1124402\n0.0\n0.2514204\n0.3971292\n0.3700998\n0.0\n1.0\n-100.0\n11.2440191\n0.1571906\n\n\n16\n1.0\n0.2260367\n0.0\n1.0\n0.0\n0.2319318\n0.3569892\n0.3561344\n0.0\n1.0\n-100.0\n0.0\n0.0\n\n\n\n\n\n\n\nModelMetricsBinomial: gbm\n** Reported on validation data. **\n\nMSE: 0.18921804289735072\nRMSE: 0.43499200325678483\nLogLoss: 0.5634654900419724\nMean Per-Class Error: 0.26399838285829796\nAUC: 0.8019001414998989\nAUCPR: 0.6808591759676361\nGini: 0.6038002829997977\n\n\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.3235163299950883\n\n\n\n0\n1\nError\nRate\n\n\n\n\n0\n61.0\n36.0\n0.3711\n(36.0/97.0)\n\n\n1\n8.0\n43.0\n0.1569\n(8.0/51.0)\n\n\nTotal\n69.0\n79.0\n0.2973\n(44.0/148.0)\n\n\n\n\n\n\n\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n\n\nmetric\nthreshold\nvalue\nidx\n\n\n\n\nmax f1\n0.3235163\n0.6615385\n78.0\n\n\nmax f2\n0.2734902\n0.7961783\n109.0\n\n\nmax f0point5\n0.4722372\n0.6451613\n25.0\n\n\nmax accuracy\n0.4722372\n0.75\n25.0\n\n\nmax precision\n0.5650105\n1.0\n0.0\n\n\nmax recall\n0.2609124\n1.0\n119.0\n\n\nmax specificity\n0.5650105\n1.0\n0.0\n\n\nmax absolute_mcc\n0.3235163\n0.4496534\n78.0\n\n\nmax min_per_class_accuracy\n0.3575934\n0.7058824\n60.0\n\n\nmax mean_per_class_accuracy\n0.3235163\n0.7360016\n78.0\n\n\nmax tns\n0.5650105\n97.0\n0.0\n\n\nmax fns\n0.5650105\n50.0\n0.0\n\n\nmax fps\n0.2298880\n97.0\n147.0\n\n\nmax tps\n0.2609124\n51.0\n119.0\n\n\nmax tnr\n0.5650105\n1.0\n0.0\n\n\nmax fnr\n0.5650105\n0.9803922\n0.0\n\n\nmax fpr\n0.2298880\n1.0\n147.0\n\n\nmax tpr\n0.2609124\n1.0\n119.0\n\n\n\n\n\n\n\n\nGains/Lift Table: Avg response rate: 34.46 %, avg score: 35.48 %\n\n\ngroup\ncumulative_data_fraction\nlower_threshold\nlift\ncumulative_lift\nresponse_rate\nscore\ncumulative_response_rate\ncumulative_score\ncapture_rate\ncumulative_capture_rate\ngain\ncumulative_gain\nkolmogorov_smirnov\n\n\n\n\n1\n0.0135135\n0.5593949\n2.9019608\n2.9019608\n1.0\n0.5628574\n1.0\n0.5628574\n0.0392157\n0.0392157\n190.1960784\n190.1960784\n0.0392157\n\n\n2\n0.0202703\n0.5570526\n2.9019608\n2.9019608\n1.0\n0.5579185\n1.0\n0.5612111\n0.0196078\n0.0588235\n190.1960784\n190.1960784\n0.0588235\n\n\n3\n0.0337838\n0.5543010\n1.4509804\n2.3215686\n0.5\n0.5558218\n0.8\n0.5590554\n0.0196078\n0.0784314\n45.0980392\n132.1568627\n0.0681221\n\n\n4\n0.0405405\n0.5506229\n2.9019608\n2.4183007\n1.0\n0.5538042\n0.8333333\n0.5581802\n0.0196078\n0.0980392\n190.1960784\n141.8300654\n0.0877299\n\n\n5\n0.0540541\n0.5456868\n2.9019608\n2.5392157\n1.0\n0.5484607\n0.875\n0.5557503\n0.0392157\n0.1372549\n190.1960784\n153.9215686\n0.1269456\n\n\n6\n0.1013514\n0.5143227\n2.0728291\n2.3215686\n0.7142857\n0.5300004\n0.8\n0.5437337\n0.0980392\n0.2352941\n107.2829132\n132.1568627\n0.2043663\n\n\n7\n0.1554054\n0.4833172\n1.8137255\n2.1449275\n0.625\n0.4954764\n0.7391304\n0.5269485\n0.0980392\n0.3333333\n81.3725490\n114.4927536\n0.2714777\n\n\n8\n0.2027027\n0.4498019\n1.6582633\n2.0313725\n0.5714286\n0.4684984\n0.7\n0.5133102\n0.0784314\n0.4117647\n65.8263305\n103.1372549\n0.3189812\n\n\n9\n0.3040541\n0.4033326\n1.5477124\n1.8701525\n0.5333333\n0.4209193\n0.6444444\n0.4825132\n0.1568627\n0.5686275\n54.7712418\n87.0152505\n0.4036790\n\n\n10\n0.3986486\n0.3619319\n1.0364146\n1.6723164\n0.3571429\n0.3829104\n0.5762712\n0.4588787\n0.0980392\n0.6666667\n3.6414566\n67.2316384\n0.4089347\n\n\n11\n0.5\n0.3313348\n0.9673203\n1.5294118\n0.3333333\n0.3453180\n0.5270270\n0.4358596\n0.0980392\n0.7647059\n-3.2679739\n52.9411765\n0.4038811\n\n\n12\n0.6013514\n0.2986642\n1.1607843\n1.4672835\n0.4\n0.3165625\n0.5056180\n0.4157533\n0.1176471\n0.8823529\n16.0784314\n46.7283543\n0.4287447\n\n\n13\n0.6959459\n0.2827940\n0.4145658\n1.3241957\n0.1428571\n0.2891424\n0.4563107\n0.3985441\n0.0392157\n0.9215686\n-58.5434174\n32.4195698\n0.3442490\n\n\n14\n0.7972973\n0.2613478\n0.5803922\n1.2296444\n0.2\n0.2710911\n0.4237288\n0.3823424\n0.0588235\n0.9803922\n-41.9607843\n22.9644400\n0.2793612\n\n\n15\n0.8986486\n0.2459740\n0.1934641\n1.1127820\n0.0666667\n0.2551804\n0.3834586\n0.3680009\n0.0196078\n1.0\n-80.6535948\n11.2781955\n0.1546392\n\n\n16\n1.0\n0.2298880\n0.0\n1.0\n0.0\n0.2375287\n0.3445946\n0.3547773\n0.0\n1.0\n-100.0\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nScoring History:\n\n\n\ntimestamp\nduration\nnumber_of_trees\ntraining_rmse\ntraining_logloss\ntraining_auc\ntraining_pr_auc\ntraining_lift\ntraining_classification_error\nvalidation_rmse\nvalidation_logloss\nvalidation_auc\nvalidation_pr_auc\nvalidation_lift\nvalidation_classification_error\n\n\n\n\n\n2022-10-02 09:29:30\n0.004 sec\n0.0\n0.4791116\n0.6516662\n0.5\n0.3569892\n1.0\n0.6430108\n0.4753975\n0.6443725\n0.5\n0.3445946\n1.0\n0.6554054\n\n\n\n2022-10-02 09:29:30\n0.071 sec\n1.0\n0.4769982\n0.6472695\n0.8658077\n0.7839194\n2.6411360\n0.2236559\n0.4739195\n0.6413185\n0.7874469\n0.6720678\n2.9019608\n0.3648649\n\n\n\n2022-10-02 09:29:30\n0.097 sec\n2.0\n0.4749531\n0.6430432\n0.9007837\n0.8396386\n2.8012048\n0.1763441\n0.4723907\n0.6381740\n0.8094805\n0.6949017\n2.9019608\n0.2972973\n\n\n\n2022-10-02 09:29:30\n0.119 sec\n3.0\n0.4728492\n0.6387192\n0.9130536\n0.8606570\n2.8012048\n0.1677419\n0.4712882\n0.6359110\n0.8022034\n0.6608272\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:30\n0.137 sec\n4.0\n0.4707947\n0.6345181\n0.9156526\n0.8682298\n2.8012048\n0.1591398\n0.4699587\n0.6331932\n0.8033151\n0.6803937\n2.9019608\n0.3310811\n\n\n\n2022-10-02 09:29:30\n0.153 sec\n5.0\n0.4694104\n0.6316968\n0.9223617\n0.8691775\n2.8012048\n0.1526882\n0.4692248\n0.6316983\n0.7984637\n0.6781010\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:30\n0.181 sec\n6.0\n0.4673539\n0.6275332\n0.9264013\n0.8765272\n2.8012048\n0.1634409\n0.4681066\n0.6294256\n0.7926016\n0.6737813\n2.9019608\n0.2837838\n\n\n\n2022-10-02 09:29:30\n0.213 sec\n7.0\n0.4653951\n0.6235869\n0.9257364\n0.8790033\n2.8012048\n0.1698925\n0.4667515\n0.6266790\n0.8012937\n0.6779514\n2.9019608\n0.2770270\n\n\n\n2022-10-02 09:29:30\n0.231 sec\n8.0\n0.4633429\n0.6194722\n0.9264919\n0.8770158\n2.8012048\n0.1655914\n0.4653257\n0.6237967\n0.8156458\n0.7009834\n2.9019608\n0.2770270\n\n\n\n2022-10-02 09:29:30\n0.270 sec\n9.0\n0.4612921\n0.6153845\n0.9243563\n0.8750852\n2.8012048\n0.1655914\n0.4639921\n0.6211111\n0.8143319\n0.7070355\n2.9019608\n0.2702703\n\n\n---\n---\n---\n---\n---\n---\n---\n---\n---\n---\n---\n---\n---\n---\n---\n---\n\n\n\n2022-10-02 09:29:31\n0.842 sec\n37.0\n0.4178853\n0.5325925\n0.9315792\n0.8833978\n2.8012048\n0.1483871\n0.4402507\n0.5740110\n0.8025066\n0.6794139\n2.9019608\n0.2972973\n\n\n\n2022-10-02 09:29:31\n0.881 sec\n38.0\n0.4165556\n0.5301480\n0.9313978\n0.8834538\n2.8012048\n0.1569892\n0.4397835\n0.5730423\n0.8025066\n0.6767335\n2.9019608\n0.2972973\n\n\n\n2022-10-02 09:29:31\n0.905 sec\n39.0\n0.4154800\n0.5281700\n0.9323851\n0.8858817\n2.8012048\n0.1548387\n0.4392927\n0.5720462\n0.8029109\n0.6725959\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:31\n0.930 sec\n40.0\n0.4142100\n0.5258196\n0.9327880\n0.8862107\n2.8012048\n0.1569892\n0.4384777\n0.5704509\n0.8031130\n0.6722814\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:31\n0.962 sec\n41.0\n0.4129595\n0.5235202\n0.9328686\n0.8866435\n2.8012048\n0.1655914\n0.4378318\n0.5691652\n0.8049323\n0.6739874\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:31\n0.980 sec\n42.0\n0.4116622\n0.5211524\n0.9336946\n0.8877616\n2.8012048\n0.1634409\n0.4373805\n0.5682585\n0.8037194\n0.6719149\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:31\n1.011 sec\n43.0\n0.4102407\n0.5185592\n0.9343696\n0.8886470\n2.8012048\n0.1591398\n0.4367366\n0.5669666\n0.8031130\n0.6720384\n2.9019608\n0.2905405\n\n\n\n2022-10-02 09:29:31\n1.040 sec\n44.0\n0.4089426\n0.5161688\n0.9347725\n0.8887239\n2.8012048\n0.1591398\n0.4360471\n0.5655702\n0.8025066\n0.6709346\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:31\n1.059 sec\n45.0\n0.4076517\n0.5137667\n0.9349539\n0.8887844\n2.8012048\n0.1591398\n0.4355522\n0.5645598\n0.8029109\n0.6733691\n2.9019608\n0.2972973\n\n\n\n2022-10-02 09:29:31\n1.076 sec\n46.0\n0.4064353\n0.5115662\n0.9354777\n0.8897930\n2.8012048\n0.1569892\n0.4349920\n0.5634655\n0.8019001\n0.6808592\n2.9019608\n0.2972973\n\n\n\n\n[47 rows x 16 columns]\n\n\n\n\nVariable Importances:\n\n\nvariable\nrelative_importance\nscaled_importance\npercentage\n\n\n\n\nGlucose\n642.6171265\n1.0\n0.4746986\n\n\nBMI\n326.4670410\n0.5080273\n0.2411598\n\n\nDiabetesPedigreeFunction\n115.8438263\n0.1802688\n0.0855733\n\n\nAge\n94.2728806\n0.1467015\n0.0696390\n\n\nPregnancies\n59.3880119\n0.0924159\n0.0438697\n\n\nSkinThickness\n41.0039215\n0.0638077\n0.0302894\n\n\nInsulin\n39.4553261\n0.0613979\n0.0291455\n\n\nBloodPressure\n34.6891441\n0.0539810\n0.0256247\n\n\n\n\n\n\n\n[tips]\nUse `model.explain()` to inspect the model.\n--\nUse `h2o.display.toggle_user_tips()` to switch on/off this section.\n\n:::\n::: {.cell _uuid=‘34c9d4d72deb864223e275e5ab13854841e756e3’ cell_id=‘00039-3d1466ef-b36f-4a1e-a71d-7e05be6ae241’ outputId=‘277c9157-fab7-43d9-c9e5-aac41889a4c3’ execution_count=15}\ngbm_tune.model_performance(valid).auc()\n\n0.8019001414998989\n\n:::\n::: {.cell _uuid=‘09c93530f3ab9e0c45011f257ef29e2b07b22de1’ cell_id=‘00041-695fc4fe-0577-495c-88cc-a3a5a9418eb8’ outputId=‘a6fa3678-14b1-4401-986a-31f38be4b27a’ execution_count=16}\nfrom h2o.grid.grid_search import H2OGridSearch\n\ngbm_grid = H2OGradientBoostingEstimator(\n    ntrees = 3000,\n    learn_rate = 0.01,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    col_sample_rate = 0.7,\n    sample_rate = 0.7,\n    seed = 1234\n) \n\nhyper_params = {'max_depth':[4,6,8,10,12]}\ngrid = H2OGridSearch(gbm_grid, hyper_params,\n                         grid_id='depth_grid',\n                         search_criteria={'strategy': \"Cartesian\"})\n#Train grid search\ngrid.train(x=predictors, \n           y=response,\n           training_frame=train,\n           validation_frame=valid)\n\ngbm Grid Build progress: |███████████████████████████████████████████████████████| (done) 100%\n\n\n\n\n      \n\n  \n\nHyper-Parameter Search Summary: ordered by increasing logloss\n\n\n\nmax_depth\nmodel_ids\nlogloss\n\n\n\n\n\n10.0\ndepth_grid_model_4\n0.5610333\n\n\n\n12.0\ndepth_grid_model_5\n0.5610432\n\n\n\n8.0\ndepth_grid_model_3\n0.5618676\n\n\n\n6.0\ndepth_grid_model_2\n0.5688369\n\n\n\n4.0\ndepth_grid_model_1\n0.5720338\n\n\n\n\n\n\n:::\n::: {.cell _uuid=‘192d673d0b7ae4270f4907a904a48f3d713c3eff’ cell_id=‘00042-e6fd92fd-7d66-4116-9521-787897322ee0’ outputId=‘e3828d16-f377-47f8-acc4-98b94cb7ccf9’ execution_count=17}\nprint(grid)\n\nHyper-Parameter Search Summary: ordered by increasing logloss\n    max_depth    model_ids           logloss\n--  -----------  ------------------  ---------\n    10           depth_grid_model_4  0.561033\n    12           depth_grid_model_5  0.561043\n    8            depth_grid_model_3  0.561868\n    6            depth_grid_model_2  0.568837\n    4            depth_grid_model_1  0.572034\n\n:::\n::: {.cell _uuid=‘6ec9dc6f496957337b2aef1995dc7b4a911b5cdc’ cell_id=‘00044-00f6373e-c217-486b-8be6-bf5356a8db22’ outputId=‘87f2fd9c-5384-4939-fb8a-795cfb50fe43’ scrolled=‘true’ execution_count=18}\nsorted_grid = grid.get_grid(sort_by='auc',decreasing=True)\nprint(sorted_grid)\n\nHyper-Parameter Search Summary: ordered by decreasing auc\n    max_depth    model_ids           auc\n--  -----------  ------------------  --------\n    10           depth_grid_model_4  0.806752\n    12           depth_grid_model_5  0.806752\n    8            depth_grid_model_3  0.803315\n    6            depth_grid_model_2  0.803113\n    4            depth_grid_model_1  0.798464\n\n:::\n::: {.cell _uuid=‘a4c2d034be186b4b95b45507b70710f48a1910bd’ cell_id=‘00046-b60171b1-0e85-45e2-92db-9a5ca354ee08’ outputId=‘9c762184-b936-48b0-f74d-11cafbea721d’ scrolled=‘true’ execution_count=19}\ncv_gbm = H2OGradientBoostingEstimator(\n    ntrees = 3000,\n    learn_rate = 0.05,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    nfolds=4, \n    seed=2018)\ncv_gbm.train(x = predictors, y = response, training_frame = train, validation_frame=valid)\ncv_summary = cv_gbm.cross_validation_metrics_summary().as_data_frame()\ncv_summary\n\ngbm Model Build progress: |██████████████████████████████████████████████████████| (done) 100%\n\n\n\n\n\n\n\n\n\n\nmean\nsd\ncv_1_valid\ncv_2_valid\ncv_3_valid\ncv_4_valid\n\n\n\n\n0\naccuracy\n0.806061\n0.028867\n0.815126\n0.841667\n0.775000\n0.792453\n\n\n1\nauc\n0.834600\n0.025335\n0.871185\n0.824937\n0.813037\n0.829241\n\n\n2\nerr\n0.193939\n0.028867\n0.184874\n0.158333\n0.225000\n0.207547\n\n\n3\nerr_count\n22.500000\n3.316625\n22.000000\n19.000000\n27.000000\n22.000000\n\n\n4\nf0point5\n0.716627\n0.023856\n0.743243\n0.699301\n0.693878\n0.730089\n\n\n5\nf1\n0.735939\n0.051860\n0.800000\n0.677966\n0.715789\n0.750000\n\n\n6\nf2\n0.758549\n0.086101\n0.866142\n0.657895\n0.739130\n0.771028\n\n\n7\nlift_top_group\n2.091386\n0.650080\n1.239583\n1.935484\n2.666667\n2.523809\n\n\n8\nlogloss\n0.484218\n0.030851\n0.460773\n0.456354\n0.520194\n0.499551\n\n\n9\nmax_per_class_error\n0.266773\n0.061059\n0.253521\n0.354839\n0.244444\n0.214286\n\n\n10\nmcc\n0.583317\n0.049473\n0.651216\n0.574659\n0.532452\n0.574940\n\n\n11\nmean_per_class_accuracy\n0.792904\n0.027116\n0.831573\n0.777637\n0.771111\n0.791295\n\n\n12\nmean_per_class_error\n0.207096\n0.027116\n0.168427\n0.222363\n0.228889\n0.208705\n\n\n13\nmse\n0.155457\n0.008760\n0.149316\n0.147334\n0.166110\n0.159068\n\n\n14\npr_auc\n0.733286\n0.085906\n0.775008\n0.609529\n0.744994\n0.803614\n\n\n15\nprecision\n0.705339\n0.017187\n0.709677\n0.714286\n0.680000\n0.717391\n\n\n16\nr2\n0.309233\n0.063386\n0.379557\n0.231025\n0.291265\n0.335087\n\n\n17\nrecall\n0.775775\n0.111681\n0.916667\n0.645161\n0.755556\n0.785714\n\n\n18\nrmse\n0.394164\n0.011076\n0.386415\n0.383841\n0.407566\n0.398833\n\n\n19\nspecificity\n0.810033\n0.070176\n0.746479\n0.910112\n0.786667\n0.796875\n\n\n\n\n\n\n:::\n::: {.cell _uuid=‘f760802df46f149151672e7427ce2afc87c254ca’ cell_id=‘00048-7e06b3d1-982f-4c59-bbfc-c2e01ee55a62’ outputId=‘12f1eaa9-4862-40b5-9bc4-b829d4d0bec6’ execution_count=20}\ncv_gbm.model_performance(valid).auc()\n\n0.8103901354356176\n\n:::\nXGBoost:\n::: {.cell _uuid=‘cae3487ac5a582f3f19b56730a6aa3c3a5be127e’ cell_id=‘00050-f398446a-42c4-45f2-91ec-9c6fb2604ea0’ outputId=‘3184a7f0-3eb1-4a58-fa87-1d5338b9f7ee’ execution_count=21}\nfrom h2o.estimators import H2OXGBoostEstimator\n\ncv_xgb = H2OXGBoostEstimator(\n    ntrees = 3000,\n    learn_rate = 0.05,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    nfolds=4, \n    seed=2018)\ncv_xgb.train(x = predictors, y = response, training_frame = train, validation_frame=valid)\ncv_xgb.model_performance(valid).auc()\n\nxgboost Model Build progress: |██████████████████████████████████████████████████| (done) 100%\n\n\n/home/gao/anaconda3/lib/python3.9/site-packages/h2o/estimators/estimator_base.py:193: RuntimeWarning: early stopping is enabled but neither score_tree_interval or score_each_iteration are defined. Early stopping will not be reproducible!\n  warnings.warn(mesg[\"message\"], RuntimeWarning)\n\n\n0.7982615726703053\n\n:::\n::: {.cell _uuid=‘3e1ee6e8c669a1d9afe6b7cb28f8d4e6bbed6b2f’ cell_id=‘00052-90298ce2-adc4-4479-80a6-254f41c7d692’ outputId=‘8a8bf0be-06aa-4b91-c7f1-84b0545764cc’ scrolled=‘true’ execution_count=22}\ncv_xgb.varimp_plot()\n\n\n\n\n&lt;h2o.plot._plot_result._MObject at 0x7f92b513f2b0&gt;\n\n\n&lt;Figure size 432x288 with 0 Axes&gt;\n\n:::\nAutoML : Automatic Machine Learning:\nFrom the H2O AutoML page,\n::: {.cell _uuid=‘e48a71a1942cf093367c03e2ae293cd3d496b9e3’ cell_id=‘00054-79bef60f-bc57-4c25-b5df-5acbc85a5a23’ outputId=‘c190c6df-85f0-4092-c12d-87d4361cb2cc’ execution_count=23}\nfrom h2o.automl import H2OAutoML\n\naml = H2OAutoML(max_models = 10, max_runtime_secs=100, seed = 1)\naml.train(x=predictors, y=response, training_frame=train, validation_frame=valid)\n\nAutoML progress: |\n09:29:47.222: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n\n███████████████████████████████████████████████████████████████| (done) 100%\n\n\n\nModel Details\n=============\nH2ORandomForestEstimator : Distributed Random Forest\nModel Key: DRF_1_AutoML_1_20221002_92947\n\n\n      \n\n  \n\nModel Summary:\n\n\n\nnumber_of_trees\nnumber_of_internal_trees\nmodel_size_in_bytes\nmin_depth\nmax_depth\nmean_depth\nmin_leaves\nmax_leaves\nmean_leaves\n\n\n\n\n\n42.0\n42.0\n45661.0\n10.0\n17.0\n13.452381\n65.0\n100.0\n81.666664\n\n\n\n\nModelMetricsBinomial: drf\n** Reported on train data. **\n\nMSE: 0.16440960271068342\nRMSE: 0.4054745401510228\nLogLoss: 0.8344361335366673\nMean Per-Class Error: 0.23919087722125965\nAUC: 0.8153785711407503\nAUCPR: 0.700970825419835\nGini: 0.6307571422815006\n\n\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.35\n\n\n\n0\n1\nError\nRate\n\n\n\n\n0\n210.0\n89.0\n0.2977\n(89.0/299.0)\n\n\n1\n30.0\n136.0\n0.1807\n(30.0/166.0)\n\n\nTotal\n240.0\n225.0\n0.2559\n(119.0/465.0)\n\n\n\n\n\n\n\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n\n\nmetric\nthreshold\nvalue\nidx\n\n\n\n\nmax f1\n0.35\n0.6956522\n85.0\n\n\nmax f2\n0.2666667\n0.7866379\n101.0\n\n\nmax f0point5\n0.4822695\n0.6818182\n56.0\n\n\nmax accuracy\n0.4822695\n0.7720430\n56.0\n\n\nmax precision\n1.0\n1.0\n0.0\n\n\nmax recall\n0.0\n1.0\n150.0\n\n\nmax specificity\n1.0\n1.0\n0.0\n\n\nmax absolute_mcc\n0.4822695\n0.5008789\n56.0\n\n\nmax min_per_class_accuracy\n0.4117647\n0.7469880\n72.0\n\n\nmax mean_per_class_accuracy\n0.35\n0.7608091\n85.0\n\n\nmax tns\n1.0\n299.0\n0.0\n\n\nmax fns\n1.0\n164.0\n0.0\n\n\nmax fps\n0.0\n299.0\n150.0\n\n\nmax tps\n0.0\n166.0\n150.0\n\n\nmax tnr\n1.0\n1.0\n0.0\n\n\nmax fnr\n1.0\n0.9879518\n0.0\n\n\nmax fpr\n0.0\n1.0\n150.0\n\n\nmax tpr\n0.0\n1.0\n150.0\n\n\n\n\n\n\n\n\nGains/Lift Table: Avg response rate: 35.70 %, avg score: 36.54 %\n\n\ngroup\ncumulative_data_fraction\nlower_threshold\nlift\ncumulative_lift\nresponse_rate\nscore\ncumulative_response_rate\ncumulative_score\ncapture_rate\ncumulative_capture_rate\ngain\ncumulative_gain\nkolmogorov_smirnov\n\n\n\n\n1\n0.0107527\n0.9388235\n2.2409639\n2.2409639\n0.8\n0.9664706\n0.8\n0.9664706\n0.0240964\n0.0240964\n124.0963855\n124.0963855\n0.0207519\n\n\n2\n0.0215054\n0.8950588\n2.8012048\n2.5210843\n1.0\n0.9283333\n0.9\n0.9474020\n0.0301205\n0.0542169\n180.1204819\n152.1084337\n0.0508724\n\n\n3\n0.0322581\n0.875\n1.6807229\n2.2409639\n0.6\n0.8779412\n0.8\n0.9242484\n0.0180723\n0.0722892\n68.0722892\n124.0963855\n0.0622557\n\n\n4\n0.0430108\n0.8571429\n2.2409639\n2.2409639\n0.8\n0.8571429\n0.8\n0.9074720\n0.0240964\n0.0963855\n124.0963855\n124.0963855\n0.0830076\n\n\n5\n0.0537634\n0.8421053\n2.8012048\n2.3530120\n1.0\n0.8460729\n0.84\n0.8951922\n0.0301205\n0.1265060\n180.1204819\n135.3012048\n0.1131281\n\n\n6\n0.1010753\n0.7879699\n2.2918949\n2.3244040\n0.8181818\n0.8171261\n0.8297872\n0.8586506\n0.1084337\n0.2349398\n129.1894852\n132.4403999\n0.2081839\n\n\n7\n0.1505376\n0.7229091\n2.0704557\n2.2409639\n0.7391304\n0.7558195\n0.8\n0.8248632\n0.1024096\n0.3373494\n107.0455736\n124.0963855\n0.2905267\n\n\n8\n0.2021505\n0.6666667\n1.7507530\n2.1158036\n0.625\n0.6899108\n0.7553191\n0.7904073\n0.0903614\n0.4277108\n75.0753012\n111.5803640\n0.3507878\n\n\n9\n0.3032258\n0.5294118\n1.6688029\n1.9668034\n0.5957447\n0.5855089\n0.7021277\n0.7221078\n0.1686747\n0.5963855\n66.8802871\n96.6803384\n0.4559173\n\n\n10\n0.4021505\n0.4285714\n1.1570194\n1.7676052\n0.4130435\n0.4780518\n0.6310160\n0.6620726\n0.1144578\n0.7108434\n15.7019382\n76.7605180\n0.4800741\n\n\n11\n0.5010753\n0.3288201\n1.0961236\n1.6350380\n0.3913043\n0.3789114\n0.5836910\n0.6061696\n0.1084337\n0.8192771\n9.6123625\n63.5038006\n0.4948624\n\n\n12\n0.6021505\n0.2272727\n0.6556011\n1.4706325\n0.2340426\n0.2747920\n0.525\n0.5505455\n0.0662651\n0.8855422\n-34.4398872\n47.0632530\n0.4407261\n\n\n13\n0.7032258\n0.1538462\n0.4172007\n1.3192218\n0.1489362\n0.1829056\n0.4709480\n0.4977043\n0.0421687\n0.9277108\n-58.2799282\n31.9221841\n0.3491155\n\n\n14\n0.8\n0.0770867\n0.2489960\n1.1897590\n0.0888889\n0.1115738\n0.4247312\n0.4509949\n0.0240964\n0.9518072\n-75.1004016\n18.9759036\n0.2360882\n\n\n15\n1.0\n0.0\n0.2409639\n1.0\n0.0860215\n0.0228685\n0.3569892\n0.3653697\n0.0481928\n1.0\n-75.9036145\n0.0\n0.0\n\n\n\n\n\n\n\nModelMetricsBinomial: drf\n** Reported on validation data. **\n\nMSE: 0.17229692194273172\nRMSE: 0.41508664389827304\nLogLoss: 0.501410044540819\nMean Per-Class Error: 0.2691530220335557\nAUC: 0.8009904992925005\nAUCPR: 0.6966438472127967\nGini: 0.6019809985850011\n\n\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.2857142857142857\n\n\n\n0\n1\nError\nRate\n\n\n\n\n0\n60.0\n37.0\n0.3814\n(37.0/97.0)\n\n\n1\n8.0\n43.0\n0.1569\n(8.0/51.0)\n\n\nTotal\n68.0\n80.0\n0.3041\n(45.0/148.0)\n\n\n\n\n\n\n\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n\n\nmetric\nthreshold\nvalue\nidx\n\n\n\n\nmax f1\n0.2857143\n0.6564885\n41.0\n\n\nmax f2\n0.1190476\n0.7987220\n54.0\n\n\nmax f0point5\n0.7619048\n0.6504065\n8.0\n\n\nmax accuracy\n0.7619048\n0.75\n8.0\n\n\nmax precision\n0.9523810\n1.0\n0.0\n\n\nmax recall\n0.0952381\n1.0\n56.0\n\n\nmax specificity\n0.9523810\n1.0\n0.0\n\n\nmax absolute_mcc\n0.3150963\n0.4407484\n39.0\n\n\nmax min_per_class_accuracy\n0.3809524\n0.7113402\n34.0\n\n\nmax mean_per_class_accuracy\n0.3150963\n0.7318577\n39.0\n\n\nmax tns\n0.9523810\n97.0\n0.0\n\n\nmax fns\n0.9523810\n50.0\n0.0\n\n\nmax fps\n0.0\n97.0\n63.0\n\n\nmax tps\n0.0952381\n51.0\n56.0\n\n\nmax tnr\n0.9523810\n1.0\n0.0\n\n\nmax fnr\n0.9523810\n0.9803922\n0.0\n\n\nmax fpr\n0.0\n1.0\n63.0\n\n\nmax tpr\n0.0952381\n1.0\n56.0\n\n\n\n\n\n\n\n\nGains/Lift Table: Avg response rate: 34.46 %, avg score: 35.67 %\n\n\ngroup\ncumulative_data_fraction\nlower_threshold\nlift\ncumulative_lift\nresponse_rate\nscore\ncumulative_response_rate\ncumulative_score\ncapture_rate\ncumulative_capture_rate\ngain\ncumulative_gain\nkolmogorov_smirnov\n\n\n\n\n1\n0.0337838\n0.9285714\n2.9019608\n2.9019608\n1.0\n0.9333333\n1.0\n0.9333333\n0.0980392\n0.0980392\n190.1960784\n190.1960784\n0.0980392\n\n\n2\n0.0337838\n0.9090476\n0.0\n2.9019608\n0.0\n0.0\n1.0\n0.9333333\n0.0\n0.0980392\n-100.0\n190.1960784\n0.0980392\n\n\n3\n0.0540541\n0.8809524\n2.9019608\n2.9019608\n1.0\n0.8809524\n1.0\n0.9136905\n0.0588235\n0.1568627\n190.1960784\n190.1960784\n0.1568627\n\n\n4\n0.0540541\n0.8726190\n0.0\n2.9019608\n0.0\n0.0\n1.0\n0.9136905\n0.0\n0.1568627\n-100.0\n190.1960784\n0.1568627\n\n\n5\n0.1013514\n0.7640476\n2.4873950\n2.7084967\n0.8571429\n0.8047619\n0.9333333\n0.8628571\n0.1176471\n0.2745098\n148.7394958\n170.8496732\n0.2642005\n\n\n6\n0.1554054\n0.7133739\n1.0882353\n2.1449275\n0.375\n0.7420635\n0.7391304\n0.8208420\n0.0588235\n0.3333333\n8.8235294\n114.4927536\n0.2714777\n\n\n7\n0.2094595\n0.6428571\n1.4509804\n1.9658444\n0.5\n0.6703394\n0.6774194\n0.7820026\n0.0784314\n0.4117647\n45.0980392\n96.5844402\n0.3086719\n\n\n8\n0.3108108\n0.5\n1.3542484\n1.7664109\n0.4666667\n0.5483114\n0.6086957\n0.7057989\n0.1372549\n0.5490196\n35.4248366\n76.6410912\n0.3634526\n\n\n9\n0.4054054\n0.4047619\n1.4509804\n1.6928105\n0.5\n0.4332791\n0.5833333\n0.6422110\n0.1372549\n0.6862745\n45.0980392\n69.2810458\n0.4285426\n\n\n10\n0.5\n0.3123100\n1.2436975\n1.6078431\n0.4285714\n0.3637516\n0.5540541\n0.5895295\n0.1176471\n0.8039216\n24.3697479\n60.7843137\n0.4637154\n\n\n11\n0.6013514\n0.2187437\n0.5803922\n1.4346772\n0.2\n0.2673276\n0.4943820\n0.5352258\n0.0588235\n0.8627451\n-41.9607843\n43.4677242\n0.3988276\n\n\n12\n0.7094595\n0.1428571\n0.7254902\n1.3266106\n0.25\n0.1661759\n0.4571429\n0.4789896\n0.0784314\n0.9411765\n-27.4509804\n32.6610644\n0.3535476\n\n\n13\n0.8040541\n0.0952381\n0.6218487\n1.2436975\n0.2142857\n0.1045376\n0.4285714\n0.4349364\n0.0588235\n1.0\n-37.8151261\n24.3697479\n0.2989691\n\n\n14\n0.8986486\n0.0440476\n0.0\n1.1127820\n0.0\n0.0620206\n0.3834586\n0.3956821\n0.0\n1.0\n-100.0\n11.2781955\n0.1546392\n\n\n15\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0110605\n0.3445946\n0.3567002\n0.0\n1.0\n-100.0\n0.0\n0.0\n\n\n\n\n\n\n\nModelMetricsBinomial: drf\n** Reported on cross-validation data. **\n\nMSE: 0.15154308101802944\nRMSE: 0.38928534652363866\nLogLoss: 0.6698533159343569\nMean Per-Class Error: 0.2301547326429464\nAUC: 0.8406938791957126\nAUCPR: 0.7538605020823073\nGini: 0.6813877583914252\n\n\n\nConfusion Matrix (Act/Pred) for max f1 @ threshold = 0.31999999999999995\n\n\n\n0\n1\nError\nRate\n\n\n\n\n0\n210.0\n89.0\n0.2977\n(89.0/299.0)\n\n\n1\n27.0\n139.0\n0.1627\n(27.0/166.0)\n\n\nTotal\n237.0\n228.0\n0.2495\n(116.0/465.0)\n\n\n\n\n\n\n\n\nMaximum Metrics: Maximum metrics at their respective thresholds\n\n\nmetric\nthreshold\nvalue\nidx\n\n\n\n\nmax f1\n0.3200000\n0.7055838\n81.0\n\n\nmax f2\n0.1800000\n0.7921811\n110.0\n\n\nmax f0point5\n0.6566667\n0.7254902\n33.0\n\n\nmax accuracy\n0.52\n0.7870968\n53.0\n\n\nmax precision\n0.9714286\n1.0\n0.0\n\n\nmax recall\n0.0\n1.0\n136.0\n\n\nmax specificity\n0.9714286\n1.0\n0.0\n\n\nmax absolute_mcc\n0.4400000\n0.5290850\n67.0\n\n\nmax min_per_class_accuracy\n0.3800000\n0.7658863\n75.0\n\n\nmax mean_per_class_accuracy\n0.3200000\n0.7698453\n81.0\n\n\nmax tns\n0.9714286\n299.0\n0.0\n\n\nmax fns\n0.9714286\n165.0\n0.0\n\n\nmax fps\n0.0\n299.0\n136.0\n\n\nmax tps\n0.0\n166.0\n136.0\n\n\nmax tnr\n0.9714286\n1.0\n0.0\n\n\nmax fnr\n0.9714286\n0.9939759\n0.0\n\n\nmax fpr\n0.0\n1.0\n136.0\n\n\nmax tpr\n0.0\n1.0\n136.0\n\n\n\n\n\n\n\n\nGains/Lift Table: Avg response rate: 35.70 %, avg score: 35.62 %\n\n\ngroup\ncumulative_data_fraction\nlower_threshold\nlift\ncumulative_lift\nresponse_rate\nscore\ncumulative_response_rate\ncumulative_score\ncapture_rate\ncumulative_capture_rate\ngain\ncumulative_gain\nkolmogorov_smirnov\n\n\n\n\n1\n0.0129032\n0.92\n2.3343373\n2.3343373\n0.8333333\n0.9419048\n0.8333333\n0.9419048\n0.0301205\n0.0301205\n133.4337349\n133.4337349\n0.0267760\n\n\n2\n0.0215054\n0.8841143\n2.8012048\n2.5210843\n1.0\n0.9\n0.9\n0.9251429\n0.0240964\n0.0542169\n180.1204819\n152.1084337\n0.0508724\n\n\n3\n0.0344086\n0.88\n2.3343373\n2.4510542\n0.8333333\n0.88\n0.875\n0.9082143\n0.0301205\n0.0843373\n133.4337349\n145.1054217\n0.0776484\n\n\n4\n0.0473118\n0.86\n2.8012048\n2.5465498\n1.0\n0.8604167\n0.9090909\n0.8951786\n0.0361446\n0.1204819\n180.1204819\n154.6549836\n0.1137930\n\n\n5\n0.0537634\n0.84\n2.8012048\n2.5771084\n1.0\n0.8433333\n0.92\n0.8889571\n0.0180723\n0.1385542\n180.1204819\n157.7108434\n0.1318653\n\n\n6\n0.1053763\n0.7428571\n2.2176205\n2.4010327\n0.7916667\n0.7819940\n0.8571429\n0.8365671\n0.1144578\n0.2530120\n121.7620482\n140.1032702\n0.2296007\n\n\n7\n0.1505376\n0.6955000\n2.5344234\n2.4410499\n0.9047619\n0.7177177\n0.8714286\n0.8009122\n0.1144578\n0.3674699\n153.4423408\n144.1049914\n0.3373695\n\n\n8\n0.2021505\n0.64\n1.9841867\n2.3244040\n0.7083333\n0.6614980\n0.8297872\n0.7653171\n0.1024096\n0.4698795\n98.4186747\n132.4403999\n0.4163678\n\n\n9\n0.3010753\n0.5126786\n1.5223939\n2.0608864\n0.5434783\n0.5755357\n0.7357143\n0.7029604\n0.1506024\n0.6204819\n52.2393924\n106.0886403\n0.4967361\n\n\n10\n0.4129032\n0.4\n1.1851251\n1.8237011\n0.4230769\n0.4510520\n0.6510417\n0.6347352\n0.1325301\n0.7530120\n18.5125116\n82.3701054\n0.5289318\n\n\n11\n0.5010753\n0.3142857\n1.0248310\n1.6831274\n0.3658537\n0.3441185\n0.6008584\n0.5835966\n0.0903614\n0.8433735\n2.4831031\n68.3127359\n0.5323367\n\n\n12\n0.6107527\n0.22\n0.4943303\n1.4696462\n0.1764706\n0.2647166\n0.5246479\n0.5263330\n0.0542169\n0.8975904\n-50.5669738\n46.9646190\n0.4460853\n\n\n13\n0.7032258\n0.1428571\n0.3908658\n1.3277882\n0.1395349\n0.1782510\n0.4740061\n0.4805607\n0.0361446\n0.9337349\n-60.9134211\n32.7788217\n0.3584841\n\n\n14\n0.8387097\n0.08\n0.3112450\n1.1635774\n0.1111111\n0.1027910\n0.4153846\n0.4195364\n0.0421687\n0.9759036\n-68.8755020\n16.3577386\n0.2133618\n\n\n15\n0.9096774\n0.04\n0.0\n1.0728018\n0.0\n0.0502012\n0.3829787\n0.3907230\n0.0\n0.9759036\n-100.0\n7.2801846\n0.1029939\n\n\n16\n1.0\n0.0\n0.2667814\n1.0\n0.0952381\n0.0079592\n0.3569892\n0.3561508\n0.0240964\n1.0\n-73.3218589\n0.0\n0.0\n\n\n\n\n\n\n\n\n\nCross-Validation Metrics Summary:\n\n\n\nmean\nsd\ncv_1_valid\ncv_2_valid\ncv_3_valid\ncv_4_valid\ncv_5_valid\n\n\n\n\naccuracy\n0.7913979\n0.0406617\n0.8387096\n0.7741935\n0.8064516\n0.7311828\n0.8064516\n\n\nauc\n0.8375493\n0.0195664\n0.8571429\n0.8125623\n0.8561422\n0.8242297\n0.8376691\n\n\nerr\n0.2086021\n0.0406617\n0.1612903\n0.2258064\n0.1935484\n0.2688172\n0.1935484\n\n\nerr_count\n19.4\n3.7815342\n15.0\n21.0\n18.0\n25.0\n18.0\n\n\nf0point5\n0.7038342\n0.0236254\n0.7407407\n0.6868132\n0.6862745\n0.6910569\n0.7142857\n\n\nf1\n0.7194626\n0.0270576\n0.7619048\n0.7042254\n0.7\n0.7311828\n0.7\n\n\nf2\n0.7367346\n0.0420632\n0.7843137\n0.7225434\n0.7142857\n0.7762557\n0.6862745\n\n\nlift_top_group\n1.8084382\n1.6601037\n3.1\n2.735294\n3.2068965\n0.0\n0.0\n\n\nlogloss\n0.6736913\n0.3070846\n0.7692305\n1.170734\n0.4359544\n0.5240929\n0.4684449\n\n\nmax_per_class_error\n0.2792964\n0.0531651\n0.2\n0.2647059\n0.2758621\n0.3333333\n0.3225806\n\n\nmcc\n0.5515387\n0.0607154\n0.6420341\n0.5233643\n0.5580524\n0.4761905\n0.5580524\n\n\nmean_per_class_accuracy\n0.7781513\n0.0329572\n0.8285714\n0.7659522\n0.7839439\n0.7380952\n0.7741935\n\n\nmean_per_class_error\n0.2218487\n0.0329572\n0.1714286\n0.2340479\n0.2160560\n0.2619048\n0.2258064\n\n\nmse\n0.1525488\n0.0142573\n0.1353005\n0.1627745\n0.1416339\n0.1696109\n0.1534241\n\n\npr_auc\n0.7505447\n0.0531491\n0.8066668\n0.7528367\n0.7662882\n0.764195\n0.6627369\n\n\nprecision\n0.6942345\n0.0290384\n0.7272728\n0.6756757\n0.6774194\n0.6666667\n0.7241379\n\n\nr2\n0.3287488\n0.0328852\n0.3808392\n0.2981872\n0.3399831\n0.3151427\n0.3095917\n\n\nrecall\n0.749275\n0.0552121\n0.8\n0.7352941\n0.7241379\n0.8095238\n0.6774194\n\n\nrmse\n0.3902321\n0.0182923\n0.3678322\n0.4034532\n0.3763427\n0.4118384\n0.3916938\n\n\nspecificity\n0.8070275\n0.0833039\n0.8571429\n0.7966102\n0.84375\n0.6666667\n0.8709678\n\n\n\n\n\n\n\n\nScoring History:\n\n\n\ntimestamp\nduration\nnumber_of_trees\ntraining_rmse\ntraining_logloss\ntraining_auc\ntraining_pr_auc\ntraining_lift\ntraining_classification_error\nvalidation_rmse\nvalidation_logloss\nvalidation_auc\nvalidation_pr_auc\nvalidation_lift\nvalidation_classification_error\n\n\n\n\n\n2022-10-02 09:29:50\n0.921 sec\n0.0\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\nnan\n\n\n\n2022-10-02 09:29:50\n0.953 sec\n5.0\n0.5139663\n6.7066483\n0.6897261\n0.5373973\n1.7168675\n0.3428571\n0.4330814\n1.3870830\n0.7669295\n0.6356146\n2.4873950\n0.3243243\n\n\n\n2022-10-02 09:29:51\n0.998 sec\n10.0\n0.4608627\n3.6270673\n0.7465098\n0.5845657\n1.8207831\n0.3167028\n0.4181826\n0.5099225\n0.7921973\n0.6704777\n2.9019608\n0.3175676\n\n\n\n2022-10-02 09:29:51\n1.029 sec\n15.0\n0.4337379\n2.1151426\n0.7820271\n0.6298664\n2.0008606\n0.2634989\n0.4101235\n0.4988519\n0.8029109\n0.7064394\n2.9019608\n0.2905405\n\n\n\n2022-10-02 09:29:51\n1.066 sec\n20.0\n0.4205906\n1.5358695\n0.7993009\n0.6530956\n2.0372399\n0.2817204\n0.4166082\n0.5082825\n0.7959369\n0.6982436\n2.9019608\n0.2837838\n\n\n\n2022-10-02 09:29:51\n1.101 sec\n25.0\n0.4182988\n1.3319800\n0.7995527\n0.6601665\n2.0008606\n0.2580645\n0.4136440\n0.4969478\n0.8015969\n0.7014673\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:51\n1.152 sec\n30.0\n0.4101680\n1.0450661\n0.8109461\n0.6788486\n2.0008606\n0.2451613\n0.4163121\n0.5049921\n0.7954316\n0.6840246\n2.9019608\n0.3040541\n\n\n\n2022-10-02 09:29:51\n1.201 sec\n35.0\n0.4106029\n0.9780555\n0.8084680\n0.6806083\n1.6807229\n0.2494624\n0.4147069\n0.5017193\n0.7985648\n0.6910329\n2.9019608\n0.2635135\n\n\n\n2022-10-02 09:29:51\n1.286 sec\n40.0\n0.4079850\n0.8385907\n0.8116614\n0.6841102\n2.2409639\n0.2580645\n0.4141854\n0.5000911\n0.8008894\n0.6925245\n2.9019608\n0.2837838\n\n\n\n2022-10-02 09:29:51\n1.340 sec\n42.0\n0.4054745\n0.8344361\n0.8153786\n0.7009708\n2.2409639\n0.2559140\n0.4150866\n0.5014100\n0.8009905\n0.6966438\n2.9019608\n0.3040541\n\n\n\n\n\n\n\n\nVariable Importances:\n\n\nvariable\nrelative_importance\nscaled_importance\npercentage\n\n\n\n\nGlucose\n929.6009521\n1.0\n0.3070341\n\n\nBMI\n478.2341614\n0.5144510\n0.1579540\n\n\nDiabetesPedigreeFunction\n369.8951416\n0.3979074\n0.1221712\n\n\nAge\n334.5044861\n0.3598366\n0.1104821\n\n\nPregnancies\n267.2053223\n0.2874409\n0.0882542\n\n\nBloodPressure\n243.6348572\n0.2620854\n0.0804692\n\n\nInsulin\n208.7045746\n0.2245099\n0.0689322\n\n\nSkinThickness\n195.8999023\n0.2107355\n0.0647030\n\n\n\n\n\n\n\n[tips]\nUse `model.explain()` to inspect the model.\n--\nUse `h2o.display.toggle_user_tips()` to switch on/off this section.\n\n:::\nautoml leaderboard:\n::: {.cell _uuid=‘4c44674f0b6c472c417265de6adfefb79104d9f0’ cell_id=‘00056-a43f750c-e6d1-48b2-bf95-d4f9cba29737’ outputId=‘5ad8cfbb-da4b-435d-bea1-b8fde5c267d8’ scrolled=‘true’ execution_count=24}\nlb = aml.leaderboard\nlb\n\n\n\n\nmodel_id\nauc\nlogloss\naucpr\nmean_per_class_error\nrmse\nmse\n\n\n\n\nDRF_1_AutoML_1_20221002_92947\n0.840694\n0.669853\n0.753861\n0.230155\n0.389285\n0.151543\n\n\nStackedEnsemble_BestOfFamily_1_AutoML_1_20221002_92947\n0.832736\n0.479362\n0.721094\n0.234889\n0.392271\n0.153877\n\n\nGBM_4_AutoML_1_20221002_92947\n0.832121\n0.473359\n0.7492\n0.235877\n0.390696\n0.152643\n\n\nGLM_1_AutoML_1_20221002_92947\n0.831638\n0.488798\n0.716653\n0.238204\n0.395915\n0.156749\n\n\nGBM_2_AutoML_1_20221002_92947\n0.829985\n0.480355\n0.727995\n0.238234\n0.394038\n0.155266\n\n\nStackedEnsemble_AllModels_1_AutoML_1_20221002_92947\n0.826027\n0.486947\n0.71757\n0.249879\n0.397346\n0.157884\n\n\nXRT_1_AutoML_1_20221002_92947\n0.825976\n0.555213\n0.72196\n0.232532\n0.399288\n0.159431\n\n\nGBM_3_AutoML_1_20221002_92947\n0.821655\n0.490684\n0.719058\n0.251904\n0.398949\n0.15916\n\n\nXGBoost_1_AutoML_1_20221002_92947\n0.820687\n0.492297\n0.703893\n0.248559\n0.402223\n0.161783\n\n\nXGBoost_2_AutoML_1_20221002_92947\n0.820264\n0.501107\n0.697644\n0.239886\n0.405695\n0.164588\n\n\n\n[12 rows x 7 columns]\n\n:::\n::: {.cell _uuid=‘e2360db29873b6076e2fbcbdfef307d700d19bb2’ cell_id=‘00058-2c28a740-535e-469f-91d7-4cb32b97c4bb’ outputId=‘3d1f4816-bb55-4abc-81a7-99a36d0482ad’ execution_count=28}\n#metalearner = h2o.get_model(aml.leader.metalearner()['name'])\n#metalearner.std_coef_plot()\n:::"
  },
  {
    "objectID": "posts/2021-01-02-SNP-SMA-EWMA.html",
    "href": "posts/2021-01-02-SNP-SMA-EWMA.html",
    "title": "Stock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages",
    "section": "",
    "text": "This post includes code and notes from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks.\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport pandas_datareader\nimport datetime\nimport pandas_datareader.data as web\n\nimport statsmodels.api as sm\nimport quandl\nstart = datetime.datetime(2019, 1, 1)\nend = pd.to_datetime('today')\nSP500 = quandl.get(\"MULTPL/SP500_REAL_PRICE_MONTH\",start_date = start,end_date = end)\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2019-01-01\n2607.39\n\n\n2019-02-01\n2754.86\n\n\n2019-03-01\n2803.98\n\n\n2019-04-01\n2903.80\n\n\n2019-05-01\n2854.71\n\n\n2019-05-31\n2752.08\n\n\n2019-06-01\n2890.17\n\n\n2019-07-01\n2996.11\n\n\n2019-08-01\n2897.50\n\n\n2019-09-01\n2982.16\n\n\n2019-10-01\n2977.68\n\n\n2019-11-01\n3104.90\n\n\n2019-12-01\n3176.75\n\n\n2019-12-31\n3230.58\n\n\n2020-01-01\n3278.20\n\n\n2020-01-31\n3225.04\n\n\n2020-02-01\n3277.31\n\n\n2020-02-28\n2954.81\n\n\n2020-03-01\n2652.39\n\n\n2020-03-31\n2584.59\n\n\n2020-04-01\n2761.98\n\n\n2020-04-30\n2912.43\n\n\n2020-05-01\n2919.61\n\n\n2020-06-01\n3104.66\n\n\n2020-06-30\n3100.29\n\n\n2020-07-01\n3207.62\n\n\n2020-07-31\n3271.12\n\n\n2020-08-01\n3391.71\n\n\n2020-08-31\n3500.31\n\n\n2020-09-01\n3365.52\n\n\n2020-09-30\n3363.00\n\n\n2020-10-01\n3418.70\n\n\n2020-11-01\n3429.33\n\n\n2020-11-30\n3621.63\n\n\n2020-12-01\n3662.45\nSP500\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2019-01-01\n2607.39\n\n\n2019-02-01\n2754.86\n\n\n2019-03-01\n2803.98\n\n\n2019-04-01\n2903.80\n\n\n2019-05-01\n2854.71\n\n\n2019-05-31\n2752.08\n\n\n2019-06-01\n2890.17\n\n\n2019-07-01\n2996.11\n\n\n2019-08-01\n2897.50\n\n\n2019-09-01\n2982.16\n\n\n2019-10-01\n2977.68\n\n\n2019-11-01\n3104.90\n\n\n2019-12-01\n3176.75\n\n\n2019-12-31\n3230.58\n\n\n2020-01-01\n3278.20\n\n\n2020-01-31\n3225.04\n\n\n2020-02-01\n3277.31\n\n\n2020-02-28\n2954.81\n\n\n2020-03-01\n2652.39\n\n\n2020-03-31\n2584.59\n\n\n2020-04-01\n2761.98\n\n\n2020-04-30\n2912.43\n\n\n2020-05-01\n2919.61\n\n\n2020-06-01\n3104.66\n\n\n2020-06-30\n3100.29\n\n\n2020-07-01\n3207.62\n\n\n2020-07-31\n3271.12\n\n\n2020-08-01\n3391.71\n\n\n2020-08-31\n3500.31\n\n\n2020-09-01\n3365.52\n\n\n2020-09-30\n3363.00\n\n\n2020-10-01\n3418.70\n\n\n2020-11-01\n3429.33\n\n\n2020-11-30\n3621.63\n\n\n2020-12-01\n3662.45\nSP500.dropna(inplace=True)\nSP500.index = pd.to_datetime(SP500.index)\nSP500.head()\n\n\n\n\n\n\n\n\nValue\n\n\nDate\n\n\n\n\n\n2019-01-01\n2607.39\n\n\n2019-02-01\n2754.86\n\n\n2019-03-01\n2803.98\n\n\n2019-04-01\n2903.80\n\n\n2019-05-01\n2854.71"
  },
  {
    "objectID": "posts/2021-01-02-SNP-SMA-EWMA.html#simple-moving-averages",
    "href": "posts/2021-01-02-SNP-SMA-EWMA.html#simple-moving-averages",
    "title": "Stock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages",
    "section": "Simple Moving Averages",
    "text": "Simple Moving Averages\n\nSP500['6-month-SMA']=SP500['Value'].rolling(window=6).mean()\nSP500['12-month-SMA']=SP500['Value'].rolling(window=12).mean()\n\n\nSP500.head()\n\n\n\n\n\n\n\n\nValue\n6-month-SMA\n12-month-SMA\n\n\nDate\n\n\n\n\n\n\n\n2019-01-01\n2607.39\nNaN\nNaN\n\n\n2019-02-01\n2754.86\nNaN\nNaN\n\n\n2019-03-01\n2803.98\nNaN\nNaN\n\n\n2019-04-01\n2903.80\nNaN\nNaN\n\n\n2019-05-01\n2854.71\nNaN\nNaN\n\n\n\n\n\n\n\n\nSP500.plot()\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "posts/2021-01-02-SNP-SMA-EWMA.html#exponentially-weighted-moving-averages",
    "href": "posts/2021-01-02-SNP-SMA-EWMA.html#exponentially-weighted-moving-averages",
    "title": "Stock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages",
    "section": "Exponentially-weighted moving averages",
    "text": "Exponentially-weighted moving averages\n\nSP500['EWMA12'] = SP500['Value'].ewm(span=12).mean()\n\n\nSP500[['Value','EWMA12']].plot()\n\n&lt;AxesSubplot:xlabel='Date'&gt;"
  },
  {
    "objectID": "posts/2020-12-23_Healthcare_Modeling_app.html",
    "href": "posts/2020-12-23_Healthcare_Modeling_app.html",
    "title": "Modeling Health Care Data App",
    "section": "",
    "text": "This notebook uses SMOTE and cross-validation.\nimport sys\nimport os\n\nfrom scipy import stats\nfrom datetime import datetime, date\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\n\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport pickle\nfrom sklearn.model_selection import GridSearchCV, train_test_split, cross_val_predict,cross_validate\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix"
  },
  {
    "objectID": "posts/2020-12-23_Healthcare_Modeling_app.html#data-prep",
    "href": "posts/2020-12-23_Healthcare_Modeling_app.html#data-prep",
    "title": "Modeling Health Care Data App",
    "section": "Data Prep",
    "text": "Data Prep\n\ndf = df.drop(columns = ['id'])\n\n\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n\n\npct_list = []\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    if round(pct_missing*100) &gt;0:\n        pct_list.append([col, round(pct_missing*100)])\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\ngender - 0%\nage - 0%\nhypertension - 0%\nheart_disease - 0%\never_married - 0%\nwork_type - 0%\nResidence_type - 0%\navg_glucose_level - 0%\nbmi - 3%\nsmoking_status - 0%\nstroke - 0%\n\n\n\ndf = df.fillna(df.mean())\n\n\ndf=df.dropna()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 43400 entries, 0 to 43399\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   gender             43400 non-null  int64  \n 1   age                43400 non-null  float64\n 2   hypertension       43400 non-null  int64  \n 3   heart_disease      43400 non-null  int64  \n 4   ever_married       43400 non-null  int64  \n 5   work_type          43400 non-null  int64  \n 6   Residence_type     43400 non-null  int64  \n 7   avg_glucose_level  43400 non-null  float64\n 8   bmi                43400 non-null  float64\n 9   smoking_status     43400 non-null  int64  \n 10  stroke             43400 non-null  int64  \ndtypes: float64(3), int64(8)\nmemory usage: 4.0 MB\n\n\n\nFeatures = ['age','heart_disease','ever_married']\nx = df[Features]\ny = df[\"stroke\"]\n\n\n# Train Test split\nX_train, X_test,y_train,y_test = train_test_split(x,y, test_size=0.2, random_state=2)\n\n\n#### Data Preprocessing\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, cross_validate\nimport xgboost as xgb\n\n\n\nclf = xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    missing=-999,\n    random_state=2021,\n    tree_method='auto'\n#    tree_method='hist'\n#    tree_method='gpu_hist'\n)\n\n\nkfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=42)\n\n\nparam_grid = { \n    'colsample_bytree':[.75,1],\n    'learning_rate':[0.01,0.05,0.1,0.3,0.5],\n    'max_depth':[1,2,3,5],\n    'subsample':[.75,1],\n    'n_estimators': list(range(50, 400, 50))\n}\n\n\ngrid_search = GridSearchCV(estimator=clf, scoring='roc_auc', param_grid=param_grid, n_jobs=-1, cv=kfold)\n\n\n%%time\ngrid_result = grid_search.fit(X_train, y_train)\n\n/home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n\n\n[12:38:13] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nCPU times: user 9.02 s, sys: 860 ms, total: 9.88 s\nWall time: 12min 43s\n\n\n\nprint(f'Best: {grid_result.best_score_} using {grid_result.best_params_}','\\n')\n\nBest: 0.8410524780191915 using {'colsample_bytree': 0.75, 'learning_rate': 0.05, 'max_depth': 1, 'n_estimators': 200, 'subsample': 0.75} \n\n\n\n\n#Set our final hyperparameters to the tuned values\nxgbcl = xgb.XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n         gamma=0.0, max_delta_step=0.0, min_child_weight=1.0,\n         missing=None, n_jobs=-1, objective='binary:logistic', random_state=42, reg_alpha=0.0,\n         reg_lambda=1.0, scale_pos_weight=1.0, tree_method='auto',\n         colsample_bytree = grid_result.best_params_['colsample_bytree'], \n         learning_rate = grid_result.best_params_['learning_rate'], \n         max_depth = grid_result.best_params_['max_depth'], \n         subsample = grid_result.best_params_['subsample'], \n         n_estimators = grid_result.best_params_['n_estimators'])\n\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n#refit the model on k-folds to get stable avg error metrics\nscores = cross_validate(estimator=xgbcl, X=X_train, y=y_train, cv=kfold, n_jobs=-1, \n                        scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'f1'])\n\nprint('Training 5-fold Cross Validation Results:\\n')\nprint('AUC: ', scores['test_roc_auc'].mean())\nprint('Accuracy: ', scores['test_accuracy'].mean())\nprint('Precision: ', scores['test_precision'].mean())\nprint('Recall: ', scores['test_recall'].mean())\nprint('F1: ', scores['test_f1'].mean(), '\\n')\n\nTraining 5-fold Cross Validation Results:\n\nAUC:  0.8429184127269972\nAccuracy:  0.9820852534562212\nPrecision:  0.0\nRecall:  0.0\nF1:  0.0 \n\n\n\n\nimport sklearn.metrics as metrics\n\n\n#Fit the final model\nxgbcl.fit(X_train, y_train)\n\n#Generate predictions against our training and test data\npred_train = xgbcl.predict(X_train)\nproba_train = xgbcl.predict_proba(X_train)\npred_test = xgbcl.predict(X_test)\nproba_test = xgbcl.predict_proba(X_test)\n\n# Print model report\nprint(\"Classification report (Test): \\n\")\nprint(metrics.classification_report(y_test, pred_test))\nprint(\"Confusion matrix (Test): \\n\")\nprint(metrics.confusion_matrix(y_test, pred_test)/len(y_test))\n\nprint ('\\nTrain Accuracy:', metrics.accuracy_score(y_train, pred_train))\nprint ('Test Accuracy:', metrics.accuracy_score(y_test, pred_test))\n\nprint ('\\nTrain AUC:', metrics.roc_auc_score(y_train, proba_train[:,1]))\nprint ('Test AUC:', metrics.roc_auc_score(y_test, proba_test[:,1]))\n\n# calculate the fpr and tpr for all thresholds of the classification\ntrain_fpr, train_tpr, train_threshold = metrics.roc_curve(y_train, proba_train[:,1])\ntest_fpr, test_tpr, test_threshold = metrics.roc_curve(y_test, proba_test[:,1])\n\ntrain_roc_auc = metrics.auc(train_fpr, train_tpr)\ntest_roc_auc = metrics.auc(test_fpr, test_tpr)\n\nimport matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=[7,5])\nplt.title('Receiver Operating Characteristic')\nplt.plot(train_fpr, train_tpr, 'b', label = 'Train AUC = %0.2f' % train_roc_auc)\nplt.plot(test_fpr, test_tpr, 'g', label = 'Test AUC = %0.2f' % test_roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n\n# plot feature importance\nxgb.plot_importance(xgbcl, importance_type='gain');\n\n[12:38:18] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\nClassification report (Test): \n\n              precision    recall  f1-score   support\n\n           0       0.98      1.00      0.99      8519\n           1       0.00      0.00      0.00       161\n\n    accuracy                           0.98      8680\n   macro avg       0.49      0.50      0.50      8680\nweighted avg       0.96      0.98      0.97      8680\n\nConfusion matrix (Test): \n\n[[0.98145161 0.        ]\n [0.01854839 0.        ]]\n\nTrain Accuracy: 0.9820852534562212\nTest Accuracy: 0.9814516129032258\n\nTrain AUC: 0.8465318377764562\nTest AUC: 0.8563193417126058\n\n\n/home/david/anaconda3/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n/home/david/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, msg_start, len(result))\n\n\n\n\n\n\n\n\n\npickle.dump(xgbcl, open('stroke_xgboost_model.pkl', 'wb'))\npickle.dump(scaler, open('scaler.pkl', 'wb'))\n\n\nmodel = pickle.load(open('stroke_xgboost_model.pkl', 'rb'))\nprint(model)\n\nXGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1.0,\n              colsample_bynode=1, colsample_bytree=0.75, gamma=0.0, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.05, max_delta_step=0.0, max_depth=1,\n              min_child_weight=1.0, missing=None, monotone_constraints='()',\n              n_estimators=200, n_jobs=-1, num_parallel_tree=1, random_state=42,\n              reg_alpha=0.0, reg_lambda=1.0, scale_pos_weight=1.0,\n              subsample=0.75, tree_method='auto', validate_parameters=1,\n              verbosity=None)"
  },
  {
    "objectID": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html",
    "href": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "",
    "text": "“A minimal example of using Pyspark for Linear Regression”"
  },
  {
    "objectID": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#bring-in-needed-imports",
    "href": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#bring-in-needed-imports",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Bring in needed imports",
    "text": "Bring in needed imports\n\nfrom pyspark.sql.functions import col\nfrom pyspark.sql.types import StringType,BooleanType,DateType,IntegerType\nfrom pyspark.sql.functions import *"
  },
  {
    "objectID": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#load-data-from-csv",
    "href": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#load-data-from-csv",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Load data from CSV",
    "text": "Load data from CSV\n\n#collapse-hide\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n\n\n\n\n_c0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\nAnhui\n147002.0\nnull\n1996\n2093.3\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\nAnhui\n151981.0\nnull\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\nAnhui\n174930.0\nnull\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\nAnhui\n285324.0\nnull\n1999\n2712.34\n26131\nnull\nnull\nnull\n1646891\nEast China\n1227364\n\n\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110\n\n\n\n\n\n\n\ndf.createOrReplaceTempView(\"fiscal_stats\")\n\nsums = spark.sql(\"\"\"\nselect year, sum(it) as total_yearly_it, sum(fr) as total_yearly_fr\nfrom fiscal_stats\ngroup by 1\norder by year asc\n\"\"\")\n\nsums.show()\n\n\n+----+---------------+---------------+\nyear|total_yearly_it|total_yearly_fr|\n+----+---------------+---------------+\n1996|       19825341|    2.9579215E7|\n1997|       21391321|    2.9110765E7|\n1998|       25511453|    3.8154711E7|\n1999|       31922107|    4.2128627E7|\n2000|       38721293|    4.8288092E7|\n2001|       50754944|    5.8910649E7|\n2002|       62375881|    6.2071474E7|\n2003|       69316709|    7.2479293E7|\n2004|       88626786|           null|\n2005|       98263665|           null|\n2006|      119517822|    1.3349148E8|\n2007|      153467611|   2.27385701E8|\n+----+---------------+---------------+"
  },
  {
    "objectID": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#describing-the-data",
    "href": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#describing-the-data",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Describing the Data",
    "text": "Describing the Data\n\ndf.describe().toPandas().transpose()\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n\n\n\n\nsummary\ncount\nmean\nstddev\nmin\nmax\n\n\n_c0\n360\n179.5\n104.06728592598157\n0\n359\n\n\nprovince\n360\nNone\nNone\nAnhui\nZhejiang\n\n\nspecific\n356\n583470.7303370787\n654055.3290782663\n8964.0\n3937966.0\n\n\ngeneral\n169\n309127.53846153844\n355423.5760674793\n0.0\n1737800.0\n\n\nyear\n360\n2001.5\n3.4568570586927794\n1996\n2007\n\n\ngdp\n360\n4428.653416666667\n4484.668659976412\n64.98\n31777.01\n\n\nfdi\n360\n196139.38333333333\n303043.97011891654\n2\n1743140\n\n\nrnr\n294\n0.0355944252244898\n0.16061503029299648\n0.0\n1.214285714\n\n\nrr\n296\n0.059688621057432424\n0.15673351824073453\n0.0\n0.84\n\n\ni\n287\n0.08376351662369343\n0.1838933104683607\n0.0\n1.05\n\n\nfr\n295\n2522449.0034013605\n3491329.8613106664\n#REF!\n9898522\n\n\nreg\n360\nNone\nNone\nEast China\nSouthwest China\n\n\nit\n360\n2165819.2583333333\n1769294.2935487411\n147897\n10533312"
  },
  {
    "objectID": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#cast-data-type",
    "href": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#cast-data-type",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Cast Data Type",
    "text": "Cast Data Type\n\ndf2 = df.withColumn(\"gdp\",col(\"gdp\").cast(IntegerType())) \\\n.withColumn(\"specific\",col(\"specific\").cast(IntegerType())) \\\n.withColumn(\"general\",col(\"general\").cast(IntegerType())) \\\n.withColumn(\"year\",col(\"year\").cast(IntegerType())) \\\n.withColumn(\"fdi\",col(\"fdi\").cast(IntegerType())) \\\n.withColumn(\"rnr\",col(\"rnr\").cast(IntegerType())) \\\n.withColumn(\"rr\",col(\"rr\").cast(IntegerType())) \\\n.withColumn(\"i\",col(\"i\").cast(IntegerType())) \\\n.withColumn(\"fr\",col(\"fr\").cast(IntegerType()))"
  },
  {
    "objectID": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#printschema",
    "href": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#printschema",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "printSchema",
    "text": "printSchema\n\ndf2.printSchema()\n\n\nroot\n-- _c0: integer (nullable = true)\n-- province: string (nullable = true)\n-- specific: integer (nullable = true)\n-- general: integer (nullable = true)\n-- year: integer (nullable = true)\n-- gdp: integer (nullable = true)\n-- fdi: integer (nullable = true)\n-- rnr: integer (nullable = true)\n-- rr: integer (nullable = true)\n-- i: integer (nullable = true)\n-- fr: integer (nullable = true)\n-- reg: string (nullable = true)\n-- it: integer (nullable = true)\n\n\n\n\n\nfrom pyspark.ml.feature import VectorAssembler\nfrom pyspark.ml.regression import LinearRegression\n\nassembler = VectorAssembler(inputCols=['gdp', 'fdi'], outputCol=\"features\")\ntrain_df = assembler.transform(df2) \n\n\n\n\n\n\ntrain_df.select(\"specific\", \"year\").show()\n\n\n+--------+----+\nspecific|year|\n+--------+----+\n  147002|1996|\n  151981|1997|\n  174930|1998|\n  285324|1999|\n  195580|2000|\n  250898|2001|\n  434149|2002|\n  619201|2003|\n  898441|2004|\n  898441|2005|\n 1457872|2006|\n 2213991|2007|\n  165957|1996|\n  165957|1997|\n  245198|1998|\n  388083|1999|\n  281769|2000|\n  441923|2001|\n  558569|2002|\n  642581|2003|\n+--------+----+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#linear-regression-in-pyspark",
    "href": "posts/2020-08-15-Pyspark-Fiscal-Data-Regression.html#linear-regression-in-pyspark",
    "title": "Pyspark Regression with Fiscal Data",
    "section": "Linear Regression in Pyspark",
    "text": "Linear Regression in Pyspark\n\nlr = LinearRegression(featuresCol = 'features', labelCol='it')\nlr_model = lr.fit(train_df)\n\ntrainingSummary = lr_model.summary\nprint(\"Coefficients: \" + str(lr_model.coefficients))\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\nprint(\"R2: %f\" % trainingSummary.r2)\n\n\nCoefficients: [495.05888709337756,-4.968141828763066]\nRMSE: 1234228.673087\nR2: 0.512023\n\n\n\n\nlr_predictions = lr_model.transform(train_df)\nlr_predictions.select(\"prediction\",\"it\",\"features\").show(5)\nfrom pyspark.ml.evaluation import RegressionEvaluator\nlr_evaluator = RegressionEvaluator(predictionCol=\"prediction\", \\\n                 labelCol=\"it\",metricName=\"r2\")\n\n\n\n+------------------+-------+----------------+\n        prediction|     it|        features|\n+------------------+-------+----------------+\n1732528.7382477913| 631930|[2093.0,50661.0]|\n1894133.7432895212| 657860|[2347.0,43443.0]|\n2069017.8229123235| 889463|[2542.0,27673.0]|\n2160838.7084181504|1227364|[2712.0,26131.0]|\n2226501.9982726825|1499110|[2902.0,31847.0]|\n+------------------+-------+----------------+\nonly showing top 5 rows\n\n\n\n\n\nprint(\"R Squared (R2) on test data = %g\" % lr_evaluator.evaluate(lr_predictions))\n\n\nR Squared (R2) on test data = 0.512023\n\n\n\n\nprint(\"numIterations: %d\" % trainingSummary.totalIterations)\nprint(\"objectiveHistory: %s\" % str(trainingSummary.objectiveHistory))\ntrainingSummary.residuals.show()\n\n\nnumIterations: 1\nobjectiveHistory: [0.0]\n+-------------------+\n          residuals|\n+-------------------+\n-1100598.7382477913|\n-1236273.7432895212|\n-1179554.8229123235|\n -933474.7084181504|\n -727391.9982726825|\n-222546.39659531135|\n -94585.30175113119|\n 108072.63313654158|\n 389732.58121094666|\n  621021.2194867637|\n  1885768.997742407|\n  3938310.059555837|\n  -554084.125169754|\n -615660.3899049093|\n -352195.3468934437|\n-348450.00565795833|\n -918476.5594253046|\n -710059.9133252408|\n-1148661.0062004486|\n  -911572.322055324|\n+-------------------+\nonly showing top 20 rows\n\n\n\n\n\npredictions = lr_model.transform(test_df)\npredictions.select(\"prediction\",\"it\",\"features\").show()\n\n\n+------------------+-------+---------------+\n        prediction|     it|       features|\n+------------------+-------+---------------+\n 976371.9212205639| 306114|   [64.0,679.0]|\n 990722.2032541803| 415547|   [91.0,481.0]|\n1016348.0830204486| 983251|  [139.0,106.0]|\n1036290.7062801318| 218361|  [184.0,576.0]|\n1034023.4471330958| 178668| [202.0,2826.0]|\n1060130.0768520113| 274994| [245.0,1856.0]|\n1023513.0851009073| 546541|[263.0,11020.0]|\n   1053250.6267921| 361358| [264.0,5134.0]|\n1123768.8091592425| 866691| [377.0,2200.0]|\n1128604.8330225947| 948521| [390.0,2522.0]|\n 810587.2575938476| 177748|[442.0,71715.0]|\n 1159703.254297337| 736165| [445.0,1743.0]|\n 1066975.770986663|1260633|[466.0,22500.0]|\n1288507.6625716756|1423771| [725.0,3718.0]|\n 1320055.238474972| 573905| [793.0,4144.0]|\n1188611.0570700848|2347862|[797.0,31000.0]|\n 1321857.482976733| 582711| [805.0,4977.0]|\n1033849.5995896922| 746784|[819.0,64343.0]|\n 1445051.792853667|1216605|[1029.0,2501.0]|\n1437887.1056682135|1258100|[1052.0,6235.0]|\n+------------------+-------+---------------+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.ml.regression import DecisionTreeRegressor\ndt = DecisionTreeRegressor(featuresCol ='features', labelCol = 'it')\ndt_model = dt.fit(train_df)\ndt_predictions = dt_model.transform(train_df)\ndt_evaluator = RegressionEvaluator(\n    labelCol=\"it\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = dt_evaluator.evaluate(dt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\n\nRoot Mean Squared Error (RMSE) on test data = 1.01114e+06\n\n\n\n\nfrom pyspark.ml.regression import GBTRegressor\ngbt = GBTRegressor(featuresCol = 'features', labelCol = 'it', maxIter=10)\ngbt_model = gbt.fit(train_df)\ngbt_predictions = gbt_model.transform(train_df)\ngbt_predictions.select('prediction', 'it', 'features').show(5)\n\n\ngbt_evaluator = RegressionEvaluator(\n    labelCol=\"it\", predictionCol=\"prediction\", metricName=\"rmse\")\nrmse = gbt_evaluator.evaluate(gbt_predictions)\nprint(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmse)\n\n\n+------------------+-------+----------------+\n        prediction|     it|        features|\n+------------------+-------+----------------+\n 1388898.308543053| 631930|[2093.0,50661.0]|\n 1388898.308543053| 657860|[2347.0,43443.0]|\n1649083.6277172007| 889463|[2542.0,27673.0]|\n1649083.6277172007|1227364|[2712.0,26131.0]|\n1649083.6277172007|1499110|[2902.0,31847.0]|\n+------------------+-------+----------------+\nonly showing top 5 rows\n\nRoot Mean Squared Error (RMSE) on test data = 778728\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-09-02-NLP_Ngram.html",
    "href": "posts/2020-09-02-NLP_Ngram.html",
    "title": "NLP ngrams With Python",
    "section": "",
    "text": "import pandas as pd\ndf=pd.read_csv('../../processed_data/nf_complete.csv')"
  },
  {
    "objectID": "posts/2020-09-02-NLP_Ngram.html#pre-processing-text",
    "href": "posts/2020-09-02-NLP_Ngram.html#pre-processing-text",
    "title": "NLP ngrams With Python",
    "section": "Pre-processing text",
    "text": "Pre-processing text\n\ndef preprocessor(text):\n    text = re.sub('&lt;[^&gt;]*&gt;', '', text)\n    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n    text = re.sub('[\\W]+', ' ', text.lower()) +\\\n        ' '.join(emoticons).replace('-', '')\n    return text"
  },
  {
    "objectID": "posts/2020-09-02-NLP_Ngram.html#find-total-word-count",
    "href": "posts/2020-09-02-NLP_Ngram.html#find-total-word-count",
    "title": "NLP ngrams With Python",
    "section": "Find Total Word Count",
    "text": "Find Total Word Count\n\ntext = \" \".join(review for review in df.abstract)\nprint (\"There are {} words in the combination of all abstracts.\".format(len(text)))\n\nThere are 272025 words in the combination of all abstracts.\n\n\n\nfrom urllib.request import urlopen\nfrom random import randint\n\ndef wordListSum(wordList):\n    sum = 0\n    for word, value in wordList.items():\n        sum += value\n    return sum\n\ndef retrieveRandomWord(wordList):\n    randIndex = randint(1, wordListSum(wordList))\n    for word, value in wordList.items():\n        randIndex -= value\n        if randIndex &lt;= 0:\n            return word\n\ndef buildWordDict(text):\n    # Remove newlines and quotes\n    text = text.replace('\\n', ' ');\n    text = text.replace('\"', '');\n\n    # Make sure punctuation marks are treated as their own \"words,\"\n    # so that they will be included in the Markov chain\n    punctuation = [',','.',';',':']\n    for symbol in punctuation:\n        text = text.replace(symbol, ' {} '.format(symbol));\n\n    words = text.split(' ')\n    # Filter out empty words\n    words = [word for word in words if word != '']\n\n    wordDict = {}\n    for i in range(1, len(words)):\n        if words[i-1] not in wordDict:\n                # Create a new dictionary for this word\n            wordDict[words[i-1]] = {}\n        if words[i] not in wordDict[words[i-1]]:\n            wordDict[words[i-1]][words[i]] = 0\n        wordDict[words[i-1]][words[i]] += 1\n    return wordDict\n\nwordDict = buildWordDict(text)\n\n#Generate a Markov chain of length 100\nlength = 100\nchain = ['Vietnam']\nfor i in range(0, length):\n    newWord = retrieveRandomWord(wordDict[chain[-1]])\n    chain.append(newWord)\n\nprint(' '.join(chain))\n\nVietnam (DRV) hampered the end it? This paper at all) and have on a viable combat jet aircraft into a corps composed overwhelmingly of radical visions of the argument in Iraqi Kurdistan , few reasons : the war experience . The group identified and how elite cues , the emphasis on the ongoing betrayal of 1971-79 under which I argue that expand our knowledge of biological weapons which it . This pushes against Axis material support for any single institutional prerogatives . My work fills an opposition organization at the generalizability of the Cold War strategy to escalate . ” and\n\n\n\ndef getFirstSentenceContaining(ngram, text):\n    #print(ngram)\n    sentences = text.upper().split(\". \")\n    for sentence in sentences: \n        if ngram in sentence:\n            return sentence+'\\n'\n    return \"\"\n\n\nprint(getFirstSentenceContaining('I', text))\n\n\n\nCIVIL-MILITARY RELATIONS ARE FREQUENTLY STUDIED AS IF THEY OPERATE ON TWO DISTINCT LEVELS OF ANALYSIS\n\n\n\n\n#text\n\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nfrom collections import Counter\n\ndef cleanSentence(sentence):\n    sentence = sentence.split(' ')\n    sentence = [word.strip(string.punctuation+string.whitespace) for word in sentence]\n    sentence = [word for word in sentence if len(word) &gt; 1 or (word.lower() == 'a' or word.lower() == 'i')]\n    return sentence\n\ndef cleanInput(content):\n    content = content.upper()\n    content = re.sub('\\n', ' ', content)\n    content = bytes(content, 'UTF-8')\n    content = content.decode('ascii', 'ignore')\n    sentences = content.split('. ')\n    return [cleanSentence(sentence) for sentence in sentences]\n\ndef getNgramsFromSentence(content, n):\n    output = []\n    for i in range(len(content)-n+1):\n        output.append(content[i:i+n])\n    return output\n\ndef getNgrams(content, n):\n    content = cleanInput(content)\n    ngrams = Counter()\n    ngrams_list = []\n    for sentence in content:\n        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n        ngrams_list.extend(newNgrams)\n        ngrams.update(newNgrams)\n    return(ngrams)\n\n\ncontent = str(text)\n\nngrams = getNgrams(content, 3)\n#print(ngrams)\n\n\nfrom urllib.request import urlopen\nfrom bs4 import BeautifulSoup\nimport re\nimport string\nfrom collections import Counter\n\ndef isCommon(ngram):\n    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n    for word in ngram:\n        if word in commonWords:\n            return True\n    return False\n\ndef getNgramsFromSentence(content, n):\n    output = []\n    for i in range(len(content)-n+1):\n        if not isCommon(content[i:i+n]):\n            output.append(content[i:i+n])\n    return output\n\nngrams = getNgrams(content, 3)\n#print(ngrams)\n\n\ndef getFirstSentenceContaining(ngram, content):\n    #print(ngram)\n    sentences = content.upper().split(\". \")\n    for sentence in sentences: \n        if ngram in sentence:\n            return sentence+'\\n'\n    return \"\"\n\n\nprint(getFirstSentenceContaining('SINO-JAPANESE WAR 1894-1895', content))\nprint(getFirstSentenceContaining('2ND VIETNAM WAR', content))\nprint(getFirstSentenceContaining('COLD WAR ARMY', content))\nprint(getFirstSentenceContaining('WORLD WAR II', content))\nprint(getFirstSentenceContaining('ARMS CONTROL AGREEMENTS', content))\n\n\n IN THE INTERNATIONAL SITUATION, THE GERMANS PROVIDED SUBSTANTIAL ADVANCES TO TECHNOLOGICAL DEVELOPMENT IN THE IMMEDIATE POST-WAR PERIOD.\n   THE HISTORIOGRAPHY ON THE 2ND VIETNAM WAR HAS FOCUSED MOSTLY ON THE AMERICAN SIDE, WHILE THE ‘OTHER SIDE,’ ESPECIALLY FOR THE EARLY VIETNAM WAR, 1964-1966, HAS NOT ATTRACTED MUCH ATTENTION\n\nCOLD WAR ARMY DURING THE PERIOD 1949 AND 1953 BY EXAMINING HOW SENIOR ARMY LEADERS WERE ABLE TO FUNDAMENTALLY BROADEN THE INSTITUTION’S INTELLECTUAL AND HISTORICAL FRAMEWORK OF “PREPAREDNESS” TO DESIGN A BLUEPRINT FOR A NEW TYPE OF GROUND FORCE THAT WOULD BE MORE ADEPT TO MEET THE CHALLENGES OF THE NEW NATURE OF WAR IMPOSED BY THE COLD WAR\n\n I ARGUE THAT A NORM PROTECTING STATES’ TERRITORIAL SOVEREIGNTY IS ONLY ENTRENCHED AFTER WORLD WAR II, ALTHOUGH IT CAN BE TRACED AT LEAST AS FAR BACK AS THE FOUNDING OF THE LEAGUE OF NATIONS\n\n IN EACH CASE I USE RIGOROUS ANALYSIS ON ORIGINAL DATA TO EXPLAIN THE WHY, WHEN, AND HOW OF THEIR DECISIONS ON THE BOMB, AS WELL AS OF THEIR DECISIONS ON RELATED ISSUES SUCH AS WHETHER TO BUILD UP NUCLEAR TECHNOLOGY, TO SEEK NUCLEAR SECURITY GUARANTEES, AND TO SIGN INTERNATIONAL NUCLEAR ARMS CONTROL AGREEMENTS.\n         THE OVERALL APPROACH INTRODUCED HERE HAS WIDE POTENTIAL APPLICABILITY\n\n\n\n\nfrom urllib.request import urlopen\nfrom random import randint\n\ndef wordListSum(wordList):\n    sum = 0\n    for word, value in wordList.items():\n        sum += value\n    return sum\n\ndef retrieveRandomWord(wordList):\n    randIndex = randint(1, wordListSum(wordList))\n    for word, value in wordList.items():\n        randIndex -= value\n        if randIndex &lt;= 0:\n            return word\n\ndef buildWordDict(text):\n    # Remove newlines and quotes\n    text = text.replace('\\n', ' ');\n    text = text.replace('\"', '');\n\n    # Make sure punctuation marks are treated as their own \"words,\"\n    # so that they will be included in the Markov chain\n    punctuation = [',','.',';',':']\n    for symbol in punctuation:\n        text = text.replace(symbol, ' {} '.format(symbol));\n\n    words = text.split(' ')\n    # Filter out empty words\n    words = [word for word in words if word != '']\n\n    wordDict = {}\n    for i in range(1, len(words)):\n        if words[i-1] not in wordDict:\n                # Create a new dictionary for this word\n            wordDict[words[i-1]] = {}\n        if words[i] not in wordDict[words[i-1]]:\n            wordDict[words[i-1]][words[i]] = 0\n        wordDict[words[i-1]][words[i]] += 1\n    return wordDict\n\nwordDict = buildWordDict(text)\n\n#Generate a Markov chain of length 100\nlength = 100\nchain = ['I']\nfor i in range(0, length):\n    newWord = retrieveRandomWord(wordDict[chain[-1]])\n    chain.append(newWord)\n\n#print(' '.join(chain))\n\n\nimport re\n\ndef getNgrams(content, n):\n    content = re.sub('\\n|[[\\d+\\]]', ' ', content)\n    content = bytes(content, 'UTF-8')\n    content = content.decode('ascii', 'ignore')\n    content = content.split(' ')\n    content = [word for word in content if word != '']\n    output = []\n    for i in range(len(content)-n+1):\n        output.append(content[i:i+n])\n    return output\n\n\nfrom collections import Counter\n\ndef getNgrams(content, n):\n    content = cleanInput(content)\n    ngrams = Counter()\n    ngrams_list = []\n    for sentence in content:\n        newNgrams = [' '.join(ngram) for ngram in getNgramsFromSentence(sentence, n)]\n        ngrams_list.extend(newNgrams)\n        ngrams.update(newNgrams)\n    return(ngrams)\n\n\n#print(getNgrams(content, 2))\n\n\ndef isCommon(ngram):\n    commonWords = ['THE', 'BE', 'AND', 'OF', 'A', 'IN', 'TO', 'HAVE', 'IT', 'I', 'THAT', 'FOR', 'YOU', 'HE', 'WITH', 'ON', 'DO', 'SAY', 'THIS', 'THEY', 'IS', 'AN', 'AT', 'BUT', 'WE', 'HIS', 'FROM', 'THAT', 'NOT', 'BY', 'SHE', 'OR', 'AS', 'WHAT', 'GO', 'THEIR', 'CAN', 'WHO', 'GET', 'IF', 'WOULD', 'HER', 'ALL', 'MY', 'MAKE', 'ABOUT', 'KNOW', 'WILL', 'AS', 'UP', 'ONE', 'TIME', 'HAS', 'BEEN', 'THERE', 'YEAR', 'SO', 'THINK', 'WHEN', 'WHICH', 'THEM', 'SOME', 'ME', 'PEOPLE', 'TAKE', 'OUT', 'INTO', 'JUST', 'SEE', 'HIM', 'YOUR', 'COME', 'COULD', 'NOW', 'THAN', 'LIKE', 'OTHER', 'HOW', 'THEN', 'ITS', 'OUR', 'TWO', 'MORE', 'THESE', 'WANT', 'WAY', 'LOOK', 'FIRST', 'ALSO', 'NEW', 'BECAUSE', 'DAY', 'MORE', 'USE', 'NO', 'MAN', 'FIND', 'HERE', 'THING', 'GIVE', 'MANY', 'WELL']\n    for word in ngram:\n        if word in commonWords:\n            return True\n    return False\n\ndef getNgramsFromSentence(text, n):\n    output = []\n    for i in range(len(text)-n+1):\n        if not isCommon(text[i:i+n]):\n            output.append(text[i:i+n])\n    return output\n\nngrams = getNgrams(text, 3)\n#print(ngrams)"
  },
  {
    "objectID": "posts/2020-10-05-StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
    "href": "posts/2020-10-05-StockMarketPortfolioAnaylsis_snp_pandas_datareader_sqlite-Copy1.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here\n\nimport pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf = pd.read_csv('https://stocks-snp-500.herokuapp.com/stocks/stocks_table.csv?_size=max')\n\n\ndf\n\n\n\n\n\n\n\n\nrowid\nDate\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\n\n\n0\n1\n2020-01-02 00:00:00.000000\n158.779999\n68.800003\nNaN\n112.980003\n\n\n1\n2\n2020-01-03 00:00:00.000000\n158.320007\n67.620003\nNaN\n112.190002\n\n\n2\n3\n2020-01-06 00:00:00.000000\n157.080002\n66.629997\nNaN\n112.589996\n\n\n3\n4\n2020-01-07 00:00:00.000000\n159.320007\n70.290001\nNaN\n112.290001\n\n\n4\n5\n2020-01-08 00:00:00.000000\n158.929993\n71.809998\nNaN\n112.839996\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n258\n259\n2021-01-11 00:00:00.000000\n218.470001\n344.980011\n295.000000\n131.750000\n\n\n259\n260\n2021-01-12 00:00:00.000000\n216.500000\n333.200012\n298.000000\n131.800003\n\n\n260\n261\n2021-01-13 00:00:00.000000\n214.020004\n360.000000\n295.000000\n132.100006\n\n\n261\n262\n2021-01-14 00:00:00.000000\n215.910004\n371.000000\n305.000000\n131.619995\n\n\n262\n263\n2021-01-15 00:00:00.000000\n213.520004\n397.709991\n306.820007\n130.679993\n\n\n\n\n263 rows × 6 columns\n\n\n\n\nstart = pd.to_datetime('2020-01-01')\nend = pd.to_datetime('today')\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Open')\n\nMSFT_stock['Open'].plot(label='Microsoft')\nZOOM_stock['Open'].plot(label='Zoom')\nSNOW_stock['Open'].plot(label='Snowflake')\nFXAIX_stock['Open'].plot(label='SNP_500')\nplt.legend()\n\nfig = plt.figure(figsize=(12, 6))\nplt.title('Volume')\n\nMSFT_stock['Volume'].plot(label='Microsoft')\nZOOM_stock['Volume'].plot(label='Zoom')\nSNOW_stock['Volume'].plot(label='Snowflake')\nFXAIX_stock['Volume'].plot(label='SNP_500')\n\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x7f7f95b65f40&gt;\n\n\n\n\n\n\n\n\n\nFXAIX_stock = web.DataReader('FXAIX', 'yahoo', start, end)\nFXAIX_stock.head()\n\nMSFT_stock = web.DataReader('MSFT', 'yahoo', start, end)\nMSFT_stock.head()\n\nZOOM_stock = web.DataReader('ZM', 'yahoo', start, end)\nZOOM_stock.head()\n\nSNOW_stock = web.DataReader('SNOW', 'yahoo', start, end)\nSNOW_stock.head()\n\n\n\n\n\n\n\n\nHigh\nLow\nOpen\nClose\nVolume\nAdj Close\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n2020-09-16\n319.0\n231.110001\n245.000000\n253.929993\n36099700\n253.929993\n\n\n2020-09-17\n241.5\n215.240005\n230.759995\n227.539993\n11907500\n227.539993\n\n\n2020-09-18\n249.0\n218.589996\n235.000000\n240.000000\n7475400\n240.000000\n\n\n2020-09-21\n241.5\n218.600006\n230.000000\n228.850006\n5524900\n228.850006\n\n\n2020-09-22\n239.0\n225.149994\n238.500000\n235.160004\n3889100\n235.160004\n\n\n\n\n\n\n\n\nstocks = pd.concat([MSFT_stock['Open'], ZOOM_stock['Open'], SNOW_stock['Open'], FXAIX_stock['Open']],\n                   axis = 1)\n\n\nstocks.reset_index(level=0, inplace=True)\n\n\nstocks\n\n\n\n\n\n\n\n\nDate\nOpen\nOpen\nOpen\nOpen\n\n\n\n\n0\n2020-01-02\n158.779999\n68.800003\nNaN\n112.980003\n\n\n1\n2020-01-03\n158.320007\n67.620003\nNaN\n112.190002\n\n\n2\n2020-01-06\n157.080002\n66.629997\nNaN\n112.589996\n\n\n3\n2020-01-07\n159.320007\n70.290001\nNaN\n112.290001\n\n\n4\n2020-01-08\n158.929993\n71.809998\nNaN\n112.839996\n\n\n...\n...\n...\n...\n...\n...\n\n\n258\n2021-01-11\n218.470001\n344.980011\n295.000000\n131.750000\n\n\n259\n2021-01-12\n216.500000\n333.200012\n298.000000\n131.800003\n\n\n260\n2021-01-13\n214.020004\n360.000000\n295.000000\n132.100006\n\n\n261\n2021-01-14\n215.910004\n371.000000\n305.000000\n131.619995\n\n\n262\n2021-01-15\n213.520004\n397.709991\n306.820007\n130.679993\n\n\n\n\n263 rows × 5 columns\n\n\n\n\nstocks.columns = ['Date','MSFT_stock','ZOOM_stock','SNOW_stock','FXAIX_stock']\n\n\nstocks\n\n\n\n\n\n\n\n\nDate\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\n\n\n0\n2020-01-02\n158.779999\n68.800003\nNaN\n112.980003\n\n\n1\n2020-01-03\n158.320007\n67.620003\nNaN\n112.190002\n\n\n2\n2020-01-06\n157.080002\n66.629997\nNaN\n112.589996\n\n\n3\n2020-01-07\n159.320007\n70.290001\nNaN\n112.290001\n\n\n4\n2020-01-08\n158.929993\n71.809998\nNaN\n112.839996\n\n\n...\n...\n...\n...\n...\n...\n\n\n258\n2021-01-11\n218.470001\n344.980011\n295.000000\n131.750000\n\n\n259\n2021-01-12\n216.500000\n333.200012\n298.000000\n131.800003\n\n\n260\n2021-01-13\n214.020004\n360.000000\n295.000000\n132.100006\n\n\n261\n2021-01-14\n215.910004\n371.000000\n305.000000\n131.619995\n\n\n262\n2021-01-15\n213.520004\n397.709991\n306.820007\n130.679993\n\n\n\n\n263 rows × 5 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///stocks.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nstocks_table = db.Table('stocks_table', metadata, \n    db.Column('Date',db.Integer, nullable=True, index=False),\n    db.Column('MSFT_stock',db.Integer, nullable=True),\n    db.Column('ZOOM_stock',db.Integer, nullable=True),\n    db.Column('SNOW_stock',db.Integer, nullable=True),\n    db.Column('FXAIX_stock', db.Numeric, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\nstocks_table\n\nTable('stocks_table', MetaData(bind=None), Column('Date', Integer(), table=&lt;stocks_table&gt;), Column('MSFT_stock', Integer(), table=&lt;stocks_table&gt;), Column('ZOOM_stock', Integer(), table=&lt;stocks_table&gt;), Column('SNOW_stock', Integer(), table=&lt;stocks_table&gt;), Column('FXAIX_stock', Numeric(), table=&lt;stocks_table&gt;), schema=None)\n\n\n\nstocks.to_sql('stocks_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT * FROM stocks_table LIMIT 10\").fetchall()\n\n[('2020-01-02 00:00:00.000000', 158.77999877929688, 68.80000305175781, None, 112.9800033569336),\n ('2020-01-03 00:00:00.000000', 158.32000732421875, 67.62000274658203, None, 112.19000244140625),\n ('2020-01-06 00:00:00.000000', 157.0800018310547, 66.62999725341797, None, 112.58999633789062),\n ('2020-01-07 00:00:00.000000', 159.32000732421875, 70.29000091552734, None, 112.29000091552734),\n ('2020-01-08 00:00:00.000000', 158.92999267578125, 71.80999755859375, None, 112.83999633789062),\n ('2020-01-09 00:00:00.000000', 161.83999633789062, 73.98999786376953, None, 113.62000274658203),\n ('2020-01-10 00:00:00.000000', 162.82000732421875, 73.08000183105469, None, 113.30000305175781),\n ('2020-01-13 00:00:00.000000', 161.75999450683594, 73.88999938964844, None, 114.08999633789062),\n ('2020-01-14 00:00:00.000000', 163.38999938964844, 74.31999969482422, None, 113.93000030517578),\n ('2020-01-15 00:00:00.000000', 162.6199951171875, 73.27999877929688, None, 114.13999938964844)]\n\n\n\nsql = \"\"\"\nSELECT\n  DATE(date) AS DATE\n, FXAIX_stock\n, MSFT_stock\n, SNOW_stock\n, row_number() OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_NBR\n, COUNT(*) OVER (PARTITION BY Date, MSFT_stock ORDER BY Date) AS REC_CNT\n, CASE WHEN FXAIX_stock &gt;= 120 THEN 'SNP_High' ELSE 'SNP_low' END AS SNP_HIGH_LOW\nFROM stocks_table\n--WHERE FXAIX_stock &gt;= 120\n\"\"\"\n\ncnxn = connection\n\n\nstocks = pd.read_sql(sql, cnxn)\n\n\nstocks.tail(30)\n\n\n\n\n\n\n\n\nDATE\nFXAIX_stock\nMSFT_stock\nSNOW_stock\nREC_NBR\nREC_CNT\nSNP_HIGH_LOW\n\n\n\n\n233\n2020-12-03\n127.540001\n214.610001\n290.540009\n1\n1\nSNP_High\n\n\n234\n2020-12-04\n128.669998\n214.220001\n335.399994\n1\n1\nSNP_High\n\n\n235\n2020-12-07\n128.419998\n214.369995\n393.500000\n1\n1\nSNP_High\n\n\n236\n2020-12-08\n128.789993\n213.970001\n388.799988\n1\n1\nSNP_High\n\n\n237\n2020-12-09\n127.769997\n215.160004\n393.399994\n1\n1\nSNP_High\n\n\n238\n2020-12-10\n127.610001\n211.770004\n362.000000\n1\n1\nSNP_High\n\n\n239\n2020-12-11\n126.870003\n210.050003\n360.399994\n1\n1\nSNP_High\n\n\n240\n2020-12-14\n126.339996\n213.100006\n352.489990\n1\n1\nSNP_High\n\n\n241\n2020-12-15\n127.970001\n215.169998\n308.980011\n1\n1\nSNP_High\n\n\n242\n2020-12-16\n128.199997\n214.750000\n328.429993\n1\n1\nSNP_High\n\n\n243\n2020-12-17\n128.940002\n219.869995\n333.820007\n1\n1\nSNP_High\n\n\n244\n2020-12-18\n128.500000\n218.589996\n332.769989\n1\n1\nSNP_High\n\n\n245\n2020-12-21\n128.000000\n217.550003\n329.000000\n1\n1\nSNP_High\n\n\n246\n2020-12-22\n127.750000\n222.690002\n349.890015\n1\n1\nSNP_High\n\n\n247\n2020-12-23\n127.839996\n223.110001\n341.160004\n1\n1\nSNP_High\n\n\n248\n2020-12-24\n128.309998\n221.419998\n334.100006\n1\n1\nSNP_High\n\n\n249\n2020-12-28\n129.429993\n224.449997\n324.869995\n1\n1\nSNP_High\n\n\n250\n2020-12-29\n129.139999\n226.309998\n305.250000\n1\n1\nSNP_High\n\n\n251\n2020-12-30\n129.330002\n225.229996\n304.000000\n1\n1\nSNP_High\n\n\n252\n2020-12-31\n130.169998\n221.699997\n299.700012\n1\n1\nSNP_High\n\n\n253\n2021-01-04\n128.259995\n222.529999\n285.410004\n1\n1\nSNP_High\n\n\n254\n2021-01-05\n129.179993\n217.259995\n280.619995\n1\n1\nSNP_High\n\n\n255\n2021-01-06\n129.919998\n212.169998\n279.989990\n1\n1\nSNP_High\n\n\n256\n2021-01-07\n131.880005\n214.039993\n272.589996\n1\n1\nSNP_High\n\n\n257\n2021-01-08\n132.619995\n218.679993\n315.000000\n1\n1\nSNP_High\n\n\n258\n2021-01-11\n131.750000\n218.470001\n295.000000\n1\n1\nSNP_High\n\n\n259\n2021-01-12\n131.800003\n216.500000\n298.000000\n1\n1\nSNP_High\n\n\n260\n2021-01-13\n132.100006\n214.020004\n295.000000\n1\n1\nSNP_High\n\n\n261\n2021-01-14\n131.619995\n215.910004\n305.000000\n1\n1\nSNP_High\n\n\n262\n2021-01-15\n130.679993\n213.520004\n306.820007\n1\n1\nSNP_High\n\n\n\n\n\n\n\n\nstocks['FXAIX_stock'].plot(figsize = (12, 8))\nplt.title('Total S&P 500 in 2020 Value')\n\nText(0.5, 1.0, 'Total S&P 500 in 2020 Value')"
  },
  {
    "objectID": "posts/2020-12-12_Healthcare_Modeling-Copy1.html",
    "href": "posts/2020-12-12_Healthcare_Modeling-Copy1.html",
    "title": "Modeling Health Care Data",
    "section": "",
    "text": "This notebook uses SMOTE and cross-validation.\nimport sys\nimport os\n\nfrom scipy import stats\nfrom datetime import datetime, date\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn import metrics\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.pipeline import make_pipeline\nimport xgboost as xgb\n\n\n%matplotlib inline\nplt.style.use(\"fivethirtyeight\")\nsns.set_context(\"notebook\")\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing"
  },
  {
    "objectID": "posts/2020-12-12_Healthcare_Modeling-Copy1.html#data-prep",
    "href": "posts/2020-12-12_Healthcare_Modeling-Copy1.html#data-prep",
    "title": "Modeling Health Care Data",
    "section": "Data Prep",
    "text": "Data Prep\n\ndf = df.drop(columns = ['id'])\n\n\n# Label Encoding\nfor f in df.columns:\n    if df[f].dtype=='object': \n        lbl = preprocessing.LabelEncoder()\n        lbl.fit(list(df[f].values))\n        df[f] = lbl.transform(list(df[f].values))\n\n\npct_list = []\nfor col in df.columns:\n    pct_missing = np.mean(df[col].isnull())\n    if round(pct_missing*100) &gt;0:\n        pct_list.append([col, round(pct_missing*100)])\n    print('{} - {}%'.format(col, round(pct_missing*100)))\n\ngender - 0%\nage - 0%\nhypertension - 0%\nheart_disease - 0%\never_married - 0%\nwork_type - 0%\nResidence_type - 0%\navg_glucose_level - 0%\nbmi - 3%\nsmoking_status - 0%\nstroke - 0%\n\n\n\ndf = df.fillna(df.mean())\n\n\ndf=df.dropna()\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 43400 entries, 0 to 43399\nData columns (total 11 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   gender             43400 non-null  int64  \n 1   age                43400 non-null  float64\n 2   hypertension       43400 non-null  int64  \n 3   heart_disease      43400 non-null  int64  \n 4   ever_married       43400 non-null  int64  \n 5   work_type          43400 non-null  int64  \n 6   Residence_type     43400 non-null  int64  \n 7   avg_glucose_level  43400 non-null  float64\n 8   bmi                43400 non-null  float64\n 9   smoking_status     43400 non-null  int64  \n 10  stroke             43400 non-null  int64  \ndtypes: float64(3), int64(8)\nmemory usage: 4.0 MB"
  },
  {
    "objectID": "posts/2021-01-18-datasette-csv-reading.html",
    "href": "posts/2021-01-18-datasette-csv-reading.html",
    "title": "Reading CSV from Datasette SQL Database",
    "section": "",
    "text": "import pandas as pd\nimport numpy as np\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf = pd.read_csv('https://stocks-snp-500.herokuapp.com/stocks/index_stock.csv?_size=max')\n\nHTTPError: HTTP Error 404: Not Found\n\n\n\ndf\n\n\n\n\n\n\n\n\nrowid\nDate\nMSFT_stock\nZOOM_stock\nSNOW_stock\nFXAIX_stock\n\n\n\n\n0\n1\n2020-01-02 00:00:00.000000\n158.779999\n68.800003\nNaN\n112.980003\n\n\n1\n2\n2020-01-03 00:00:00.000000\n158.320007\n67.620003\nNaN\n112.190002\n\n\n2\n3\n2020-01-06 00:00:00.000000\n157.080002\n66.629997\nNaN\n112.589996\n\n\n3\n4\n2020-01-07 00:00:00.000000\n159.320007\n70.290001\nNaN\n112.290001\n\n\n4\n5\n2020-01-08 00:00:00.000000\n158.929993\n71.809998\nNaN\n112.839996\n\n\n...\n...\n...\n...\n...\n...\n...\n\n\n258\n259\n2021-01-11 00:00:00.000000\n218.470001\n344.980011\n295.000000\n131.750000\n\n\n259\n260\n2021-01-12 00:00:00.000000\n216.500000\n333.200012\n298.000000\n131.800003\n\n\n260\n261\n2021-01-13 00:00:00.000000\n214.020004\n360.000000\n295.000000\n132.100006\n\n\n261\n262\n2021-01-14 00:00:00.000000\n215.910004\n371.000000\n305.000000\n131.619995\n\n\n262\n263\n2021-01-15 00:00:00.000000\n213.520004\n397.709991\n306.820007\n130.679993\n\n\n\n\n263 rows × 6 columns\n\n\n\n\ndf['FXAIX_stock'].plot()\n\n&lt;AxesSubplot:&gt;"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "",
    "text": "Spark uses Java Virtual Machine (JVM) objects Resilient Distributed Datasets (RDD) which are calculated and stored in memory.\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\nfrom pyspark.sql.functions import *\n\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#resilient-distributed-datasets",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#resilient-distributed-datasets",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "",
    "text": "Spark uses Java Virtual Machine (JVM) objects Resilient Distributed Datasets (RDD) which are calculated and stored in memory.\n\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.types import StructField,StringType,IntegerType,StructType, DoubleType, FloatType\nfrom pyspark.sql.functions import *\n\n\ndata_schema = [\nStructField(\"_c0\", IntegerType(), True)\n,StructField(\"province\", StringType(), True)\n,StructField(\"specific\", DoubleType(), True)\n,StructField(\"general\", DoubleType(), True)\n,StructField(\"year\", IntegerType(), True)\n,StructField(\"gdp\", FloatType(), True)\n,StructField(\"fdi\", FloatType(), True)\n,StructField(\"rnr\", DoubleType(), True)\n,StructField(\"rr\", FloatType(), True)\n,StructField(\"i\", FloatType(), True)\n,StructField(\"fr\", IntegerType(), True)\n,StructField(\"reg\", StringType(), True)\n,StructField(\"it\", IntegerType(), True)\n]\n\nfinal_struc = StructType(fields=data_schema)\n\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").schema(final_struc).option(\"header\", True).load(file_location)\n\n#df.printSchema()\n\ndf.show()\n\n\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n_c0|province| specific| general|year|    gdp|     fdi| rnr|       rr|        i|     fr|        reg|     it|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\n  0|   Anhui| 147002.0|    null|1996| 2093.3| 50661.0| 0.0|      0.0|      0.0|1128873| East China| 631930|\n  1|   Anhui| 151981.0|    null|1997|2347.32| 43443.0| 0.0|      0.0|      0.0|1356287| East China| 657860|\n  2|   Anhui| 174930.0|    null|1998|2542.96| 27673.0| 0.0|      0.0|      0.0|1518236| East China| 889463|\n  3|   Anhui| 285324.0|    null|1999|2712.34| 26131.0|null|     null|     null|1646891| East China|1227364|\n  4|   Anhui| 195580.0| 32100.0|2000|2902.09| 31847.0| 0.0|      0.0|      0.0|1601508| East China|1499110|\n  5|   Anhui| 250898.0|    null|2001|3246.71| 33672.0| 0.0|      0.0|      0.0|1672445| East China|2165189|\n  6|   Anhui| 434149.0| 66529.0|2002|3519.72| 38375.0| 0.0|      0.0|      0.0|1677840| East China|2404936|\n  7|   Anhui| 619201.0| 52108.0|2003|3923.11| 36720.0| 0.0|      0.0|      0.0|1896479| East China|2815820|\n  8|   Anhui| 898441.0|349699.0|2004| 4759.3| 54669.0| 0.0|      0.0|      0.0|   null| East China|3422176|\n  9|   Anhui| 898441.0|    null|2005|5350.17| 69000.0| 0.0|      0.0|0.3243243|   null| East China|3874846|\n 10|   Anhui|1457872.0|279052.0|2006| 6112.5|139354.0| 0.0|      0.0|0.3243243|3434548| East China|5167300|\n 11|   Anhui|2213991.0|178705.0|2007|7360.92|299892.0| 0.0|      0.0|0.3243243|4468640| East China|7040099|\n 12| Beijing| 165957.0|    null|1996| 1789.2|155290.0|null|     null|     null| 634562|North China| 508135|\n 13| Beijing| 165957.0|    null|1997|2077.09|159286.0| 0.0|      0.0|      0.6| 634562|North China| 569283|\n 14| Beijing| 245198.0|    null|1998|2377.18|216800.0| 0.0|      0.0|     0.53| 938788|North China| 695528|\n 15| Beijing| 388083.0|    null|1999|2678.82|197525.0| 0.0|      0.0|     0.53|   null|North China| 944047|\n 16| Beijing| 281769.0|188633.0|2000|3161.66|168368.0| 0.0|      0.0|     0.53|1667114|North China| 757990|\n 17| Beijing| 441923.0|    null|2001|3707.96|176818.0| 0.0|      0.0|     0.53|2093925|North China|1194728|\n 18| Beijing| 558569.0|280277.0|2002| 4315.0|172464.0| 0.0|      0.0|     0.53|2511249|North China|1078754|\n 19| Beijing| 642581.0|269596.0|2003|5007.21|219126.0| 0.0|0.7948718|      0.0|2823366|North China|1426600|\n+---+--------+---------+--------+----+-------+--------+----+---------+---------+-------+-----------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#using-topandas-to-look-at-the-data",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#using-topandas-to-look-at-the-data",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Using toPandas to look at the data",
    "text": "Using toPandas to look at the data\n\ndf.limit(10).toPandas()\n\n\n\n\n\n\n\n\n_c0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.300049\n50661.0\n0.0\n0.0\n0.000000\n1128873.0\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.320068\n43443.0\n0.0\n0.0\n0.000000\n1356287.0\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.959961\n27673.0\n0.0\n0.0\n0.000000\n1518236.0\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.340088\n26131.0\nNaN\nNaN\nNaN\n1646891.0\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.090088\n31847.0\n0.0\n0.0\n0.000000\n1601508.0\nEast China\n1499110\n\n\n5\n5\nAnhui\n250898.0\nNaN\n2001\n3246.709961\n33672.0\n0.0\n0.0\n0.000000\n1672445.0\nEast China\n2165189\n\n\n6\n6\nAnhui\n434149.0\n66529.0\n2002\n3519.719971\n38375.0\n0.0\n0.0\n0.000000\n1677840.0\nEast China\n2404936\n\n\n7\n7\nAnhui\n619201.0\n52108.0\n2003\n3923.110107\n36720.0\n0.0\n0.0\n0.000000\n1896479.0\nEast China\n2815820\n\n\n8\n8\nAnhui\n898441.0\n349699.0\n2004\n4759.299805\n54669.0\n0.0\n0.0\n0.000000\nNaN\nEast China\n3422176\n\n\n9\n9\nAnhui\n898441.0\nNaN\n2005\n5350.169922\n69000.0\n0.0\n0.0\n0.324324\nNaN\nEast China\n3874846"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#renaming-columns",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#renaming-columns",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Renaming Columns",
    "text": "Renaming Columns\n\ndf = df.withColumnRenamed(\"reg\",\"region\")\n\n\n\n\n\n\ndf.limit(10).toPandas()\n\n\n\n\n\n\n\n\n_c0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nregion\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.300049\n50661.0\n0.0\n0.0\n0.000000\n1128873.0\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.320068\n43443.0\n0.0\n0.0\n0.000000\n1356287.0\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.959961\n27673.0\n0.0\n0.0\n0.000000\n1518236.0\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.340088\n26131.0\nNaN\nNaN\nNaN\n1646891.0\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.090088\n31847.0\n0.0\n0.0\n0.000000\n1601508.0\nEast China\n1499110\n\n\n5\n5\nAnhui\n250898.0\nNaN\n2001\n3246.709961\n33672.0\n0.0\n0.0\n0.000000\n1672445.0\nEast China\n2165189\n\n\n6\n6\nAnhui\n434149.0\n66529.0\n2002\n3519.719971\n38375.0\n0.0\n0.0\n0.000000\n1677840.0\nEast China\n2404936\n\n\n7\n7\nAnhui\n619201.0\n52108.0\n2003\n3923.110107\n36720.0\n0.0\n0.0\n0.000000\n1896479.0\nEast China\n2815820\n\n\n8\n8\nAnhui\n898441.0\n349699.0\n2004\n4759.299805\n54669.0\n0.0\n0.0\n0.000000\nNaN\nEast China\n3422176\n\n\n9\n9\nAnhui\n898441.0\nNaN\n2005\n5350.169922\n69000.0\n0.0\n0.0\n0.324324\nNaN\nEast China\n3874846\n\n\n\n\n\n\n\n\n# df = df.toDF(*['year', 'region', 'province', 'gdp', 'fdi', 'specific', 'general', 'it', 'fr', 'rnr', 'rr', 'i', '_c0', 'specific_classification', 'provinceIndex', 'regionIndex'])"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#selecting-columns-of-interest",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#selecting-columns-of-interest",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Selecting Columns of Interest",
    "text": "Selecting Columns of Interest\n\ndf = df.select('year','region','province','gdp', 'fdi')\n\n\n\n\n\n\ndf.sort(\"gdp\").show()\n\n\n+----+---------------+--------+------+-------+\nyear|         region|province|   gdp|    fdi|\n+----+---------------+--------+------+-------+\n1996|Southwest China|   Tibet| 64.98|  679.0|\n1997|Southwest China|   Tibet| 77.24|   63.0|\n1998|Southwest China|   Tibet|  91.5|  481.0|\n1999|Southwest China|   Tibet|105.98|  196.0|\n2000|Southwest China|   Tibet| 117.8|    2.0|\n2001|Southwest China|   Tibet|139.16|  106.0|\n2002|Southwest China|   Tibet|162.04|  293.0|\n1996|Northwest China| Qinghai|184.17|  576.0|\n2003|Southwest China|   Tibet|185.09|  467.0|\n1997|Northwest China| Qinghai|202.79|  247.0|\n1996|Northwest China| Ningxia| 202.9| 2826.0|\n2004|Southwest China|   Tibet|220.34| 2699.0|\n1998|Northwest China| Qinghai|220.92| 1010.0|\n1997|Northwest China| Ningxia|224.59|  671.0|\n1999|Northwest China| Qinghai|239.38|  459.0|\n1998|Northwest China| Ningxia|245.44| 1856.0|\n2005|Southwest China|   Tibet| 248.8| 1151.0|\n2000|Northwest China| Qinghai|263.68|11020.0|\n1999|Northwest China| Ningxia|264.58| 5134.0|\n2006|Southwest China|   Tibet|290.76| 1522.0|\n+----+---------------+--------+------+-------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#sorting-rdds-by-columns",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#sorting-rdds-by-columns",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Sorting RDDs by Columns",
    "text": "Sorting RDDs by Columns\n\nfrom pyspark.sql import functions as F\ndf.sort(F.desc(\"gdp\")).show()\n\n\n+----+-------------------+---------+--------+---------+\nyear|             region| province|     gdp|      fdi|\n+----+-------------------+---------+--------+---------+\n2007|South Central China|Guangdong|31777.01|1712603.0|\n2006|South Central China|Guangdong|26587.76|1451065.0|\n2007|         East China| Shandong|25776.91|1101159.0|\n2005|South Central China|Guangdong|22557.37|1236400.0|\n2006|         East China| Shandong|21900.19|1000069.0|\n2007|         East China|  Jiangsu|21742.05|1743140.0|\n2004|South Central China|Guangdong|18864.62|1001158.0|\n2007|         East China| Zhejiang|18753.73|1036576.0|\n2006|         East China|  Jiangsu|18598.69|1318339.0|\n2005|         East China| Shandong|18366.87| 897000.0|\n2003|South Central China|Guangdong|15844.64| 782294.0|\n2006|         East China| Zhejiang|15718.47| 888935.0|\n2004|         East China| Shandong|15021.84| 870064.0|\n2007|South Central China|    Henan|15012.46| 306162.0|\n2005|         East China|  Jiangsu| 15003.6|1213800.0|\n2007|        North China|    Hebei|13607.32| 241621.0|\n2002|South Central China|Guangdong|13502.42|1133400.0|\n2005|         East China| Zhejiang|13417.68| 772000.0|\n2007|         East China| Shanghai|12494.01| 792000.0|\n2004|         East China|  Jiangsu|12442.87|1056365.0|\n+----+-------------------+---------+--------+---------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#casting-data-types",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#casting-data-types",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Casting Data Types",
    "text": "Casting Data Types\n\nfrom pyspark.sql.types import IntegerType, StringType, DoubleType\ndf = df.withColumn('gdp', F.col('gdp').cast(DoubleType()))\n\n\n\n\n\n\ndf = df.withColumn('province', F.col('province').cast(StringType()))\n\n\n\n\n\n\ndf.filter((df.gdp&gt;10000) & (df.region=='East China')).show()\n\n\n+----+----------+--------+----------------+---------+\nyear|    region|province|             gdp|      fdi|\n+----+----------+--------+----------------+---------+\n2003|East China| Jiangsu| 10606.849609375|1018960.0|\n2004|East China| Jiangsu|12442.8701171875|1056365.0|\n2005|East China| Jiangsu| 15003.599609375|1213800.0|\n2006|East China| Jiangsu| 18598.689453125|1318339.0|\n2007|East China| Jiangsu|  21742.05078125|1743140.0|\n2002|East China|Shandong|         10275.5| 473404.0|\n2003|East China|Shandong| 12078.150390625| 601617.0|\n2004|East China|Shandong|  15021.83984375| 870064.0|\n2005|East China|Shandong| 18366.869140625| 897000.0|\n2006|East China|Shandong| 21900.189453125|1000069.0|\n2007|East China|Shandong|  25776.91015625|1101159.0|\n2006|East China|Shanghai| 10572.240234375| 710700.0|\n2007|East China|Shanghai| 12494.009765625| 792000.0|\n2004|East China|Zhejiang|11648.7001953125| 668128.0|\n2005|East China|Zhejiang|   13417.6796875| 772000.0|\n2006|East China|Zhejiang|15718.4697265625| 888935.0|\n2007|East China|Zhejiang|  18753.73046875|1036576.0|\n+----+----------+--------+----------------+---------+"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#aggregating-using-groupby-.agg-and-summax",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#aggregating-using-groupby-.agg-and-summax",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Aggregating using groupBy, .agg and sum/max",
    "text": "Aggregating using groupBy, .agg and sum/max\n\nfrom pyspark.sql import functions as F\n\ndf.groupBy([\"region\",\"province\"]).agg(F.sum(\"gdp\") ,F.max(\"gdp\")).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|          sum(gdp)|          max(gdp)|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy([\"region\",\"province\"]).agg(F.sum(\"gdp\").alias(\"SumGDP\"),F.max(\"gdp\").alias(\"MaxGDP\")).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|               GDP|            MaxGDP|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.groupBy([\"region\",\"province\"]).agg(\n    F.sum(\"gdp\").alias(\"SumGDP\"),\\\n    F.max(\"gdp\").alias(\"MaxGDP\")\\\n    ).show()\n\n\n+-------------------+---------+------------------+------------------+\n             region| province|            SumGDP|            MaxGDP|\n+-------------------+---------+------------------+------------------+\nSouth Central China|    Hunan| 57190.69970703125|    9439.599609375|\n        North China|  Tianjin|30343.979858398438|    5252.759765625|\n    Northwest China| Xinjiang| 21946.75994873047| 3523.159912109375|\n        North China|  Beijing|56081.439208984375|   9846.8095703125|\nSouth Central China|Guangdong|  184305.376953125|   31777.009765625|\nSouth Central China|    Henan| 86507.60034179688|  15012.4599609375|\n         East China|  Jiangsu|129142.15966796875|    21742.05078125|\n    Northwest China|    Gansu| 16773.99005126953|  2703.97998046875|\n    Southwest China|  Guizhou|17064.130249023438| 2884.110107421875|\n    Southwest China|  Sichuan|64533.479736328125|  10562.3896484375|\nSouth Central China|   Hainan| 8240.570068359375|1254.1700439453125|\n         East China| Shandong| 147888.0283203125|    25776.91015625|\n    Southwest China|Chongqing|29732.549926757812|   4676.1298828125|\n    Northwest China|  Shaanxi|31896.409790039062|   5757.2900390625|\n         East China| Shanghai|  77189.4501953125|   12494.009765625|\n    Southwest China|    Tibet| 2045.120002746582|341.42999267578125|\n        North China|    Hebei|  83241.8994140625|     13607.3203125|\n    Northeast China|    Jilin|27298.250366210938|   4275.1201171875|\n         East China| Zhejiang|109657.81884765625|    18753.73046875|\n        North China|   Shanxi| 33806.52990722656|   6024.4501953125|\n+-------------------+---------+------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\ndf.limit(10).toPandas()\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\ngdp\nfdi\n\n\n\n\n0\n1996\nEast China\nAnhui\n2093.300049\n50661.0\n\n\n1\n1997\nEast China\nAnhui\n2347.320068\n43443.0\n\n\n2\n1998\nEast China\nAnhui\n2542.959961\n27673.0\n\n\n3\n1999\nEast China\nAnhui\n2712.340088\n26131.0\n\n\n4\n2000\nEast China\nAnhui\n2902.090088\n31847.0\n\n\n5\n2001\nEast China\nAnhui\n3246.709961\n33672.0\n\n\n6\n2002\nEast China\nAnhui\n3519.719971\n38375.0\n\n\n7\n2003\nEast China\nAnhui\n3923.110107\n36720.0\n\n\n8\n2004\nEast China\nAnhui\n4759.299805\n54669.0\n\n\n9\n2005\nEast China\nAnhui\n5350.169922\n69000.0"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#exponentials-using-exp",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#exponentials-using-exp",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Exponentials using exp",
    "text": "Exponentials using exp\n\ndf = df.withColumn(\"Exp_GDP\", F.exp(\"gdp\"))\ndf.show()\n\n\n+----+-----------+--------+-----------------+--------+--------+\nyear|     region|province|              gdp|     fdi| Exp_GDP|\n+----+-----------+--------+-----------------+--------+--------+\n1996| East China|   Anhui|2093.300048828125| 50661.0|Infinity|\n1997| East China|   Anhui|2347.320068359375| 43443.0|Infinity|\n1998| East China|   Anhui|  2542.9599609375| 27673.0|Infinity|\n1999| East China|   Anhui|2712.340087890625| 26131.0|Infinity|\n2000| East China|   Anhui|2902.090087890625| 31847.0|Infinity|\n2001| East China|   Anhui|  3246.7099609375| 33672.0|Infinity|\n2002| East China|   Anhui|3519.719970703125| 38375.0|Infinity|\n2003| East China|   Anhui|3923.110107421875| 36720.0|Infinity|\n2004| East China|   Anhui|  4759.2998046875| 54669.0|Infinity|\n2005| East China|   Anhui|   5350.169921875| 69000.0|Infinity|\n2006| East China|   Anhui|           6112.5|139354.0|Infinity|\n2007| East China|   Anhui|   7360.919921875|299892.0|Infinity|\n1996|North China| Beijing|1789.199951171875|155290.0|Infinity|\n1997|North China| Beijing|2077.090087890625|159286.0|Infinity|\n1998|North China| Beijing|2377.179931640625|216800.0|Infinity|\n1999|North China| Beijing|2678.820068359375|197525.0|Infinity|\n2000|North China| Beijing|3161.659912109375|168368.0|Infinity|\n2001|North China| Beijing|  3707.9599609375|176818.0|Infinity|\n2002|North China| Beijing|           4315.0|172464.0|Infinity|\n2003|North China| Beijing|  5007.2099609375|219126.0|Infinity|\n+----+-----------+--------+-----------------+--------+--------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#window-functions",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#window-functions",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Window functions",
    "text": "Window functions\n\nNote: Window functions\n\n\n# Window functions\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy(F.desc('gdp'))\ndf.withColumn(\"rank\",F.rank().over(windowSpec)).show()\n\n\n+----+-------------------+---------+-----------------+---------+--------+----+\nyear|             region| province|              gdp|      fdi| Exp_GDP|rank|\n+----+-------------------+---------+-----------------+---------+--------+----+\n2007|South Central China|Guangdong|  31777.009765625|1712603.0|Infinity|   1|\n2006|South Central China|Guangdong|  26587.759765625|1451065.0|Infinity|   2|\n2005|South Central China|Guangdong|  22557.369140625|1236400.0|Infinity|   3|\n2004|South Central China|Guangdong|  18864.619140625|1001158.0|Infinity|   4|\n2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity|   5|\n2002|South Central China|Guangdong|  13502.419921875|1133400.0|Infinity|   6|\n2001|South Central China|Guangdong|         12039.25|1193203.0|Infinity|   7|\n2000|South Central China|Guangdong|         10741.25|1128091.0|Infinity|   8|\n1999|South Central China|Guangdong|     9250.6796875|1165750.0|Infinity|   9|\n1998|South Central China|Guangdong|  8530.8798828125|1201994.0|Infinity|  10|\n1997|South Central China|Guangdong| 7774.52978515625|1171083.0|Infinity|  11|\n1996|South Central China|Guangdong| 6834.97021484375|1162362.0|Infinity|  12|\n2007|South Central China|    Hunan|   9439.599609375| 327051.0|Infinity|   1|\n2006|South Central China|    Hunan|   7688.669921875| 259335.0|Infinity|   2|\n2005|South Central China|    Hunan| 6596.10009765625| 207200.0|Infinity|   3|\n2004|South Central China|    Hunan| 5641.93994140625| 141800.0|Infinity|   4|\n2003|South Central China|    Hunan|   4659.990234375| 101835.0|Infinity|   5|\n2002|South Central China|    Hunan|  4151.5400390625|  90022.0|Infinity|   6|\n2001|South Central China|    Hunan| 3831.89990234375|  81011.0|Infinity|   7|\n2000|South Central China|    Hunan|3551.489990234375|  67833.0|Infinity|   8|\n+----+-------------------+---------+-----------------+---------+--------+----+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy('year')"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#lagging-variables",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#lagging-variables",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Lagging Variables",
    "text": "Lagging Variables\n\ndfWithLag = df.withColumn(\"lag_7\",F.lag(\"gdp\", 7).over(windowSpec))\n\n\n\n\n\n\ndf.filter(df.year&gt;'2000').show()\n\n\n+----+---------------+---------+------------------+--------+--------+\nyear|         region| province|               gdp|     fdi| Exp_GDP|\n+----+---------------+---------+------------------+--------+--------+\n2001|     East China|    Anhui|   3246.7099609375| 33672.0|Infinity|\n2002|     East China|    Anhui| 3519.719970703125| 38375.0|Infinity|\n2003|     East China|    Anhui| 3923.110107421875| 36720.0|Infinity|\n2004|     East China|    Anhui|   4759.2998046875| 54669.0|Infinity|\n2005|     East China|    Anhui|    5350.169921875| 69000.0|Infinity|\n2006|     East China|    Anhui|            6112.5|139354.0|Infinity|\n2007|     East China|    Anhui|    7360.919921875|299892.0|Infinity|\n2001|    North China|  Beijing|   3707.9599609375|176818.0|Infinity|\n2002|    North China|  Beijing|            4315.0|172464.0|Infinity|\n2003|    North China|  Beijing|   5007.2099609375|219126.0|Infinity|\n2004|    North China|  Beijing|   6033.2099609375|308354.0|Infinity|\n2005|    North China|  Beijing|  6969.52001953125|352638.0|Infinity|\n2006|    North China|  Beijing|  8117.77978515625|455191.0|Infinity|\n2007|    North China|  Beijing|   9846.8095703125|506572.0|Infinity|\n2001|Southwest China|Chongqing|1976.8599853515625| 25649.0|Infinity|\n2002|Southwest China|Chongqing| 2232.860107421875| 19576.0|Infinity|\n2003|Southwest China|Chongqing| 2555.719970703125| 26083.0|Infinity|\n2004|Southwest China|Chongqing|    3034.580078125| 40508.0|Infinity|\n2005|Southwest China|Chongqing| 3467.719970703125| 51600.0|Infinity|\n2006|Southwest China|Chongqing|  3907.22998046875| 69595.0|Infinity|\n+----+---------------+---------+------------------+--------+--------+\nonly showing top 20 rows"
  },
  {
    "objectID": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#looking-at-windows-within-the-data",
    "href": "posts/2020-08-22-Window functions and Pivot Tables with Pyspark.html#looking-at-windows-within-the-data",
    "title": "Window functions and Pivot Tables with Pyspark",
    "section": "Looking at windows within the data",
    "text": "Looking at windows within the data\n\nfrom pyspark.sql.window import Window\n\nwindowSpec = Window().partitionBy(['province']).orderBy('year').rowsBetween(-6,0)\n\n\n\n\n\n\ndfWithRoll = df.withColumn(\"roll_7_confirmed\",F.mean(\"gdp\").over(windowSpec))\n\n\n\n\n\n\ndfWithRoll.filter(dfWithLag.year&gt;'2001').show()\n\n\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\nyear|             region| province|               gdp|      fdi|             Exp_GDP|  roll_7_confirmed|\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\n2002|South Central China|Guangdong|   13502.419921875|1133400.0|            Infinity|  9810.56849888393|\n2003|South Central China|Guangdong|  15844.6396484375| 782294.0|            Infinity|11097.664132254464|\n2004|South Central China|Guangdong|   18864.619140625|1001158.0|            Infinity|12681.962611607143|\n2005|South Central China|Guangdong|   22557.369140625|1236400.0|            Infinity|14685.746791294643|\n2006|South Central China|Guangdong|   26587.759765625|1451065.0|            Infinity|17162.472516741072|\n2007|South Central China|Guangdong|   31777.009765625|1712603.0|            Infinity|  20167.5810546875|\n2002|South Central China|    Hunan|   4151.5400390625|  90022.0|            Infinity|3309.1999860491073|\n2003|South Central China|    Hunan|    4659.990234375| 101835.0|            Infinity| 3612.037179129464|\n2004|South Central China|    Hunan|  5641.93994140625| 141800.0|            Infinity|4010.9900251116073|\n2005|South Central China|    Hunan|  6596.10009765625| 207200.0|            Infinity|  4521.07146344866|\n2006|South Central China|    Hunan|    7688.669921875| 259335.0|            Infinity| 5160.232875279018|\n2007|South Central China|    Hunan|    9439.599609375| 327051.0|            Infinity| 6001.391392299107|\n2002|        North China|   Shanxi| 2324.800048828125|  21164.0|            Infinity|1749.4771379743304|\n2003|        North China|   Shanxi|  2855.22998046875|  21361.0|            Infinity| 1972.779994419643|\n2004|        North China|   Shanxi|   3571.3701171875|  62184.0|            Infinity| 2272.118582589286|\n2005|        North China|   Shanxi|  4230.52978515625|  27516.0|            Infinity| 2646.325701032366|\n2006|        North China|   Shanxi|  4878.60986328125|  47199.0|            Infinity|3105.1128278459823|\n2007|        North China|   Shanxi|   6024.4501953125| 134283.0|            Infinity| 3702.074288504464|\n2002|    Southwest China|    Tibet| 162.0399932861328|    293.0|2.360885537826244E70|108.38571493966239|\n2003|    Southwest China|    Tibet|185.08999633789062|    467.0|2.418600091901801E80|125.54428536551339|\n+----+-------------------+---------+------------------+---------+--------------------+------------------+\nonly showing top 20 rows\n\n\n\n\n\nfrom pyspark.sql.window import Window\nwindowSpec = Window().partitionBy(['province']).orderBy('year').rowsBetween(Window.unboundedPreceding,Window.currentRow)\n\n\n\n\n\n\ndfWithRoll = df.withColumn(\"cumulative_gdp\",F.sum(\"gdp\").over(windowSpec))\n\n\n\n\n\n\ndfWithRoll.filter(dfWithLag.year&gt;'1999').show()\n\n\n+----+-------------------+---------+-----------------+---------+--------+------------------+\nyear|             region| province|              gdp|      fdi| Exp_GDP|    cumulative_gdp|\n+----+-------------------+---------+-----------------+---------+--------+------------------+\n2000|South Central China|Guangdong|         10741.25|1128091.0|Infinity|  43132.3095703125|\n2001|South Central China|Guangdong|         12039.25|1193203.0|Infinity|  55171.5595703125|\n2002|South Central China|Guangdong|  13502.419921875|1133400.0|Infinity|  68673.9794921875|\n2003|South Central China|Guangdong| 15844.6396484375| 782294.0|Infinity|   84518.619140625|\n2004|South Central China|Guangdong|  18864.619140625|1001158.0|Infinity|   103383.23828125|\n2005|South Central China|Guangdong|  22557.369140625|1236400.0|Infinity|  125940.607421875|\n2006|South Central China|Guangdong|  26587.759765625|1451065.0|Infinity|    152528.3671875|\n2007|South Central China|Guangdong|  31777.009765625|1712603.0|Infinity|  184305.376953125|\n2000|South Central China|    Hunan|3551.489990234375|  67833.0|Infinity|  15180.9599609375|\n2001|South Central China|    Hunan| 3831.89990234375|  81011.0|Infinity| 19012.85986328125|\n2002|South Central China|    Hunan|  4151.5400390625|  90022.0|Infinity| 23164.39990234375|\n2003|South Central China|    Hunan|   4659.990234375| 101835.0|Infinity| 27824.39013671875|\n2004|South Central China|    Hunan| 5641.93994140625| 141800.0|Infinity|   33466.330078125|\n2005|South Central China|    Hunan| 6596.10009765625| 207200.0|Infinity| 40062.43017578125|\n2006|South Central China|    Hunan|   7688.669921875| 259335.0|Infinity| 47751.10009765625|\n2007|South Central China|    Hunan|   9439.599609375| 327051.0|Infinity| 57190.69970703125|\n2000|        North China|   Shanxi|1845.719970703125|  22472.0|Infinity|7892.0098876953125|\n2001|        North China|   Shanxi|2029.530029296875|  23393.0|Infinity| 9921.539916992188|\n2002|        North China|   Shanxi|2324.800048828125|  21164.0|Infinity|12246.339965820312|\n2003|        North China|   Shanxi| 2855.22998046875|  21361.0|Infinity|15101.569946289062|\n+----+-------------------+---------+-----------------+---------+--------+------------------+\nonly showing top 20 rows\n\n\n\n\n\nPivot Dataframes\n\nNote: Pivot Dataframes\n\n\npivoted_df = df.groupBy('year').pivot('province') \\\n                      .agg(F.sum('gdp').alias('gdp') , F.sum('fdi').alias('fdi'))\npivoted_df.limit(10).toPandas()\n\n\n\n\n\n\n\n\nyear\nAnhui_gdp\nAnhui_fdi\nBeijing_gdp\nBeijing_fdi\nChongqing_gdp\nChongqing_fdi\nFujian_gdp\nFujian_fdi\nGansu_gdp\nGansu_fdi\nGuangdong_gdp\nGuangdong_fdi\nGuangxi_gdp\nGuangxi_fdi\nGuizhou_gdp\nGuizhou_fdi\nHainan_gdp\nHainan_fdi\nHebei_gdp\nHebei_fdi\nHeilongjiang_gdp\nHeilongjiang_fdi\nHenan_gdp\nHenan_fdi\nHubei_gdp\nHubei_fdi\nHunan_gdp\nHunan_fdi\nJiangsu_gdp\nJiangsu_fdi\nJiangxi_gdp\nJiangxi_fdi\nJilin_gdp\nJilin_fdi\nLiaoning_gdp\nLiaoning_fdi\nNingxia_gdp\nNingxia_fdi\nQinghai_gdp\nQinghai_fdi\nShaanxi_gdp\nShaanxi_fdi\nShandong_gdp\nShandong_fdi\nShanghai_gdp\nShanghai_fdi\nShanxi_gdp\nShanxi_fdi\nSichuan_gdp\nSichuan_fdi\nTianjin_gdp\nTianjin_fdi\nTibet_gdp\nTibet_fdi\nXinjiang_gdp\nXinjiang_fdi\nYunnan_gdp\nYunnan_fdi\nZhejiang_gdp\nZhejiang_fdi\n\n\n\n\n0\n2003\n3923.110107\n36720.0\n5007.209961\n219126.0\n2555.719971\n26083.0\n4983.669922\n259903.0\n1399.829956\n2342.0\n15844.639648\n782294.0\n2821.110107\n41856.0\n1426.339966\n4521.0\n713.960022\n42125.0\n6921.290039\n96405.0\n4057.399902\n32180.0\n6867.700195\n53903.0\n4757.450195\n156886.0\n4659.990234\n101835.0\n10606.849609\n1018960.0\n2450.479980\n108197.0\n2348.540039\n24468.0\n5458.220215\n341168.0\n445.359985\n1743.0\n390.200012\n2522.0\n2587.719971\n33190.0\n12078.150391\n601617.0\n6694.229980\n546849.0\n2855.229980\n21361.0\n5333.089844\n41231.0\n2578.030029\n153473.0\n185.089996\n467.0\n1886.349976\n1534.0\n2556.020020\n8384.0\n9705.019531\n498055.0\n\n\n1\n2007\n7360.919922\n299892.0\n9846.809570\n506572.0\n4676.129883\n108534.0\n9248.530273\n406058.0\n2703.979980\n11802.0\n31777.009766\n1712603.0\n5823.410156\n68396.0\n2884.110107\n12651.0\n1254.170044\n112001.0\n13607.320312\n241621.0\n7104.000000\n208508.0\n15012.459961\n306162.0\n9333.400391\n276622.0\n9439.599609\n327051.0\n21742.050781\n1743140.0\n4820.529785\n280657.0\n4275.120117\n76064.0\n9304.519531\n598554.0\n919.109985\n5047.0\n797.349976\n31000.0\n5757.290039\n119516.0\n25776.910156\n1101159.0\n12494.009766\n792000.0\n6024.450195\n134283.0\n10562.389648\n149322.0\n5252.759766\n527776.0\n341.429993\n2418.0\n3523.159912\n12484.0\n4772.520020\n39453.0\n18753.730469\n1036576.0\n\n\n2\n2006\n6112.500000\n139354.0\n8117.779785\n455191.0\n3907.229980\n69595.0\n7583.850098\n322047.0\n2277.350098\n2954.0\n26587.759766\n1451065.0\n4746.160156\n44740.0\n2338.979980\n9384.0\n1065.670044\n74878.0\n11467.599609\n201434.0\n6211.799805\n170801.0\n12362.790039\n184526.0\n7617.470215\n244853.0\n7688.669922\n259335.0\n18598.689453\n1318339.0\n4056.760010\n242000.0\n3620.270020\n66100.0\n8047.259766\n359000.0\n725.900024\n3718.0\n648.500000\n27500.0\n4743.609863\n92489.0\n21900.189453\n1000069.0\n10572.240234\n710700.0\n4878.609863\n47199.0\n8690.240234\n120819.0\n4462.740234\n413077.0\n290.760010\n1522.0\n3045.260010\n10366.0\n3988.139893\n30234.0\n15718.469727\n888935.0\n\n\n3\n1997\n2347.320068\n43443.0\n2077.090088\n159286.0\n1509.750000\n38675.0\n2870.899902\n419666.0\n793.570007\n4144.0\n7774.529785\n1171083.0\n1817.250000\n87986.0\n805.789978\n4977.0\n411.160004\n70554.0\n3953.780029\n110064.0\n2667.500000\n73485.0\n4041.090088\n69204.0\n2856.469971\n79019.0\n2849.270020\n91702.0\n6004.209961\n507208.0\n1409.739990\n30068.0\n1346.790039\n45155.0\n3157.689941\n167142.0\n224.589996\n671.0\n202.789993\n247.0\n1363.599976\n62816.0\n6537.069824\n249294.0\n3438.790039\n422536.0\n1476.000000\n26592.0\n3241.469971\n24846.0\n1264.630005\n251135.0\n77.239998\n63.0\n1039.849976\n2472.0\n1676.170044\n16566.0\n4686.109863\n150345.0\n\n\n4\n2004\n4759.299805\n54669.0\n6033.209961\n308354.0\n3034.580078\n40508.0\n5763.350098\n474801.0\n1688.489990\n3539.0\n18864.619141\n1001158.0\n3433.500000\n29579.0\n1677.800049\n6533.0\n819.659973\n64343.0\n8477.629883\n162341.0\n4750.600098\n123639.0\n8553.790039\n87367.0\n5633.240234\n207126.0\n5641.939941\n141800.0\n12442.870117\n1056365.0\n2807.409912\n161202.0\n2662.080078\n19059.0\n6002.540039\n282410.0\n537.109985\n6689.0\n466.100006\n22500.0\n3175.580078\n52664.0\n15021.839844\n870064.0\n8072.830078\n654100.0\n3571.370117\n62184.0\n6379.629883\n70129.0\n3110.969971\n247243.0\n220.339996\n2699.0\n2209.090088\n4586.0\n3081.909912\n14200.0\n11648.700195\n668128.0\n\n\n5\n1996\n2093.300049\n50661.0\n1789.199951\n155290.0\n1315.119995\n21878.0\n2484.250000\n407876.0\n722.520020\n9002.0\n6834.970215\n1162362.0\n1697.900024\n66618.0\n723.179993\n3138.0\n389.679993\n78960.0\n3452.969971\n123652.0\n2370.500000\n54841.0\n3634.689941\n52566.0\n2499.770020\n68878.0\n2540.129883\n70344.0\n5155.250000\n478058.0\n1169.729980\n28818.0\n1137.229980\n39876.0\n2793.370117\n140405.0\n202.899994\n2826.0\n184.169998\n576.0\n1215.839966\n33008.0\n5883.799805\n259041.0\n2957.550049\n471578.0\n1292.109985\n13802.0\n2871.649902\n22519.0\n1121.930054\n200587.0\n64.980003\n679.0\n900.929993\n6639.0\n1517.689941\n18000.0\n4188.529785\n152021.0\n\n\n6\n1998\n2542.959961\n27673.0\n2377.179932\n216800.0\n1602.380005\n43107.0\n3159.909912\n421211.0\n887.669983\n3864.0\n8530.879883\n1201994.0\n1911.300049\n88613.0\n858.390015\n4535.0\n442.130005\n71715.0\n4256.009766\n142868.0\n2774.399902\n52639.0\n4308.240234\n61654.0\n3114.020020\n97294.0\n3025.530029\n81816.0\n6680.339844\n543511.0\n1605.770020\n47768.0\n1464.339966\n40227.0\n3582.459961\n220470.0\n245.440002\n1856.0\n220.919998\n1010.0\n1458.400024\n30010.0\n7021.350098\n220274.0\n3801.090088\n360150.0\n1611.079956\n24451.0\n3474.090088\n37248.0\n1374.599976\n211361.0\n91.500000\n481.0\n1106.949951\n2167.0\n1831.329956\n14568.0\n5052.620117\n131802.0\n\n\n7\n2001\n3246.709961\n33672.0\n3707.959961\n176818.0\n1976.859985\n25649.0\n4072.850098\n391804.0\n1125.369995\n7439.0\n12039.250000\n1193203.0\n2279.340088\n38416.0\n1133.270020\n2829.0\n579.169983\n46691.0\n5516.759766\n66989.0\n3390.100098\n34114.0\n5533.009766\n45729.0\n3880.530029\n118860.0\n3831.899902\n81011.0\n8553.690430\n642550.0\n2003.069946\n22724.0\n1951.510010\n33701.0\n4669.060059\n204446.0\n337.440002\n1680.0\n300.130005\n3649.0\n2010.619995\n35174.0\n9195.040039\n352093.0\n5210.120117\n429159.0\n2029.530029\n23393.0\n4293.490234\n58188.0\n1919.089966\n213348.0\n139.160004\n106.0\n1491.599976\n2035.0\n2138.310059\n6457.0\n6898.339844\n221162.0\n\n\n8\n2005\n5350.169922\n69000.0\n6969.520020\n352638.0\n3467.719971\n51600.0\n6554.689941\n260800.0\n1933.979980\n2000.0\n22557.369141\n1236400.0\n3984.100098\n37866.0\n2005.420044\n10768.0\n918.750000\n68400.0\n10012.110352\n191000.0\n5513.700195\n145000.0\n10587.419922\n123000.0\n6590.189941\n218500.0\n6596.100098\n207200.0\n15003.599609\n1213800.0\n3456.699951\n205238.0\n3122.010010\n45266.0\n6672.000000\n540679.0\n612.609985\n14100.0\n543.320007\n26600.0\n3933.719971\n62800.0\n18366.869141\n897000.0\n9247.660156\n685000.0\n4230.529785\n27516.0\n7385.100098\n88686.0\n3905.639893\n332885.0\n248.800003\n1151.0\n2604.189941\n4700.0\n3462.729980\n17352.0\n13417.679688\n772000.0\n\n\n9\n2000\n2902.090088\n31847.0\n3161.659912\n168368.0\n1791.000000\n24436.0\n3764.540039\n343191.0\n1052.880005\n6235.0\n10741.250000\n1128091.0\n2080.040039\n52466.0\n1029.920044\n2501.0\n526.820007\n43080.0\n5043.959961\n67923.0\n3151.399902\n30086.0\n5052.990234\n56403.0\n3545.389893\n94368.0\n3551.489990\n67833.0\n7697.819824\n607756.0\n1853.650024\n32080.0\n1672.959961\n30120.0\n4171.689941\n106173.0\n295.019989\n1741.0\n263.679993\n11020.0\n1804.000000\n28842.0\n8337.469727\n297119.0\n4771.169922\n316014.0\n1845.719971\n22472.0\n3928.199951\n43694.0\n1701.880005\n116601.0\n117.800003\n2.0\n1363.560059\n1911.0\n2011.189941\n12812.0\n6141.029785\n161266.0\n\n\n\n\n\n\n\n\npivoted_df.columns\n\n\nOut[55]: ['year',\n 'Anhui_gdp',\n 'Anhui_fdi',\n 'Beijing_gdp',\n 'Beijing_fdi',\n 'Chongqing_gdp',\n 'Chongqing_fdi',\n 'Fujian_gdp',\n 'Fujian_fdi',\n 'Gansu_gdp',\n 'Gansu_fdi',\n 'Guangdong_gdp',\n 'Guangdong_fdi',\n 'Guangxi_gdp',\n 'Guangxi_fdi',\n 'Guizhou_gdp',\n 'Guizhou_fdi',\n 'Hainan_gdp',\n 'Hainan_fdi',\n 'Hebei_gdp',\n 'Hebei_fdi',\n 'Heilongjiang_gdp',\n 'Heilongjiang_fdi',\n 'Henan_gdp',\n 'Henan_fdi',\n 'Hubei_gdp',\n 'Hubei_fdi',\n 'Hunan_gdp',\n 'Hunan_fdi',\n 'Jiangsu_gdp',\n 'Jiangsu_fdi',\n 'Jiangxi_gdp',\n 'Jiangxi_fdi',\n 'Jilin_gdp',\n 'Jilin_fdi',\n 'Liaoning_gdp',\n 'Liaoning_fdi',\n 'Ningxia_gdp',\n 'Ningxia_fdi',\n 'Qinghai_gdp',\n 'Qinghai_fdi',\n 'Shaanxi_gdp',\n 'Shaanxi_fdi',\n 'Shandong_gdp',\n 'Shandong_fdi',\n 'Shanghai_gdp',\n 'Shanghai_fdi',\n 'Shanxi_gdp',\n 'Shanxi_fdi',\n 'Sichuan_gdp',\n 'Sichuan_fdi',\n 'Tianjin_gdp',\n 'Tianjin_fdi',\n 'Tibet_gdp',\n 'Tibet_fdi',\n 'Xinjiang_gdp',\n 'Xinjiang_fdi',\n 'Yunnan_gdp',\n 'Yunnan_fdi',\n 'Zhejiang_gdp',\n 'Zhejiang_fdi']\n\n\n\nnewColnames = [x.replace(\"-\",\"_\") for x in pivoted_df.columns]\n\n\n\n\n\n\npivoted_df = pivoted_df.toDF(*newColnames)\n\n\n\n\n\n\nexpression = \"\"\ncnt=0\nfor column in pivoted_df.columns:\n    if column!='year':\n        cnt +=1\n        expression += f\"'{column}' , {column},\"\n        \nexpression = f\"stack({cnt}, {expression[:-1]}) as (Type,Value)\"\n\n\n\n\n\n\n\nUnpivoting RDDs\n\nunpivoted_df = pivoted_df.select('year',F.expr(expression))\nunpivoted_df.show()\n\n\n+----+-------------+------------------+\nyear|         Type|             Value|\n+----+-------------+------------------+\n2003|    Anhui_gdp| 3923.110107421875|\n2003|    Anhui_fdi|           36720.0|\n2003|  Beijing_gdp|   5007.2099609375|\n2003|  Beijing_fdi|          219126.0|\n2003|Chongqing_gdp| 2555.719970703125|\n2003|Chongqing_fdi|           26083.0|\n2003|   Fujian_gdp|    4983.669921875|\n2003|   Fujian_fdi|          259903.0|\n2003|    Gansu_gdp|1399.8299560546875|\n2003|    Gansu_fdi|            2342.0|\n2003|Guangdong_gdp|  15844.6396484375|\n2003|Guangdong_fdi|          782294.0|\n2003|  Guangxi_gdp| 2821.110107421875|\n2003|  Guangxi_fdi|           41856.0|\n2003|  Guizhou_gdp|1426.3399658203125|\n2003|  Guizhou_fdi|            4521.0|\n2003|   Hainan_gdp| 713.9600219726562|\n2003|   Hainan_fdi|           42125.0|\n2003|    Hebei_gdp|   6921.2900390625|\n2003|    Hebei_fdi|           96405.0|\n+----+-------------+------------------+\nonly showing top 20 rows\n\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2020-08-19-Pyspark-Filtering.html",
    "href": "posts/2020-08-19-Pyspark-Filtering.html",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "",
    "text": "from pyspark.sql import SparkSession\n\n# Load data from a CSV\nfile_location = \"/FileStore/tables/df_panel_fix.csv\"\ndf = spark.read.format(\"CSV\").option(\"inferSchema\", True).option(\"header\", True).load(file_location)\ndisplay(df.take(5))\n\n\n\n\n\n\n_c0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\nAnhui\n147002.0\nnull\n1996\n2093.3\n50661\n0.0\n0.0\n0.0\n1128873\nEast China\n631930\n\n\n1\nAnhui\n151981.0\nnull\n1997\n2347.32\n43443\n0.0\n0.0\n0.0\n1356287\nEast China\n657860\n\n\n2\nAnhui\n174930.0\nnull\n1998\n2542.96\n27673\n0.0\n0.0\n0.0\n1518236\nEast China\n889463\n\n\n3\nAnhui\n285324.0\nnull\n1999\n2712.34\n26131\nnull\nnull\nnull\n1646891\nEast China\n1227364\n\n\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.0\n1601508\nEast China\n1499110"
  },
  {
    "objectID": "posts/2020-08-19-Pyspark-Filtering.html#filtering-on-values-in-a-column",
    "href": "posts/2020-08-19-Pyspark-Filtering.html#filtering-on-values-in-a-column",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "Filtering on values in a column",
    "text": "Filtering on values in a column\n\ndf.filter(\"specific&lt;10000\").show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n269|Shanghai|  9834.0|   null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n\n\n\n\n\ndf.filter(\"specific&lt;10000\").select('province').show()\n\n\n+--------+\nprovince|\n+--------+\nShanghai|\nShanghai|\n+--------+\n\n\n\n\n\ndf.filter(\"specific&lt;10000\").select(['province','year']).show()\n\n\n+--------+----+\nprovince|year|\n+--------+----+\nShanghai|2000|\nShanghai|2001|\n+--------+----+\n\n\n\n\n\ndf.filter(df[\"specific\"] &lt; 10000).show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n269|Shanghai|  9834.0|   null|2001|5210.12|429159|0.0|0.0|0.44|2947285|East China|1053917|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+"
  },
  {
    "objectID": "posts/2020-08-19-Pyspark-Filtering.html#filtering-on-values-in-2-columns",
    "href": "posts/2020-08-19-Pyspark-Filtering.html#filtering-on-values-in-2-columns",
    "title": "Dataframe Filitering and Operations with Pyspark",
    "section": "Filtering on values in 2+ columns",
    "text": "Filtering on values in 2+ columns\n\ndf.filter((df[\"specific\"] &lt; 55000) & (df['gdp'] &gt; 200) ).show()\n\n\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n_c0|province|specific|general|year|     gdp|   fdi| rnr|  rr|   i|     fr|                reg|     it|\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n 98|  Hainan| 54462.0|   null|1998|  442.13| 71715|null|null|null| 236461|South Central China| 177748|\n216| Ningxia| 32088.0|   null|1996|   202.9|  2826|null|null|null|  90805|    Northwest China| 178668|\n217| Ningxia| 44267.0|   null|1997|  224.59|   671|null|null|null| 102083|    Northwest China| 195295|\n268|Shanghai|  8964.0|   null|2000| 4771.17|316014| 0.0| 0.0|0.44|2224124|         East China|1212473|\n269|Shanghai|  9834.0|   null|2001| 5210.12|429159| 0.0| 0.0|0.44|2947285|         East China|1053917|\n270|Shanghai| 19985.0|   null|2002| 5741.03|427229| 0.0| 0.0|0.44|3380397|         East China|1572208|\n271|Shanghai| 23547.0|   null|2003| 6694.23|546849| 0.0|0.53| 0.0|4461153|         East China|2031496|\n272|Shanghai| 29943.0|   null|2004| 8072.83|654100| 0.0|0.53| 0.0|   null|         East China|2703643|\n273|Shanghai| 29943.0|   null|2005| 9247.66|685000| 0.0|0.53| 0.0|   null|         East China|2140461|\n274|Shanghai| 42928.0|   null|2006|10572.24|710700| 0.0|0.53| 0.0|8175966|         East China|2239987|\n302| Tianjin| 39364.0|   null|1998|  1374.6|211361|null|null|null| 540178|        North China| 361723|\n303| Tianjin| 45463.0|   null|1999| 1500.95|176399| 0.0| 0.0| 0.0| 605662|        North China| 422522|\n304| Tianjin| 51821.0|   null|2000| 1701.88|116601| 0.0| 0.0| 0.0| 757464|        North China| 547120|\n305| Tianjin| 35084.0|   null|2001| 1919.09|213348| 0.0| 0.0| 0.0| 942763|        North China| 688810|\n+---+--------+--------+-------+----+--------+------+----+----+----+-------+-------------------+-------+\n\n\n\n\n\ndf.filter((df[\"specific\"] &lt; 55000) | (df['gdp'] &gt; 20000) ).show()\n\n\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\n_c0| province| specific| general|year|     gdp|    fdi|                 rnr|  rr|          i|      fr|                reg|     it|\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\n 69|Guangdong|1491588.0|    null|2005|22557.37|1236400|0.027027027000000002| 0.0|        0.0|    null|South Central China|4327217|\n 70|Guangdong|1897575.0|498913.0|2006|26587.76|1451065|0.027027027000000002| 0.0|        0.0|16804703|South Central China|4559252|\n 71|Guangdong| 859482.0|     0.0|2007|31777.01|1712603|0.027027027000000002| 0.0|        0.0|27858007|South Central China|4947824|\n 98|   Hainan|  54462.0|    null|1998|  442.13|  71715|                null|null|       null|  236461|South Central China| 177748|\n179|  Jiangsu|1188989.0|     0.0|2007|21742.05|1743140|                 0.0| 0.0|0.275862069|22377276|         East China|3557071|\n216|  Ningxia|  32088.0|    null|1996|   202.9|   2826|                null|null|       null|   90805|    Northwest China| 178668|\n217|  Ningxia|  44267.0|    null|1997|  224.59|    671|                null|null|       null|  102083|    Northwest China| 195295|\n228|  Qinghai|  37976.0|    null|1996|  184.17|    576|                null|null|       null|   73260|    Northwest China| 218361|\n262| Shandong|1204547.0|112137.0|2006|21900.19|1000069|                 0.0| 0.0|        0.0|11673659|         East China|5304833|\n263| Shandong|2121243.0|581800.0|2007|25776.91|1101159|                 0.0| 0.0|        0.0|16753980|         East China|6357869|\n268| Shanghai|   8964.0|    null|2000| 4771.17| 316014|                 0.0| 0.0|       0.44| 2224124|         East China|1212473|\n269| Shanghai|   9834.0|    null|2001| 5210.12| 429159|                 0.0| 0.0|       0.44| 2947285|         East China|1053917|\n270| Shanghai|  19985.0|    null|2002| 5741.03| 427229|                 0.0| 0.0|       0.44| 3380397|         East China|1572208|\n271| Shanghai|  23547.0|    null|2003| 6694.23| 546849|                 0.0|0.53|        0.0| 4461153|         East China|2031496|\n272| Shanghai|  29943.0|    null|2004| 8072.83| 654100|                 0.0|0.53|        0.0|    null|         East China|2703643|\n273| Shanghai|  29943.0|    null|2005| 9247.66| 685000|                 0.0|0.53|        0.0|    null|         East China|2140461|\n274| Shanghai|  42928.0|    null|2006|10572.24| 710700|                 0.0|0.53|        0.0| 8175966|         East China|2239987|\n302|  Tianjin|  39364.0|    null|1998|  1374.6| 211361|                null|null|       null|  540178|        North China| 361723|\n303|  Tianjin|  45463.0|    null|1999| 1500.95| 176399|                 0.0| 0.0|        0.0|  605662|        North China| 422522|\n304|  Tianjin|  51821.0|    null|2000| 1701.88| 116601|                 0.0| 0.0|        0.0|  757464|        North China| 547120|\n+---+---------+---------+--------+----+--------+-------+--------------------+----+-----------+--------+-------------------+-------+\nonly showing top 20 rows\n\n\n\n\n\ndf.filter((df[\"specific\"] &lt; 55000) & ~(df['gdp'] &gt; 20000) ).show()\n\n\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n_c0|province|specific|general|year|     gdp|   fdi|        rnr|  rr|   i|     fr|                reg|     it|\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n 98|  Hainan| 54462.0|   null|1998|  442.13| 71715|       null|null|null| 236461|South Central China| 177748|\n216| Ningxia| 32088.0|   null|1996|   202.9|  2826|       null|null|null|  90805|    Northwest China| 178668|\n217| Ningxia| 44267.0|   null|1997|  224.59|   671|       null|null|null| 102083|    Northwest China| 195295|\n228| Qinghai| 37976.0|   null|1996|  184.17|   576|       null|null|null|  73260|    Northwest China| 218361|\n268|Shanghai|  8964.0|   null|2000| 4771.17|316014|        0.0| 0.0|0.44|2224124|         East China|1212473|\n269|Shanghai|  9834.0|   null|2001| 5210.12|429159|        0.0| 0.0|0.44|2947285|         East China|1053917|\n270|Shanghai| 19985.0|   null|2002| 5741.03|427229|        0.0| 0.0|0.44|3380397|         East China|1572208|\n271|Shanghai| 23547.0|   null|2003| 6694.23|546849|        0.0|0.53| 0.0|4461153|         East China|2031496|\n272|Shanghai| 29943.0|   null|2004| 8072.83|654100|        0.0|0.53| 0.0|   null|         East China|2703643|\n273|Shanghai| 29943.0|   null|2005| 9247.66|685000|        0.0|0.53| 0.0|   null|         East China|2140461|\n274|Shanghai| 42928.0|   null|2006|10572.24|710700|        0.0|0.53| 0.0|8175966|         East China|2239987|\n302| Tianjin| 39364.0|   null|1998|  1374.6|211361|       null|null|null| 540178|        North China| 361723|\n303| Tianjin| 45463.0|   null|1999| 1500.95|176399|        0.0| 0.0| 0.0| 605662|        North China| 422522|\n304| Tianjin| 51821.0|   null|2000| 1701.88|116601|        0.0| 0.0| 0.0| 757464|        North China| 547120|\n305| Tianjin| 35084.0|   null|2001| 1919.09|213348|        0.0| 0.0| 0.0| 942763|        North China| 688810|\n312|   Tibet| 18829.0|   null|1996|   64.98|   679|0.181818182| 0.0| 0.0|  27801|    Southwest China| 306114|\n313|   Tibet| 25185.0|   null|1997|   77.24|    63|0.181818182| 0.0| 0.0|  33787|    Southwest China| 346368|\n314|   Tibet| 48197.0|   null|1998|    91.5|   481|        0.0|0.24| 0.0|   3810|    Southwest China| 415547|\n+---+--------+--------+-------+----+--------+------+-----------+----+----+-------+-------------------+-------+\n\n\n\n\n\ndf.filter(df[\"specific\"] == 8964.0).show()\n\n\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n_c0|province|specific|general|year|    gdp|   fdi|rnr| rr|   i|     fr|       reg|     it|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n268|Shanghai|  8964.0|   null|2000|4771.17|316014|0.0|0.0|0.44|2224124|East China|1212473|\n+---+--------+--------+-------+----+-------+------+---+---+----+-------+----------+-------+\n\n\n\n\n\ndf.filter(df[\"province\"] == \"Zhejiang\").show()\n\n\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n_c0|province| specific| general|year|     gdp|    fdi|        rnr|         rr|          i|      fr|       reg|     it|\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n348|Zhejiang| 273253.0|    null|1996| 4188.53| 152021|        0.0|        0.0|        0.0| 1291252|East China| 740327|\n349|Zhejiang| 330558.0|    null|1997| 4686.11| 150345|        0.0|        0.0|        0.0| 1432453|East China| 814253|\n350|Zhejiang| 426756.0|    null|1998| 5052.62| 131802|        0.0|        0.0|        0.0| 1761084|East China| 923455|\n351|Zhejiang| 586457.0|    null|1999| 5443.92| 123262|        0.0|        0.0|        0.0| 2146200|East China|1001703|\n352|Zhejiang| 408151.0|    null|2000| 6141.03| 161266|        0.0|        0.0|        0.0| 2955508|East China|1135215|\n353|Zhejiang| 358714.0|    null|2001| 6898.34| 221162|        0.0|        0.0|        0.0| 4436868|East China|1203372|\n354|Zhejiang| 365437.0|321686.0|2002| 8003.67| 307610|        0.0|        0.0|        0.0| 4958329|East China|1962633|\n355|Zhejiang| 391292.0|260313.0|2003| 9705.02| 498055|1.214285714|0.035714286|0.035714286| 6217715|East China|2261631|\n356|Zhejiang| 656175.0|276652.0|2004| 11648.7| 668128|1.214285714|0.035714286|0.035714286|    null|East China|3162299|\n357|Zhejiang| 656175.0|    null|2005|13417.68| 772000|1.214285714|0.035714286|0.035714286|    null|East China|2370200|\n358|Zhejiang|1017303.0|394795.0|2006|15718.47| 888935|1.214285714|0.035714286|0.035714286|11537149|East China|2553268|\n359|Zhejiang| 844647.0|     0.0|2007|18753.73|1036576|0.047619048|        0.0|        0.0|16494981|East China|2939778|\n+---+--------+---------+--------+----+--------+-------+-----------+-----------+-----------+--------+----------+-------+\n\n\n\n\n\ndf.filter(df[\"specific\"] == 8964.0).collect()\n\n\nOut[15]: [Row(_c0=268, province='Shanghai', specific=8964.0, general=None, year=2000, gdp=4771.17, fdi=316014, rnr=0.0, rr=0.0, i=0.44, fr='2224124', reg='East China', it=1212473)]\n\n\n\nresult = df.filter(df[\"specific\"] == 8964.0).collect()\n\n\n\n\n\n\ntype(result[0])\n\n\nOut[17]: pyspark.sql.types.Row\n\n\n\nrow = result[0]\n\n\n\n\n\n\nrow.asDict()\n\n\nOut[19]: {'_c0': 268,\n 'province': 'Shanghai',\n 'specific': 8964.0,\n 'general': None,\n 'year': 2000,\n 'gdp': 4771.17,\n 'fdi': 316014,\n 'rnr': 0.0,\n 'rr': 0.0,\n 'i': 0.44,\n 'fr': '2224124',\n 'reg': 'East China',\n 'it': 1212473}\n\n\n\nfor item in result[0]:\n    print(item)\n\n\n268\nShanghai\n8964.0\nNone\n2000\n4771.17\n316014\n0.0\n0.0\n0.44\n2224124\nEast China\n1212473\n\n\n\nThis post includes code adapted from Spark and Python for Big Data udemy course and Spark and Python for Big Data notebooks."
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html",
    "href": "posts/2021-05-31-pandas-interoperability.html",
    "title": "Pandas Interoperability",
    "section": "",
    "text": "#code adapted from https://github.com/thomasjpfan/ml-workshop-intermediate-1-of-2\nimport pandas as pd\nurl = 'https://raw.githubusercontent.com/davidrkearney/Kearney_Data_Science/master/_notebooks/df_panel_fix.csv'\ndf = pd.read_csv(url, error_bad_lines=False)\ndf\n\nimport pandas as pd\nimport sklearn\nfrom sklearn.datasets import fetch_openml\nfrom sklearn.model_selection import train_test_split\n\n\n\ndf.columns\n\nsklearn.set_config(display='diagram')\n\ndf=df.dropna()\n\ndf.isnull().sum()\n\n\n\nX, y = df.drop(['specific', 'Unnamed: 0'], axis = 1), df['specific']\n\nX\n\n\n_ = X.hist(figsize=(30, 15), layout=(5, 8))\nimport numpy as np\nimport sklearn\nsklearn.set_config(display='diagram')"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#categorical-data",
    "href": "posts/2021-05-31-pandas-interoperability.html#categorical-data",
    "title": "Pandas Interoperability",
    "section": "Categorical Data",
    "text": "Categorical Data\n\nOridinalEncoder\n\nfrom sklearn.preprocessing import OrdinalEncoder\n\n\nord_encoder = OrdinalEncoder()\nord_encoder.fit_transform(df)\n\narray([[  0.,   0.,  17., ...,  24.,   0.,  28.],\n       [  1.,   0.,  48., ...,  29.,   0.,  60.],\n       [  2.,   0.,  64., ...,  40.,   0.,  67.],\n       ...,\n       [115.,  27.,  44., ...,  97.,   0.,  53.],\n       [116.,  27.,  84., ...,   4.,   0.,  64.],\n       [117.,  27.,  78., ...,  25.,   0.,  71.]])\n\n\n\nord_encoder.categories_\n\n[array([  4,   6,   7,  10,  11,  16,  18,  19,  22,  23,  34,  35,  40,\n         42,  43,  46,  47,  52,  54,  58,  64,  66,  67,  70,  71,  76,\n         78,  79,  82,  83,  88,  90,  91,  94,  95, 107, 112, 119, 124,\n        126, 127, 130, 131, 136, 138, 139, 142, 143, 148, 150, 151, 154,\n        155, 160, 162, 163, 166, 167, 172, 174, 175, 178, 179, 184, 186,\n        187, 190, 191, 196, 198, 199, 202, 203, 220, 222, 223, 226, 227,\n        232, 234, 235, 239, 244, 246, 247, 250, 251, 258, 259, 262, 263,\n        280, 282, 283, 292, 294, 295, 298, 310, 316, 318, 319, 322, 323,\n        328, 330, 331, 334, 335, 340, 342, 343, 346, 347, 354, 355, 358,\n        359]),\n array(['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong',\n        'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan',\n        'Hubei', 'Hunan', 'Jiangsu', 'Jiangxi', 'Jilin', 'Ningxia',\n        'Qinghai', 'Shaanxi', 'Shandong', 'Shanxi', 'Sichuan', 'Tianjin',\n        'Tibet', 'Xinjiang', 'Yunnan', 'Zhejiang'], dtype=object),\n array([  72939.,   91405.,   96825.,  103041.,  107046.,  107687.,\n         119536.,  127819.,  133858.,  137190.,  142650.,  144839.,\n         147749.,  148812.,  160637.,  179235.,  181409.,  195580.,\n         207615.,  217707.,  223984.,  228043.,  237948.,  248903.,\n         251539.,  254002.,  262197.,  265770.,  271297.,  271499.,\n         281769.,  295133.,  319710.,  331999.,  337894.,  340372.,\n         353776.,  354775.,  359275.,  365437.,  367568.,  369552.,\n         370049.,  379186.,  391292.,  395775.,  397517.,  430577.,\n         434149.,  436189.,  447709.,  458201.,  469514.,  472786.,\n         484715.,  487155.,  510656.,  518022.,  531994.,  558569.,\n         575550.,  591088.,  601485.,  615593.,  619201.,  642581.,\n         675931.,  681676.,  684767.,  685732.,  688887.,  714572.,\n         752279.,  753552.,  761081.,  821750.,  833423.,  833430.,\n         844647.,  859482.,  875877.,  909559.,  978069.,  985851.,\n        1017303., 1035872., 1097470., 1188989., 1204547., 1207353.,\n        1224179., 1235386., 1246484., 1315102., 1331590., 1358528.,\n        1388043., 1457872., 1550764., 1562694., 1791403., 1890650.,\n        1897575., 1956261., 2018158., 2022957., 2045869., 2121243.,\n        2213991., 2225220., 2482173., 2663667., 2669238., 2922784.,\n        2981235., 3156087., 3847672., 3860764.]),\n array([      0.,    2990.,    8115.,   11755.,   11767.,   17400.,\n          20842.,   26300.,   27302.,   27387.,   29646.,   30705.,\n          32100.,   32119.,   32868.,   34842.,   36670.,   36946.,\n          40604.,   44623.,   45683.,   50097.,   52108.,   53900.,\n          56070.,   58533.,   59263.,   60560.,   60906.,   62000.,\n          66100.,   66529.,   68142.,   71807.,   80361.,   80609.,\n          81879.,   86256.,   88007.,   93323.,   95648.,  100000.,\n         100900.,  107658.,  108624.,  112137.,  113000.,  114418.,\n         116000.,  119658.,  123317.,  123546.,  124582.,  124647.,\n         129791.,  135765.,  143800.,  145000.,  149549.,  150000.,\n         153640.,  154364.,  165071.,  173552.,  173556.,  178705.,\n         179252.,  188633.,  197539.,  229895.,  241282.,  260313.,\n         264185.,  269596.,  279052.,  280277.,  302600.,  309582.,\n         317700.,  320627.,  321686.,  363054.,  394795.,  400000.,\n         405966.,  423049.,  429591.,  434318.,  447900.,  460668.,\n         498913.,  516342.,  527300.,  540479.,  564400.,  570723.,\n         581800.,  605400.,  655919.,  659400.,  694400.,  763953.,\n        1016400., 1023453., 1046700., 1081000., 1131615., 1187958.,\n        1197400., 1214100., 1239200., 1263500., 1272600., 1329200.,\n        1737800.]),\n array([2000, 2002, 2003, 2006, 2007]),\n array([  117.8 ,   162.04,   185.09,   263.68,   290.76,   295.02,\n          340.65,   341.43,   377.16,   390.2 ,   445.36,   725.9 ,\n          797.35,   919.11,  1029.92,  1052.88,  1232.03,  1243.43,\n         1254.17,  1363.56,  1426.34,  1612.65,  1672.96,  1804.  ,\n         1845.72,  1853.65,  1886.35,  2011.19,  2080.04,  2120.35,\n         2175.68,  2253.39,  2277.35,  2312.82,  2324.8 ,  2338.98,\n         2348.54,  2450.48,  2523.73,  2556.02,  2587.72,  2821.11,\n         2855.23,  2884.11,  2902.09,  3045.26,  3151.4 ,  3161.66,\n         3519.72,  3523.16,  3545.39,  3551.49,  3620.27,  3637.2 ,\n         3764.54,  3907.23,  3923.11,  3928.2 ,  3988.14,  4056.76,\n         4057.4 ,  4151.54,  4212.82,  4275.12,  4315.  ,  4462.74,\n         4467.55,  4659.99,  4676.13,  4725.01,  4743.61,  4746.16,\n         4757.45,  4772.52,  4820.53,  4983.67,  5007.21,  5043.96,\n         5052.99,  5333.09,  5757.29,  5823.41,  6035.48,  6112.5 ,\n         6211.8 ,  6867.7 ,  7104.  ,  7360.92,  7583.85,  7617.47,\n         7688.67,  7697.82,  8003.67,  8117.78,  8690.24,  9248.53,\n         9333.4 ,  9439.6 ,  9456.84,  9705.02,  9846.81, 10275.5 ,\n        10606.85, 10741.25, 12078.15, 12362.79, 13502.42, 13607.32,\n        15012.46, 15718.47, 15844.64, 18598.69, 18753.73, 21742.05,\n        21900.19, 25776.91, 26587.76, 31777.01]),\n array([      2,     293,     467,    1522,    1534,    1741,    1743,\n           1899,    1911,    2200,    2418,    2501,    2522,    2954,\n           3718,    3821,    4521,    4726,    5047,    6121,    6235,\n           8384,    9384,   10366,   11020,   11169,   12484,   12651,\n          12812,   21164,   21361,   22472,   24468,   28842,   30086,\n          30120,   30234,   31000,   31847,   32080,   32180,   33190,\n          33766,   35511,   36005,   36720,   38375,   39453,   39575,\n          40463,   41231,   41726,   41856,   43694,   44740,   52466,\n          53903,   55583,   56403,   66100,   67833,   67923,   68396,\n          69595,   76064,   90022,   92489,   94368,  101835,  108197,\n         108534,  112001,  119516,  120819,  139354,  142665,  156886,\n         168368,  170801,  172464,  184526,  208508,  219126,  241621,\n         242000,  244853,  259335,  259903,  276622,  280657,  299892,\n         306162,  307610,  322047,  327051,  343191,  383837,  406058,\n         413077,  455191,  473404,  498055,  506572,  601617,  607756,\n         691482,  782294,  888935, 1000069, 1018960, 1036576, 1101159,\n        1128091, 1133400, 1318339, 1451065, 1712603, 1743140]),\n array([0.        , 0.02702703, 0.03      , 0.03125   , 0.04761905,\n        0.09677419, 0.20512821, 0.22      , 0.4       , 1.21428571]),\n array([0.        , 0.03      , 0.03571429, 0.10869565, 0.11111111,\n        0.13      , 0.13888889, 0.15384615, 0.16      , 0.24      ,\n        0.27027027, 0.3       , 0.31      , 0.4       , 0.41025641,\n        0.4375    , 0.5       , 0.7948718 ]),\n array([0.        , 0.03571429, 0.05128205, 0.12820513, 0.13      ,\n        0.21621622, 0.22222222, 0.23076923, 0.27586207, 0.3       ,\n        0.32432432, 0.4       , 0.40625   , 0.47      , 0.51612903,\n        0.53      , 0.55      , 0.71052632, 0.8125    ]),\n array(['1060812', '1082935', '1089674', '1108348', '11537149', '1163113',\n        '11673659', '118013', '1212843', '123888', '1292604', '1310512',\n        '1321004', '1389153', '1443753', '147235', '14740022', '14926380',\n        '1514364', '1514799', '1543658', '1548155', '157652', '1600475',\n        '1601508', '16494981', '1667114', '16753980', '1675757', '1677840',\n        '16804703', '169770', '1710605', '1723026', '1755299', '1762409',\n        '1802055', '1807967', '1841592', '1851377', '1896479', '1913563',\n        '1925862', '1938812', '201412', '2018672', '2024337', '202761',\n        '2110577', '2125369', '2195820', '22377276', '2308652', '2329505',\n        '233299', '2373047', '2419708', '2450874', '2511249', '2523352',\n        '2525301', '2567976', '2648861', '27858007', '2823366', '2823413',\n        '2851375', '2858600', '2972212', '3206892', '3434548', '3444533',\n        '3816261', '3898510', '4032810', '4188265', '4247403', '4404689',\n        '4427000', '4468640', '447643', '4752398', '4830320', '4830392',\n        '4867146', '4958329', '505196', '50819', '5145006', '5596906',\n        '567083', '5903552', '597159', '59841', '6065508', '6166904',\n        '6212824', '6217715', '6879383', '693750', '6994577', '70048',\n        '7071605', '740947', '776120', '7891198', '800312', '830159',\n        '8620804', '8818088', '919235', '924080', '932549', '960708',\n        '966606', '971485', '974325', '9898522'], dtype=object),\n array(['East China', 'North China', 'Northeast China', 'Northwest China',\n        'South Central China', 'Southwest China'], dtype=object),\n array([  475184,   546541,   632880,   736165,   757990,   819028,\n          866691,   948521,   976396,  1047698,  1078754,  1109537,\n         1174622,  1184990,  1210637,  1216605,  1228569,  1258100,\n         1308445,  1333133,  1364344,  1364980,  1423771,  1426600,\n         1428990,  1440939,  1472622,  1492835,  1499110,  1554999,\n         1648826,  1658350,  1742585,  1782317,  1845611,  1873822,\n         1898911,  1927102,  1962192,  1962633,  1986738,  2017594,\n         2023674,  2047192,  2052220,  2053980,  2072426,  2135224,\n         2138158,  2138758,  2143190,  2150325,  2254281,  2261631,\n         2268499,  2339769,  2347862,  2355164,  2376983,  2378616,\n         2404936,  2444270,  2455900,  2545841,  2553268,  2649011,\n         2764053,  2815820,  2867525,  2907301,  2926542,  2939778,\n         2940367,  2977880,  3035767,  3051103,  3101537,  3114638,\n         3124234,  3343228,  3388449,  3545004,  3557071,  3586373,\n         3847158,  3893879,  3923569,  4039036,  4062020,  4073606,\n         4133488,  4229821,  4390259,  4559252,  4607955,  4613724,\n         4686125,  4947824,  5046865,  5167300,  5304833,  5502470,\n         5639838,  6003791,  6033279,  6185600,  6308151,  6349262,\n         6357869,  6832541,  7040099,  7537692,  7601825,  7646885,\n         7666512,  7968319,  8340692, 10533312])]\n\n\n\nord_encoder.transform(df)\n\narray([[  0.,   0.,  17., ...,  24.,   0.,  28.],\n       [  1.,   0.,  48., ...,  29.,   0.,  60.],\n       [  2.,   0.,  64., ...,  40.,   0.,  67.],\n       ...,\n       [115.,  27.,  44., ...,  97.,   0.,  53.],\n       [116.,  27.,  84., ...,   4.,   0.,  64.],\n       [117.,  27.,  78., ...,  25.,   0.,  71.]])\n\n\n\n\nCategories that are unknown during fit\n\n\nHow to handle unknown categories in OridinalEncoder?\n\n\nProvide all the categories in the constructor"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#onehotencoder",
    "href": "posts/2021-05-31-pandas-interoperability.html#onehotencoder",
    "title": "Pandas Interoperability",
    "section": "OneHotEncoder",
    "text": "OneHotEncoder\n\nfrom sklearn.preprocessing import OneHotEncoder\n\nohe = OneHotEncoder()\nX_trans = ohe.fit_transform(df)\nX_trans\n\n&lt;118x909 sparse matrix of type '&lt;class 'numpy.float64'&gt;'\n    with 1534 stored elements in Compressed Sparse Row format&gt;\n\n\n\nX_trans.toarray()\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nSwitch to dense\n\nohe = OneHotEncoder(sparse=False)\nohe.fit_transform(df)\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\n\nUnknown categories during transform?\n\n\nOHE can handle unknowns\n\nohe = OneHotEncoder(sparse=False, handle_unknown='ignore')\nohe.fit(df)\n\nOneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\n\nohe.transform(df)\n\narray([[1., 0., 0., ..., 0., 0., 0.],\n       [0., 1., 0., ..., 0., 0., 0.],\n       [0., 0., 1., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])\n\n\n\nohe.categories_\n\n[array([  4,   6,   7,  10,  11,  16,  18,  19,  22,  23,  34,  35,  40,\n         42,  43,  46,  47,  52,  54,  58,  64,  66,  67,  70,  71,  76,\n         78,  79,  82,  83,  88,  90,  91,  94,  95, 107, 112, 119, 124,\n        126, 127, 130, 131, 136, 138, 139, 142, 143, 148, 150, 151, 154,\n        155, 160, 162, 163, 166, 167, 172, 174, 175, 178, 179, 184, 186,\n        187, 190, 191, 196, 198, 199, 202, 203, 220, 222, 223, 226, 227,\n        232, 234, 235, 239, 244, 246, 247, 250, 251, 258, 259, 262, 263,\n        280, 282, 283, 292, 294, 295, 298, 310, 316, 318, 319, 322, 323,\n        328, 330, 331, 334, 335, 340, 342, 343, 346, 347, 354, 355, 358,\n        359]),\n array(['Anhui', 'Beijing', 'Chongqing', 'Fujian', 'Gansu', 'Guangdong',\n        'Guangxi', 'Guizhou', 'Hainan', 'Hebei', 'Heilongjiang', 'Henan',\n        'Hubei', 'Hunan', 'Jiangsu', 'Jiangxi', 'Jilin', 'Ningxia',\n        'Qinghai', 'Shaanxi', 'Shandong', 'Shanxi', 'Sichuan', 'Tianjin',\n        'Tibet', 'Xinjiang', 'Yunnan', 'Zhejiang'], dtype=object),\n array([  72939.,   91405.,   96825.,  103041.,  107046.,  107687.,\n         119536.,  127819.,  133858.,  137190.,  142650.,  144839.,\n         147749.,  148812.,  160637.,  179235.,  181409.,  195580.,\n         207615.,  217707.,  223984.,  228043.,  237948.,  248903.,\n         251539.,  254002.,  262197.,  265770.,  271297.,  271499.,\n         281769.,  295133.,  319710.,  331999.,  337894.,  340372.,\n         353776.,  354775.,  359275.,  365437.,  367568.,  369552.,\n         370049.,  379186.,  391292.,  395775.,  397517.,  430577.,\n         434149.,  436189.,  447709.,  458201.,  469514.,  472786.,\n         484715.,  487155.,  510656.,  518022.,  531994.,  558569.,\n         575550.,  591088.,  601485.,  615593.,  619201.,  642581.,\n         675931.,  681676.,  684767.,  685732.,  688887.,  714572.,\n         752279.,  753552.,  761081.,  821750.,  833423.,  833430.,\n         844647.,  859482.,  875877.,  909559.,  978069.,  985851.,\n        1017303., 1035872., 1097470., 1188989., 1204547., 1207353.,\n        1224179., 1235386., 1246484., 1315102., 1331590., 1358528.,\n        1388043., 1457872., 1550764., 1562694., 1791403., 1890650.,\n        1897575., 1956261., 2018158., 2022957., 2045869., 2121243.,\n        2213991., 2225220., 2482173., 2663667., 2669238., 2922784.,\n        2981235., 3156087., 3847672., 3860764.]),\n array([      0.,    2990.,    8115.,   11755.,   11767.,   17400.,\n          20842.,   26300.,   27302.,   27387.,   29646.,   30705.,\n          32100.,   32119.,   32868.,   34842.,   36670.,   36946.,\n          40604.,   44623.,   45683.,   50097.,   52108.,   53900.,\n          56070.,   58533.,   59263.,   60560.,   60906.,   62000.,\n          66100.,   66529.,   68142.,   71807.,   80361.,   80609.,\n          81879.,   86256.,   88007.,   93323.,   95648.,  100000.,\n         100900.,  107658.,  108624.,  112137.,  113000.,  114418.,\n         116000.,  119658.,  123317.,  123546.,  124582.,  124647.,\n         129791.,  135765.,  143800.,  145000.,  149549.,  150000.,\n         153640.,  154364.,  165071.,  173552.,  173556.,  178705.,\n         179252.,  188633.,  197539.,  229895.,  241282.,  260313.,\n         264185.,  269596.,  279052.,  280277.,  302600.,  309582.,\n         317700.,  320627.,  321686.,  363054.,  394795.,  400000.,\n         405966.,  423049.,  429591.,  434318.,  447900.,  460668.,\n         498913.,  516342.,  527300.,  540479.,  564400.,  570723.,\n         581800.,  605400.,  655919.,  659400.,  694400.,  763953.,\n        1016400., 1023453., 1046700., 1081000., 1131615., 1187958.,\n        1197400., 1214100., 1239200., 1263500., 1272600., 1329200.,\n        1737800.]),\n array([2000, 2002, 2003, 2006, 2007]),\n array([  117.8 ,   162.04,   185.09,   263.68,   290.76,   295.02,\n          340.65,   341.43,   377.16,   390.2 ,   445.36,   725.9 ,\n          797.35,   919.11,  1029.92,  1052.88,  1232.03,  1243.43,\n         1254.17,  1363.56,  1426.34,  1612.65,  1672.96,  1804.  ,\n         1845.72,  1853.65,  1886.35,  2011.19,  2080.04,  2120.35,\n         2175.68,  2253.39,  2277.35,  2312.82,  2324.8 ,  2338.98,\n         2348.54,  2450.48,  2523.73,  2556.02,  2587.72,  2821.11,\n         2855.23,  2884.11,  2902.09,  3045.26,  3151.4 ,  3161.66,\n         3519.72,  3523.16,  3545.39,  3551.49,  3620.27,  3637.2 ,\n         3764.54,  3907.23,  3923.11,  3928.2 ,  3988.14,  4056.76,\n         4057.4 ,  4151.54,  4212.82,  4275.12,  4315.  ,  4462.74,\n         4467.55,  4659.99,  4676.13,  4725.01,  4743.61,  4746.16,\n         4757.45,  4772.52,  4820.53,  4983.67,  5007.21,  5043.96,\n         5052.99,  5333.09,  5757.29,  5823.41,  6035.48,  6112.5 ,\n         6211.8 ,  6867.7 ,  7104.  ,  7360.92,  7583.85,  7617.47,\n         7688.67,  7697.82,  8003.67,  8117.78,  8690.24,  9248.53,\n         9333.4 ,  9439.6 ,  9456.84,  9705.02,  9846.81, 10275.5 ,\n        10606.85, 10741.25, 12078.15, 12362.79, 13502.42, 13607.32,\n        15012.46, 15718.47, 15844.64, 18598.69, 18753.73, 21742.05,\n        21900.19, 25776.91, 26587.76, 31777.01]),\n array([      2,     293,     467,    1522,    1534,    1741,    1743,\n           1899,    1911,    2200,    2418,    2501,    2522,    2954,\n           3718,    3821,    4521,    4726,    5047,    6121,    6235,\n           8384,    9384,   10366,   11020,   11169,   12484,   12651,\n          12812,   21164,   21361,   22472,   24468,   28842,   30086,\n          30120,   30234,   31000,   31847,   32080,   32180,   33190,\n          33766,   35511,   36005,   36720,   38375,   39453,   39575,\n          40463,   41231,   41726,   41856,   43694,   44740,   52466,\n          53903,   55583,   56403,   66100,   67833,   67923,   68396,\n          69595,   76064,   90022,   92489,   94368,  101835,  108197,\n         108534,  112001,  119516,  120819,  139354,  142665,  156886,\n         168368,  170801,  172464,  184526,  208508,  219126,  241621,\n         242000,  244853,  259335,  259903,  276622,  280657,  299892,\n         306162,  307610,  322047,  327051,  343191,  383837,  406058,\n         413077,  455191,  473404,  498055,  506572,  601617,  607756,\n         691482,  782294,  888935, 1000069, 1018960, 1036576, 1101159,\n        1128091, 1133400, 1318339, 1451065, 1712603, 1743140]),\n array([0.        , 0.02702703, 0.03      , 0.03125   , 0.04761905,\n        0.09677419, 0.20512821, 0.22      , 0.4       , 1.21428571]),\n array([0.        , 0.03      , 0.03571429, 0.10869565, 0.11111111,\n        0.13      , 0.13888889, 0.15384615, 0.16      , 0.24      ,\n        0.27027027, 0.3       , 0.31      , 0.4       , 0.41025641,\n        0.4375    , 0.5       , 0.7948718 ]),\n array([0.        , 0.03571429, 0.05128205, 0.12820513, 0.13      ,\n        0.21621622, 0.22222222, 0.23076923, 0.27586207, 0.3       ,\n        0.32432432, 0.4       , 0.40625   , 0.47      , 0.51612903,\n        0.53      , 0.55      , 0.71052632, 0.8125    ]),\n array(['1060812', '1082935', '1089674', '1108348', '11537149', '1163113',\n        '11673659', '118013', '1212843', '123888', '1292604', '1310512',\n        '1321004', '1389153', '1443753', '147235', '14740022', '14926380',\n        '1514364', '1514799', '1543658', '1548155', '157652', '1600475',\n        '1601508', '16494981', '1667114', '16753980', '1675757', '1677840',\n        '16804703', '169770', '1710605', '1723026', '1755299', '1762409',\n        '1802055', '1807967', '1841592', '1851377', '1896479', '1913563',\n        '1925862', '1938812', '201412', '2018672', '2024337', '202761',\n        '2110577', '2125369', '2195820', '22377276', '2308652', '2329505',\n        '233299', '2373047', '2419708', '2450874', '2511249', '2523352',\n        '2525301', '2567976', '2648861', '27858007', '2823366', '2823413',\n        '2851375', '2858600', '2972212', '3206892', '3434548', '3444533',\n        '3816261', '3898510', '4032810', '4188265', '4247403', '4404689',\n        '4427000', '4468640', '447643', '4752398', '4830320', '4830392',\n        '4867146', '4958329', '505196', '50819', '5145006', '5596906',\n        '567083', '5903552', '597159', '59841', '6065508', '6166904',\n        '6212824', '6217715', '6879383', '693750', '6994577', '70048',\n        '7071605', '740947', '776120', '7891198', '800312', '830159',\n        '8620804', '8818088', '919235', '924080', '932549', '960708',\n        '966606', '971485', '974325', '9898522'], dtype=object),\n array(['East China', 'North China', 'Northeast China', 'Northwest China',\n        'South Central China', 'Southwest China'], dtype=object),\n array([  475184,   546541,   632880,   736165,   757990,   819028,\n          866691,   948521,   976396,  1047698,  1078754,  1109537,\n         1174622,  1184990,  1210637,  1216605,  1228569,  1258100,\n         1308445,  1333133,  1364344,  1364980,  1423771,  1426600,\n         1428990,  1440939,  1472622,  1492835,  1499110,  1554999,\n         1648826,  1658350,  1742585,  1782317,  1845611,  1873822,\n         1898911,  1927102,  1962192,  1962633,  1986738,  2017594,\n         2023674,  2047192,  2052220,  2053980,  2072426,  2135224,\n         2138158,  2138758,  2143190,  2150325,  2254281,  2261631,\n         2268499,  2339769,  2347862,  2355164,  2376983,  2378616,\n         2404936,  2444270,  2455900,  2545841,  2553268,  2649011,\n         2764053,  2815820,  2867525,  2907301,  2926542,  2939778,\n         2940367,  2977880,  3035767,  3051103,  3101537,  3114638,\n         3124234,  3343228,  3388449,  3545004,  3557071,  3586373,\n         3847158,  3893879,  3923569,  4039036,  4062020,  4073606,\n         4133488,  4229821,  4390259,  4559252,  4607955,  4613724,\n         4686125,  4947824,  5046865,  5167300,  5304833,  5502470,\n         5639838,  6003791,  6033279,  6185600,  6308151,  6349262,\n         6357869,  6832541,  7040099,  7537692,  7601825,  7646885,\n         7666512,  7968319,  8340692, 10533312])]"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#two-categorical-features",
    "href": "posts/2021-05-31-pandas-interoperability.html#two-categorical-features",
    "title": "Pandas Interoperability",
    "section": "Two categorical features",
    "text": "Two categorical features\n\ndf_train = pd.DataFrame({\n    \"province\": [\"Zhejiang\", \"Beijing\", \"Shanghai\"],\n    \"region\": [\"East China\", \"North China\", \"Southwest China\"]\n})\n\n\nohe.fit(df_train)\n\nOneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\n\nohe.categories_\n\n[array(['Beijing', 'Shanghai', 'Zhejiang'], dtype=object),\n array(['East China', 'North China', 'Southwest China'], dtype=object)]\n\n\n\nohe.transform(df_train)\n\narray([[0., 0., 1., 1., 0., 0.],\n       [1., 0., 0., 0., 1., 0.],\n       [0., 1., 0., 0., 0., 1.]])"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#with-oridinalencoder",
    "href": "posts/2021-05-31-pandas-interoperability.html#with-oridinalencoder",
    "title": "Pandas Interoperability",
    "section": "With OridinalEncoder",
    "text": "With OridinalEncoder\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\n\n\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.000000\n0.000000\n0.000000\n1601508\nEast China\n1499110\n\n\n6\n6\nAnhui\n434149.0\n66529.0\n2002\n3519.72\n38375\n0.000000\n0.000000\n0.000000\n1677840\nEast China\n2404936\n\n\n7\n7\nAnhui\n619201.0\n52108.0\n2003\n3923.11\n36720\n0.000000\n0.000000\n0.000000\n1896479\nEast China\n2815820\n\n\n10\n10\nAnhui\n1457872.0\n279052.0\n2006\n6112.50\n139354\n0.000000\n0.000000\n0.324324\n3434548\nEast China\n5167300\n\n\n11\n11\nAnhui\n2213991.0\n178705.0\n2007\n7360.92\n299892\n0.000000\n0.000000\n0.324324\n4468640\nEast China\n7040099\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n347\n347\nYunnan\n2482173.0\n564400.0\n2007\n4772.52\n39453\n0.000000\n0.000000\n0.000000\n4867146\nSouthwest China\n6832541\n\n\n354\n354\nZhejiang\n365437.0\n321686.0\n2002\n8003.67\n307610\n0.000000\n0.000000\n0.000000\n4958329\nEast China\n1962633\n\n\n355\n355\nZhejiang\n391292.0\n260313.0\n2003\n9705.02\n498055\n1.214286\n0.035714\n0.035714\n6217715\nEast China\n2261631\n\n\n358\n358\nZhejiang\n1017303.0\n394795.0\n2006\n15718.47\n888935\n1.214286\n0.035714\n0.035714\n11537149\nEast China\n2553268\n\n\n359\n359\nZhejiang\n844647.0\n0.0\n2007\n18753.73\n1036576\n0.047619\n0.000000\n0.000000\n16494981\nEast China\n2939778\n\n\n\n\n118 rows × 13 columns\n\n\n\n\nct = ColumnTransformer([\n    ('numerical', StandardScaler(), ['fdi', 'gdp']),\n    ('categorical', OrdinalEncoder(), ['reg'])\n])\n\nct.fit_transform(df)\n\narray([[-0.54088554, -0.48498461,  0.        ],\n       [-0.52313911, -0.37867596,  0.        ],\n       [-0.52763824, -0.30924305,  0.        ],\n       [-0.24862668,  0.06760246,  0.        ],\n       [ 0.18779748,  0.28248491,  0.        ],\n       [-0.16975183, -0.44030651,  1.        ],\n       [-0.15861681, -0.24178957,  1.        ],\n       [-0.0317657 , -0.12264395,  1.        ],\n       [ 0.60997938,  0.41275831,  1.        ],\n       [ 0.74965914,  0.71036505,  1.        ],\n       [-0.43826722, -0.31197638,  5.        ],\n       [-0.33241116, -0.1796306 ,  5.        ],\n       [ 0.30550625, -0.33653668,  0.        ],\n       [ 0.41600281, -0.21553212,  0.        ],\n       [ 0.07908699, -0.12669573,  0.        ],\n       [ 0.24802608,  0.3208564 ,  0.        ],\n       [ 0.47641082,  0.60738699,  0.        ],\n       [-0.61051202, -0.80327715,  3.        ],\n       [-0.61082193, -0.77244122,  3.        ],\n       [-0.61943145, -0.59251706,  3.        ],\n       [ 2.43926479,  0.86431921,  4.        ],\n       [ 2.45369736,  1.33958151,  4.        ],\n       [ 1.49921217,  1.74273266,  4.        ],\n       [ 3.31727285,  3.5918763 ,  4.        ],\n       [ 4.02826654,  4.48506828,  4.        ],\n       [-0.48483258, -0.62647875,  4.        ],\n       [-0.51402938, -0.55010927,  4.        ],\n       [-0.51367597, -0.49892317,  4.        ],\n       [-0.50583579, -0.16757679,  4.        ],\n       [-0.44152672,  0.01784327,  4.        ],\n       [-0.62066294, -0.8072291 ,  5.        ],\n       [-0.6170745 , -0.77047901,  5.        ],\n       [-0.61517155, -0.7389959 ,  5.        ],\n       [-0.60195143, -0.58190909,  5.        ],\n       [-0.59307006, -0.48807939,  5.        ],\n       [-0.32298609, -0.7686304 ,  4.        ],\n       [-0.44281257, -0.11631841,  1.        ],\n       [ 0.02938719,  1.35763727,  1.        ],\n       [-0.54567284, -0.4420725 ,  2.        ],\n       [-0.53092492, -0.35845489,  2.        ],\n       [-0.53998028, -0.28612859,  2.        ],\n       [-0.1631377 ,  0.08469433,  2.        ],\n       [-0.06063084,  0.23826293,  2.        ],\n       [-0.47412981, -0.11476413,  4.        ],\n       [-0.51746286,  0.05434551,  4.        ],\n       [-0.48092608,  0.19759014,  4.        ],\n       [-0.12582615,  1.14342438,  4.        ],\n       [ 0.20484254,  1.59949491,  4.        ],\n       [-0.37092157, -0.37425755,  4.        ],\n       [-0.23962569, -0.25937715,  4.        ],\n       [-0.20096576, -0.16563352,  4.        ],\n       [ 0.03817341,  0.3266432 ,  4.        ],\n       [ 0.12453776,  0.62199511,  4.        ],\n       [-0.44305724, -0.3732076 ,  4.        ],\n       [-0.38273622, -0.26992488,  4.        ],\n       [-0.35062246, -0.18240867,  4.        ],\n       [ 0.07754287,  0.33889839,  4.        ],\n       [ 0.2616295 ,  0.64027463,  4.        ],\n       [ 1.02472886,  0.34047332,  0.        ],\n       [ 1.25233884,  0.64324204,  0.        ],\n       [ 2.14259107,  0.84118581,  0.        ],\n       [ 2.95645589,  2.21676944,  0.        ],\n       [ 4.11128168,  2.75781563,  0.        ],\n       [-0.54025213, -0.6654458 ,  0.        ],\n       [-0.5198769 , -0.61001686,  0.        ],\n       [-0.3333273 , -0.56271731,  0.        ],\n       [ 0.0304175 , -0.28623875,  0.        ],\n       [ 0.13550694, -0.15477596,  0.        ],\n       [-0.54558041, -0.69654679,  2.        ],\n       [-0.53566872, -0.61954045,  2.        ],\n       [-0.56094543, -0.58026359,  2.        ],\n       [-0.44776842, -0.36136894,  2.        ],\n       [-0.42068118, -0.24865385,  2.        ],\n       [-0.62272901, -0.93372268,  3.        ],\n       [-0.62148121, -0.91958445,  3.        ],\n       [-0.62272357, -0.90784563,  3.        ],\n       [-0.61735451, -0.8595581 ,  3.        ],\n       [-0.61374161, -0.82630211,  3.        ],\n       [-0.59750395, -0.93911703,  3.        ],\n       [-0.61461425, -0.92586868,  3.        ],\n       [-0.62060585, -0.91733996,  3.        ],\n       [-0.54318812, -0.84725987,  3.        ],\n       [-0.54905466, -0.67399173,  3.        ],\n       [-0.52958198, -0.59664114,  3.        ],\n       [-0.53723458, -0.53909508,  3.        ],\n       [-0.37602966, -0.16801571,  3.        ],\n       [-0.30255648,  0.00646247,  3.        ],\n       [ 0.6594916 ,  0.78415268,  0.        ],\n       [ 1.00803993,  1.09443114,  0.        ],\n       [ 2.0912357 ,  2.78503525,  0.        ],\n       [ 2.36604988,  3.45230994,  0.        ],\n       [-0.56637157, -0.66681073,  1.        ],\n       [-0.56992738, -0.5843498 ,  1.        ],\n       [-0.56939184, -0.49305032,  1.        ],\n       [-0.50867935, -0.30836695,  5.        ],\n       [-0.47635899, -0.1712172 ,  5.        ],\n       [-0.51537504, -0.06655233,  5.        ],\n       [-0.29901427,  0.51129215,  5.        ],\n       [ 0.49549204, -0.21636004,  1.        ],\n       [-0.62745649, -0.96422641,  5.        ],\n       [-0.62666541, -0.95661166,  5.        ],\n       [-0.62619239, -0.95264422,  5.        ],\n       [-0.62332436, -0.93445592,  5.        ],\n       [-0.62088857, -0.92573443,  5.        ],\n       [-0.62226686, -0.74980181,  3.        ],\n       [-0.62229948, -0.70692756,  3.        ],\n       [-0.62329174, -0.65981736,  3.        ],\n       [-0.59928186, -0.46034169,  3.        ],\n       [-0.59352405, -0.37808386,  3.        ],\n       [-0.59263238, -0.63832946,  5.        ],\n       [-0.59709889, -0.58641184,  5.        ],\n       [-0.60466994, -0.5445514 ,  5.        ],\n       [-0.5452705 , -0.29804986,  5.        ],\n       [-0.52020855, -0.16303961,  5.        ],\n       [ 0.20877895,  0.3931173 ,  0.        ],\n       [ 0.72650559,  0.68595965,  0.        ],\n       [ 1.7891168 ,  1.72101584,  0.        ],\n       [ 2.19048034,  2.24345547,  0.        ]])"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#with-onehotencoder",
    "href": "posts/2021-05-31-pandas-interoperability.html#with-onehotencoder",
    "title": "Pandas Interoperability",
    "section": "With OneHotEncoder",
    "text": "With OneHotEncoder\n\nct = ColumnTransformer([\n    ('numerical', StandardScaler(), ['fdi', 'gdp']),\n    ('categorical', OneHotEncoder(), ['reg'])\n])\n\n\nct.fit_transform(df)\n\narray([[-0.54088554, -0.48498461,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.52313911, -0.37867596,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.52763824, -0.30924305,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.24862668,  0.06760246,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.18779748,  0.28248491,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.16975183, -0.44030651,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.15861681, -0.24178957,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.0317657 , -0.12264395,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.60997938,  0.41275831,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.74965914,  0.71036505,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.43826722, -0.31197638,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.33241116, -0.1796306 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [ 0.30550625, -0.33653668,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.41600281, -0.21553212,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.07908699, -0.12669573,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.24802608,  0.3208564 ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.47641082,  0.60738699,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.61051202, -0.80327715,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61082193, -0.77244122,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61943145, -0.59251706,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [ 2.43926479,  0.86431921,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 2.45369736,  1.33958151,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 1.49921217,  1.74273266,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 3.31727285,  3.5918763 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 4.02826654,  4.48506828,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.48483258, -0.62647875,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.51402938, -0.55010927,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.51367597, -0.49892317,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.50583579, -0.16757679,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.44152672,  0.01784327,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.62066294, -0.8072291 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.6170745 , -0.77047901,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.61517155, -0.7389959 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.60195143, -0.58190909,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.59307006, -0.48807939,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.32298609, -0.7686304 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.44281257, -0.11631841,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.02938719,  1.35763727,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.54567284, -0.4420725 ,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.53092492, -0.35845489,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.53998028, -0.28612859,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.1631377 ,  0.08469433,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.06063084,  0.23826293,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.47412981, -0.11476413,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.51746286,  0.05434551,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.48092608,  0.19759014,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.12582615,  1.14342438,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.20484254,  1.59949491,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.37092157, -0.37425755,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.23962569, -0.25937715,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.20096576, -0.16563352,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.03817341,  0.3266432 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.12453776,  0.62199511,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.44305724, -0.3732076 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.38273622, -0.26992488,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [-0.35062246, -0.18240867,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.07754287,  0.33889839,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 0.2616295 ,  0.64027463,  0.        ,  0.        ,  0.        ,\n         0.        ,  1.        ,  0.        ],\n       [ 1.02472886,  0.34047332,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 1.25233884,  0.64324204,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.14259107,  0.84118581,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.95645589,  2.21676944,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 4.11128168,  2.75781563,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.54025213, -0.6654458 ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.5198769 , -0.61001686,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.3333273 , -0.56271731,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.0304175 , -0.28623875,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.13550694, -0.15477596,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.54558041, -0.69654679,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.53566872, -0.61954045,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.56094543, -0.58026359,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.44776842, -0.36136894,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.42068118, -0.24865385,  0.        ,  0.        ,  1.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.62272901, -0.93372268,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62148121, -0.91958445,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62272357, -0.90784563,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61735451, -0.8595581 ,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61374161, -0.82630211,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.59750395, -0.93911703,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.61461425, -0.92586868,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62060585, -0.91733996,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.54318812, -0.84725987,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.54905466, -0.67399173,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.52958198, -0.59664114,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.53723458, -0.53909508,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.37602966, -0.16801571,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.30255648,  0.00646247,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [ 0.6594916 ,  0.78415268,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 1.00803993,  1.09443114,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.0912357 ,  2.78503525,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.36604988,  3.45230994,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.56637157, -0.66681073,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.56992738, -0.5843498 ,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.56939184, -0.49305032,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.50867935, -0.30836695,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.47635899, -0.1712172 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.51537504, -0.06655233,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.29901427,  0.51129215,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [ 0.49549204, -0.21636004,  0.        ,  1.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [-0.62745649, -0.96422641,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62666541, -0.95661166,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62619239, -0.95264422,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62332436, -0.93445592,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62088857, -0.92573443,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.62226686, -0.74980181,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62229948, -0.70692756,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.62329174, -0.65981736,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.59928186, -0.46034169,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.59352405, -0.37808386,  0.        ,  0.        ,  0.        ,\n         1.        ,  0.        ,  0.        ],\n       [-0.59263238, -0.63832946,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.59709889, -0.58641184,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.60466994, -0.5445514 ,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.5452705 , -0.29804986,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [-0.52020855, -0.16303961,  0.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  1.        ],\n       [ 0.20877895,  0.3931173 ,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 0.72650559,  0.68595965,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 1.7891168 ,  1.72101584,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ],\n       [ 2.19048034,  2.24345547,  1.        ,  0.        ,  0.        ,\n         0.        ,  0.        ,  0.        ]])\n\n\n\n# df\n\n\ny\n\n4       195580.0\n6       434149.0\n7       619201.0\n10     1457872.0\n11     2213991.0\n         ...    \n347    2482173.0\n354     365437.0\n355     391292.0\n358    1017303.0\n359     844647.0\nName: specific, Length: 118, dtype: float64\n\n\n\nX.head()\n\n\n\n\n\n\n\n\nprovince\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n4\nAnhui\n32100.0\n2000\n2902.09\n31847\n0.0\n0.0\n0.000000\n1601508\nEast China\n1499110\n\n\n6\nAnhui\n66529.0\n2002\n3519.72\n38375\n0.0\n0.0\n0.000000\n1677840\nEast China\n2404936\n\n\n7\nAnhui\n52108.0\n2003\n3923.11\n36720\n0.0\n0.0\n0.000000\n1896479\nEast China\n2815820\n\n\n10\nAnhui\n279052.0\n2006\n6112.50\n139354\n0.0\n0.0\n0.324324\n3434548\nEast China\n5167300\n\n\n11\nAnhui\n178705.0\n2007\n7360.92\n299892\n0.0\n0.0\n0.324324\n4468640\nEast China\n7040099\n\n\n\n\n\n\n\n\nAre three categories already encoded in the dataset?\n\nX.dtypes\n\nprovince     object\ngeneral     float64\nyear          int64\ngdp         float64\nfdi           int64\nrnr         float64\nrr          float64\ni           float64\nfr           object\nreg          object\nit            int64\ndtype: object\n\n\n\n\nAre there missing values in the dataset?\n\nmissing_values = pd.concat({\"na_cnt\": X.isna().sum(), \"dtypes\": X.dtypes}, axis='columns')\nmissing_values\n\n\n\n\n\n\n\n\nna_cnt\ndtypes\n\n\n\n\nprovince\n0\nobject\n\n\ngeneral\n0\nfloat64\n\n\nyear\n0\nint64\n\n\ngdp\n0\nfloat64\n\n\nfdi\n0\nint64\n\n\nrnr\n0\nfloat64\n\n\nrr\n0\nfloat64\n\n\ni\n0\nfloat64\n\n\nfr\n0\nobject\n\n\nreg\n0\nobject\n\n\nit\n0\nint64\n\n\n\n\n\n\n\n\n\nSplit data into training and test set\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, random_state=42)"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#columntransformer",
    "href": "posts/2021-05-31-pandas-interoperability.html#columntransformer",
    "title": "Pandas Interoperability",
    "section": "ColumnTransformer",
    "text": "ColumnTransformer\n\nmissing_values\n\n\n\n\n\n\n\n\nna_cnt\ndtypes\n\n\n\n\nprovince\n0\nobject\n\n\ngeneral\n0\nfloat64\n\n\nyear\n0\nint64\n\n\ngdp\n0\nfloat64\n\n\nfdi\n0\nint64\n\n\nrnr\n0\nfloat64\n\n\nrr\n0\nfloat64\n\n\ni\n0\nfloat64\n\n\nfr\n0\nobject\n\n\nreg\n0\nobject\n\n\nit\n0\nint64\n\n\n\n\n\n\n\n\nX\n\n\n\n\n\n\n\n\nprovince\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n4\nAnhui\n32100.0\n2000\n2902.09\n31847\n0.000000\n0.000000\n0.000000\n1601508\nEast China\n1499110\n\n\n6\nAnhui\n66529.0\n2002\n3519.72\n38375\n0.000000\n0.000000\n0.000000\n1677840\nEast China\n2404936\n\n\n7\nAnhui\n52108.0\n2003\n3923.11\n36720\n0.000000\n0.000000\n0.000000\n1896479\nEast China\n2815820\n\n\n10\nAnhui\n279052.0\n2006\n6112.50\n139354\n0.000000\n0.000000\n0.324324\n3434548\nEast China\n5167300\n\n\n11\nAnhui\n178705.0\n2007\n7360.92\n299892\n0.000000\n0.000000\n0.324324\n4468640\nEast China\n7040099\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n347\nYunnan\n564400.0\n2007\n4772.52\n39453\n0.000000\n0.000000\n0.000000\n4867146\nSouthwest China\n6832541\n\n\n354\nZhejiang\n321686.0\n2002\n8003.67\n307610\n0.000000\n0.000000\n0.000000\n4958329\nEast China\n1962633\n\n\n355\nZhejiang\n260313.0\n2003\n9705.02\n498055\n1.214286\n0.035714\n0.035714\n6217715\nEast China\n2261631\n\n\n358\nZhejiang\n394795.0\n2006\n15718.47\n888935\n1.214286\n0.035714\n0.035714\n11537149\nEast China\n2553268\n\n\n359\nZhejiang\n0.0\n2007\n18753.73\n1036576\n0.047619\n0.000000\n0.000000\n16494981\nEast China\n2939778\n\n\n\n\n118 rows × 11 columns\n\n\n\n\nNumerical preprocessing\n\nnumerical_features = ['general', 'gdp', 'fdi', 'i', 'rr']\n\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\nnum_prep = Pipeline([\n    ('imputer', SimpleImputer()),\n    ('scaler', StandardScaler())\n])\n\n\nnum_prep\n\nPipelinePipeline(steps=[('imputer', SimpleImputer()), ('scaler', StandardScaler())])SimpleImputerSimpleImputer()StandardScalerStandardScaler()\n\n\n\nRunning only on numerical features\n\nnum_trans = num_prep.fit_transform(X_train[numerical_features])\nnum_trans\n\narray([[ 0.84445696, -0.23824571,  0.05133332, -0.46886997,  1.84019633],\n       [ 0.24766853,  2.07395803,  1.57444985, -0.46886997, -0.43195321],\n       [-0.72214407, -0.0446055 , -0.43939868, -0.46886997, -0.43195321],\n       [-0.48856958, -0.11992158, -0.34379901, -0.46886997, -0.43195321],\n       [ 0.85337791, -0.87758205, -0.54348651, -0.46886997, -0.43195321],\n       [-0.62992082, -0.83196322, -0.62382665, -0.46886997,  0.17004293],\n       [-0.83650183,  3.23074054,  4.28312405,  1.06326271, -0.43195321],\n       [ 1.76828095, -0.19541419, -0.41644878, -0.46886997, -0.43195321],\n       [ 1.78635605,  0.89747804,  0.79717537, -0.46886997,  3.97033652],\n       [-0.01481382, -0.57518987, -0.60442312,  3.47737968, -0.43195321],\n       [-0.33025804, -0.71766019, -0.62552372, -0.46886997, -0.43195321],\n       [-0.75781248, -0.70583032, -0.54596728, -0.46886997, -0.43195321],\n       [ 0.18859962, -0.10301925, -0.50475274, -0.46886997,  1.78339259],\n       [ 3.61705142,  1.91072514,  0.2322093 , -0.46886997, -0.43195321],\n       [-0.52047074, -0.52639929, -0.53731279, -0.46886997, -0.43195321],\n       [-0.35308192, -0.41382042, -0.15623907,  2.47473995, -0.43195321],\n       [ 0.51483816, -0.85369871, -0.61664935, -0.46886997, -0.43195321],\n       [ 2.42485843, -0.08843151,  0.16030939, -0.46886997,  1.84019633],\n       [-0.63055638, -0.3205428 , -0.53076977, -0.46886997, -0.43195321],\n       [-0.81570508, -0.2076344 , -0.22869714,  0.25314756, -0.43195321],\n       [-0.06101258, -0.78797637, -0.31514053, -0.46886997, -0.43195321],\n       [-0.022315  ,  0.78012485,  0.51382125, -0.46886997, -0.43195321],\n       [-0.37712312, -0.55331904, -0.3258642 ,  2.14150109, -0.43195321],\n       [ 2.33926246, -0.46826211, -0.59521328,  3.47737968, -0.43195321],\n       [ 0.62612061, -0.10351944, -0.37014586, -0.46886997, -0.43195321],\n       [-0.1455937 , -0.05181405, -0.01314966, -0.46886997,  3.97033652],\n       [-0.69280823, -0.04283426, -0.47187415, -0.46886997, -0.43195321],\n       [-0.15946076, -0.25170559, -0.5456459 , -0.46886997, -0.43195321],\n       [-0.80637666, -0.98226192, -0.59981116, -0.46886997, -0.43195321],\n       [-0.45208878, -0.48061962, -0.51288289, -0.46886997,  1.78339259],\n       [ 0.4867555 , -0.58727866, -0.62254962,  0.24317887, -0.43195321],\n       [-0.74721037, -0.26346288, -0.50770147, -0.46886997, -0.26580227],\n       [ 0.71498921, -0.96701118, -0.62406064,  0.76534802, -0.43195321],\n       [-0.6776111 , -0.6259812 , -0.48297275,  1.75272242, -0.43195321],\n       [ 0.20388901,  0.55832718,  0.6523297 , -0.46886997,  3.97033652],\n       [-0.68462536, -0.15766887,  0.45117911, -0.46886997,  1.22955614],\n       [-0.68130147, -0.59197843, -0.52937716, -0.46886997, -0.43195321],\n       [-0.1213603 ,  0.16498962, -0.23803102,  1.33242116, -0.43195321],\n       [-0.68649617, -0.8274596 , -0.61330032, -0.46886997,  0.42010287],\n       [-0.73244378, -0.41583293, -0.54606312, -0.46886997, -0.43195321],\n       [-0.75418874, -0.67194329, -0.56752738, -0.46886997, -0.43195321],\n       [-0.11822093, -0.18759169, -0.14469223,  2.47473995, -0.43195321],\n       [-0.5606009 , -0.89159703, -0.62039587, -0.46886997, -0.43195321],\n       [-0.70811556, -0.99767742, -0.6295606 ,  1.33242116, -0.43195321],\n       [-0.44276036, -0.47392695, -0.57065935, -0.46886997, -0.43195321],\n       [-0.61544895, -0.21965453, -0.37710046, -0.46886997, -0.43195321],\n       [-0.57791999, -0.10080471, -0.18860741, -0.46886997,  0.18342062],\n       [ 2.23213936, -0.34291185, -0.59568406, -0.46886997, -0.43195321],\n       [ 2.06354861,  1.39098988, -0.11068883, -0.46886997, -0.43195321],\n       [-0.79190991, -0.9574449 , -0.62376745,  0.24317887, -0.43195321],\n       [ 0.26443406,  2.61416753,  3.08558822,  1.06326271, -0.43195321],\n       [-0.01209987,  0.53594439,  0.23629128, -0.46886997, -0.43195321],\n       [-0.80634591, -0.96716418, -0.61755426, -0.46886997, -0.43195321],\n       [-0.16938374,  0.8696658 ,  0.77316551, -0.27051351, -0.23415447],\n       [-0.04311944,  0.18446741, -0.14938031,  0.73199078, -0.43195321],\n       [-0.68041476,  0.47595163,  1.08241827, -0.18405044, -0.43195321],\n       [-0.76631569, -0.33855142, -0.36484886, -0.46886997,  1.06490206],\n       [-0.71942755, -1.0021987 , -0.63005112, -0.46886997,  1.28493979],\n       [ 2.56991029,  0.81760341,  0.29109646, -0.46886997,  1.99108126],\n       [-0.74181833, -0.94662522, -0.62596349, -0.46886997, -0.43195321],\n       [-0.39172056, -0.75420507, -0.61813217,  0.73199078, -0.43195321],\n       [-0.74252565, -0.58032118, -0.59939112, -0.46886997, -0.43195321],\n       [-0.52984786,  0.82098506,  1.31844616, -0.46886997, -0.43195321],\n       [-0.75226924, -0.67038781, -0.54044193, -0.46886997, -0.43195321],\n       [-0.51722886, -0.15861235,  0.53360817, -0.46886997, -0.43195321],\n       [ 0.94307429,  1.63510561,  0.0502649 , -0.46886997,  2.33722904],\n       [ 2.27493734,  0.10828435, -0.43806527,  0.81281795, -0.43195321],\n       [-0.62666612, -0.76651942, -0.62548989, -0.46886997, -0.43195321],\n       [-0.66600439, -0.34358661, -0.52269601, -0.46886997, -0.43195321],\n       [ 2.40153737,  0.79677218,  0.14893452, -0.46886997,  0.18342062],\n       [ 0.09391613,  1.61452938,  2.56423569, -0.46886997, -0.43195321],\n       [-0.70296186, -0.26446128, -0.52736154, -0.46886997, -0.43195321],\n       [ 0.44208927,  4.18123112,  3.45974963, -0.46886997, -0.43195321],\n       [-0.78308892, -1.01087642, -0.63087146, -0.46886997,  0.89725427],\n       [-0.54691067,  0.98156598,  0.70367308, -0.46886997, -0.43195321],\n       [ 0.60991632, -0.09784872, -0.51965707, -0.46886997, -0.43195321],\n       [-0.76052643, -0.96000271, -0.62467519, -0.46886997,  0.33726408],\n       [ 1.93383484,  0.09531485, -0.29395536,  4.04373958, -0.43195321],\n       [-0.76653353, -0.6801267 , -0.54957002, -0.46886997, -0.43195321],\n       [-0.83650183,  2.64457877,  2.29128386, -0.46886997, -0.43195321],\n       [-0.51706228,  1.07292328,  2.54926935, -0.46886997, -0.43195321],\n       [-0.39173081,  1.0465606 ,  2.24162345, -0.46886997, -0.43195321],\n       [ 0.54861269, -0.32386364, -0.44453781, -0.46886997, -0.43195321],\n       [-0.41346552, -0.66397367, -0.62655268, -0.46886997, -0.43195321],\n       [-0.66187067, -0.05643144,  0.1018028 ,  1.19732432, -0.43195321],\n       [-0.61096157, -0.57797129, -0.5712147 , -0.46886997, -0.43195321],\n       [ 0.34407676,  0.4601909 ,  0.05937608, -0.46886997,  0.18342062],\n       [-0.46490255, -0.97695015, -0.62658651,  1.7874348 , -0.43195321]])\n\n\n\nnum_trans.shape\n\n(88, 5)\n\n\n\nX\n\n\n\n\n\n\n\n\nprovince\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n4\nAnhui\n32100.0\n2000\n2902.09\n31847\n0.000000\n0.000000\n0.000000\n1601508\nEast China\n1499110\n\n\n6\nAnhui\n66529.0\n2002\n3519.72\n38375\n0.000000\n0.000000\n0.000000\n1677840\nEast China\n2404936\n\n\n7\nAnhui\n52108.0\n2003\n3923.11\n36720\n0.000000\n0.000000\n0.000000\n1896479\nEast China\n2815820\n\n\n10\nAnhui\n279052.0\n2006\n6112.50\n139354\n0.000000\n0.000000\n0.324324\n3434548\nEast China\n5167300\n\n\n11\nAnhui\n178705.0\n2007\n7360.92\n299892\n0.000000\n0.000000\n0.324324\n4468640\nEast China\n7040099\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n347\nYunnan\n564400.0\n2007\n4772.52\n39453\n0.000000\n0.000000\n0.000000\n4867146\nSouthwest China\n6832541\n\n\n354\nZhejiang\n321686.0\n2002\n8003.67\n307610\n0.000000\n0.000000\n0.000000\n4958329\nEast China\n1962633\n\n\n355\nZhejiang\n260313.0\n2003\n9705.02\n498055\n1.214286\n0.035714\n0.035714\n6217715\nEast China\n2261631\n\n\n358\nZhejiang\n394795.0\n2006\n15718.47\n888935\n1.214286\n0.035714\n0.035714\n11537149\nEast China\n2553268\n\n\n359\nZhejiang\n0.0\n2007\n18753.73\n1036576\n0.047619\n0.000000\n0.000000\n16494981\nEast China\n2939778\n\n\n\n\n118 rows × 11 columns\n\n\n\n\n\n\nCategorical preprocessing\n\ncategorical_features = ['province', 'reg']\n\n\ncat_prep = Pipeline([\n    ('imputer', SimpleImputer(strategy='constant', fill_value='sk_missing')),\n    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))\n])\n\n\ncat_prep\n\nPipelinePipeline(steps=[('imputer',\n                 SimpleImputer(fill_value='sk_missing', strategy='constant')),\n                ('ohe', OneHotEncoder(handle_unknown='ignore', sparse=False))])SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\n\nRunning only on the categorical features\n\ncat_trans = cat_prep.fit_transform(X_train[categorical_features])\ncat_trans\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n\n\n\ncat_trans.shape\n\n(88, 33)"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#columntransformer-1",
    "href": "posts/2021-05-31-pandas-interoperability.html#columntransformer-1",
    "title": "Pandas Interoperability",
    "section": "ColumnTransformer!",
    "text": "ColumnTransformer!\n\nct = ColumnTransformer([\n   ('numerical', num_prep, numerical_features),\n   ('categorical', cat_prep, categorical_features)\n])\n\n\nct\n\nColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)\n\n\n\nX_trans = ct.fit_transform(X_train)\n\n\nX_trans[:, :5]\n\narray([[ 0.84445696, -0.23824571,  0.05133332, -0.46886997,  1.84019633],\n       [ 0.24766853,  2.07395803,  1.57444985, -0.46886997, -0.43195321],\n       [-0.72214407, -0.0446055 , -0.43939868, -0.46886997, -0.43195321],\n       [-0.48856958, -0.11992158, -0.34379901, -0.46886997, -0.43195321],\n       [ 0.85337791, -0.87758205, -0.54348651, -0.46886997, -0.43195321],\n       [-0.62992082, -0.83196322, -0.62382665, -0.46886997,  0.17004293],\n       [-0.83650183,  3.23074054,  4.28312405,  1.06326271, -0.43195321],\n       [ 1.76828095, -0.19541419, -0.41644878, -0.46886997, -0.43195321],\n       [ 1.78635605,  0.89747804,  0.79717537, -0.46886997,  3.97033652],\n       [-0.01481382, -0.57518987, -0.60442312,  3.47737968, -0.43195321],\n       [-0.33025804, -0.71766019, -0.62552372, -0.46886997, -0.43195321],\n       [-0.75781248, -0.70583032, -0.54596728, -0.46886997, -0.43195321],\n       [ 0.18859962, -0.10301925, -0.50475274, -0.46886997,  1.78339259],\n       [ 3.61705142,  1.91072514,  0.2322093 , -0.46886997, -0.43195321],\n       [-0.52047074, -0.52639929, -0.53731279, -0.46886997, -0.43195321],\n       [-0.35308192, -0.41382042, -0.15623907,  2.47473995, -0.43195321],\n       [ 0.51483816, -0.85369871, -0.61664935, -0.46886997, -0.43195321],\n       [ 2.42485843, -0.08843151,  0.16030939, -0.46886997,  1.84019633],\n       [-0.63055638, -0.3205428 , -0.53076977, -0.46886997, -0.43195321],\n       [-0.81570508, -0.2076344 , -0.22869714,  0.25314756, -0.43195321],\n       [-0.06101258, -0.78797637, -0.31514053, -0.46886997, -0.43195321],\n       [-0.022315  ,  0.78012485,  0.51382125, -0.46886997, -0.43195321],\n       [-0.37712312, -0.55331904, -0.3258642 ,  2.14150109, -0.43195321],\n       [ 2.33926246, -0.46826211, -0.59521328,  3.47737968, -0.43195321],\n       [ 0.62612061, -0.10351944, -0.37014586, -0.46886997, -0.43195321],\n       [-0.1455937 , -0.05181405, -0.01314966, -0.46886997,  3.97033652],\n       [-0.69280823, -0.04283426, -0.47187415, -0.46886997, -0.43195321],\n       [-0.15946076, -0.25170559, -0.5456459 , -0.46886997, -0.43195321],\n       [-0.80637666, -0.98226192, -0.59981116, -0.46886997, -0.43195321],\n       [-0.45208878, -0.48061962, -0.51288289, -0.46886997,  1.78339259],\n       [ 0.4867555 , -0.58727866, -0.62254962,  0.24317887, -0.43195321],\n       [-0.74721037, -0.26346288, -0.50770147, -0.46886997, -0.26580227],\n       [ 0.71498921, -0.96701118, -0.62406064,  0.76534802, -0.43195321],\n       [-0.6776111 , -0.6259812 , -0.48297275,  1.75272242, -0.43195321],\n       [ 0.20388901,  0.55832718,  0.6523297 , -0.46886997,  3.97033652],\n       [-0.68462536, -0.15766887,  0.45117911, -0.46886997,  1.22955614],\n       [-0.68130147, -0.59197843, -0.52937716, -0.46886997, -0.43195321],\n       [-0.1213603 ,  0.16498962, -0.23803102,  1.33242116, -0.43195321],\n       [-0.68649617, -0.8274596 , -0.61330032, -0.46886997,  0.42010287],\n       [-0.73244378, -0.41583293, -0.54606312, -0.46886997, -0.43195321],\n       [-0.75418874, -0.67194329, -0.56752738, -0.46886997, -0.43195321],\n       [-0.11822093, -0.18759169, -0.14469223,  2.47473995, -0.43195321],\n       [-0.5606009 , -0.89159703, -0.62039587, -0.46886997, -0.43195321],\n       [-0.70811556, -0.99767742, -0.6295606 ,  1.33242116, -0.43195321],\n       [-0.44276036, -0.47392695, -0.57065935, -0.46886997, -0.43195321],\n       [-0.61544895, -0.21965453, -0.37710046, -0.46886997, -0.43195321],\n       [-0.57791999, -0.10080471, -0.18860741, -0.46886997,  0.18342062],\n       [ 2.23213936, -0.34291185, -0.59568406, -0.46886997, -0.43195321],\n       [ 2.06354861,  1.39098988, -0.11068883, -0.46886997, -0.43195321],\n       [-0.79190991, -0.9574449 , -0.62376745,  0.24317887, -0.43195321],\n       [ 0.26443406,  2.61416753,  3.08558822,  1.06326271, -0.43195321],\n       [-0.01209987,  0.53594439,  0.23629128, -0.46886997, -0.43195321],\n       [-0.80634591, -0.96716418, -0.61755426, -0.46886997, -0.43195321],\n       [-0.16938374,  0.8696658 ,  0.77316551, -0.27051351, -0.23415447],\n       [-0.04311944,  0.18446741, -0.14938031,  0.73199078, -0.43195321],\n       [-0.68041476,  0.47595163,  1.08241827, -0.18405044, -0.43195321],\n       [-0.76631569, -0.33855142, -0.36484886, -0.46886997,  1.06490206],\n       [-0.71942755, -1.0021987 , -0.63005112, -0.46886997,  1.28493979],\n       [ 2.56991029,  0.81760341,  0.29109646, -0.46886997,  1.99108126],\n       [-0.74181833, -0.94662522, -0.62596349, -0.46886997, -0.43195321],\n       [-0.39172056, -0.75420507, -0.61813217,  0.73199078, -0.43195321],\n       [-0.74252565, -0.58032118, -0.59939112, -0.46886997, -0.43195321],\n       [-0.52984786,  0.82098506,  1.31844616, -0.46886997, -0.43195321],\n       [-0.75226924, -0.67038781, -0.54044193, -0.46886997, -0.43195321],\n       [-0.51722886, -0.15861235,  0.53360817, -0.46886997, -0.43195321],\n       [ 0.94307429,  1.63510561,  0.0502649 , -0.46886997,  2.33722904],\n       [ 2.27493734,  0.10828435, -0.43806527,  0.81281795, -0.43195321],\n       [-0.62666612, -0.76651942, -0.62548989, -0.46886997, -0.43195321],\n       [-0.66600439, -0.34358661, -0.52269601, -0.46886997, -0.43195321],\n       [ 2.40153737,  0.79677218,  0.14893452, -0.46886997,  0.18342062],\n       [ 0.09391613,  1.61452938,  2.56423569, -0.46886997, -0.43195321],\n       [-0.70296186, -0.26446128, -0.52736154, -0.46886997, -0.43195321],\n       [ 0.44208927,  4.18123112,  3.45974963, -0.46886997, -0.43195321],\n       [-0.78308892, -1.01087642, -0.63087146, -0.46886997,  0.89725427],\n       [-0.54691067,  0.98156598,  0.70367308, -0.46886997, -0.43195321],\n       [ 0.60991632, -0.09784872, -0.51965707, -0.46886997, -0.43195321],\n       [-0.76052643, -0.96000271, -0.62467519, -0.46886997,  0.33726408],\n       [ 1.93383484,  0.09531485, -0.29395536,  4.04373958, -0.43195321],\n       [-0.76653353, -0.6801267 , -0.54957002, -0.46886997, -0.43195321],\n       [-0.83650183,  2.64457877,  2.29128386, -0.46886997, -0.43195321],\n       [-0.51706228,  1.07292328,  2.54926935, -0.46886997, -0.43195321],\n       [-0.39173081,  1.0465606 ,  2.24162345, -0.46886997, -0.43195321],\n       [ 0.54861269, -0.32386364, -0.44453781, -0.46886997, -0.43195321],\n       [-0.41346552, -0.66397367, -0.62655268, -0.46886997, -0.43195321],\n       [-0.66187067, -0.05643144,  0.1018028 ,  1.19732432, -0.43195321],\n       [-0.61096157, -0.57797129, -0.5712147 , -0.46886997, -0.43195321],\n       [ 0.34407676,  0.4601909 ,  0.05937608, -0.46886997,  0.18342062],\n       [-0.46490255, -0.97695015, -0.62658651,  1.7874348 , -0.43195321]])\n\n\n\nX_trans[:, 5:]\n\narray([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 1., 0.],\n       [0., 0., 0., ..., 0., 0., 1.]])\n\n\n\nX_trans.shape\n\n(88, 38)\n\n\n\nLinear model\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n\n\nlin_reg = Pipeline([\n    ('preprocess', ct),\n    ('lin_reg', LinearRegression())\n])\nlin_reg\n\nPipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['general', 'gdp', 'fdi', 'i',\n                                                   'rr']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='sk_missing',\n                                                                                 strategy='constant')),\n                                                                  ('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['province', 'reg'])])),\n                ('lin_reg', LinearRegression())])preprocess: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)LinearRegressionLinearRegression()\n\n\n\nlin_reg.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['general', 'gdp', 'fdi', 'i',\n                                                   'rr']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='sk_missing',\n                                                                                 strategy='constant')),\n                                                                  ('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['province', 'reg'])])),\n                ('lin_reg', LinearRegression())])preprocess: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)LinearRegressionLinearRegression()\n\n\n\nlin_reg.score(X_train, y_train)\n\n0.9195619190476331"
  },
  {
    "objectID": "posts/2021-05-31-pandas-interoperability.html#random-forest",
    "href": "posts/2021-05-31-pandas-interoperability.html#random-forest",
    "title": "Pandas Interoperability",
    "section": "Random Forest",
    "text": "Random Forest\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nrf = Pipeline([\n    ('preprocess', ct),\n    ('log_reg', RandomForestRegressor(random_state=42))\n])\nrf\n\nPipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['general', 'gdp', 'fdi', 'i',\n                                                   'rr']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='sk_missing',\n                                                                                 strategy='constant')),\n                                                                  ('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['province', 'reg'])])),\n                ('log_reg', RandomForestRegressor(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nrf.fit(X_train, y_train)\n\nPipelinePipeline(steps=[('preprocess',\n                 ColumnTransformer(transformers=[('numerical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer()),\n                                                                  ('scaler',\n                                                                   StandardScaler())]),\n                                                  ['general', 'gdp', 'fdi', 'i',\n                                                   'rr']),\n                                                 ('categorical',\n                                                  Pipeline(steps=[('imputer',\n                                                                   SimpleImputer(fill_value='sk_missing',\n                                                                                 strategy='constant')),\n                                                                  ('ohe',\n                                                                   OneHotEncoder(handle_unknown='ignore',\n                                                                                 sparse=False))]),\n                                                  ['province', 'reg'])])),\n                ('log_reg', RandomForestRegressor(random_state=42))])preprocess: ColumnTransformerColumnTransformer(transformers=[('numerical',\n                                 Pipeline(steps=[('imputer', SimpleImputer()),\n                                                 ('scaler', StandardScaler())]),\n                                 ['general', 'gdp', 'fdi', 'i', 'rr']),\n                                ('categorical',\n                                 Pipeline(steps=[('imputer',\n                                                  SimpleImputer(fill_value='sk_missing',\n                                                                strategy='constant')),\n                                                 ('ohe',\n                                                  OneHotEncoder(handle_unknown='ignore',\n                                                                sparse=False))]),\n                                 ['province', 'reg'])])numerical['general', 'gdp', 'fdi', 'i', 'rr']SimpleImputerSimpleImputer()StandardScalerStandardScaler()categorical['province', 'reg']SimpleImputerSimpleImputer(fill_value='sk_missing', strategy='constant')OneHotEncoderOneHotEncoder(handle_unknown='ignore', sparse=False)RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\nrf.score(X_train, y_train)\n\n0.9503718502648469"
  },
  {
    "objectID": "posts/2020-10-07-Fiscal_data-sqlitedb-Copy1.html",
    "href": "posts/2020-10-07-Fiscal_data-sqlitedb-Copy1.html",
    "title": "Using sqlalchemy and pandas to read and write from and to a local sqlite database for Fiscal Data",
    "section": "",
    "text": "This post includes code adapted from python for finance and trading algorithms udemy course and python for finance and trading algorithms udemy course notebooks and the documentation here\n\nimport pandas as pd\nimport numpy as np\nimport pandas_datareader.data as web\nimport datetime\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\ndf=pd.read_csv('df_panel_fix.csv')\n\n\ndf\n\n\n\n\n\n\n\n\nUnnamed: 0\nprovince\nspecific\ngeneral\nyear\ngdp\nfdi\nrnr\nrr\ni\nfr\nreg\nit\n\n\n\n\n0\n0\nAnhui\n147002.0\nNaN\n1996\n2093.30\n50661\n0.000000\n0.000000\n0.000000\n1128873\nEast China\n631930\n\n\n1\n1\nAnhui\n151981.0\nNaN\n1997\n2347.32\n43443\n0.000000\n0.000000\n0.000000\n1356287\nEast China\n657860\n\n\n2\n2\nAnhui\n174930.0\nNaN\n1998\n2542.96\n27673\n0.000000\n0.000000\n0.000000\n1518236\nEast China\n889463\n\n\n3\n3\nAnhui\n285324.0\nNaN\n1999\n2712.34\n26131\nNaN\nNaN\nNaN\n1646891\nEast China\n1227364\n\n\n4\n4\nAnhui\n195580.0\n32100.0\n2000\n2902.09\n31847\n0.000000\n0.000000\n0.000000\n1601508\nEast China\n1499110\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n355\nZhejiang\n391292.0\n260313.0\n2003\n9705.02\n498055\n1.214286\n0.035714\n0.035714\n6217715\nEast China\n2261631\n\n\n356\n356\nZhejiang\n656175.0\n276652.0\n2004\n11648.70\n668128\n1.214286\n0.035714\n0.035714\nNaN\nEast China\n3162299\n\n\n357\n357\nZhejiang\n656175.0\nNaN\n2005\n13417.68\n772000\n1.214286\n0.035714\n0.035714\nNaN\nEast China\n2370200\n\n\n358\n358\nZhejiang\n1017303.0\n394795.0\n2006\n15718.47\n888935\n1.214286\n0.035714\n0.035714\n11537149\nEast China\n2553268\n\n\n359\n359\nZhejiang\n844647.0\n0.0\n2007\n18753.73\n1036576\n0.047619\n0.000000\n0.000000\n16494981\nEast China\n2939778\n\n\n\n\n360 rows × 13 columns\n\n\n\n\ndf_subset = df[[\"year\", \"reg\", \"province\", \"it\", \"specific\", 'gdp',\"fdi\"]]\ndf_subset\n\n\n\n\n\n\n\n\nyear\nreg\nprovince\nit\nspecific\ngdp\nfdi\n\n\n\n\n0\n1996\nEast China\nAnhui\n631930\n147002.0\n2093.30\n50661\n\n\n1\n1997\nEast China\nAnhui\n657860\n151981.0\n2347.32\n43443\n\n\n2\n1998\nEast China\nAnhui\n889463\n174930.0\n2542.96\n27673\n\n\n3\n1999\nEast China\nAnhui\n1227364\n285324.0\n2712.34\n26131\n\n\n4\n2000\nEast China\nAnhui\n1499110\n195580.0\n2902.09\n31847\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n2261631\n391292.0\n9705.02\n498055\n\n\n356\n2004\nEast China\nZhejiang\n3162299\n656175.0\n11648.70\n668128\n\n\n357\n2005\nEast China\nZhejiang\n2370200\n656175.0\n13417.68\n772000\n\n\n358\n2006\nEast China\nZhejiang\n2553268\n1017303.0\n15718.47\n888935\n\n\n359\n2007\nEast China\nZhejiang\n2939778\n844647.0\n18753.73\n1036576\n\n\n\n\n360 rows × 7 columns\n\n\n\n\ndf_subset.columns = [\"year\", \"region\", \"province\", \"it\", \"specific\", 'gdp',\"fdi\"]\n\n\ndf_subset\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\nit\nspecific\ngdp\nfdi\n\n\n\n\n0\n1996\nEast China\nAnhui\n631930\n147002.0\n2093.30\n50661\n\n\n1\n1997\nEast China\nAnhui\n657860\n151981.0\n2347.32\n43443\n\n\n2\n1998\nEast China\nAnhui\n889463\n174930.0\n2542.96\n27673\n\n\n3\n1999\nEast China\nAnhui\n1227364\n285324.0\n2712.34\n26131\n\n\n4\n2000\nEast China\nAnhui\n1499110\n195580.0\n2902.09\n31847\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n355\n2003\nEast China\nZhejiang\n2261631\n391292.0\n9705.02\n498055\n\n\n356\n2004\nEast China\nZhejiang\n3162299\n656175.0\n11648.70\n668128\n\n\n357\n2005\nEast China\nZhejiang\n2370200\n656175.0\n13417.68\n772000\n\n\n358\n2006\nEast China\nZhejiang\n2553268\n1017303.0\n15718.47\n888935\n\n\n359\n2007\nEast China\nZhejiang\n2939778\n844647.0\n18753.73\n1036576\n\n\n\n\n360 rows × 7 columns\n\n\n\n\nimport sqlalchemy as db\nfrom sqlalchemy import create_engine\nimport sqlite3\nimport pandas as pd\n\n\nengine = db.create_engine('sqlite:///fiscal.db')\nconnection = engine.connect()\nmetadata = db.MetaData()\n\n\nfiscal_table = db.Table('fiscal_table', metadata, \n    db.Column('year',db.Integer, nullable=True, index=False),\n    db.Column('region',db.Integer, nullable=True),\n    db.Column('province',db.Integer, nullable=True),\n    db.Column('it',db.Integer, nullable=True),\n    db.Column('specific',db.Integer, nullable=True),\n    db.Column('gdp',db.Integer, nullable=True),\n    db.Column('fdi', db.Numeric, nullable=True)\n)\n\n\nmetadata.create_all(engine) #Creates the table\n\n\ndf_subset.to_sql('fiscal_table', con=engine, if_exists='append', index=False)\n\n\nengine.execute(\"SELECT * FROM fiscal_table LIMIT 10\").fetchall()\n\n[(1996, 'East China', 'Anhui', 631930, 147002, 2093.3, 50661),\n (1997, 'East China', 'Anhui', 657860, 151981, 2347.32, 43443),\n (1998, 'East China', 'Anhui', 889463, 174930, 2542.96, 27673),\n (1999, 'East China', 'Anhui', 1227364, 285324, 2712.34, 26131),\n (2000, 'East China', 'Anhui', 1499110, 195580, 2902.09, 31847),\n (2001, 'East China', 'Anhui', 2165189, 250898, 3246.71, 33672),\n (2002, 'East China', 'Anhui', 2404936, 434149, 3519.72, 38375),\n (2003, 'East China', 'Anhui', 2815820, 619201, 3923.11, 36720),\n (2004, 'East China', 'Anhui', 3422176, 898441, 4759.3, 54669),\n (2005, 'East China', 'Anhui', 3874846, 898441, 5350.17, 69000)]\n\n\n\nsql = \"\"\"\nSELECT\n  year\n, region\n, province\n, it\n--, CURRENT_DATE()\nFROM fiscal_table\n\"\"\"\n\ncnxn = connection\n\n\ndf = pd.read_sql(sql, cnxn)\n\n\ndf.tail(30)\n\n\n\n\n\n\n\n\nyear\nregion\nprovince\nit\n\n\n\n\n330\n2002\nNorthwest China\nXinjiang\n2150325\n\n\n331\n2003\nNorthwest China\nXinjiang\n2355164\n\n\n332\n2004\nNorthwest China\nXinjiang\n2838346\n\n\n333\n2005\nNorthwest China\nXinjiang\n3421743\n\n\n334\n2006\nNorthwest China\nXinjiang\n4686125\n\n\n335\n2007\nNorthwest China\nXinjiang\n5502470\n\n\n336\n1996\nSouthwest China\nYunnan\n1374111\n\n\n337\n1997\nSouthwest China\nYunnan\n1452425\n\n\n338\n1998\nSouthwest China\nYunnan\n1617463\n\n\n339\n1999\nSouthwest China\nYunnan\n1888666\n\n\n340\n2000\nSouthwest China\nYunnan\n2254281\n\n\n341\n2001\nSouthwest China\nYunnan\n2856307\n\n\n342\n2002\nSouthwest China\nYunnan\n3035767\n\n\n343\n2003\nSouthwest China\nYunnan\n3388449\n\n\n344\n2004\nSouthwest China\nYunnan\n3957158\n\n\n345\n2005\nSouthwest China\nYunnan\n4280994\n\n\n346\n2006\nSouthwest China\nYunnan\n5046865\n\n\n347\n2007\nSouthwest China\nYunnan\n6832541\n\n\n348\n1996\nEast China\nZhejiang\n740327\n\n\n349\n1997\nEast China\nZhejiang\n814253\n\n\n350\n1998\nEast China\nZhejiang\n923455\n\n\n351\n1999\nEast China\nZhejiang\n1001703\n\n\n352\n2000\nEast China\nZhejiang\n1135215\n\n\n353\n2001\nEast China\nZhejiang\n1203372\n\n\n354\n2002\nEast China\nZhejiang\n1962633\n\n\n355\n2003\nEast China\nZhejiang\n2261631\n\n\n356\n2004\nEast China\nZhejiang\n3162299\n\n\n357\n2005\nEast China\nZhejiang\n2370200\n\n\n358\n2006\nEast China\nZhejiang\n2553268\n\n\n359\n2007\nEast China\nZhejiang\n2939778\n\n\n\n\n\n\n\n\n#df['it'].plot(figsize = (12, 8))"
  },
  {
    "objectID": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html",
    "href": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html",
    "title": "Text summarizer in Python, Notes from underground",
    "section": "",
    "text": "# Natural Language Tool Kit (NLTK)\nimport nltk\nnltk.download('stopwords')\n\n# Regular Expression for text preprocessing\nimport re\n\n# Heap (priority) queue algorithm to get the top sentences\nimport heapq\n\n# NumPy for numerical computing\nimport numpy as np\n\n# pandas for creating DataFrames\nimport pandas as pd\n\n# matplotlib for plot\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n[nltk_data] Downloading package stopwords to /home/gao/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#credit-code-from-httpsgithub.comlouisteo9personal-text-summarizer",
    "href": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#credit-code-from-httpsgithub.comlouisteo9personal-text-summarizer",
    "title": "Text summarizer in Python, Notes from underground",
    "section": "",
    "text": "# Natural Language Tool Kit (NLTK)\nimport nltk\nnltk.download('stopwords')\n\n# Regular Expression for text preprocessing\nimport re\n\n# Heap (priority) queue algorithm to get the top sentences\nimport heapq\n\n# NumPy for numerical computing\nimport numpy as np\n\n# pandas for creating DataFrames\nimport pandas as pd\n\n# matplotlib for plot\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n[nltk_data] Downloading package stopwords to /home/gao/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#load-text-data",
    "href": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#load-text-data",
    "title": "Text summarizer in Python, Notes from underground",
    "section": "Load text data",
    "text": "Load text data\n\nimport requests\nimport re\n\nr = requests.get(\"https://www.gutenberg.org/cache/epub/600/pg600.txt\")\nraw_text = r.text\n#print(raw_text[0:1000])\n\n\n# # load text file\n# with open('https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt', 'r') as f:\n#     file_data = f.read()\n\n\n# text_file = open(\"https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt\", \"r\")\n#lines = raw_text.readlines()\n\n\n#lines = raw_text.readlines()\n\n\n# text_file.close()\n\n\n# df = pd.read_txt('https://raw.githubusercontent.com/davidrkearney/colab-notebooks/main/datasets/NotesfromtheUnderground_Dostoevsky.txt')\n\nLet’s take a look at the text.\n\n# view text data\nprint(raw_text)\n\n﻿The Project Gutenberg eBook of Notes from the Underground, by Fyodor Dostoyevsky\n\nThis eBook is for the use of anyone anywhere in the United States and\nmost other parts of the world at no cost and with almost no restrictions\nwhatsoever. You may copy it, give it away or re-use it under the terms\nof the Project Gutenberg License included with this eBook or online at\nwww.gutenberg.org. If you are not located in the United States, you\nwill have to check the laws of the country where you are located before\nusing this eBook.\n\nTitle: Notes from the Underground\n\nAuthor: Fyodor Dostoyevsky\n\nTranslator: Constance Garnett\n\nRelease Date: July, 1996 [eBook #600]\n[Most recently updated: December 26, 2021]\n\nLanguage: English\n\n\nProduced by: Judith Boss. HTML version by Al Haines\n\n*** START OF THE PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND ***\n\n\n\n\nNotes from the Underground\n\nby Fyodor Dostoyevsky\n\n\nContents\n\n NOTES FROM THE UNDERGROUND\n\n PART I Underground\n I\n II\n III\n IV\n V\n VI\n VII\n VIII\n IX\n X\n XI\n\n PART II À Propos of the Wet Snow\n I\n II\n III\n IV\n V\n VI\n VII\n VIII\n IX\n X\n\n\n\n\nNOTES FROM THE UNDERGROUND[*]\nA NOVEL\n\n\n* The author of the diary and the diary itself are, of course,\nimaginary. Nevertheless it is clear that such persons as the writer of\nthese notes not only may, but positively must, exist in our society,\nwhen we consider the circumstances in the midst of which our society is\nformed. I have tried to expose to the view of the public more\ndistinctly than is commonly done, one of the characters of the recent\npast. He is one of the representatives of a generation still living. In\nthis fragment, entitled “Underground,” this person introduces himself\nand his views, and, as it were, tries to explain the causes owing to\nwhich he has made his appearance and was bound to make his appearance\nin our midst. In the second fragment there are added the actual notes\nof this person concerning certain events in his life.—AUTHOR’S NOTE.\n\n\n\n\nPART I\nUnderground\n\n\n\n\nI\n\n\nI am a sick man.... I am a spiteful man. I am an unattractive man. I\nbelieve my liver is diseased. However, I know nothing at all about my\ndisease, and do not know for certain what ails me. I don’t consult a\ndoctor for it, and never have, though I have a respect for medicine and\ndoctors. Besides, I am extremely superstitious, sufficiently so to\nrespect medicine, anyway (I am well-educated enough not to be\nsuperstitious, but I am superstitious). No, I refuse to consult a\ndoctor from spite. That you probably will not understand. Well, I\nunderstand it, though. Of course, I can’t explain who it is precisely\nthat I am mortifying in this case by my spite: I am perfectly well\naware that I cannot “pay out” the doctors by not consulting them; I\nknow better than anyone that by all this I am only injuring myself and\nno one else. But still, if I don’t consult a doctor it is from spite.\nMy liver is bad, well—let it get worse!\n\nI have been going on like that for a long time—twenty years. Now I am\nforty. I used to be in the government service, but am no longer. I was\na spiteful official. I was rude and took pleasure in being so. I did\nnot take bribes, you see, so I was bound to find a recompense in that,\nat least. (A poor jest, but I will not scratch it out. I wrote it\nthinking it would sound very witty; but now that I have seen myself\nthat I only wanted to show off in a despicable way, I will not scratch\nit out on purpose!)\n\nWhen petitioners used to come for information to the table at which I\nsat, I used to grind my teeth at them, and felt intense enjoyment when\nI succeeded in making anybody unhappy. I almost did succeed. For the\nmost part they were all timid people—of course, they were petitioners.\nBut of the uppish ones there was one officer in particular I could not\nendure. He simply would not be humble, and clanked his sword in a\ndisgusting way. I carried on a feud with him for eighteen months over\nthat sword. At last I got the better of him. He left off clanking it.\nThat happened in my youth, though.\n\nBut do you know, gentlemen, what was the chief point about my spite?\nWhy, the whole point, the real sting of it lay in the fact that\ncontinually, even in the moment of the acutest spleen, I was inwardly\nconscious with shame that I was not only not a spiteful but not even an\nembittered man, that I was simply scaring sparrows at random and\namusing myself by it. I might foam at the mouth, but bring me a doll to\nplay with, give me a cup of tea with sugar in it, and maybe I should be\nappeased. I might even be genuinely touched, though probably I should\ngrind my teeth at myself afterwards and lie awake at night with shame\nfor months after. That was my way.\n\nI was lying when I said just now that I was a spiteful official. I was\nlying from spite. I was simply amusing myself with the petitioners and\nwith the officer, and in reality I never could become spiteful. I was\nconscious every moment in myself of many, very many elements absolutely\nopposite to that. I felt them positively swarming in me, these opposite\nelements. I knew that they had been swarming in me all my life and\ncraving some outlet from me, but I would not let them, would not let\nthem, purposely would not let them come out. They tormented me till I\nwas ashamed: they drove me to convulsions and—sickened me, at last, how\nthey sickened me! Now, are not you fancying, gentlemen, that I am\nexpressing remorse for something now, that I am asking your forgiveness\nfor something? I am sure you are fancying that ... However, I assure\nyou I do not care if you are....\n\nIt was not only that I could not become spiteful, I did not know how to\nbecome anything; neither spiteful nor kind, neither a rascal nor an\nhonest man, neither a hero nor an insect. Now, I am living out my life\nin my corner, taunting myself with the spiteful and useless consolation\nthat an intelligent man cannot become anything seriously, and it is\nonly the fool who becomes anything. Yes, a man in the nineteenth\ncentury must and morally ought to be pre-eminently a characterless\ncreature; a man of character, an active man is pre-eminently a limited\ncreature. That is my conviction of forty years. I am forty years old\nnow, and you know forty years is a whole lifetime; you know it is\nextreme old age. To live longer than forty years is bad manners, is\nvulgar, immoral. Who does live beyond forty? Answer that, sincerely and\nhonestly I will tell you who do: fools and worthless fellows. I tell\nall old men that to their face, all these venerable old men, all these\nsilver-haired and reverend seniors! I tell the whole world that to its\nface! I have a right to say so, for I shall go on living to sixty\nmyself. To seventy! To eighty! ... Stay, let me take breath ...\n\nYou imagine no doubt, gentlemen, that I want to amuse you. You are\nmistaken in that, too. I am by no means such a mirthful person as you\nimagine, or as you may imagine; however, irritated by all this babble\n(and I feel that you are irritated) you think fit to ask me who I\nam—then my answer is, I am a collegiate assessor. I was in the service\nthat I might have something to eat (and solely for that reason), and\nwhen last year a distant relation left me six thousand roubles in his\nwill I immediately retired from the service and settled down in my\ncorner. I used to live in this corner before, but now I have settled\ndown in it. My room is a wretched, horrid one in the outskirts of the\ntown. My servant is an old country-woman, ill-natured from stupidity,\nand, moreover, there is always a nasty smell about her. I am told that\nthe Petersburg climate is bad for me, and that with my small means it\nis very expensive to live in Petersburg. I know all that better than\nall these sage and experienced counsellors and monitors.... But I am\nremaining in Petersburg; I am not going away from Petersburg! I am not\ngoing away because ... ech! Why, it is absolutely no matter whether I\nam going away or not going away.\n\nBut what can a decent man speak of with most pleasure?\n\nAnswer: Of himself.\n\nWell, so I will talk about myself.\n\n\n\n\nII\n\n\nI want now to tell you, gentlemen, whether you care to hear it or not,\nwhy I could not even become an insect. I tell you solemnly, that I have\nmany times tried to become an insect. But I was not equal even to that.\nI swear, gentlemen, that to be too conscious is an illness—a real\nthorough-going illness. For man’s everyday needs, it would have been\nquite enough to have the ordinary human consciousness, that is, half or\na quarter of the amount which falls to the lot of a cultivated man of\nour unhappy nineteenth century, especially one who has the fatal\nill-luck to inhabit Petersburg, the most theoretical and intentional\ntown on the whole terrestrial globe. (There are intentional and\nunintentional towns.) It would have been quite enough, for instance, to\nhave the consciousness by which all so-called direct persons and men of\naction live. I bet you think I am writing all this from affectation, to\nbe witty at the expense of men of action; and what is more, that from\nill-bred affectation, I am clanking a sword like my officer. But,\ngentlemen, whoever can pride himself on his diseases and even swagger\nover them?\n\nThough, after all, everyone does do that; people do pride themselves on\ntheir diseases, and I do, may be, more than anyone. We will not dispute\nit; my contention was absurd. But yet I am firmly persuaded that a\ngreat deal of consciousness, every sort of consciousness, in fact, is a\ndisease. I stick to that. Let us leave that, too, for a minute. Tell me\nthis: why does it happen that at the very, yes, at the very moments\nwhen I am most capable of feeling every refinement of all that is\n“sublime and beautiful,” as they used to say at one time, it would, as\nthough of design, happen to me not only to feel but to do such ugly\nthings, such that ... Well, in short, actions that all, perhaps,\ncommit; but which, as though purposely, occurred to me at the very time\nwhen I was most conscious that they ought not to be committed. The more\nconscious I was of goodness and of all that was “sublime and\nbeautiful,” the more deeply I sank into my mire and the more ready I\nwas to sink in it altogether. But the chief point was that all this\nwas, as it were, not accidental in me, but as though it were bound to\nbe so. It was as though it were my most normal condition, and not in\nthe least disease or depravity, so that at last all desire in me to\nstruggle against this depravity passed. It ended by my almost believing\n(perhaps actually believing) that this was perhaps my normal condition.\nBut at first, in the beginning, what agonies I endured in that\nstruggle! I did not believe it was the same with other people, and all\nmy life I hid this fact about myself as a secret. I was ashamed (even\nnow, perhaps, I am ashamed): I got to the point of feeling a sort of\nsecret abnormal, despicable enjoyment in returning home to my corner on\nsome disgusting Petersburg night, acutely conscious that that day I had\ncommitted a loathsome action again, that what was done could never be\nundone, and secretly, inwardly gnawing, gnawing at myself for it,\ntearing and consuming myself till at last the bitterness turned into a\nsort of shameful accursed sweetness, and at last—into positive real\nenjoyment! Yes, into enjoyment, into enjoyment! I insist upon that. I\nhave spoken of this because I keep wanting to know for a fact whether\nother people feel such enjoyment? I will explain; the enjoyment was\njust from the too intense consciousness of one’s own degradation; it\nwas from feeling oneself that one had reached the last barrier, that it\nwas horrible, but that it could not be otherwise; that there was no\nescape for you; that you never could become a different man; that even\nif time and faith were still left you to change into something\ndifferent you would most likely not wish to change; or if you did wish\nto, even then you would do nothing; because perhaps in reality there\nwas nothing for you to change into.\n\nAnd the worst of it was, and the root of it all, that it was all in\naccord with the normal fundamental laws of over-acute consciousness,\nand with the inertia that was the direct result of those laws, and that\nconsequently one was not only unable to change but could do absolutely\nnothing. Thus it would follow, as the result of acute consciousness,\nthat one is not to blame in being a scoundrel; as though that were any\nconsolation to the scoundrel once he has come to realise that he\nactually is a scoundrel. But enough.... Ech, I have talked a lot of\nnonsense, but what have I explained? How is enjoyment in this to be\nexplained? But I will explain it. I will get to the bottom of it! That\nis why I have taken up my pen....\n\nI, for instance, have a great deal of _amour propre_. I am as\nsuspicious and prone to take offence as a humpback or a dwarf. But upon\nmy word I sometimes have had moments when if I had happened to be\nslapped in the face I should, perhaps, have been positively glad of it.\nI say, in earnest, that I should probably have been able to discover\neven in that a peculiar sort of enjoyment—the enjoyment, of course, of\ndespair; but in despair there are the most intense enjoyments,\nespecially when one is very acutely conscious of the hopelessness of\none’s position. And when one is slapped in the face—why then the\nconsciousness of being rubbed into a pulp would positively overwhelm\none. The worst of it is, look at it which way one will, it still turns\nout that I was always the most to blame in everything. And what is most\nhumiliating of all, to blame for no fault of my own but, so to say,\nthrough the laws of nature. In the first place, to blame because I am\ncleverer than any of the people surrounding me. (I have always\nconsidered myself cleverer than any of the people surrounding me, and\nsometimes, would you believe it, have been positively ashamed of it. At\nany rate, I have all my life, as it were, turned my eyes away and never\ncould look people straight in the face.) To blame, finally, because\neven if I had had magnanimity, I should only have had more suffering\nfrom the sense of its uselessness. I should certainly have never been\nable to do anything from being magnanimous—neither to forgive, for my\nassailant would perhaps have slapped me from the laws of nature, and\none cannot forgive the laws of nature; nor to forget, for even if it\nwere owing to the laws of nature, it is insulting all the same.\nFinally, even if I had wanted to be anything but magnanimous, had\ndesired on the contrary to revenge myself on my assailant, I could not\nhave revenged myself on any one for anything because I should certainly\nnever have made up my mind to do anything, even if I had been able to.\nWhy should I not have made up my mind? About that in particular I want\nto say a few words.\n\n\n\n\nIII\n\n\nWith people who know how to revenge themselves and to stand up for\nthemselves in general, how is it done? Why, when they are possessed,\nlet us suppose, by the feeling of revenge, then for the time there is\nnothing else but that feeling left in their whole being. Such a\ngentleman simply dashes straight for his object like an infuriated bull\nwith its horns down, and nothing but a wall will stop him. (By the way:\nfacing the wall, such gentlemen—that is, the “direct” persons and men\nof action—are genuinely nonplussed. For them a wall is not an evasion,\nas for us people who think and consequently do nothing; it is not an\nexcuse for turning aside, an excuse for which we are always very glad,\nthough we scarcely believe in it ourselves, as a rule. No, they are\nnonplussed in all sincerity. The wall has for them something\ntranquillising, morally soothing, final—maybe even something mysterious\n... but of the wall later.)\n\nWell, such a direct person I regard as the real normal man, as his\ntender mother nature wished to see him when she graciously brought him\ninto being on the earth. I envy such a man till I am green in the face.\nHe is stupid. I am not disputing that, but perhaps the normal man\nshould be stupid, how do you know? Perhaps it is very beautiful, in\nfact. And I am the more persuaded of that suspicion, if one can call it\nso, by the fact that if you take, for instance, the antithesis of the\nnormal man, that is, the man of acute consciousness, who has come, of\ncourse, not out of the lap of nature but out of a retort (this is\nalmost mysticism, gentlemen, but I suspect this, too), this retort-made\nman is sometimes so nonplussed in the presence of his antithesis that\nwith all his exaggerated consciousness he genuinely thinks of himself\nas a mouse and not a man. It may be an acutely conscious mouse, yet it\nis a mouse, while the other is a man, and therefore, et caetera, et\ncaetera. And the worst of it is, he himself, his very own self, looks\non himself as a mouse; no one asks him to do so; and that is an\nimportant point. Now let us look at this mouse in action. Let us\nsuppose, for instance, that it feels insulted, too (and it almost\nalways does feel insulted), and wants to revenge itself, too. There may\neven be a greater accumulation of spite in it than in _l’homme de la\nnature et de la vérité_. The base and nasty desire to vent that spite\non its assailant rankles perhaps even more nastily in it than in\n_l’homme de la nature et de la vérité_. For through his innate\nstupidity the latter looks upon his revenge as justice pure and simple;\nwhile in consequence of his acute consciousness the mouse does not\nbelieve in the justice of it. To come at last to the deed itself, to\nthe very act of revenge. Apart from the one fundamental nastiness the\nluckless mouse succeeds in creating around it so many other nastinesses\nin the form of doubts and questions, adds to the one question so many\nunsettled questions that there inevitably works up around it a sort of\nfatal brew, a stinking mess, made up of its doubts, emotions, and of\nthe contempt spat upon it by the direct men of action who stand\nsolemnly about it as judges and arbitrators, laughing at it till their\nhealthy sides ache. Of course the only thing left for it is to dismiss\nall that with a wave of its paw, and, with a smile of assumed contempt\nin which it does not even itself believe, creep ignominiously into its\nmouse-hole. There in its nasty, stinking, underground home our\ninsulted, crushed and ridiculed mouse promptly becomes absorbed in\ncold, malignant and, above all, everlasting spite. For forty years\ntogether it will remember its injury down to the smallest, most\nignominious details, and every time will add, of itself, details still\nmore ignominious, spitefully teasing and tormenting itself with its own\nimagination. It will itself be ashamed of its imaginings, but yet it\nwill recall it all, it will go over and over every detail, it will\ninvent unheard of things against itself, pretending that those things\nmight happen, and will forgive nothing. Maybe it will begin to revenge\nitself, too, but, as it were, piecemeal, in trivial ways, from behind\nthe stove, incognito, without believing either in its own right to\nvengeance, or in the success of its revenge, knowing that from all its\nefforts at revenge it will suffer a hundred times more than he on whom\nit revenges itself, while he, I daresay, will not even scratch himself.\nOn its deathbed it will recall it all over again, with interest\naccumulated over all the years and ...\n\nBut it is just in that cold, abominable half despair, half belief, in\nthat conscious burying oneself alive for grief in the underworld for\nforty years, in that acutely recognised and yet partly doubtful\nhopelessness of one’s position, in that hell of unsatisfied desires\nturned inward, in that fever of oscillations, of resolutions determined\nfor ever and repented of again a minute later—that the savour of that\nstrange enjoyment of which I have spoken lies. It is so subtle, so\ndifficult of analysis, that persons who are a little limited, or even\nsimply persons of strong nerves, will not understand a single atom of\nit. “Possibly,” you will add on your own account with a grin, “people\nwill not understand it either who have never received a slap in the\nface,” and in that way you will politely hint to me that I, too,\nperhaps, have had the experience of a slap in the face in my life, and\nso I speak as one who knows. I bet that you are thinking that. But set\nyour minds at rest, gentlemen, I have not received a slap in the face,\nthough it is absolutely a matter of indifference to me what you may\nthink about it. Possibly, I even regret, myself, that I have given so\nfew slaps in the face during my life. But enough ... not another word\non that subject of such extreme interest to you.\n\nI will continue calmly concerning persons with strong nerves who do not\nunderstand a certain refinement of enjoyment. Though in certain\ncircumstances these gentlemen bellow their loudest like bulls, though\nthis, let us suppose, does them the greatest credit, yet, as I have\nsaid already, confronted with the impossible they subside at once. The\nimpossible means the stone wall! What stone wall? Why, of course, the\nlaws of nature, the deductions of natural science, mathematics. As soon\nas they prove to you, for instance, that you are descended from a\nmonkey, then it is no use scowling, accept it for a fact. When they\nprove to you that in reality one drop of your own fat must be dearer to\nyou than a hundred thousand of your fellow-creatures, and that this\nconclusion is the final solution of all so-called virtues and duties\nand all such prejudices and fancies, then you have just to accept it,\nthere is no help for it, for twice two is a law of mathematics. Just\ntry refuting it.\n\n“Upon my word, they will shout at you, it is no use protesting: it is a\ncase of twice two makes four! Nature does not ask your permission, she\nhas nothing to do with your wishes, and whether you like her laws or\ndislike them, you are bound to accept her as she is, and consequently\nall her conclusions. A wall, you see, is a wall ... and so on, and so\non.”\n\nMerciful Heavens! but what do I care for the laws of nature and\narithmetic, when, for some reason I dislike those laws and the fact\nthat twice two makes four? Of course I cannot break through the wall by\nbattering my head against it if I really have not the strength to knock\nit down, but I am not going to be reconciled to it simply because it is\na stone wall and I have not the strength.\n\nAs though such a stone wall really were a consolation, and really did\ncontain some word of conciliation, simply because it is as true as\ntwice two makes four. Oh, absurdity of absurdities! How much better it\nis to understand it all, to recognise it all, all the impossibilities\nand the stone wall; not to be reconciled to one of those\nimpossibilities and stone walls if it disgusts you to be reconciled to\nit; by the way of the most inevitable, logical combinations to reach\nthe most revolting conclusions on the everlasting theme, that even for\nthe stone wall you are yourself somehow to blame, though again it is as\nclear as day you are not to blame in the least, and therefore grinding\nyour teeth in silent impotence to sink into luxurious inertia, brooding\non the fact that there is no one even for you to feel vindictive\nagainst, that you have not, and perhaps never will have, an object for\nyour spite, that it is a sleight of hand, a bit of juggling, a\ncard-sharper’s trick, that it is simply a mess, no knowing what and no\nknowing who, but in spite of all these uncertainties and jugglings,\nstill there is an ache in you, and the more you do not know, the worse\nthe ache.\n\n\n\n\nIV\n\n\n“Ha, ha, ha! You will be finding enjoyment in toothache next,” you cry,\nwith a laugh.\n\n“Well, even in toothache there is enjoyment,” I answer. I had toothache\nfor a whole month and I know there is. In that case, of course, people\nare not spiteful in silence, but moan; but they are not candid moans,\nthey are malignant moans, and the malignancy is the whole point. The\nenjoyment of the sufferer finds expression in those moans; if he did\nnot feel enjoyment in them he would not moan. It is a good example,\ngentlemen, and I will develop it. Those moans express in the first\nplace all the aimlessness of your pain, which is so humiliating to your\nconsciousness; the whole legal system of nature on which you spit\ndisdainfully, of course, but from which you suffer all the same while\nshe does not. They express the consciousness that you have no enemy to\npunish, but that you have pain; the consciousness that in spite of all\npossible Wagenheims you are in complete slavery to your teeth; that if\nsomeone wishes it, your teeth will leave off aching, and if he does\nnot, they will go on aching another three months; and that finally if\nyou are still contumacious and still protest, all that is left you for\nyour own gratification is to thrash yourself or beat your wall with\nyour fist as hard as you can, and absolutely nothing more. Well, these\nmortal insults, these jeers on the part of someone unknown, end at last\nin an enjoyment which sometimes reaches the highest degree of\nvoluptuousness. I ask you, gentlemen, listen sometimes to the moans of\nan educated man of the nineteenth century suffering from toothache, on\nthe second or third day of the attack, when he is beginning to moan,\nnot as he moaned on the first day, that is, not simply because he has\ntoothache, not just as any coarse peasant, but as a man affected by\nprogress and European civilisation, a man who is “divorced from the\nsoil and the national elements,” as they express it now-a-days. His\nmoans become nasty, disgustingly malignant, and go on for whole days\nand nights. And of course he knows himself that he is doing himself no\nsort of good with his moans; he knows better than anyone that he is\nonly lacerating and harassing himself and others for nothing; he knows\nthat even the audience before whom he is making his efforts, and his\nwhole family, listen to him with loathing, do not put a ha’porth of\nfaith in him, and inwardly understand that he might moan differently,\nmore simply, without trills and flourishes, and that he is only amusing\nhimself like that from ill-humour, from malignancy. Well, in all these\nrecognitions and disgraces it is that there lies a voluptuous pleasure.\nAs though he would say: “I am worrying you, I am lacerating your\nhearts, I am keeping everyone in the house awake. Well, stay awake\nthen, you, too, feel every minute that I have toothache. I am not a\nhero to you now, as I tried to seem before, but simply a nasty person,\nan impostor. Well, so be it, then! I am very glad that you see through\nme. It is nasty for you to hear my despicable moans: well, let it be\nnasty; here I will let you have a nastier flourish in a minute....” You\ndo not understand even now, gentlemen? No, it seems our development and\nour consciousness must go further to understand all the intricacies of\nthis pleasure. You laugh? Delighted. My jests, gentlemen, are of course\nin bad taste, jerky, involved, lacking self-confidence. But of course\nthat is because I do not respect myself. Can a man of perception\nrespect himself at all?\n\n\n\n\nV\n\n\nCome, can a man who attempts to find enjoyment in the very feeling of\nhis own degradation possibly have a spark of respect for himself? I am\nnot saying this now from any mawkish kind of remorse. And, indeed, I\ncould never endure saying, “Forgive me, Papa, I won’t do it again,” not\nbecause I am incapable of saying that—on the contrary, perhaps just\nbecause I have been too capable of it, and in what a way, too. As\nthough of design I used to get into trouble in cases when I was not to\nblame in any way. That was the nastiest part of it. At the same time I\nwas genuinely touched and penitent, I used to shed tears and, of\ncourse, deceived myself, though I was not acting in the least and there\nwas a sick feeling in my heart at the time.... For that one could not\nblame even the laws of nature, though the laws of nature have\ncontinually all my life offended me more than anything. It is loathsome\nto remember it all, but it was loathsome even then. Of course, a minute\nor so later I would realise wrathfully that it was all a lie, a\nrevolting lie, an affected lie, that is, all this penitence, this\nemotion, these vows of reform. You will ask why did I worry myself with\nsuch antics: answer, because it was very dull to sit with one’s hands\nfolded, and so one began cutting capers. That is really it. Observe\nyourselves more carefully, gentlemen, then you will understand that it\nis so. I invented adventures for myself and made up a life, so as at\nleast to live in some way. How many times it has happened to me—well,\nfor instance, to take offence simply on purpose, for nothing; and one\nknows oneself, of course, that one is offended at nothing; that one is\nputting it on, but yet one brings oneself at last to the point of being\nreally offended. All my life I have had an impulse to play such pranks,\nso that in the end I could not control it in myself. Another time,\ntwice, in fact, I tried hard to be in love. I suffered, too, gentlemen,\nI assure you. In the depth of my heart there was no faith in my\nsuffering, only a faint stir of mockery, but yet I did suffer, and in\nthe real, orthodox way; I was jealous, beside myself ... and it was all\nfrom _ennui_, gentlemen, all from _ennui;_ inertia overcame me. You\nknow the direct, legitimate fruit of consciousness is inertia, that is,\nconscious sitting-with-the-hands-folded. I have referred to this\nalready. I repeat, I repeat with emphasis: all “direct” persons and men\nof action are active just because they are stupid and limited. How\nexplain that? I will tell you: in consequence of their limitation they\ntake immediate and secondary causes for primary ones, and in that way\npersuade themselves more quickly and easily than other people do that\nthey have found an infallible foundation for their activity, and their\nminds are at ease and you know that is the chief thing. To begin to\nact, you know, you must first have your mind completely at ease and no\ntrace of doubt left in it. Why, how am I, for example, to set my mind\nat rest? Where are the primary causes on which I am to build? Where are\nmy foundations? Where am I to get them from? I exercise myself in\nreflection, and consequently with me every primary cause at once draws\nafter itself another still more primary, and so on to infinity. That is\njust the essence of every sort of consciousness and reflection. It must\nbe a case of the laws of nature again. What is the result of it in the\nend? Why, just the same. Remember I spoke just now of vengeance. (I am\nsure you did not take it in.) I said that a man revenges himself\nbecause he sees justice in it. Therefore he has found a primary cause,\nthat is, justice. And so he is at rest on all sides, and consequently\nhe carries out his revenge calmly and successfully, being persuaded\nthat he is doing a just and honest thing. But I see no justice in it, I\nfind no sort of virtue in it either, and consequently if I attempt to\nrevenge myself, it is only out of spite. Spite, of course, might\novercome everything, all my doubts, and so might serve quite\nsuccessfully in place of a primary cause, precisely because it is not a\ncause. But what is to be done if I have not even spite (I began with\nthat just now, you know). In consequence again of those accursed laws\nof consciousness, anger in me is subject to chemical disintegration.\nYou look into it, the object flies off into air, your reasons\nevaporate, the criminal is not to be found, the wrong becomes not a\nwrong but a phantom, something like the toothache, for which no one is\nto blame, and consequently there is only the same outlet left\nagain—that is, to beat the wall as hard as you can. So you give it up\nwith a wave of the hand because you have not found a fundamental cause.\nAnd try letting yourself be carried away by your feelings, blindly,\nwithout reflection, without a primary cause, repelling consciousness at\nleast for a time; hate or love, if only not to sit with your hands\nfolded. The day after tomorrow, at the latest, you will begin despising\nyourself for having knowingly deceived yourself. Result: a soap-bubble\nand inertia. Oh, gentlemen, do you know, perhaps I consider myself an\nintelligent man, only because all my life I have been able neither to\nbegin nor to finish anything. Granted I am a babbler, a harmless\nvexatious babbler, like all of us. But what is to be done if the direct\nand sole vocation of every intelligent man is babble, that is, the\nintentional pouring of water through a sieve?\n\n\n\n\nVI\n\n\nOh, if I had done nothing simply from laziness! Heavens, how I should\nhave respected myself, then. I should have respected myself because I\nshould at least have been capable of being lazy; there would at least\nhave been one quality, as it were, positive in me, in which I could\nhave believed myself. Question: What is he? Answer: A sluggard; how\nvery pleasant it would have been to hear that of oneself! It would mean\nthat I was positively defined, it would mean that there was something\nto say about me. “Sluggard”—why, it is a calling and vocation, it is a\ncareer. Do not jest, it is so. I should then be a member of the best\nclub by right, and should find my occupation in continually respecting\nmyself. I knew a gentleman who prided himself all his life on being a\nconnoisseur of Lafitte. He considered this as his positive virtue, and\nnever doubted himself. He died, not simply with a tranquil, but with a\ntriumphant conscience, and he was quite right, too. Then I should have\nchosen a career for myself, I should have been a sluggard and a\nglutton, not a simple one, but, for instance, one with sympathies for\neverything sublime and beautiful. How do you like that? I have long had\nvisions of it. That “sublime and beautiful” weighs heavily on my mind\nat forty But that is at forty; then—oh, then it would have been\ndifferent! I should have found for myself a form of activity in keeping\nwith it, to be precise, drinking to the health of everything “sublime\nand beautiful.” I should have snatched at every opportunity to drop a\ntear into my glass and then to drain it to all that is “sublime and\nbeautiful.” I should then have turned everything into the sublime and\nthe beautiful; in the nastiest, unquestionable trash, I should have\nsought out the sublime and the beautiful. I should have exuded tears\nlike a wet sponge. An artist, for instance, paints a picture worthy of\nGay. At once I drink to the health of the artist who painted the\npicture worthy of Gay, because I love all that is “sublime and\nbeautiful.” An author has written _As you will:_ at once I drink to the\nhealth of “anyone you will” because I love all that is “sublime and\nbeautiful.”\n\nI should claim respect for doing so. I should persecute anyone who\nwould not show me respect. I should live at ease, I should die with\ndignity, why, it is charming, perfectly charming! And what a good round\nbelly I should have grown, what a treble chin I should have\nestablished, what a ruby nose I should have coloured for myself, so\nthat everyone would have said, looking at me: “Here is an asset! Here\nis something real and solid!” And, say what you like, it is very\nagreeable to hear such remarks about oneself in this negative age.\n\n\n\n\nVII\n\n\nBut these are all golden dreams. Oh, tell me, who was it first\nannounced, who was it first proclaimed, that man only does nasty things\nbecause he does not know his own interests; and that if he were\nenlightened, if his eyes were opened to his real normal interests, man\nwould at once cease to do nasty things, would at once become good and\nnoble because, being enlightened and understanding his real advantage,\nhe would see his own advantage in the good and nothing else, and we all\nknow that not one man can, consciously, act against his own interests,\nconsequently, so to say, through necessity, he would begin doing good?\nOh, the babe! Oh, the pure, innocent child! Why, in the first place,\nwhen in all these thousands of years has there been a time when man has\nacted only from his own interest? What is to be done with the millions\nof facts that bear witness that men, _consciously_, that is fully\nunderstanding their real interests, have left them in the background\nand have rushed headlong on another path, to meet peril and danger,\ncompelled to this course by nobody and by nothing, but, as it were,\nsimply disliking the beaten track, and have obstinately, wilfully,\nstruck out another difficult, absurd way, seeking it almost in the\ndarkness. So, I suppose, this obstinacy and perversity were pleasanter\nto them than any advantage.... Advantage! What is advantage? And will\nyou take it upon yourself to define with perfect accuracy in what the\nadvantage of man consists? And what if it so happens that a man’s\nadvantage, _sometimes_, not only may, but even must, consist in his\ndesiring in certain cases what is harmful to himself and not\nadvantageous. And if so, if there can be such a case, the whole\nprinciple falls into dust. What do you think—are there such cases? You\nlaugh; laugh away, gentlemen, but only answer me: have man’s advantages\nbeen reckoned up with perfect certainty? Are there not some which not\nonly have not been included but cannot possibly be included under any\nclassification? You see, you gentlemen have, to the best of my\nknowledge, taken your whole register of human advantages from the\naverages of statistical figures and politico-economical formulas. Your\nadvantages are prosperity, wealth, freedom, peace—and so on, and so on.\nSo that the man who should, for instance, go openly and knowingly in\nopposition to all that list would to your thinking, and indeed mine,\ntoo, of course, be an obscurantist or an absolute madman: would not he?\nBut, you know, this is what is surprising: why does it so happen that\nall these statisticians, sages and lovers of humanity, when they reckon\nup human advantages invariably leave out one? They don’t even take it\ninto their reckoning in the form in which it should be taken, and the\nwhole reckoning depends upon that. It would be no greater matter, they\nwould simply have to take it, this advantage, and add it to the list.\nBut the trouble is, that this strange advantage does not fall under any\nclassification and is not in place in any list. I have a friend for\ninstance ... Ech! gentlemen, but of course he is your friend, too; and\nindeed there is no one, no one to whom he is not a friend! When he\nprepares for any undertaking this gentleman immediately explains to\nyou, elegantly and clearly, exactly how he must act in accordance with\nthe laws of reason and truth. What is more, he will talk to you with\nexcitement and passion of the true normal interests of man; with irony\nhe will upbraid the short-sighted fools who do not understand their own\ninterests, nor the true significance of virtue; and, within a quarter\nof an hour, without any sudden outside provocation, but simply through\nsomething inside him which is stronger than all his interests, he will\ngo off on quite a different tack—that is, act in direct opposition to\nwhat he has just been saying about himself, in opposition to the laws\nof reason, in opposition to his own advantage, in fact in opposition to\neverything ... I warn you that my friend is a compound personality and\ntherefore it is difficult to blame him as an individual. The fact is,\ngentlemen, it seems there must really exist something that is dearer to\nalmost every man than his greatest advantages, or (not to be illogical)\nthere is a most advantageous advantage (the very one omitted of which\nwe spoke just now) which is more important and more advantageous than\nall other advantages, for the sake of which a man if necessary is ready\nto act in opposition to all laws; that is, in opposition to reason,\nhonour, peace, prosperity—in fact, in opposition to all those excellent\nand useful things if only he can attain that fundamental, most\nadvantageous advantage which is dearer to him than all. “Yes, but it’s\nadvantage all the same,” you will retort. But excuse me, I’ll make the\npoint clear, and it is not a case of playing upon words. What matters\nis, that this advantage is remarkable from the very fact that it breaks\ndown all our classifications, and continually shatters every system\nconstructed by lovers of mankind for the benefit of mankind. In fact,\nit upsets everything. But before I mention this advantage to you, I\nwant to compromise myself personally, and therefore I boldly declare\nthat all these fine systems, all these theories for explaining to\nmankind their real normal interests, in order that inevitably striving\nto pursue these interests they may at once become good and noble—are,\nin my opinion, so far, mere logical exercises! Yes, logical exercises.\nWhy, to maintain this theory of the regeneration of mankind by means of\nthe pursuit of his own advantage is to my mind almost the same thing\n... as to affirm, for instance, following Buckle, that through\ncivilisation mankind becomes softer, and consequently less bloodthirsty\nand less fitted for warfare. Logically it does seem to follow from his\narguments. But man has such a predilection for systems and abstract\ndeductions that he is ready to distort the truth intentionally, he is\nready to deny the evidence of his senses only to justify his logic. I\ntake this example because it is the most glaring instance of it. Only\nlook about you: blood is being spilt in streams, and in the merriest\nway, as though it were champagne. Take the whole of the nineteenth\ncentury in which Buckle lived. Take Napoleon—the Great and also the\npresent one. Take North America—the eternal union. Take the farce of\nSchleswig-Holstein.... And what is it that civilisation softens in us?\nThe only gain of civilisation for mankind is the greater capacity for\nvariety of sensations—and absolutely nothing more. And through the\ndevelopment of this many-sidedness man may come to finding enjoyment in\nbloodshed. In fact, this has already happened to him. Have you noticed\nthat it is the most civilised gentlemen who have been the subtlest\nslaughterers, to whom the Attilas and Stenka Razins could not hold a\ncandle, and if they are not so conspicuous as the Attilas and Stenka\nRazins it is simply because they are so often met with, are so ordinary\nand have become so familiar to us. In any case civilisation has made\nmankind if not more bloodthirsty, at least more vilely, more\nloathsomely bloodthirsty. In old days he saw justice in bloodshed and\nwith his conscience at peace exterminated those he thought proper. Now\nwe do think bloodshed abominable and yet we engage in this abomination,\nand with more energy than ever. Which is worse? Decide that for\nyourselves. They say that Cleopatra (excuse an instance from Roman\nhistory) was fond of sticking gold pins into her slave-girls’ breasts\nand derived gratification from their screams and writhings. You will\nsay that that was in the comparatively barbarous times; that these are\nbarbarous times too, because also, comparatively speaking, pins are\nstuck in even now; that though man has now learned to see more clearly\nthan in barbarous ages, he is still far from having learnt to act as\nreason and science would dictate. But yet you are fully convinced that\nhe will be sure to learn when he gets rid of certain old bad habits,\nand when common sense and science have completely re-educated human\nnature and turned it in a normal direction. You are confident that then\nman will cease from _intentional_ error and will, so to say, be\ncompelled not to want to set his will against his normal interests.\nThat is not all; then, you say, science itself will teach man (though\nto my mind it’s a superfluous luxury) that he never has really had any\ncaprice or will of his own, and that he himself is something of the\nnature of a piano-key or the stop of an organ, and that there are,\nbesides, things called the laws of nature; so that everything he does\nis not done by his willing it, but is done of itself, by the laws of\nnature. Consequently we have only to discover these laws of nature, and\nman will no longer have to answer for his actions and life will become\nexceedingly easy for him. All human actions will then, of course, be\ntabulated according to these laws, mathematically, like tables of\nlogarithms up to 108,000, and entered in an index; or, better still,\nthere would be published certain edifying works of the nature of\nencyclopaedic lexicons, in which everything will be so clearly\ncalculated and explained that there will be no more incidents or\nadventures in the world.\n\nThen—this is all what you say—new economic relations will be\nestablished, all ready-made and worked out with mathematical\nexactitude, so that every possible question will vanish in the\ntwinkling of an eye, simply because every possible answer to it will be\nprovided. Then the “Palace of Crystal” will be built. Then ... In fact,\nthose will be halcyon days. Of course there is no guaranteeing (this is\nmy comment) that it will not be, for instance, frightfully dull then\n(for what will one have to do when everything will be calculated and\ntabulated), but on the other hand everything will be extraordinarily\nrational. Of course boredom may lead you to anything. It is boredom\nsets one sticking golden pins into people, but all that would not\nmatter. What is bad (this is my comment again) is that I dare say\npeople will be thankful for the gold pins then. Man is stupid, you\nknow, phenomenally stupid; or rather he is not at all stupid, but he is\nso ungrateful that you could not find another like him in all creation.\nI, for instance, would not be in the least surprised if all of a\nsudden, _à propos_ of nothing, in the midst of general prosperity a\ngentleman with an ignoble, or rather with a reactionary and ironical,\ncountenance were to arise and, putting his arms akimbo, say to us all:\n“I say, gentleman, hadn’t we better kick over the whole show and\nscatter rationalism to the winds, simply to send these logarithms to\nthe devil, and to enable us to live once more at our own sweet foolish\nwill!” That again would not matter, but what is annoying is that he\nwould be sure to find followers—such is the nature of man. And all that\nfor the most foolish reason, which, one would think, was hardly worth\nmentioning: that is, that man everywhere and at all times, whoever he\nmay be, has preferred to act as he chose and not in the least as his\nreason and advantage dictated. And one may choose what is contrary to\none’s own interests, and sometimes one _positively ought_ (that is my\nidea). One’s own free unfettered choice, one’s own caprice, however\nwild it may be, one’s own fancy worked up at times to frenzy—is that\nvery “most advantageous advantage” which we have overlooked, which\ncomes under no classification and against which all systems and\ntheories are continually being shattered to atoms. And how do these\nwiseacres know that man wants a normal, a virtuous choice? What has\nmade them conceive that man must want a rationally advantageous choice?\nWhat man wants is simply _independent_ choice, whatever that\nindependence may cost and wherever it may lead. And choice, of course,\nthe devil only knows what choice.\n\n\n\n\nVIII\n\n\n“Ha! ha! ha! But you know there is no such thing as choice in reality,\nsay what you like,” you will interpose with a chuckle. “Science has\nsucceeded in so far analysing man that we know already that choice and\nwhat is called freedom of will is nothing else than—”\n\nStay, gentlemen, I meant to begin with that myself I confess, I was\nrather frightened. I was just going to say that the devil only knows\nwhat choice depends on, and that perhaps that was a very good thing,\nbut I remembered the teaching of science ... and pulled myself up. And\nhere you have begun upon it. Indeed, if there really is some day\ndiscovered a formula for all our desires and caprices—that is, an\nexplanation of what they depend upon, by what laws they arise, how they\ndevelop, what they are aiming at in one case and in another and so on,\nthat is a real mathematical formula—then, most likely, man will at once\ncease to feel desire, indeed, he will be certain to. For who would want\nto choose by rule? Besides, he will at once be transformed from a human\nbeing into an organ-stop or something of the sort; for what is a man\nwithout desires, without free will and without choice, if not a stop in\nan organ? What do you think? Let us reckon the chances—can such a thing\nhappen or not?\n\n“H’m!” you decide. “Our choice is usually mistaken from a false view of\nour advantage. We sometimes choose absolute nonsense because in our\nfoolishness we see in that nonsense the easiest means for attaining a\nsupposed advantage. But when all that is explained and worked out on\npaper (which is perfectly possible, for it is contemptible and\nsenseless to suppose that some laws of nature man will never\nunderstand), then certainly so-called desires will no longer exist. For\nif a desire should come into conflict with reason we shall then reason\nand not desire, because it will be impossible retaining our reason to\nbe _senseless_ in our desires, and in that way knowingly act against\nreason and desire to injure ourselves. And as all choice and reasoning\ncan be really calculated—because there will some day be discovered the\nlaws of our so-called free will—so, joking apart, there may one day be\nsomething like a table constructed of them, so that we really shall\nchoose in accordance with it. If, for instance, some day they calculate\nand prove to me that I made a long nose at someone because I could not\nhelp making a long nose at him and that I had to do it in that\nparticular way, what _freedom_ is left me, especially if I am a learned\nman and have taken my degree somewhere? Then I should be able to\ncalculate my whole life for thirty years beforehand. In short, if this\ncould be arranged there would be nothing left for us to do; anyway, we\nshould have to understand that. And, in fact, we ought unwearyingly to\nrepeat to ourselves that at such and such a time and in such and such\ncircumstances nature does not ask our leave; that we have got to take\nher as she is and not fashion her to suit our fancy, and if we really\naspire to formulas and tables of rules, and well, even ... to the\nchemical retort, there’s no help for it, we must accept the retort too,\nor else it will be accepted without our consent....”\n\nYes, but here I come to a stop! Gentlemen, you must excuse me for being\nover-philosophical; it’s the result of forty years underground! Allow\nme to indulge my fancy. You see, gentlemen, reason is an excellent\nthing, there’s no disputing that, but reason is nothing but reason and\nsatisfies only the rational side of man’s nature, while will is a\nmanifestation of the whole life, that is, of the whole human life\nincluding reason and all the impulses. And although our life, in this\nmanifestation of it, is often worthless, yet it is life and not simply\nextracting square roots. Here I, for instance, quite naturally want to\nlive, in order to satisfy all my capacities for life, and not simply my\ncapacity for reasoning, that is, not simply one twentieth of my\ncapacity for life. What does reason know? Reason only knows what it has\nsucceeded in learning (some things, perhaps, it will never learn; this\nis a poor comfort, but why not say so frankly?) and human nature acts\nas a whole, with everything that is in it, consciously or\nunconsciously, and, even if it goes wrong, it lives. I suspect,\ngentlemen, that you are looking at me with compassion; you tell me\nagain that an enlightened and developed man, such, in short, as the\nfuture man will be, cannot consciously desire anything disadvantageous\nto himself, that that can be proved mathematically. I thoroughly agree,\nit can—by mathematics. But I repeat for the hundredth time, there is\none case, one only, when man may consciously, purposely, desire what is\ninjurious to himself, what is stupid, very stupid—simply in order to\nhave the right to desire for himself even what is very stupid and not\nto be bound by an obligation to desire only what is sensible. Of\ncourse, this very stupid thing, this caprice of ours, may be in\nreality, gentlemen, more advantageous for us than anything else on\nearth, especially in certain cases. And in particular it may be more\nadvantageous than any advantage even when it does us obvious harm, and\ncontradicts the soundest conclusions of our reason concerning our\nadvantage—for in any circumstances it preserves for us what is most\nprecious and most important—that is, our personality, our\nindividuality. Some, you see, maintain that this really is the most\nprecious thing for mankind; choice can, of course, if it chooses, be in\nagreement with reason; and especially if this be not abused but kept\nwithin bounds. It is profitable and sometimes even praiseworthy. But\nvery often, and even most often, choice is utterly and stubbornly\nopposed to reason ... and ... and ... do you know that that, too, is\nprofitable, sometimes even praiseworthy? Gentlemen, let us suppose that\nman is not stupid. (Indeed one cannot refuse to suppose that, if only\nfrom the one consideration, that, if man is stupid, then who is wise?)\nBut if he is not stupid, he is monstrously ungrateful! Phenomenally\nungrateful. In fact, I believe that the best definition of man is the\nungrateful biped. But that is not all, that is not his worst defect;\nhis worst defect is his perpetual moral obliquity, perpetual—from the\ndays of the Flood to the Schleswig-Holstein period. Moral obliquity and\nconsequently lack of good sense; for it has long been accepted that\nlack of good sense is due to no other cause than moral obliquity. Put\nit to the test and cast your eyes upon the history of mankind. What\nwill you see? Is it a grand spectacle? Grand, if you like. Take the\nColossus of Rhodes, for instance, that’s worth something. With good\nreason Mr. Anaevsky testifies of it that some say that it is the work\nof man’s hands, while others maintain that it has been created by\nnature herself. Is it many-coloured? May be it is many-coloured, too:\nif one takes the dress uniforms, military and civilian, of all peoples\nin all ages—that alone is worth something, and if you take the undress\nuniforms you will never get to the end of it; no historian would be\nequal to the job. Is it monotonous? May be it’s monotonous too: it’s\nfighting and fighting; they are fighting now, they fought first and\nthey fought last—you will admit, that it is almost too monotonous. In\nshort, one may say anything about the history of the world—anything\nthat might enter the most disordered imagination. The only thing one\ncan’t say is that it’s rational. The very word sticks in one’s throat.\nAnd, indeed, this is the odd thing that is continually happening: there\nare continually turning up in life moral and rational persons, sages\nand lovers of humanity who make it their object to live all their lives\nas morally and rationally as possible, to be, so to speak, a light to\ntheir neighbours simply in order to show them that it is possible to\nlive morally and rationally in this world. And yet we all know that\nthose very people sooner or later have been false to themselves,\nplaying some queer trick, often a most unseemly one. Now I ask you:\nwhat can be expected of man since he is a being endowed with strange\nqualities? Shower upon him every earthly blessing, drown him in a sea\nof happiness, so that nothing but bubbles of bliss can be seen on the\nsurface; give him economic prosperity, such that he should have nothing\nelse to do but sleep, eat cakes and busy himself with the continuation\nof his species, and even then out of sheer ingratitude, sheer spite,\nman would play you some nasty trick. He would even risk his cakes and\nwould deliberately desire the most fatal rubbish, the most uneconomical\nabsurdity, simply to introduce into all this positive good sense his\nfatal fantastic element. It is just his fantastic dreams, his vulgar\nfolly that he will desire to retain, simply in order to prove to\nhimself—as though that were so necessary—that men still are men and not\nthe keys of a piano, which the laws of nature threaten to control so\ncompletely that soon one will be able to desire nothing but by the\ncalendar. And that is not all: even if man really were nothing but a\npiano-key, even if this were proved to him by natural science and\nmathematics, even then he would not become reasonable, but would\npurposely do something perverse out of simple ingratitude, simply to\ngain his point. And if he does not find means he will contrive\ndestruction and chaos, will contrive sufferings of all sorts, only to\ngain his point! He will launch a curse upon the world, and as only man\ncan curse (it is his privilege, the primary distinction between him and\nother animals), may be by his curse alone he will attain his\nobject—that is, convince himself that he is a man and not a piano-key!\nIf you say that all this, too, can be calculated and tabulated—chaos\nand darkness and curses, so that the mere possibility of calculating it\nall beforehand would stop it all, and reason would reassert itself,\nthen man would purposely go mad in order to be rid of reason and gain\nhis point! I believe in it, I answer for it, for the whole work of man\nreally seems to consist in nothing but proving to himself every minute\nthat he is a man and not a piano-key! It may be at the cost of his\nskin, it may be by cannibalism! And this being so, can one help being\ntempted to rejoice that it has not yet come off, and that desire still\ndepends on something we don’t know?\n\nYou will scream at me (that is, if you condescend to do so) that no one\nis touching my free will, that all they are concerned with is that my\nwill should of itself, of its own free will, coincide with my own\nnormal interests, with the laws of nature and arithmetic.\n\nGood heavens, gentlemen, what sort of free will is left when we come to\ntabulation and arithmetic, when it will all be a case of twice two make\nfour? Twice two makes four without my will. As if free will meant that!\n\n\n\n\nIX\n\n\nGentlemen, I am joking, and I know myself that my jokes are not\nbrilliant, but you know one can take everything as a joke. I am,\nperhaps, jesting against the grain. Gentlemen, I am tormented by\nquestions; answer them for me. You, for instance, want to cure men of\ntheir old habits and reform their will in accordance with science and\ngood sense. But how do you know, not only that it is possible, but also\nthat it is _desirable_ to reform man in that way? And what leads you to\nthe conclusion that man’s inclinations _need_ reforming? In short, how\ndo you know that such a reformation will be a benefit to man? And to go\nto the root of the matter, why are you so positively convinced that not\nto act against his real normal interests guaranteed by the conclusions\nof reason and arithmetic is certainly always advantageous for man and\nmust always be a law for mankind? So far, you know, this is only your\nsupposition. It may be the law of logic, but not the law of humanity.\nYou think, gentlemen, perhaps that I am mad? Allow me to defend myself.\nI agree that man is pre-eminently a creative animal, predestined to\nstrive consciously for an object and to engage in engineering—that is,\nincessantly and eternally to make new roads, _wherever they may lead_.\nBut the reason why he wants sometimes to go off at a tangent may just\nbe that he is _predestined_ to make the road, and perhaps, too, that\nhowever stupid the “direct” practical man may be, the thought sometimes\nwill occur to him that the road almost always does lead _somewhere_,\nand that the destination it leads to is less important than the process\nof making it, and that the chief thing is to save the well-conducted\nchild from despising engineering, and so giving way to the fatal\nidleness, which, as we all know, is the mother of all the vices. Man\nlikes to make roads and to create, that is a fact beyond dispute. But\nwhy has he such a passionate love for destruction and chaos also? Tell\nme that! But on that point I want to say a couple of words myself. May\nit not be that he loves chaos and destruction (there can be no\ndisputing that he does sometimes love it) because he is instinctively\nafraid of attaining his object and completing the edifice he is\nconstructing? Who knows, perhaps he only loves that edifice from a\ndistance, and is by no means in love with it at close quarters; perhaps\nhe only loves building it and does not want to live in it, but will\nleave it, when completed, for the use of _les animaux domestiques_—such\nas the ants, the sheep, and so on. Now the ants have quite a different\ntaste. They have a marvellous edifice of that pattern which endures for\never—the ant-heap.\n\nWith the ant-heap the respectable race of ants began and with the\nant-heap they will probably end, which does the greatest credit to\ntheir perseverance and good sense. But man is a frivolous and\nincongruous creature, and perhaps, like a chess player, loves the\nprocess of the game, not the end of it. And who knows (there is no\nsaying with certainty), perhaps the only goal on earth to which mankind\nis striving lies in this incessant process of attaining, in other\nwords, in life itself, and not in the thing to be attained, which must\nalways be expressed as a formula, as positive as twice two makes four,\nand such positiveness is not life, gentlemen, but is the beginning of\ndeath. Anyway, man has always been afraid of this mathematical\ncertainty, and I am afraid of it now. Granted that man does nothing but\nseek that mathematical certainty, he traverses oceans, sacrifices his\nlife in the quest, but to succeed, really to find it, dreads, I assure\nyou. He feels that when he has found it there will be nothing for him\nto look for. When workmen have finished their work they do at least\nreceive their pay, they go to the tavern, then they are taken to the\npolice-station—and there is occupation for a week. But where can man\ngo? Anyway, one can observe a certain awkwardness about him when he has\nattained such objects. He loves the process of attaining, but does not\nquite like to have attained, and that, of course, is very absurd. In\nfact, man is a comical creature; there seems to be a kind of jest in it\nall. But yet mathematical certainty is after all, something\ninsufferable. Twice two makes four seems to me simply a piece of\ninsolence. Twice two makes four is a pert coxcomb who stands with arms\nakimbo barring your path and spitting. I admit that twice two makes\nfour is an excellent thing, but if we are to give everything its due,\ntwice two makes five is sometimes a very charming thing too.\n\nAnd why are you so firmly, so triumphantly, convinced that only the\nnormal and the positive—in other words, only what is conducive to\nwelfare—is for the advantage of man? Is not reason in error as regards\nadvantage? Does not man, perhaps, love something besides well-being?\nPerhaps he is just as fond of suffering? Perhaps suffering is just as\ngreat a benefit to him as well-being? Man is sometimes extraordinarily,\npassionately, in love with suffering, and that is a fact. There is no\nneed to appeal to universal history to prove that; only ask yourself,\nif you are a man and have lived at all. As far as my personal opinion\nis concerned, to care only for well-being seems to me positively\nill-bred. Whether it’s good or bad, it is sometimes very pleasant, too,\nto smash things. I hold no brief for suffering nor for well-being\neither. I am standing for ... my caprice, and for its being guaranteed\nto me when necessary. Suffering would be out of place in vaudevilles,\nfor instance; I know that. In the “Palace of Crystal” it is\nunthinkable; suffering means doubt, negation, and what would be the\ngood of a “palace of crystal” if there could be any doubt about it? And\nyet I think man will never renounce real suffering, that is,\ndestruction and chaos. Why, suffering is the sole origin of\nconsciousness. Though I did lay it down at the beginning that\nconsciousness is the greatest misfortune for man, yet I know man prizes\nit and would not give it up for any satisfaction. Consciousness, for\ninstance, is infinitely superior to twice two makes four. Once you have\nmathematical certainty there is nothing left to do or to understand.\nThere will be nothing left but to bottle up your five senses and plunge\ninto contemplation. While if you stick to consciousness, even though\nthe same result is attained, you can at least flog yourself at times,\nand that will, at any rate, liven you up. Reactionary as it is,\ncorporal punishment is better than nothing.\n\n\n\n\nX\n\n\nYou believe in a palace of crystal that can never be destroyed—a palace\nat which one will not be able to put out one’s tongue or make a long\nnose on the sly. And perhaps that is just why I am afraid of this\nedifice, that it is of crystal and can never be destroyed and that one\ncannot put one’s tongue out at it even on the sly.\n\nYou see, if it were not a palace, but a hen-house, I might creep into\nit to avoid getting wet, and yet I would not call the hen-house a\npalace out of gratitude to it for keeping me dry. You laugh and say\nthat in such circumstances a hen-house is as good as a mansion. Yes, I\nanswer, if one had to live simply to keep out of the rain.\n\nBut what is to be done if I have taken it into my head that that is not\nthe only object in life, and that if one must live one had better live\nin a mansion? That is my choice, my desire. You will only eradicate it\nwhen you have changed my preference. Well, do change it, allure me with\nsomething else, give me another ideal. But meanwhile I will not take a\nhen-house for a mansion. The palace of crystal may be an idle dream, it\nmay be that it is inconsistent with the laws of nature and that I have\ninvented it only through my own stupidity, through the old-fashioned\nirrational habits of my generation. But what does it matter to me that\nit is inconsistent? That makes no difference since it exists in my\ndesires, or rather exists as long as my desires exist. Perhaps you are\nlaughing again? Laugh away; I will put up with any mockery rather than\npretend that I am satisfied when I am hungry. I know, anyway, that I\nwill not be put off with a compromise, with a recurring zero, simply\nbecause it is consistent with the laws of nature and actually exists. I\nwill not accept as the crown of my desires a block of buildings with\ntenements for the poor on a lease of a thousand years, and perhaps with\na sign-board of a dentist hanging out. Destroy my desires, eradicate my\nideals, show me something better, and I will follow you. You will say,\nperhaps, that it is not worth your trouble; but in that case I can give\nyou the same answer. We are discussing things seriously; but if you\nwon’t deign to give me your attention, I will drop your acquaintance. I\ncan retreat into my underground hole.\n\nBut while I am alive and have desires I would rather my hand were\nwithered off than bring one brick to such a building! Don’t remind me\nthat I have just rejected the palace of crystal for the sole reason\nthat one cannot put out one’s tongue at it. I did not say because I am\nso fond of putting my tongue out. Perhaps the thing I resented was,\nthat of all your edifices there has not been one at which one could not\nput out one’s tongue. On the contrary, I would let my tongue be cut off\nout of gratitude if things could be so arranged that I should lose all\ndesire to put it out. It is not my fault that things cannot be so\narranged, and that one must be satisfied with model flats. Then why am\nI made with such desires? Can I have been constructed simply in order\nto come to the conclusion that all my construction is a cheat? Can this\nbe my whole purpose? I do not believe it.\n\nBut do you know what: I am convinced that we underground folk ought to\nbe kept on a curb. Though we may sit forty years underground without\nspeaking, when we do come out into the light of day and break out we\ntalk and talk and talk....\n\n\n\n\nXI\n\n\nThe long and the short of it is, gentlemen, that it is better to do\nnothing! Better conscious inertia! And so hurrah for underground!\nThough I have said that I envy the normal man to the last drop of my\nbile, yet I should not care to be in his place such as he is now\n(though I shall not cease envying him). No, no; anyway the underground\nlife is more advantageous. There, at any rate, one can ... Oh, but even\nnow I am lying! I am lying because I know myself that it is not\nunderground that is better, but something different, quite different,\nfor which I am thirsting, but which I cannot find! Damn underground!\n\nI will tell you another thing that would be better, and that is, if I\nmyself believed in anything of what I have just written. I swear to\nyou, gentlemen, there is not one thing, not one word of what I have\nwritten that I really believe. That is, I believe it, perhaps, but at\nthe same time I feel and suspect that I am lying like a cobbler.\n\n“Then why have you written all this?” you will say to me. “I ought to\nput you underground for forty years without anything to do and then\ncome to you in your cellar, to find out what stage you have reached!\nHow can a man be left with nothing to do for forty years?”\n\n“Isn’t that shameful, isn’t that humiliating?” you will say, perhaps,\nwagging your heads contemptuously. “You thirst for life and try to\nsettle the problems of life by a logical tangle. And how persistent,\nhow insolent are your sallies, and at the same time what a scare you\nare in! You talk nonsense and are pleased with it; you say impudent\nthings and are in continual alarm and apologising for them. You declare\nthat you are afraid of nothing and at the same time try to ingratiate\nyourself in our good opinion. You declare that you are gnashing your\nteeth and at the same time you try to be witty so as to amuse us. You\nknow that your witticisms are not witty, but you are evidently well\nsatisfied with their literary value. You may, perhaps, have really\nsuffered, but you have no respect for your own suffering. You may have\nsincerity, but you have no modesty; out of the pettiest vanity you\nexpose your sincerity to publicity and ignominy. You doubtlessly mean\nto say something, but hide your last word through fear, because you\nhave not the resolution to utter it, and only have a cowardly\nimpudence. You boast of consciousness, but you are not sure of your\nground, for though your mind works, yet your heart is darkened and\ncorrupt, and you cannot have a full, genuine consciousness without a\npure heart. And how intrusive you are, how you insist and grimace!\nLies, lies, lies!”\n\nOf course I have myself made up all the things you say. That, too, is\nfrom underground. I have been for forty years listening to you through\na crack under the floor. I have invented them myself, there was nothing\nelse I could invent. It is no wonder that I have learned it by heart\nand it has taken a literary form....\n\nBut can you really be so credulous as to think that I will print all\nthis and give it to you to read too? And another problem: why do I call\nyou “gentlemen,” why do I address you as though you really were my\nreaders? Such confessions as I intend to make are never printed nor\ngiven to other people to read. Anyway, I am not strong-minded enough\nfor that, and I don’t see why I should be. But you see a fancy has\noccurred to me and I want to realise it at all costs. Let me explain.\n\nEvery man has reminiscences which he would not tell to everyone, but\nonly to his friends. He has other matters in his mind which he would\nnot reveal even to his friends, but only to himself, and that in\nsecret. But there are other things which a man is afraid to tell even\nto himself, and every decent man has a number of such things stored\naway in his mind. The more decent he is, the greater the number of such\nthings in his mind. Anyway, I have only lately determined to remember\nsome of my early adventures. Till now I have always avoided them, even\nwith a certain uneasiness. Now, when I am not only recalling them, but\nhave actually decided to write an account of them, I want to try the\nexperiment whether one can, even with oneself, be perfectly open and\nnot take fright at the whole truth. I will observe, in parenthesis,\nthat Heine says that a true autobiography is almost an impossibility,\nand that man is bound to lie about himself. He considers that Rousseau\ncertainly told lies about himself in his confessions, and even\nintentionally lied, out of vanity. I am convinced that Heine is right;\nI quite understand how sometimes one may, out of sheer vanity,\nattribute regular crimes to oneself, and indeed I can very well\nconceive that kind of vanity. But Heine judged of people who made their\nconfessions to the public. I write only for myself, and I wish to\ndeclare once and for all that if I write as though I were addressing\nreaders, that is simply because it is easier for me to write in that\nform. It is a form, an empty form—I shall never have readers. I have\nmade this plain already ...\n\nI don’t wish to be hampered by any restrictions in the compilation of\nmy notes. I shall not attempt any system or method. I will jot things\ndown as I remember them.\n\nBut here, perhaps, someone will catch at the word and ask me: if you\nreally don’t reckon on readers, why do you make such compacts with\nyourself—and on paper too—that is, that you won’t attempt any system or\nmethod, that you jot things down as you remember them, and so on, and\nso on? Why are you explaining? Why do you apologise?\n\nWell, there it is, I answer.\n\nThere is a whole psychology in all this, though. Perhaps it is simply\nthat I am a coward. And perhaps that I purposely imagine an audience\nbefore me in order that I may be more dignified while I write. There\nare perhaps thousands of reasons. Again, what is my object precisely in\nwriting? If it is not for the benefit of the public why should I not\nsimply recall these incidents in my own mind without putting them on\npaper?\n\nQuite so; but yet it is more imposing on paper. There is something more\nimpressive in it; I shall be better able to criticise myself and\nimprove my style. Besides, I shall perhaps obtain actual relief from\nwriting. Today, for instance, I am particularly oppressed by one memory\nof a distant past. It came back vividly to my mind a few days ago, and\nhas remained haunting me like an annoying tune that one cannot get rid\nof. And yet I must get rid of it somehow. I have hundreds of such\nreminiscences; but at times some one stands out from the hundred and\noppresses me. For some reason I believe that if I write it down I\nshould get rid of it. Why not try?\n\nBesides, I am bored, and I never have anything to do. Writing will be a\nsort of work. They say work makes man kind-hearted and honest. Well,\nhere is a chance for me, anyway.\n\nSnow is falling today, yellow and dingy. It fell yesterday, too, and a\nfew days ago. I fancy it is the wet snow that has reminded me of that\nincident which I cannot shake off now. And so let it be a story _à\npropos_ of the falling snow.\n\n\n\n\nPART II\nÀ Propos of the Wet Snow\n\n\nWhen from dark error’s subjugation\nMy words of passionate exhortation\n    Had wrenched thy fainting spirit free;\nAnd writhing prone in thine affliction\nThou didst recall with malediction\n    The vice that had encompassed thee:\nAnd when thy slumbering conscience, fretting\n    By recollection’s torturing flame,\nThou didst reveal the hideous setting\n    Of thy life’s current ere I came:\nWhen suddenly I saw thee sicken,\n    And weeping, hide thine anguished face,\nRevolted, maddened, horror-stricken,\n    At memories of foul disgrace.\n                    NEKRASSOV (_translated by Juliet Soskice_).\n\n\n\n\nI\n\n\nAt that time I was only twenty-four. My life was even then gloomy,\nill-regulated, and as solitary as that of a savage. I made friends with\nno one and positively avoided talking, and buried myself more and more\nin my hole. At work in the office I never looked at anyone, and was\nperfectly well aware that my companions looked upon me, not only as a\nqueer fellow, but even looked upon me—I always fancied this—with a sort\nof loathing. I sometimes wondered why it was that nobody except me\nfancied that he was looked upon with aversion? One of the clerks had a\nmost repulsive, pock-marked face, which looked positively villainous. I\nbelieve I should not have dared to look at anyone with such an\nunsightly countenance. Another had such a very dirty old uniform that\nthere was an unpleasant odour in his proximity. Yet not one of these\ngentlemen showed the slightest self-consciousness—either about their\nclothes or their countenance or their character in any way. Neither of\nthem ever imagined that they were looked at with repulsion; if they had\nimagined it they would not have minded—so long as their superiors did\nnot look at them in that way. It is clear to me now that, owing to my\nunbounded vanity and to the high standard I set for myself, I often\nlooked at myself with furious discontent, which verged on loathing, and\nso I inwardly attributed the same feeling to everyone. I hated my face,\nfor instance: I thought it disgusting, and even suspected that there\nwas something base in my expression, and so every day when I turned up\nat the office I tried to behave as independently as possible, and to\nassume a lofty expression, so that I might not be suspected of being\nabject. “My face may be ugly,” I thought, “but let it be lofty,\nexpressive, and, above all, _extremely_ intelligent.” But I was\npositively and painfully certain that it was impossible for my\ncountenance ever to express those qualities. And what was worst of all,\nI thought it actually stupid looking, and I would have been quite\nsatisfied if I could have looked intelligent. In fact, I would even\nhave put up with looking base if, at the same time, my face could have\nbeen thought strikingly intelligent.\n\nOf course, I hated my fellow clerks one and all, and I despised them\nall, yet at the same time I was, as it were, afraid of them. In fact,\nit happened at times that I thought more highly of them than of myself.\nIt somehow happened quite suddenly that I alternated between despising\nthem and thinking them superior to myself. A cultivated and decent man\ncannot be vain without setting a fearfully high standard for himself,\nand without despising and almost hating himself at certain moments. But\nwhether I despised them or thought them superior I dropped my eyes\nalmost every time I met anyone. I even made experiments whether I could\nface so and so’s looking at me, and I was always the first to drop my\neyes. This worried me to distraction. I had a sickly dread, too, of\nbeing ridiculous, and so had a slavish passion for the conventional in\neverything external. I loved to fall into the common rut, and had a\nwhole-hearted terror of any kind of eccentricity in myself. But how\ncould I live up to it? I was morbidly sensitive as a man of our age\nshould be. They were all stupid, and as like one another as so many\nsheep. Perhaps I was the only one in the office who fancied that I was\na coward and a slave, and I fancied it just because I was more highly\ndeveloped. But it was not only that I fancied it, it really was so. I\nwas a coward and a slave. I say this without the slightest\nembarrassment. Every decent man of our age must be a coward and a\nslave. That is his normal condition. Of that I am firmly persuaded. He\nis made and constructed to that very end. And not only at the present\ntime owing to some casual circumstances, but always, at all times, a\ndecent man is bound to be a coward and a slave. It is the law of nature\nfor all decent people all over the earth. If anyone of them happens to\nbe valiant about something, he need not be comforted nor carried away\nby that; he would show the white feather just the same before something\nelse. That is how it invariably and inevitably ends. Only donkeys and\nmules are valiant, and they only till they are pushed up to the wall.\nIt is not worth while to pay attention to them for they really are of\nno consequence.\n\nAnother circumstance, too, worried me in those days: that there was no\none like me and I was unlike anyone else. “I am alone and they are\n_everyone_,” I thought—and pondered.\n\nFrom that it is evident that I was still a youngster.\n\nThe very opposite sometimes happened. It was loathsome sometimes to go\nto the office; things reached such a point that I often came home ill.\nBut all at once, _à propos_ of nothing, there would come a phase of\nscepticism and indifference (everything happened in phases to me), and\nI would laugh myself at my intolerance and fastidiousness, I would\nreproach myself with being _romantic_. At one time I was unwilling to\nspeak to anyone, while at other times I would not only talk, but go to\nthe length of contemplating making friends with them. All my\nfastidiousness would suddenly, for no rhyme or reason, vanish. Who\nknows, perhaps I never had really had it, and it had simply been\naffected, and got out of books. I have not decided that question even\nnow. Once I quite made friends with them, visited their homes, played\npreference, drank vodka, talked of promotions.... But here let me make\na digression.\n\nWe Russians, speaking generally, have never had those foolish\ntranscendental “romantics”—German, and still more French—on whom\nnothing produces any effect; if there were an earthquake, if all France\nperished at the barricades, they would still be the same, they would\nnot even have the decency to affect a change, but would still go on\nsinging their transcendental songs to the hour of their death, because\nthey are fools. We, in Russia, have no fools; that is well known. That\nis what distinguishes us from foreign lands. Consequently these\ntranscendental natures are not found amongst us in their pure form. The\nidea that they are is due to our “realistic” journalists and critics of\nthat day, always on the look out for Kostanzhoglos and Uncle Pyotr\nIvanitchs and foolishly accepting them as our ideal; they have\nslandered our romantics, taking them for the same transcendental sort\nas in Germany or France. On the contrary, the characteristics of our\n“romantics” are absolutely and directly opposed to the transcendental\nEuropean type, and no European standard can be applied to them. (Allow\nme to make use of this word “romantic”—an old-fashioned and much\nrespected word which has done good service and is familiar to all.) The\ncharacteristics of our romantic are to understand everything, _to see\neverything and to see it often incomparably more clearly than our most\nrealistic minds see it;_ to refuse to accept anyone or anything, but at\nthe same time not to despise anything; to give way, to yield, from\npolicy; never to lose sight of a useful practical object (such as\nrent-free quarters at the government expense, pensions, decorations),\nto keep their eye on that object through all the enthusiasms and\nvolumes of lyrical poems, and at the same time to preserve “the sublime\nand the beautiful” inviolate within them to the hour of their death,\nand to preserve themselves also, incidentally, like some precious jewel\nwrapped in cotton wool if only for the benefit of “the sublime and the\nbeautiful.” Our “romantic” is a man of great breadth and the greatest\nrogue of all our rogues, I assure you.... I can assure you from\nexperience, indeed. Of course, that is, if he is intelligent. But what\nam I saying! The romantic is always intelligent, and I only meant to\nobserve that although we have had foolish romantics they don’t count,\nand they were only so because in the flower of their youth they\ndegenerated into Germans, and to preserve their precious jewel more\ncomfortably, settled somewhere out there—by preference in Weimar or the\nBlack Forest.\n\nI, for instance, genuinely despised my official work and did not openly\nabuse it simply because I was in it myself and got a salary for it.\nAnyway, take note, I did not openly abuse it. Our romantic would rather\ngo out of his mind—a thing, however, which very rarely happens—than\ntake to open abuse, unless he had some other career in view; and he is\nnever kicked out. At most, they would take him to the lunatic asylum as\n“the King of Spain” if he should go very mad. But it is only the thin,\nfair people who go out of their minds in Russia. Innumerable\n“romantics” attain later in life to considerable rank in the service.\nTheir many-sidedness is remarkable! And what a faculty they have for\nthe most contradictory sensations! I was comforted by this thought even\nin those days, and I am of the same opinion now. That is why there are\nso many “broad natures” among us who never lose their ideal even in the\ndepths of degradation; and though they never stir a finger for their\nideal, though they are arrant thieves and knaves, yet they tearfully\ncherish their first ideal and are extraordinarily honest at heart. Yes,\nit is only among us that the most incorrigible rogue can be absolutely\nand loftily honest at heart without in the least ceasing to be a rogue.\nI repeat, our romantics, frequently, become such accomplished rascals\n(I use the term “rascals” affectionately), suddenly display such a\nsense of reality and practical knowledge that their bewildered\nsuperiors and the public generally can only ejaculate in amazement.\n\nTheir many-sidedness is really amazing, and goodness knows what it may\ndevelop into later on, and what the future has in store for us. It is\nnot a poor material! I do not say this from any foolish or boastful\npatriotism. But I feel sure that you are again imagining that I am\njoking. Or perhaps it’s just the contrary and you are convinced that I\nreally think so. Anyway, gentlemen, I shall welcome both views as an\nhonour and a special favour. And do forgive my digression.\n\nI did not, of course, maintain friendly relations with my comrades and\nsoon was at loggerheads with them, and in my youth and inexperience I\neven gave up bowing to them, as though I had cut off all relations.\nThat, however, only happened to me once. As a rule, I was always alone.\n\nIn the first place I spent most of my time at home, reading. I tried to\nstifle all that was continually seething within me by means of external\nimpressions. And the only external means I had was reading. Reading, of\ncourse, was a great help—exciting me, giving me pleasure and pain. But\nat times it bored me fearfully. One longed for movement in spite of\neverything, and I plunged all at once into dark, underground, loathsome\nvice of the pettiest kind. My wretched passions were acute, smarting,\nfrom my continual, sickly irritability I had hysterical impulses, with\ntears and convulsions. I had no resource except reading, that is, there\nwas nothing in my surroundings which I could respect and which\nattracted me. I was overwhelmed with depression, too; I had an\nhysterical craving for incongruity and for contrast, and so I took to\nvice. I have not said all this to justify myself.... But, no! I am\nlying. I did want to justify myself. I make that little observation for\nmy own benefit, gentlemen. I don’t want to lie. I vowed to myself I\nwould not.\n\nAnd so, furtively, timidly, in solitude, at night, I indulged in filthy\nvice, with a feeling of shame which never deserted me, even at the most\nloathsome moments, and which at such moments nearly made me curse.\nAlready even then I had my underground world in my soul. I was\nfearfully afraid of being seen, of being met, of being recognised. I\nvisited various obscure haunts.\n\nOne night as I was passing a tavern I saw through a lighted window some\ngentlemen fighting with billiard cues, and saw one of them thrown out\nof the window. At other times I should have felt very much disgusted,\nbut I was in such a mood at the time, that I actually envied the\ngentleman thrown out of the window—and I envied him so much that I even\nwent into the tavern and into the billiard-room. “Perhaps,” I thought,\n“I’ll have a fight, too, and they’ll throw me out of the window.”\n\nI was not drunk—but what is one to do—depression will drive a man to\nsuch a pitch of hysteria? But nothing happened. It seemed that I was\nnot even equal to being thrown out of the window and I went away\nwithout having my fight.\n\nAn officer put me in my place from the first moment.\n\nI was standing by the billiard-table and in my ignorance blocking up\nthe way, and he wanted to pass; he took me by the shoulders and without\na word—without a warning or explanation—moved me from where I was\nstanding to another spot and passed by as though he had not noticed me.\nI could have forgiven blows, but I could not forgive his having moved\nme without noticing me.\n\nDevil knows what I would have given for a real regular quarrel—a more\ndecent, a more _literary_ one, so to speak. I had been treated like a\nfly. This officer was over six foot, while I was a spindly little\nfellow. But the quarrel was in my hands. I had only to protest and I\ncertainly would have been thrown out of the window. But I changed my\nmind and preferred to beat a resentful retreat.\n\nI went out of the tavern straight home, confused and troubled, and the\nnext night I went out again with the same lewd intentions, still more\nfurtively, abjectly and miserably than before, as it were, with tears\nin my eyes—but still I did go out again. Don’t imagine, though, it was\ncowardice made me slink away from the officer; I never have been a\ncoward at heart, though I have always been a coward in action. Don’t be\nin a hurry to laugh—I assure you I can explain it all.\n\nOh, if only that officer had been one of the sort who would consent to\nfight a duel! But no, he was one of those gentlemen (alas, long\nextinct!) who preferred fighting with cues or, like Gogol’s Lieutenant\nPirogov, appealing to the police. They did not fight duels and would\nhave thought a duel with a civilian like me an utterly unseemly\nprocedure in any case—and they looked upon the duel altogether as\nsomething impossible, something free-thinking and French. But they were\nquite ready to bully, especially when they were over six foot.\n\nI did not slink away through cowardice, but through an unbounded\nvanity. I was afraid not of his six foot, not of getting a sound\nthrashing and being thrown out of the window; I should have had\nphysical courage enough, I assure you; but I had not the moral courage.\nWhat I was afraid of was that everyone present, from the insolent\nmarker down to the lowest little stinking, pimply clerk in a greasy\ncollar, would jeer at me and fail to understand when I began to protest\nand to address them in literary language. For of the point of\nhonour—not of honour, but of the point of honour (_point\nd’honneur_)—one cannot speak among us except in literary language. You\ncan’t allude to the “point of honour” in ordinary language. I was fully\nconvinced (the sense of reality, in spite of all my romanticism!) that\nthey would all simply split their sides with laughter, and that the\nofficer would not simply beat me, that is, without insulting me, but\nwould certainly prod me in the back with his knee, kick me round the\nbilliard-table, and only then perhaps have pity and drop me out of the\nwindow.\n\nOf course, this trivial incident could not with me end in that. I often\nmet that officer afterwards in the street and noticed him very\ncarefully. I am not quite sure whether he recognised me, I imagine not;\nI judge from certain signs. But I—I stared at him with spite and hatred\nand so it went on ... for several years! My resentment grew even deeper\nwith years. At first I began making stealthy inquiries about this\nofficer. It was difficult for me to do so, for I knew no one. But one\nday I heard someone shout his surname in the street as I was following\nhim at a distance, as though I were tied to him—and so I learnt his\nsurname. Another time I followed him to his flat, and for ten kopecks\nlearned from the porter where he lived, on which storey, whether he\nlived alone or with others, and so on—in fact, everything one could\nlearn from a porter. One morning, though I had never tried my hand with\nthe pen, it suddenly occurred to me to write a satire on this officer\nin the form of a novel which would unmask his villainy. I wrote the\nnovel with relish. I did unmask his villainy, I even exaggerated it; at\nfirst I so altered his surname that it could easily be recognised, but\non second thoughts I changed it, and sent the story to the\n_Otetchestvenniya Zapiski_. But at that time such attacks were not the\nfashion and my story was not printed. That was a great vexation to me.\n\nSometimes I was positively choked with resentment. At last I determined\nto challenge my enemy to a duel. I composed a splendid, charming letter\nto him, imploring him to apologise to me, and hinting rather plainly at\na duel in case of refusal. The letter was so composed that if the\nofficer had had the least understanding of the sublime and the\nbeautiful he would certainly have flung himself on my neck and have\noffered me his friendship. And how fine that would have been! How we\nshould have got on together! “He could have shielded me with his higher\nrank, while I could have improved his mind with my culture, and, well\n... my ideas, and all sorts of things might have happened.” Only fancy,\nthis was two years after his insult to me, and my challenge would have\nbeen a ridiculous anachronism, in spite of all the ingenuity of my\nletter in disguising and explaining away the anachronism. But, thank\nGod (to this day I thank the Almighty with tears in my eyes) I did not\nsend the letter to him. Cold shivers run down my back when I think of\nwhat might have happened if I had sent it.\n\nAnd all at once I revenged myself in the simplest way, by a stroke of\ngenius! A brilliant thought suddenly dawned upon me. Sometimes on\nholidays I used to stroll along the sunny side of the Nevsky about four\no’clock in the afternoon. Though it was hardly a stroll so much as a\nseries of innumerable miseries, humiliations and resentments; but no\ndoubt that was just what I wanted. I used to wriggle along in a most\nunseemly fashion, like an eel, continually moving aside to make way for\ngenerals, for officers of the guards and the hussars, or for ladies. At\nsuch minutes there used to be a convulsive twinge at my heart, and I\nused to feel hot all down my back at the mere thought of the\nwretchedness of my attire, of the wretchedness and abjectness of my\nlittle scurrying figure. This was a regular martyrdom, a continual,\nintolerable humiliation at the thought, which passed into an incessant\nand direct sensation, that I was a mere fly in the eyes of all this\nworld, a nasty, disgusting fly—more intelligent, more highly developed,\nmore refined in feeling than any of them, of course—but a fly that was\ncontinually making way for everyone, insulted and injured by everyone.\nWhy I inflicted this torture upon myself, why I went to the Nevsky, I\ndon’t know. I felt simply drawn there at every possible opportunity.\n\nAlready then I began to experience a rush of the enjoyment of which I\nspoke in the first chapter. After my affair with the officer I felt\neven more drawn there than before: it was on the Nevsky that I met him\nmost frequently, there I could admire him. He, too, went there chiefly\non holidays, He, too, turned out of his path for generals and persons\nof high rank, and he too, wriggled between them like an eel; but\npeople, like me, or even better dressed than me, he simply walked over;\nhe made straight for them as though there was nothing but empty space\nbefore him, and never, under any circumstances, turned aside. I gloated\nover my resentment watching him and ... always resentfully made way for\nhim. It exasperated me that even in the street I could not be on an\neven footing with him.\n\n“Why must you invariably be the first to move aside?” I kept asking\nmyself in hysterical rage, waking up sometimes at three o’clock in the\nmorning. “Why is it you and not he? There’s no regulation about it;\nthere’s no written law. Let the making way be equal as it usually is\nwhen refined people meet; he moves half-way and you move half-way; you\npass with mutual respect.”\n\nBut that never happened, and I always moved aside, while he did not\neven notice my making way for him. And lo and behold a bright idea\ndawned upon me! “What,” I thought, “if I meet him and don’t move on one\nside? What if I don’t move aside on purpose, even if I knock up against\nhim? How would that be?” This audacious idea took such a hold on me\nthat it gave me no peace. I was dreaming of it continually, horribly,\nand I purposely went more frequently to the Nevsky in order to picture\nmore vividly how I should do it when I did do it. I was delighted. This\nintention seemed to me more and more practical and possible.\n\n“Of course I shall not really push him,” I thought, already more\ngood-natured in my joy. “I will simply not turn aside, will run up\nagainst him, not very violently, but just shouldering each other—just\nas much as decency permits. I will push against him just as much as he\npushes against me.” At last I made up my mind completely. But my\npreparations took a great deal of time. To begin with, when I carried\nout my plan I should need to be looking rather more decent, and so I\nhad to think of my get-up. “In case of emergency, if, for instance,\nthere were any sort of public scandal (and the public there is of the\nmost _recherché:_ the Countess walks there; Prince D. walks there; all\nthe literary world is there), I must be well dressed; that inspires\nrespect and of itself puts us on an equal footing in the eyes of the\nsociety.”\n\nWith this object I asked for some of my salary in advance, and bought\nat Tchurkin’s a pair of black gloves and a decent hat. Black gloves\nseemed to me both more dignified and _bon ton_ than the lemon-coloured\nones which I had contemplated at first. “The colour is too gaudy, it\nlooks as though one were trying to be conspicuous,” and I did not take\nthe lemon-coloured ones. I had got ready long beforehand a good shirt,\nwith white bone studs; my overcoat was the only thing that held me\nback. The coat in itself was a very good one, it kept me warm; but it\nwas wadded and it had a raccoon collar which was the height of\nvulgarity. I had to change the collar at any sacrifice, and to have a\nbeaver one like an officer’s. For this purpose I began visiting the\nGostiny Dvor and after several attempts I pitched upon a piece of cheap\nGerman beaver. Though these German beavers soon grow shabby and look\nwretched, yet at first they look exceedingly well, and I only needed it\nfor the occasion. I asked the price; even so, it was too expensive.\nAfter thinking it over thoroughly I decided to sell my raccoon collar.\nThe rest of the money—a considerable sum for me, I decided to borrow\nfrom Anton Antonitch Syetotchkin, my immediate superior, an unassuming\nperson, though grave and judicious. He never lent money to anyone, but\nI had, on entering the service, been specially recommended to him by an\nimportant personage who had got me my berth. I was horribly worried. To\nborrow from Anton Antonitch seemed to me monstrous and shameful. I did\nnot sleep for two or three nights. Indeed, I did not sleep well at that\ntime, I was in a fever; I had a vague sinking at my heart or else a\nsudden throbbing, throbbing, throbbing! Anton Antonitch was surprised\nat first, then he frowned, then he reflected, and did after all lend me\nthe money, receiving from me a written authorisation to take from my\nsalary a fortnight later the sum that he had lent me.\n\nIn this way everything was at last ready. The handsome beaver replaced\nthe mean-looking raccoon, and I began by degrees to get to work. It\nwould never have done to act offhand, at random; the plan had to be\ncarried out skilfully, by degrees. But I must confess that after many\nefforts I began to despair: we simply could not run into each other. I\nmade every preparation, I was quite determined—it seemed as though we\nshould run into one another directly—and before I knew what I was doing\nI had stepped aside for him again and he had passed without noticing\nme. I even prayed as I approached him that God would grant me\ndetermination. One time I had made up my mind thoroughly, but it ended\nin my stumbling and falling at his feet because at the very last\ninstant when I was six inches from him my courage failed me. He very\ncalmly stepped over me, while I flew on one side like a ball. That\nnight I was ill again, feverish and delirious.\n\nAnd suddenly it ended most happily. The night before I had made up my\nmind not to carry out my fatal plan and to abandon it all, and with\nthat object I went to the Nevsky for the last time, just to see how I\nwould abandon it all. Suddenly, three paces from my enemy, I\nunexpectedly made up my mind—I closed my eyes, and we ran full tilt,\nshoulder to shoulder, against one another! I did not budge an inch and\npassed him on a perfectly equal footing! He did not even look round and\npretended not to notice it; but he was only pretending, I am convinced\nof that. I am convinced of that to this day! Of course, I got the worst\nof it—he was stronger, but that was not the point. The point was that I\nhad attained my object, I had kept up my dignity, I had not yielded a\nstep, and had put myself publicly on an equal social footing with him.\nI returned home feeling that I was fully avenged for everything. I was\ndelighted. I was triumphant and sang Italian arias. Of course, I will\nnot describe to you what happened to me three days later; if you have\nread my first chapter you can guess for yourself. The officer was\nafterwards transferred; I have not seen him now for fourteen years.\nWhat is the dear fellow doing now? Whom is he walking over?\n\n\n\n\nII\n\n\nBut the period of my dissipation would end and I always felt very sick\nafterwards. It was followed by remorse—I tried to drive it away; I felt\ntoo sick. By degrees, however, I grew used to that too. I grew used to\neverything, or rather I voluntarily resigned myself to enduring it. But\nI had a means of escape that reconciled everything—that was to find\nrefuge in “the sublime and the beautiful,” in dreams, of course. I was\na terrible dreamer, I would dream for three months on end, tucked away\nin my corner, and you may believe me that at those moments I had no\nresemblance to the gentleman who, in the perturbation of his chicken\nheart, put a collar of German beaver on his great-coat. I suddenly\nbecame a hero. I would not have admitted my six-foot lieutenant even if\nhe had called on me. I could not even picture him before me then. What\nwere my dreams and how I could satisfy myself with them—it is hard to\nsay now, but at the time I was satisfied with them. Though, indeed,\neven now, I am to some extent satisfied with them. Dreams were\nparticularly sweet and vivid after a spell of dissipation; they came\nwith remorse and with tears, with curses and transports. There were\nmoments of such positive intoxication, of such happiness, that there\nwas not the faintest trace of irony within me, on my honour. I had\nfaith, hope, love. I believed blindly at such times that by some\nmiracle, by some external circumstance, all this would suddenly open\nout, expand; that suddenly a vista of suitable activity—beneficent,\ngood, and, above all, _ready made_ (what sort of activity I had no\nidea, but the great thing was that it should be all ready for me)—would\nrise up before me—and I should come out into the light of day, almost\nriding a white horse and crowned with laurel. Anything but the foremost\nplace I could not conceive for myself, and for that very reason I quite\ncontentedly occupied the lowest in reality. Either to be a hero or to\ngrovel in the mud—there was nothing between. That was my ruin, for when\nI was in the mud I comforted myself with the thought that at other\ntimes I was a hero, and the hero was a cloak for the mud: for an\nordinary man it was shameful to defile himself, but a hero was too\nlofty to be utterly defiled, and so he might defile himself. It is\nworth noting that these attacks of the “sublime and the beautiful”\nvisited me even during the period of dissipation and just at the times\nwhen I was touching the bottom. They came in separate spurts, as though\nreminding me of themselves, but did not banish the dissipation by their\nappearance. On the contrary, they seemed to add a zest to it by\ncontrast, and were only sufficiently present to serve as an appetising\nsauce. That sauce was made up of contradictions and sufferings, of\nagonising inward analysis, and all these pangs and pin-pricks gave a\ncertain piquancy, even a significance to my dissipation—in fact,\ncompletely answered the purpose of an appetising sauce. There was a\ncertain depth of meaning in it. And I could hardly have resigned myself\nto the simple, vulgar, direct debauchery of a clerk and have endured\nall the filthiness of it. What could have allured me about it then and\nhave drawn me at night into the street? No, I had a lofty way of\ngetting out of it all.\n\nAnd what loving-kindness, oh Lord, what loving-kindness I felt at times\nin those dreams of mine! in those “flights into the sublime and the\nbeautiful”; though it was fantastic love, though it was never applied\nto anything human in reality, yet there was so much of this love that\none did not feel afterwards even the impulse to apply it in reality;\nthat would have been superfluous. Everything, however, passed\nsatisfactorily by a lazy and fascinating transition into the sphere of\nart, that is, into the beautiful forms of life, lying ready, largely\nstolen from the poets and novelists and adapted to all sorts of needs\nand uses. I, for instance, was triumphant over everyone; everyone, of\ncourse, was in dust and ashes, and was forced spontaneously to\nrecognise my superiority, and I forgave them all. I was a poet and a\ngrand gentleman, I fell in love; I came in for countless millions and\nimmediately devoted them to humanity, and at the same time I confessed\nbefore all the people my shameful deeds, which, of course, were not\nmerely shameful, but had in them much that was “sublime and beautiful”\nsomething in the Manfred style. Everyone would kiss me and weep (what\nidiots they would be if they did not), while I should go barefoot and\nhungry preaching new ideas and fighting a victorious Austerlitz against\nthe obscurantists. Then the band would play a march, an amnesty would\nbe declared, the Pope would agree to retire from Rome to Brazil; then\nthere would be a ball for the whole of Italy at the Villa Borghese on\nthe shores of Lake Como, Lake Como being for that purpose transferred\nto the neighbourhood of Rome; then would come a scene in the bushes,\nand so on, and so on—as though you did not know all about it? You will\nsay that it is vulgar and contemptible to drag all this into public\nafter all the tears and transports which I have myself confessed. But\nwhy is it contemptible? Can you imagine that I am ashamed of it all,\nand that it was stupider than anything in your life, gentlemen? And I\ncan assure you that some of these fancies were by no means badly\ncomposed.... It did not all happen on the shores of Lake Como. And yet\nyou are right—it really is vulgar and contemptible. And most\ncontemptible of all it is that now I am attempting to justify myself to\nyou. And even more contemptible than that is my making this remark now.\nBut that’s enough, or there will be no end to it; each step will be\nmore contemptible than the last....\n\nI could never stand more than three months of dreaming at a time\nwithout feeling an irresistible desire to plunge into society. To\nplunge into society meant to visit my superior at the office, Anton\nAntonitch Syetotchkin. He was the only permanent acquaintance I have\nhad in my life, and I wonder at the fact myself now. But I only went to\nsee him when that phase came over me, and when my dreams had reached\nsuch a point of bliss that it became essential at once to embrace my\nfellows and all mankind; and for that purpose I needed, at least, one\nhuman being, actually existing. I had to call on Anton Antonitch,\nhowever, on Tuesday—his at-home day; so I had always to time my\npassionate desire to embrace humanity so that it might fall on a\nTuesday.\n\nThis Anton Antonitch lived on the fourth storey in a house in Five\nCorners, in four low-pitched rooms, one smaller than the other, of a\nparticularly frugal and sallow appearance. He had two daughters and\ntheir aunt, who used to pour out the tea. Of the daughters one was\nthirteen and another fourteen, they both had snub noses, and I was\nawfully shy of them because they were always whispering and giggling\ntogether. The master of the house usually sat in his study on a leather\ncouch in front of the table with some grey-headed gentleman, usually a\ncolleague from our office or some other department. I never saw more\nthan two or three visitors there, always the same. They talked about\nthe excise duty; about business in the senate, about salaries, about\npromotions, about His Excellency, and the best means of pleasing him,\nand so on. I had the patience to sit like a fool beside these people\nfor four hours at a stretch, listening to them without knowing what to\nsay to them or venturing to say a word. I became stupefied, several\ntimes I felt myself perspiring, I was overcome by a sort of paralysis;\nbut this was pleasant and good for me. On returning home I deferred for\na time my desire to embrace all mankind.\n\nI had however one other acquaintance of a sort, Simonov, who was an old\nschoolfellow. I had a number of schoolfellows, indeed, in Petersburg,\nbut I did not associate with them and had even given up nodding to them\nin the street. I believe I had transferred into the department I was in\nsimply to avoid their company and to cut off all connection with my\nhateful childhood. Curses on that school and all those terrible years\nof penal servitude! In short, I parted from my schoolfellows as soon as\nI got out into the world. There were two or three left to whom I nodded\nin the street. One of them was Simonov, who had in no way been\ndistinguished at school, was of a quiet and equable disposition; but I\ndiscovered in him a certain independence of character and even honesty\nI don’t even suppose that he was particularly stupid. I had at one time\nspent some rather soulful moments with him, but these had not lasted\nlong and had somehow been suddenly clouded over. He was evidently\nuncomfortable at these reminiscences, and was, I fancy, always afraid\nthat I might take up the same tone again. I suspected that he had an\naversion for me, but still I went on going to see him, not being quite\ncertain of it.\n\nAnd so on one occasion, unable to endure my solitude and knowing that\nas it was Thursday Anton Antonitch’s door would be closed, I thought of\nSimonov. Climbing up to his fourth storey I was thinking that the man\ndisliked me and that it was a mistake to go and see him. But as it\nalways happened that such reflections impelled me, as though purposely,\nto put myself into a false position, I went in. It was almost a year\nsince I had last seen Simonov.\n\n\n\n\nIII\n\n\nI found two of my old schoolfellows with him. They seemed to be\ndiscussing an important matter. All of them took scarcely any notice of\nmy entrance, which was strange, for I had not met them for years.\nEvidently they looked upon me as something on the level of a common\nfly. I had not been treated like that even at school, though they all\nhated me. I knew, of course, that they must despise me now for my lack\nof success in the service, and for my having let myself sink so low,\ngoing about badly dressed and so on—which seemed to them a sign of my\nincapacity and insignificance. But I had not expected such contempt.\nSimonov was positively surprised at my turning up. Even in old days he\nhad always seemed surprised at my coming. All this disconcerted me: I\nsat down, feeling rather miserable, and began listening to what they\nwere saying.\n\nThey were engaged in warm and earnest conversation about a farewell\ndinner which they wanted to arrange for the next day to a comrade of\ntheirs called Zverkov, an officer in the army, who was going away to a\ndistant province. This Zverkov had been all the time at school with me\ntoo. I had begun to hate him particularly in the upper forms. In the\nlower forms he had simply been a pretty, playful boy whom everybody\nliked. I had hated him, however, even in the lower forms, just because\nhe was a pretty and playful boy. He was always bad at his lessons and\ngot worse and worse as he went on; however, he left with a good\ncertificate, as he had powerful interests. During his last year at\nschool he came in for an estate of two hundred serfs, and as almost all\nof us were poor he took up a swaggering tone among us. He was vulgar in\nthe extreme, but at the same time he was a good-natured fellow, even in\nhis swaggering. In spite of superficial, fantastic and sham notions of\nhonour and dignity, all but very few of us positively grovelled before\nZverkov, and the more so the more he swaggered. And it was not from any\ninterested motive that they grovelled, but simply because he had been\nfavoured by the gifts of nature. Moreover, it was, as it were, an\naccepted idea among us that Zverkov was a specialist in regard to tact\nand the social graces. This last fact particularly infuriated me. I\nhated the abrupt self-confident tone of his voice, his admiration of\nhis own witticisms, which were often frightfully stupid, though he was\nbold in his language; I hated his handsome, but stupid face (for which\nI would, however, have gladly exchanged my intelligent one), and the\nfree-and-easy military manners in fashion in the “’forties.” I hated\nthe way in which he used to talk of his future conquests of women (he\ndid not venture to begin his attack upon women until he had the\nepaulettes of an officer, and was looking forward to them with\nimpatience), and boasted of the duels he would constantly be fighting.\nI remember how I, invariably so taciturn, suddenly fastened upon\nZverkov, when one day talking at a leisure moment with his\nschoolfellows of his future relations with the fair sex, and growing as\nsportive as a puppy in the sun, he all at once declared that he would\nnot leave a single village girl on his estate unnoticed, that that was\nhis _droit de seigneur_, and that if the peasants dared to protest he\nwould have them all flogged and double the tax on them, the bearded\nrascals. Our servile rabble applauded, but I attacked him, not from\ncompassion for the girls and their fathers, but simply because they\nwere applauding such an insect. I got the better of him on that\noccasion, but though Zverkov was stupid he was lively and impudent, and\nso laughed it off, and in such a way that my victory was not really\ncomplete; the laugh was on his side. He got the better of me on several\noccasions afterwards, but without malice, jestingly, casually. I\nremained angrily and contemptuously silent and would not answer him.\nWhen we left school he made advances to me; I did not rebuff them, for\nI was flattered, but we soon parted and quite naturally. Afterwards I\nheard of his barrack-room success as a lieutenant, and of the fast life\nhe was leading. Then there came other rumours—of his successes in the\nservice. By then he had taken to cutting me in the street, and I\nsuspected that he was afraid of compromising himself by greeting a\npersonage as insignificant as me. I saw him once in the theatre, in the\nthird tier of boxes. By then he was wearing shoulder-straps. He was\ntwisting and twirling about, ingratiating himself with the daughters of\nan ancient General. In three years he had gone off considerably, though\nhe was still rather handsome and adroit. One could see that by the time\nhe was thirty he would be corpulent. So it was to this Zverkov that my\nschoolfellows were going to give a dinner on his departure. They had\nkept up with him for those three years, though privately they did not\nconsider themselves on an equal footing with him, I am convinced of\nthat.\n\nOf Simonov’s two visitors, one was Ferfitchkin, a Russianised German—a\nlittle fellow with the face of a monkey, a blockhead who was always\nderiding everyone, a very bitter enemy of mine from our days in the\nlower forms—a vulgar, impudent, swaggering fellow, who affected a most\nsensitive feeling of personal honour, though, of course, he was a\nwretched little coward at heart. He was one of those worshippers of\nZverkov who made up to the latter from interested motives, and often\nborrowed money from him. Simonov’s other visitor, Trudolyubov, was a\nperson in no way remarkable—a tall young fellow, in the army, with a\ncold face, fairly honest, though he worshipped success of every sort,\nand was only capable of thinking of promotion. He was some sort of\ndistant relation of Zverkov’s, and this, foolish as it seems, gave him\na certain importance among us. He always thought me of no consequence\nwhatever; his behaviour to me, though not quite courteous, was\ntolerable.\n\n“Well, with seven roubles each,” said Trudolyubov, “twenty-one roubles\nbetween the three of us, we ought to be able to get a good dinner.\nZverkov, of course, won’t pay.”\n\n“Of course not, since we are inviting him,” Simonov decided.\n\n“Can you imagine,” Ferfitchkin interrupted hotly and conceitedly, like\nsome insolent flunkey boasting of his master the General’s decorations,\n“can you imagine that Zverkov will let us pay alone? He will accept\nfrom delicacy, but he will order half a dozen bottles of champagne.”\n\n“Do we want half a dozen for the four of us?” observed Trudolyubov,\ntaking notice only of the half dozen.\n\n“So the three of us, with Zverkov for the fourth, twenty-one roubles,\nat the Hôtel de Paris at five o’clock tomorrow,” Simonov, who had been\nasked to make the arrangements, concluded finally.\n\n“How twenty-one roubles?” I asked in some agitation, with a show of\nbeing offended; “if you count me it will not be twenty-one, but\ntwenty-eight roubles.”\n\nIt seemed to me that to invite myself so suddenly and unexpectedly\nwould be positively graceful, and that they would all be conquered at\nonce and would look at me with respect.\n\n“Do you want to join, too?” Simonov observed, with no appearance of\npleasure, seeming to avoid looking at me. He knew me through and\nthrough.\n\nIt infuriated me that he knew me so thoroughly.\n\n“Why not? I am an old schoolfellow of his, too, I believe, and I must\nown I feel hurt that you have left me out,” I said, boiling over again.\n\n“And where were we to find you?” Ferfitchkin put in roughly.\n\n“You never were on good terms with Zverkov,” Trudolyubov added,\nfrowning.\n\nBut I had already clutched at the idea and would not give it up.\n\n“It seems to me that no one has a right to form an opinion upon that,”\nI retorted in a shaking voice, as though something tremendous had\nhappened. “Perhaps that is just my reason for wishing it now, that I\nhave not always been on good terms with him.”\n\n“Oh, there’s no making you out ... with these refinements,” Trudolyubov\njeered.\n\n“We’ll put your name down,” Simonov decided, addressing me. “Tomorrow\nat five-o’clock at the Hôtel de Paris.”\n\n“What about the money?” Ferfitchkin began in an undertone, indicating\nme to Simonov, but he broke off, for even Simonov was embarrassed.\n\n“That will do,” said Trudolyubov, getting up. “If he wants to come so\nmuch, let him.”\n\n“But it’s a private thing, between us friends,” Ferfitchkin said\ncrossly, as he, too, picked up his hat. “It’s not an official\ngathering.”\n\n“We do not want at all, perhaps ...”\n\nThey went away. Ferfitchkin did not greet me in any way as he went out,\nTrudolyubov barely nodded. Simonov, with whom I was left _tête-à-tête_,\nwas in a state of vexation and perplexity, and looked at me queerly. He\ndid not sit down and did not ask me to.\n\n“H’m ... yes ... tomorrow, then. Will you pay your subscription now? I\njust ask so as to know,” he muttered in embarrassment.\n\nI flushed crimson, as I did so I remembered that I had owed Simonov\nfifteen roubles for ages—which I had, indeed, never forgotten, though I\nhad not paid it.\n\n“You will understand, Simonov, that I could have no idea when I came\nhere.... I am very much vexed that I have forgotten....”\n\n“All right, all right, that doesn’t matter. You can pay tomorrow after\nthe dinner. I simply wanted to know.... Please don’t...”\n\nHe broke off and began pacing the room still more vexed. As he walked\nhe began to stamp with his heels.\n\n“Am I keeping you?” I asked, after two minutes of silence.\n\n“Oh!” he said, starting, “that is—to be truthful—yes. I have to go and\nsee someone ... not far from here,” he added in an apologetic voice,\nsomewhat abashed.\n\n“My goodness, why didn’t you say so?” I cried, seizing my cap, with an\nastonishingly free-and-easy air, which was the last thing I should have\nexpected of myself.\n\n“It’s close by ... not two paces away,” Simonov repeated, accompanying\nme to the front door with a fussy air which did not suit him at all.\n“So five o’clock, punctually, tomorrow,” he called down the stairs\nafter me. He was very glad to get rid of me. I was in a fury.\n\n“What possessed me, what possessed me to force myself upon them?” I\nwondered, grinding my teeth as I strode along the street, “for a\nscoundrel, a pig like that Zverkov! Of course I had better not go; of\ncourse, I must just snap my fingers at them. I am not bound in any way.\nI’ll send Simonov a note by tomorrow’s post....”\n\nBut what made me furious was that I knew for certain that I should go,\nthat I should make a point of going; and the more tactless, the more\nunseemly my going would be, the more certainly I would go.\n\nAnd there was a positive obstacle to my going: I had no money. All I\nhad was nine roubles, I had to give seven of that to my servant,\nApollon, for his monthly wages. That was all I paid him—he had to keep\nhimself.\n\nNot to pay him was impossible, considering his character. But I will\ntalk about that fellow, about that plague of mine, another time.\n\nHowever, I knew I should go and should not pay him his wages.\n\nThat night I had the most hideous dreams. No wonder; all the evening I\nhad been oppressed by memories of my miserable days at school, and I\ncould not shake them off. I was sent to the school by distant\nrelations, upon whom I was dependent and of whom I have heard nothing\nsince—they sent me there a forlorn, silent boy, already crushed by\ntheir reproaches, already troubled by doubt, and looking with savage\ndistrust at everyone. My schoolfellows met me with spiteful and\nmerciless jibes because I was not like any of them. But I could not\nendure their taunts; I could not give in to them with the ignoble\nreadiness with which they gave in to one another. I hated them from the\nfirst, and shut myself away from everyone in timid, wounded and\ndisproportionate pride. Their coarseness revolted me. They laughed\ncynically at my face, at my clumsy figure; and yet what stupid faces\nthey had themselves. In our school the boys’ faces seemed in a special\nway to degenerate and grow stupider. How many fine-looking boys came to\nus! In a few years they became repulsive. Even at sixteen I wondered at\nthem morosely; even then I was struck by the pettiness of their\nthoughts, the stupidity of their pursuits, their games, their\nconversations. They had no understanding of such essential things, they\ntook no interest in such striking, impressive subjects, that I could\nnot help considering them inferior to myself. It was not wounded vanity\nthat drove me to it, and for God’s sake do not thrust upon me your\nhackneyed remarks, repeated to nausea, that “I was only a dreamer,”\nwhile they even then had an understanding of life. They understood\nnothing, they had no idea of real life, and I swear that that was what\nmade me most indignant with them. On the contrary, the most obvious,\nstriking reality they accepted with fantastic stupidity and even at\nthat time were accustomed to respect success. Everything that was just,\nbut oppressed and looked down upon, they laughed at heartlessly and\nshamefully. They took rank for intelligence; even at sixteen they were\nalready talking about a snug berth. Of course, a great deal of it was\ndue to their stupidity, to the bad examples with which they had always\nbeen surrounded in their childhood and boyhood. They were monstrously\ndepraved. Of course a great deal of that, too, was superficial and an\nassumption of cynicism; of course there were glimpses of youth and\nfreshness even in their depravity; but even that freshness was not\nattractive, and showed itself in a certain rakishness. I hated them\nhorribly, though perhaps I was worse than any of them. They repaid me\nin the same way, and did not conceal their aversion for me. But by then\nI did not desire their affection: on the contrary, I continually longed\nfor their humiliation. To escape from their derision I purposely began\nto make all the progress I could with my studies and forced my way to\nthe very top. This impressed them. Moreover, they all began by degrees\nto grasp that I had already read books none of them could read, and\nunderstood things (not forming part of our school curriculum) of which\nthey had not even heard. They took a savage and sarcastic view of it,\nbut were morally impressed, especially as the teachers began to notice\nme on those grounds. The mockery ceased, but the hostility remained,\nand cold and strained relations became permanent between us. In the end\nI could not put up with it: with years a craving for society, for\nfriends, developed in me. I attempted to get on friendly terms with\nsome of my schoolfellows; but somehow or other my intimacy with them\nwas always strained and soon ended of itself. Once, indeed, I did have\na friend. But I was already a tyrant at heart; I wanted to exercise\nunbounded sway over him; I tried to instil into him a contempt for his\nsurroundings; I required of him a disdainful and complete break with\nthose surroundings. I frightened him with my passionate affection; I\nreduced him to tears, to hysterics. He was a simple and devoted soul;\nbut when he devoted himself to me entirely I began to hate him\nimmediately and repulsed him—as though all I needed him for was to win\na victory over him, to subjugate him and nothing else. But I could not\nsubjugate all of them; my friend was not at all like them either, he\nwas, in fact, a rare exception. The first thing I did on leaving school\nwas to give up the special job for which I had been destined so as to\nbreak all ties, to curse my past and shake the dust from off my\nfeet.... And goodness knows why, after all that, I should go trudging\noff to Simonov’s!\n\nEarly next morning I roused myself and jumped out of bed with\nexcitement, as though it were all about to happen at once. But I\nbelieved that some radical change in my life was coming, and would\ninevitably come that day. Owing to its rarity, perhaps, any external\nevent, however trivial, always made me feel as though some radical\nchange in my life were at hand. I went to the office, however, as\nusual, but sneaked away home two hours earlier to get ready. The great\nthing, I thought, is not to be the first to arrive, or they will think\nI am overjoyed at coming. But there were thousands of such great points\nto consider, and they all agitated and overwhelmed me. I polished my\nboots a second time with my own hands; nothing in the world would have\ninduced Apollon to clean them twice a day, as he considered that it was\nmore than his duties required of him. I stole the brushes to clean them\nfrom the passage, being careful he should not detect it, for fear of\nhis contempt. Then I minutely examined my clothes and thought that\neverything looked old, worn and threadbare. I had let myself get too\nslovenly. My uniform, perhaps, was tidy, but I could not go out to\ndinner in my uniform. The worst of it was that on the knee of my\ntrousers was a big yellow stain. I had a foreboding that that stain\nwould deprive me of nine-tenths of my personal dignity. I knew, too,\nthat it was very poor to think so. “But this is no time for thinking:\nnow I am in for the real thing,” I thought, and my heart sank. I knew,\ntoo, perfectly well even then, that I was monstrously exaggerating the\nfacts. But how could I help it? I could not control myself and was\nalready shaking with fever. With despair I pictured to myself how\ncoldly and disdainfully that “scoundrel” Zverkov would meet me; with\nwhat dull-witted, invincible contempt the blockhead Trudolyubov would\nlook at me; with what impudent rudeness the insect Ferfitchkin would\nsnigger at me in order to curry favour with Zverkov; how completely\nSimonov would take it all in, and how he would despise me for the\nabjectness of my vanity and lack of spirit—and, worst of all, how\npaltry, _unliterary_, commonplace it would all be. Of course, the best\nthing would be not to go at all. But that was most impossible of all:\nif I feel impelled to do anything, I seem to be pitchforked into it. I\nshould have jeered at myself ever afterwards: “So you funked it, you\nfunked it, you funked the _real thing!_” On the contrary, I\npassionately longed to show all that “rabble” that I was by no means\nsuch a spiritless creature as I seemed to myself. What is more, even in\nthe acutest paroxysm of this cowardly fever, I dreamed of getting the\nupper hand, of dominating them, carrying them away, making them like\nme—if only for my “elevation of thought and unmistakable wit.” They\nwould abandon Zverkov, he would sit on one side, silent and ashamed,\nwhile I should crush him. Then, perhaps, we would be reconciled and\ndrink to our everlasting friendship; but what was most bitter and\nhumiliating for me was that I knew even then, knew fully and for\ncertain, that I needed nothing of all this really, that I did not\nreally want to crush, to subdue, to attract them, and that I did not\ncare a straw really for the result, even if I did achieve it. Oh, how I\nprayed for the day to pass quickly! In unutterable anguish I went to\nthe window, opened the movable pane and looked out into the troubled\ndarkness of the thickly falling wet snow. At last my wretched little\nclock hissed out five. I seized my hat and, trying not to look at\nApollon, who had been all day expecting his month’s wages, but in his\nfoolishness was unwilling to be the first to speak about it, I slipped\nbetween him and the door and, jumping into a high-class sledge, on\nwhich I spent my last half rouble, I drove up in grand style to the\nHôtel de Paris.\n\n\n\n\nIV\n\n\nI had been certain the day before that I should be the first to arrive.\nBut it was not a question of being the first to arrive. Not only were\nthey not there, but I had difficulty in finding our room. The table was\nnot laid even. What did it mean? After a good many questions I elicited\nfrom the waiters that the dinner had been ordered not for five, but for\nsix o’clock. This was confirmed at the buffet too. I felt really\nashamed to go on questioning them. It was only twenty-five minutes past\nfive. If they changed the dinner hour they ought at least to have let\nme know—that is what the post is for, and not to have put me in an\nabsurd position in my own eyes and ... and even before the waiters. I\nsat down; the servant began laying the table; I felt even more\nhumiliated when he was present. Towards six o’clock they brought in\ncandles, though there were lamps burning in the room. It had not\noccurred to the waiter, however, to bring them in at once when I\narrived. In the next room two gloomy, angry-looking persons were eating\ntheir dinners in silence at two different tables. There was a great\ndeal of noise, even shouting, in a room further away; one could hear\nthe laughter of a crowd of people, and nasty little shrieks in French:\nthere were ladies at the dinner. It was sickening, in fact. I rarely\npassed more unpleasant moments, so much so that when they did arrive\nall together punctually at six I was overjoyed to see them, as though\nthey were my deliverers, and even forgot that it was incumbent upon me\nto show resentment.\n\nZverkov walked in at the head of them; evidently he was the leading\nspirit. He and all of them were laughing; but, seeing me, Zverkov drew\nhimself up a little, walked up to me deliberately with a slight, rather\njaunty bend from the waist. He shook hands with me in a friendly, but\nnot over-friendly, fashion, with a sort of circumspect courtesy like\nthat of a General, as though in giving me his hand he were warding off\nsomething. I had imagined, on the contrary, that on coming in he would\nat once break into his habitual thin, shrill laugh and fall to making\nhis insipid jokes and witticisms. I had been preparing for them ever\nsince the previous day, but I had not expected such condescension, such\nhigh-official courtesy. So, then, he felt himself ineffably superior to\nme in every respect! If he only meant to insult me by that\nhigh-official tone, it would not matter, I thought—I could pay him back\nfor it one way or another. But what if, in reality, without the least\ndesire to be offensive, that sheepshead had a notion in earnest that he\nwas superior to me and could only look at me in a patronising way? The\nvery supposition made me gasp.\n\n“I was surprised to hear of your desire to join us,” he began, lisping\nand drawling, which was something new. “You and I seem to have seen\nnothing of one another. You fight shy of us. You shouldn’t. We are not\nsuch terrible people as you think. Well, anyway, I am glad to renew our\nacquaintance.”\n\nAnd he turned carelessly to put down his hat on the window.\n\n“Have you been waiting long?” Trudolyubov inquired.\n\n“I arrived at five o’clock as you told me yesterday,” I answered aloud,\nwith an irritability that threatened an explosion.\n\n“Didn’t you let him know that we had changed the hour?” said\nTrudolyubov to Simonov.\n\n“No, I didn’t. I forgot,” the latter replied, with no sign of regret,\nand without even apologising to me he went off to order the _hors\nd’œuvres_.\n\n“So you’ve been here a whole hour? Oh, poor fellow!” Zverkov cried\nironically, for to his notions this was bound to be extremely funny.\nThat rascal Ferfitchkin followed with his nasty little snigger like a\npuppy yapping. My position struck him, too, as exquisitely ludicrous\nand embarrassing.\n\n“It isn’t funny at all!” I cried to Ferfitchkin, more and more\nirritated. “It wasn’t my fault, but other people’s. They neglected to\nlet me know. It was ... it was ... it was simply absurd.”\n\n“It’s not only absurd, but something else as well,” muttered\nTrudolyubov, naively taking my part. “You are not hard enough upon it.\nIt was simply rudeness—unintentional, of course. And how could Simonov\n... h’m!”\n\n“If a trick like that had been played on me,” observed Ferfitchkin, “I\nshould ...”\n\n“But you should have ordered something for yourself,” Zverkov\ninterrupted, “or simply asked for dinner without waiting for us.”\n\n“You will allow that I might have done that without your permission,” I\nrapped out. “If I waited, it was ...”\n\n“Let us sit down, gentlemen,” cried Simonov, coming in. “Everything is\nready; I can answer for the champagne; it is capitally frozen.... You\nsee, I did not know your address, where was I to look for you?” he\nsuddenly turned to me, but again he seemed to avoid looking at me.\nEvidently he had something against me. It must have been what happened\nyesterday.\n\nAll sat down; I did the same. It was a round table. Trudolyubov was on\nmy left, Simonov on my right, Zverkov was sitting opposite, Ferfitchkin\nnext to him, between him and Trudolyubov.\n\n“Tell me, are you ... in a government office?” Zverkov went on\nattending to me. Seeing that I was embarrassed he seriously thought\nthat he ought to be friendly to me, and, so to speak, cheer me up.\n\n“Does he want me to throw a bottle at his head?” I thought, in a fury.\nIn my novel surroundings I was unnaturally ready to be irritated.\n\n“In the N—— office,” I answered jerkily, with my eyes on my plate.\n\n“And ha-ave you a go-od berth? I say, what ma-a-de you leave your\noriginal job?”\n\n“What ma-a-de me was that I wanted to leave my original job,” I drawled\nmore than he, hardly able to control myself. Ferfitchkin went off into\na guffaw. Simonov looked at me ironically. Trudolyubov left off eating\nand began looking at me with curiosity.\n\nZverkov winced, but he tried not to notice it.\n\n“And the remuneration?”\n\n“What remuneration?”\n\n“I mean, your sa-a-lary?”\n\n“Why are you cross-examining me?” However, I told him at once what my\nsalary was. I turned horribly red.\n\n“It is not very handsome,” Zverkov observed majestically.\n\n“Yes, you can’t afford to dine at cafés on that,” Ferfitchkin added\ninsolently.\n\n“To my thinking it’s very poor,” Trudolyubov observed gravely.\n\n“And how thin you have grown! How you have changed!” added Zverkov,\nwith a shade of venom in his voice, scanning me and my attire with a\nsort of insolent compassion.\n\n“Oh, spare his blushes,” cried Ferfitchkin, sniggering.\n\n“My dear sir, allow me to tell you I am not blushing,” I broke out at\nlast; “do you hear? I am dining here, at this cafe, at my own expense,\nnot at other people’s—note that, Mr. Ferfitchkin.”\n\n“Wha-at? Isn’t every one here dining at his own expense? You would seem\nto be ...” Ferfitchkin flew out at me, turning as red as a lobster, and\nlooking me in the face with fury.\n\n“Tha-at,” I answered, feeling I had gone too far, “and I imagine it\nwould be better to talk of something more intelligent.”\n\n“You intend to show off your intelligence, I suppose?”\n\n“Don’t disturb yourself, that would be quite out of place here.”\n\n“Why are you clacking away like that, my good sir, eh? Have you gone\nout of your wits in your office?”\n\n“Enough, gentlemen, enough!” Zverkov cried, authoritatively.\n\n“How stupid it is!” muttered Simonov.\n\n“It really is stupid. We have met here, a company of friends, for a\nfarewell dinner to a comrade and you carry on an altercation,” said\nTrudolyubov, rudely addressing himself to me alone. “You invited\nyourself to join us, so don’t disturb the general harmony.”\n\n“Enough, enough!” cried Zverkov. “Give over, gentlemen, it’s out of\nplace. Better let me tell you how I nearly got married the day before\nyesterday....”\n\nAnd then followed a burlesque narrative of how this gentleman had\nalmost been married two days before. There was not a word about the\nmarriage, however, but the story was adorned with generals, colonels\nand kammer-junkers, while Zverkov almost took the lead among them. It\nwas greeted with approving laughter; Ferfitchkin positively squealed.\n\nNo one paid any attention to me, and I sat crushed and humiliated.\n\n“Good Heavens, these are not the people for me!” I thought. “And what a\nfool I have made of myself before them! I let Ferfitchkin go too far,\nthough. The brutes imagine they are doing me an honour in letting me\nsit down with them. They don’t understand that it’s an honour to them\nand not to me! I’ve grown thinner! My clothes! Oh, damn my trousers!\nZverkov noticed the yellow stain on the knee as soon as he came in....\nBut what’s the use! I must get up at once, this very minute, take my\nhat and simply go without a word ... with contempt! And tomorrow I can\nsend a challenge. The scoundrels! As though I cared about the seven\nroubles. They may think.... Damn it! I don’t care about the seven\nroubles. I’ll go this minute!”\n\nOf course I remained. I drank sherry and Lafitte by the glassful in my\ndiscomfiture. Being unaccustomed to it, I was quickly affected. My\nannoyance increased as the wine went to my head. I longed all at once\nto insult them all in a most flagrant manner and then go away. To seize\nthe moment and show what I could do, so that they would say, “He’s\nclever, though he is absurd,” and ... and ... in fact, damn them all!\n\nI scanned them all insolently with my drowsy eyes. But they seemed to\nhave forgotten me altogether. They were noisy, vociferous, cheerful.\nZverkov was talking all the time. I began listening. Zverkov was\ntalking of some exuberant lady whom he had at last led on to declaring\nher love (of course, he was lying like a horse), and how he had been\nhelped in this affair by an intimate friend of his, a Prince Kolya, an\nofficer in the hussars, who had three thousand serfs.\n\n“And yet this Kolya, who has three thousand serfs, has not put in an\nappearance here tonight to see you off,” I cut in suddenly.\n\nFor one minute every one was silent. “You are drunk already.”\nTrudolyubov deigned to notice me at last, glancing contemptuously in my\ndirection. Zverkov, without a word, examined me as though I were an\ninsect. I dropped my eyes. Simonov made haste to fill up the glasses\nwith champagne.\n\nTrudolyubov raised his glass, as did everyone else but me.\n\n“Your health and good luck on the journey!” he cried to Zverkov. “To\nold times, to our future, hurrah!”\n\nThey all tossed off their glasses, and crowded round Zverkov to kiss\nhim. I did not move; my full glass stood untouched before me.\n\n“Why, aren’t you going to drink it?” roared Trudolyubov, losing\npatience and turning menacingly to me.\n\n“I want to make a speech separately, on my own account ... and then\nI’ll drink it, Mr. Trudolyubov.”\n\n“Spiteful brute!” muttered Simonov. I drew myself up in my chair and\nfeverishly seized my glass, prepared for something extraordinary,\nthough I did not know myself precisely what I was going to say.\n\n“_Silence!_” cried Ferfitchkin. “Now for a display of wit!”\n\nZverkov waited very gravely, knowing what was coming.\n\n“Mr. Lieutenant Zverkov,” I began, “let me tell you that I hate\nphrases, phrasemongers and men in corsets ... that’s the first point,\nand there is a second one to follow it.”\n\nThere was a general stir.\n\n“The second point is: I hate ribaldry and ribald talkers. Especially\nribald talkers! The third point: I love justice, truth and honesty.” I\nwent on almost mechanically, for I was beginning to shiver with horror\nmyself and had no idea how I came to be talking like this. “I love\nthought, Monsieur Zverkov; I love true comradeship, on an equal footing\nand not ... H’m ... I love ... But, however, why not? I will drink your\nhealth, too, Mr. Zverkov. Seduce the Circassian girls, shoot the\nenemies of the fatherland and ... and ... to your health, Monsieur\nZverkov!”\n\nZverkov got up from his seat, bowed to me and said:\n\n“I am very much obliged to you.” He was frightfully offended and turned\npale.\n\n“Damn the fellow!” roared Trudolyubov, bringing his fist down on the\ntable.\n\n“Well, he wants a punch in the face for that,” squealed Ferfitchkin.\n\n“We ought to turn him out,” muttered Simonov.\n\n“Not a word, gentlemen, not a movement!” cried Zverkov solemnly,\nchecking the general indignation. “I thank you all, but I can show him\nfor myself how much value I attach to his words.”\n\n“Mr. Ferfitchkin, you will give me satisfaction tomorrow for your words\njust now!” I said aloud, turning with dignity to Ferfitchkin.\n\n“A duel, you mean? Certainly,” he answered. But probably I was so\nridiculous as I challenged him and it was so out of keeping with my\nappearance that everyone including Ferfitchkin was prostrate with\nlaughter.\n\n“Yes, let him alone, of course! He is quite drunk,” Trudolyubov said\nwith disgust.\n\n“I shall never forgive myself for letting him join us,” Simonov\nmuttered again.\n\n“Now is the time to throw a bottle at their heads,” I thought to\nmyself. I picked up the bottle ... and filled my glass.... “No, I’d\nbetter sit on to the end,” I went on thinking; “you would be pleased,\nmy friends, if I went away. Nothing will induce me to go. I’ll go on\nsitting here and drinking to the end, on purpose, as a sign that I\ndon’t think you of the slightest consequence. I will go on sitting and\ndrinking, because this is a public-house and I paid my entrance money.\nI’ll sit here and drink, for I look upon you as so many pawns, as\ninanimate pawns. I’ll sit here and drink ... and sing if I want to,\nyes, sing, for I have the right to ... to sing ... H’m!”\n\nBut I did not sing. I simply tried not to look at any of them. I\nassumed most unconcerned attitudes and waited with impatience for them\nto speak _first_. But alas, they did not address me! And oh, how I\nwished, how I wished at that moment to be reconciled to them! It struck\neight, at last nine. They moved from the table to the sofa. Zverkov\nstretched himself on a lounge and put one foot on a round table. Wine\nwas brought there. He did, as a fact, order three bottles on his own\naccount. I, of course, was not invited to join them. They all sat round\nhim on the sofa. They listened to him, almost with reverence. It was\nevident that they were fond of him. “What for? What for?” I wondered.\nFrom time to time they were moved to drunken enthusiasm and kissed each\nother. They talked of the Caucasus, of the nature of true passion, of\nsnug berths in the service, of the income of an hussar called\nPodharzhevsky, whom none of them knew personally, and rejoiced in the\nlargeness of it, of the extraordinary grace and beauty of a Princess\nD., whom none of them had ever seen; then it came to Shakespeare’s\nbeing immortal.\n\nI smiled contemptuously and walked up and down the other side of the\nroom, opposite the sofa, from the table to the stove and back again. I\ntried my very utmost to show them that I could do without them, and yet\nI purposely made a noise with my boots, thumping with my heels. But it\nwas all in vain. They paid no attention. I had the patience to walk up\nand down in front of them from eight o’clock till eleven, in the same\nplace, from the table to the stove and back again. “I walk up and down\nto please myself and no one can prevent me.” The waiter who came into\nthe room stopped, from time to time, to look at me. I was somewhat\ngiddy from turning round so often; at moments it seemed to me that I\nwas in delirium. During those three hours I was three times soaked with\nsweat and dry again. At times, with an intense, acute pang I was\nstabbed to the heart by the thought that ten years, twenty years, forty\nyears would pass, and that even in forty years I would remember with\nloathing and humiliation those filthiest, most ludicrous, and most\nawful moments of my life. No one could have gone out of his way to\ndegrade himself more shamelessly, and I fully realised it, fully, and\nyet I went on pacing up and down from the table to the stove. “Oh, if\nyou only knew what thoughts and feelings I am capable of, how cultured\nI am!” I thought at moments, mentally addressing the sofa on which my\nenemies were sitting. But my enemies behaved as though I were not in\nthe room. Once—only once—they turned towards me, just when Zverkov was\ntalking about Shakespeare, and I suddenly gave a contemptuous laugh. I\nlaughed in such an affected and disgusting way that they all at once\nbroke off their conversation, and silently and gravely for two minutes\nwatched me walking up and down from the table to the stove, _taking no\nnotice of them_. But nothing came of it: they said nothing, and two\nminutes later they ceased to notice me again. It struck eleven.\n\n“Friends,” cried Zverkov getting up from the sofa, “let us all be off\nnow, _there!_”\n\n“Of course, of course,” the others assented. I turned sharply to\nZverkov. I was so harassed, so exhausted, that I would have cut my\nthroat to put an end to it. I was in a fever; my hair, soaked with\nperspiration, stuck to my forehead and temples.\n\n“Zverkov, I beg your pardon,” I said abruptly and resolutely.\n“Ferfitchkin, yours too, and everyone’s, everyone’s: I have insulted\nyou all!”\n\n“Aha! A duel is not in your line, old man,” Ferfitchkin hissed\nvenomously.\n\nIt sent a sharp pang to my heart.\n\n“No, it’s not the duel I am afraid of, Ferfitchkin! I am ready to fight\nyou tomorrow, after we are reconciled. I insist upon it, in fact, and\nyou cannot refuse. I want to show you that I am not afraid of a duel.\nYou shall fire first and I shall fire into the air.”\n\n“He is comforting himself,” said Simonov.\n\n“He’s simply raving,” said Trudolyubov.\n\n“But let us pass. Why are you barring our way? What do you want?”\nZverkov answered disdainfully.\n\nThey were all flushed, their eyes were bright: they had been drinking\nheavily.\n\n“I ask for your friendship, Zverkov; I insulted you, but ...”\n\n“Insulted? _You_ insulted _me?_ Understand, sir, that you never, under\nany circumstances, could possibly insult _me_.”\n\n“And that’s enough for you. Out of the way!” concluded Trudolyubov.\n\n“Olympia is mine, friends, that’s agreed!” cried Zverkov.\n\n“We won’t dispute your right, we won’t dispute your right,” the others\nanswered, laughing.\n\nI stood as though spat upon. The party went noisily out of the room.\nTrudolyubov struck up some stupid song. Simonov remained behind for a\nmoment to tip the waiters. I suddenly went up to him.\n\n“Simonov! give me six roubles!” I said, with desperate resolution.\n\nHe looked at me in extreme amazement, with vacant eyes. He, too, was\ndrunk.\n\n“You don’t mean you are coming with us?”\n\n“Yes.”\n\n“I’ve no money,” he snapped out, and with a scornful laugh he went out\nof the room.\n\nI clutched at his overcoat. It was a nightmare.\n\n“Simonov, I saw you had money. Why do you refuse me? Am I a scoundrel?\nBeware of refusing me: if you knew, if you knew why I am asking! My\nwhole future, my whole plans depend upon it!”\n\nSimonov pulled out the money and almost flung it at me.\n\n“Take it, if you have no sense of shame!” he pronounced pitilessly, and\nran to overtake them.\n\nI was left for a moment alone. Disorder, the remains of dinner, a\nbroken wine-glass on the floor, spilt wine, cigarette ends, fumes of\ndrink and delirium in my brain, an agonising misery in my heart and\nfinally the waiter, who had seen and heard all and was looking\ninquisitively into my face.\n\n“I am going there!” I cried. “Either they shall all go down on their\nknees to beg for my friendship, or I will give Zverkov a slap in the\nface!”\n\n\n\n\nV\n\n\n“So this is it, this is it at last—contact with real life,” I muttered\nas I ran headlong downstairs. “This is very different from the Pope’s\nleaving Rome and going to Brazil, very different from the ball on Lake\nComo!”\n\n“You are a scoundrel,” a thought flashed through my mind, “if you laugh\nat this now.”\n\n“No matter!” I cried, answering myself. “Now everything is lost!”\n\nThere was no trace to be seen of them, but that made no difference—I\nknew where they had gone.\n\nAt the steps was standing a solitary night sledge-driver in a rough\npeasant coat, powdered over with the still falling, wet, and as it were\nwarm, snow. It was hot and steamy. The little shaggy piebald horse was\nalso covered with snow and coughing, I remember that very well. I made\na rush for the roughly made sledge; but as soon as I raised my foot to\nget into it, the recollection of how Simonov had just given me six\nroubles seemed to double me up and I tumbled into the sledge like a\nsack.\n\n“No, I must do a great deal to make up for all that,” I cried. “But I\nwill make up for it or perish on the spot this very night. Start!”\n\nWe set off. There was a perfect whirl in my head.\n\n“They won’t go down on their knees to beg for my friendship. That is a\nmirage, cheap mirage, revolting, romantic and fantastical—that’s\nanother ball on Lake Como. And so I am bound to slap Zverkov’s face! It\nis my duty to. And so it is settled; I am flying to give him a slap in\nthe face. Hurry up!”\n\nThe driver tugged at the reins.\n\n“As soon as I go in I’ll give it him. Ought I before giving him the\nslap to say a few words by way of preface? No. I’ll simply go in and\ngive it him. They will all be sitting in the drawing-room, and he with\nOlympia on the sofa. That damned Olympia! She laughed at my looks on\none occasion and refused me. I’ll pull Olympia’s hair, pull Zverkov’s\nears! No, better one ear, and pull him by it round the room. Maybe they\nwill all begin beating me and will kick me out. That’s most likely,\nindeed. No matter! Anyway, I shall first slap him; the initiative will\nbe mine; and by the laws of honour that is everything: he will be\nbranded and cannot wipe off the slap by any blows, by nothing but a\nduel. He will be forced to fight. And let them beat me now. Let them,\nthe ungrateful wretches! Trudolyubov will beat me hardest, he is so\nstrong; Ferfitchkin will be sure to catch hold sideways and tug at my\nhair. But no matter, no matter! That’s what I am going for. The\nblockheads will be forced at last to see the tragedy of it all! When\nthey drag me to the door I shall call out to them that in reality they\nare not worth my little finger. Get on, driver, get on!” I cried to the\ndriver. He started and flicked his whip, I shouted so savagely.\n\n“We shall fight at daybreak, that’s a settled thing. I’ve done with the\noffice. Ferfitchkin made a joke about it just now. But where can I get\npistols? Nonsense! I’ll get my salary in advance and buy them. And\npowder, and bullets? That’s the second’s business. And how can it all\nbe done by daybreak? and where am I to get a second? I have no friends.\nNonsense!” I cried, lashing myself up more and more. “It’s of no\nconsequence! The first person I meet in the street is bound to be my\nsecond, just as he would be bound to pull a drowning man out of water.\nThe most eccentric things may happen. Even if I were to ask the\ndirector himself to be my second tomorrow, he would be bound to\nconsent, if only from a feeling of chivalry, and to keep the secret!\nAnton Antonitch....”\n\nThe fact is, that at that very minute the disgusting absurdity of my\nplan and the other side of the question was clearer and more vivid to\nmy imagination than it could be to anyone on earth. But ....\n\n“Get on, driver, get on, you rascal, get on!”\n\n“Ugh, sir!” said the son of toil.\n\nCold shivers suddenly ran down me. Wouldn’t it be better ... to go\nstraight home? My God, my God! Why did I invite myself to this dinner\nyesterday? But no, it’s impossible. And my walking up and down for\nthree hours from the table to the stove? No, they, they and no one else\nmust pay for my walking up and down! They must wipe out this dishonour!\nDrive on!\n\nAnd what if they give me into custody? They won’t dare! They’ll be\nafraid of the scandal. And what if Zverkov is so contemptuous that he\nrefuses to fight a duel? He is sure to; but in that case I’ll show them\n... I will turn up at the posting station when he’s setting off\ntomorrow, I’ll catch him by the leg, I’ll pull off his coat when he\ngets into the carriage. I’ll get my teeth into his hand, I’ll bite him.\n“See what lengths you can drive a desperate man to!” He may hit me on\nthe head and they may belabour me from behind. I will shout to the\nassembled multitude: “Look at this young puppy who is driving off to\ncaptivate the Circassian girls after letting me spit in his face!”\n\nOf course, after that everything will be over! The office will have\nvanished off the face of the earth. I shall be arrested, I shall be\ntried, I shall be dismissed from the service, thrown in prison, sent to\nSiberia. Never mind! In fifteen years when they let me out of prison I\nwill trudge off to him, a beggar, in rags. I shall find him in some\nprovincial town. He will be married and happy. He will have a grown-up\ndaughter.... I shall say to him: “Look, monster, at my hollow cheeks\nand my rags! I’ve lost everything—my career, my happiness, art,\nscience, _the woman I loved_, and all through you. Here are pistols. I\nhave come to discharge my pistol and ... and I ... forgive you. Then I\nshall fire into the air and he will hear nothing more of me....”\n\nI was actually on the point of tears, though I knew perfectly well at\nthat moment that all this was out of Pushkin’s _Silvio_ and Lermontov’s\n_Masquerade_. And all at once I felt horribly ashamed, so ashamed that\nI stopped the horse, got out of the sledge, and stood still in the snow\nin the middle of the street. The driver gazed at me, sighing and\nastonished.\n\nWhat was I to do? I could not go on there—it was evidently stupid, and\nI could not leave things as they were, because that would seem as\nthough ... Heavens, how could I leave things! And after such insults!\n“No!” I cried, throwing myself into the sledge again. “It is ordained!\nIt is fate! Drive on, drive on!”\n\nAnd in my impatience I punched the sledge-driver on the back of the\nneck.\n\n“What are you up to? What are you hitting me for?” the peasant shouted,\nbut he whipped up his nag so that it began kicking.\n\nThe wet snow was falling in big flakes; I unbuttoned myself, regardless\nof it. I forgot everything else, for I had finally decided on the slap,\nand felt with horror that it was going to happen _now, at once_, and\nthat _no force could stop it_. The deserted street lamps gleamed\nsullenly in the snowy darkness like torches at a funeral. The snow\ndrifted under my great-coat, under my coat, under my cravat, and melted\nthere. I did not wrap myself up—all was lost, anyway.\n\nAt last we arrived. I jumped out, almost unconscious, ran up the steps\nand began knocking and kicking at the door. I felt fearfully weak,\nparticularly in my legs and knees. The door was opened quickly as\nthough they knew I was coming. As a fact, Simonov had warned them that\nperhaps another gentleman would arrive, and this was a place in which\none had to give notice and to observe certain precautions. It was one\nof those “millinery establishments” which were abolished by the police\na good time ago. By day it really was a shop; but at night, if one had\nan introduction, one might visit it for other purposes.\n\nI walked rapidly through the dark shop into the familiar drawing-room,\nwhere there was only one candle burning, and stood still in amazement:\nthere was no one there. “Where are they?” I asked somebody. But by now,\nof course, they had separated. Before me was standing a person with a\nstupid smile, the “madam” herself, who had seen me before. A minute\nlater a door opened and another person came in.\n\nTaking no notice of anything I strode about the room, and, I believe, I\ntalked to myself. I felt as though I had been saved from death and was\nconscious of this, joyfully, all over: I should have given that slap, I\nshould certainly, certainly have given it! But now they were not here\nand ... everything had vanished and changed! I looked round. I could\nnot realise my condition yet. I looked mechanically at the girl who had\ncome in: and had a glimpse of a fresh, young, rather pale face, with\nstraight, dark eyebrows, and with grave, as it were wondering, eyes\nthat attracted me at once; I should have hated her if she had been\nsmiling. I began looking at her more intently and, as it were, with\neffort. I had not fully collected my thoughts. There was something\nsimple and good-natured in her face, but something strangely grave. I\nam sure that this stood in her way here, and no one of those fools had\nnoticed her. She could not, however, have been called a beauty, though\nshe was tall, strong-looking, and well built. She was very simply\ndressed. Something loathsome stirred within me. I went straight up to\nher.\n\nI chanced to look into the glass. My harassed face struck me as\nrevolting in the extreme, pale, angry, abject, with dishevelled hair.\n“No matter, I am glad of it,” I thought; “I am glad that I shall seem\nrepulsive to her; I like that.”\n\n\n\n\nVI\n\n\n... Somewhere behind a screen a clock began wheezing, as though\noppressed by something, as though someone were strangling it. After an\nunnaturally prolonged wheezing there followed a shrill, nasty, and as\nit were unexpectedly rapid, chime—as though someone were suddenly\njumping forward. It struck two. I woke up, though I had indeed not been\nasleep but lying half-conscious.\n\nIt was almost completely dark in the narrow, cramped, low-pitched room,\ncumbered up with an enormous wardrobe and piles of cardboard boxes and\nall sorts of frippery and litter. The candle end that had been burning\non the table was going out and gave a faint flicker from time to time.\nIn a few minutes there would be complete darkness.\n\nI was not long in coming to myself; everything came back to my mind at\nonce, without an effort, as though it had been in ambush to pounce upon\nme again. And, indeed, even while I was unconscious a point seemed\ncontinually to remain in my memory unforgotten, and round it my dreams\nmoved drearily. But strange to say, everything that had happened to me\nin that day seemed to me now, on waking, to be in the far, far away\npast, as though I had long, long ago lived all that down.\n\nMy head was full of fumes. Something seemed to be hovering over me,\nrousing me, exciting me, and making me restless. Misery and spite\nseemed surging up in me again and seeking an outlet. Suddenly I saw\nbeside me two wide open eyes scrutinising me curiously and\npersistently. The look in those eyes was coldly detached, sullen, as it\nwere utterly remote; it weighed upon me.\n\nA grim idea came into my brain and passed all over my body, as a\nhorrible sensation, such as one feels when one goes into a damp and\nmouldy cellar. There was something unnatural in those two eyes,\nbeginning to look at me only now. I recalled, too, that during those\ntwo hours I had not said a single word to this creature, and had, in\nfact, considered it utterly superfluous; in fact, the silence had for\nsome reason gratified me. Now I suddenly realised vividly the hideous\nidea—revolting as a spider—of vice, which, without love, grossly and\nshamelessly begins with that in which true love finds its consummation.\nFor a long time we gazed at each other like that, but she did not drop\nher eyes before mine and her expression did not change, so that at last\nI felt uncomfortable.\n\n“What is your name?” I asked abruptly, to put an end to it.\n\n“Liza,” she answered almost in a whisper, but somehow far from\ngraciously, and she turned her eyes away.\n\nI was silent.\n\n“What weather! The snow ... it’s disgusting!” I said, almost to myself,\nputting my arm under my head despondently, and gazing at the ceiling.\n\nShe made no answer. This was horrible.\n\n“Have you always lived in Petersburg?” I asked a minute later, almost\nangrily, turning my head slightly towards her.\n\n“No.”\n\n“Where do you come from?”\n\n“From Riga,” she answered reluctantly.\n\n“Are you a German?”\n\n“No, Russian.”\n\n“Have you been here long?”\n\n“Where?”\n\n“In this house?”\n\n“A fortnight.”\n\nShe spoke more and more jerkily. The candle went out; I could no longer\ndistinguish her face.\n\n“Have you a father and mother?”\n\n“Yes ... no ... I have.”\n\n“Where are they?”\n\n“There ... in Riga.”\n\n“What are they?”\n\n“Oh, nothing.”\n\n“Nothing? Why, what class are they?”\n\n“Tradespeople.”\n\n“Have you always lived with them?”\n\n“Yes.”\n\n“How old are you?”\n\n“Twenty.”\n\n“Why did you leave them?”\n\n“Oh, for no reason.”\n\nThat answer meant “Let me alone; I feel sick, sad.”\n\nWe were silent.\n\nGod knows why I did not go away. I felt myself more and more sick and\ndreary. The images of the previous day began of themselves, apart from\nmy will, flitting through my memory in confusion. I suddenly recalled\nsomething I had seen that morning when, full of anxious thoughts, I was\nhurrying to the office.\n\n“I saw them carrying a coffin out yesterday and they nearly dropped\nit,” I suddenly said aloud, not that I desired to open the\nconversation, but as it were by accident.\n\n“A coffin?”\n\n“Yes, in the Haymarket; they were bringing it up out of a cellar.”\n\n“From a cellar?”\n\n“Not from a cellar, but a basement. Oh, you know ... down below ...\nfrom a house of ill-fame. It was filthy all round ... Egg-shells,\nlitter ... a stench. It was loathsome.”\n\nSilence.\n\n“A nasty day to be buried,” I began, simply to avoid being silent.\n\n“Nasty, in what way?”\n\n“The snow, the wet.” (I yawned.)\n\n“It makes no difference,” she said suddenly, after a brief silence.\n\n“No, it’s horrid.” (I yawned again). “The gravediggers must have sworn\nat getting drenched by the snow. And there must have been water in the\ngrave.”\n\n“Why water in the grave?” she asked, with a sort of curiosity, but\nspeaking even more harshly and abruptly than before.\n\nI suddenly began to feel provoked.\n\n“Why, there must have been water at the bottom a foot deep. You can’t\ndig a dry grave in Volkovo Cemetery.”\n\n“Why?”\n\n“Why? Why, the place is waterlogged. It’s a regular marsh. So they bury\nthem in water. I’ve seen it myself ... many times.”\n\n(I had never seen it once, indeed I had never been in Volkovo, and had\nonly heard stories of it.)\n\n“Do you mean to say, you don’t mind how you die?”\n\n“But why should I die?” she answered, as though defending herself.\n\n“Why, some day you will die, and you will die just the same as that\ndead woman. She was ... a girl like you. She died of consumption.”\n\n“A wench would have died in hospital ...” (She knows all about it\nalready: she said “wench,” not “girl.”)\n\n“She was in debt to her madam,” I retorted, more and more provoked by\nthe discussion; “and went on earning money for her up to the end,\nthough she was in consumption. Some sledge-drivers standing by were\ntalking about her to some soldiers and telling them so. No doubt they\nknew her. They were laughing. They were going to meet in a pot-house to\ndrink to her memory.”\n\nA great deal of this was my invention. Silence followed, profound\nsilence. She did not stir.\n\n“And is it better to die in a hospital?”\n\n“Isn’t it just the same? Besides, why should I die?” she added\nirritably.\n\n“If not now, a little later.”\n\n“Why a little later?”\n\n“Why, indeed? Now you are young, pretty, fresh, you fetch a high price.\nBut after another year of this life you will be very different—you will\ngo off.”\n\n“In a year?”\n\n“Anyway, in a year you will be worth less,” I continued malignantly.\n“You will go from here to something lower, another house; a year\nlater—to a third, lower and lower, and in seven years you will come to\na basement in the Haymarket. That will be if you were lucky. But it\nwould be much worse if you got some disease, consumption, say ... and\ncaught a chill, or something or other. It’s not easy to get over an\nillness in your way of life. If you catch anything you may not get rid\nof it. And so you would die.”\n\n“Oh, well, then I shall die,” she answered, quite vindictively, and she\nmade a quick movement.\n\n“But one is sorry.”\n\n“Sorry for whom?”\n\n“Sorry for life.” Silence.\n\n“Have you been engaged to be married? Eh?”\n\n“What’s that to you?”\n\n“Oh, I am not cross-examining you. It’s nothing to me. Why are you so\ncross? Of course you may have had your own troubles. What is it to me?\nIt’s simply that I felt sorry.”\n\n“Sorry for whom?”\n\n“Sorry for you.”\n\n“No need,” she whispered hardly audibly, and again made a faint\nmovement.\n\nThat incensed me at once. What! I was so gentle with her, and she....\n\n“Why, do you think that you are on the right path?”\n\n“I don’t think anything.”\n\n“That’s what’s wrong, that you don’t think. Realise it while there is\nstill time. There still is time. You are still young, good-looking; you\nmight love, be married, be happy....”\n\n“Not all married women are happy,” she snapped out in the rude abrupt\ntone she had used at first.\n\n“Not all, of course, but anyway it is much better than the life here.\nInfinitely better. Besides, with love one can live even without\nhappiness. Even in sorrow life is sweet; life is sweet, however one\nlives. But here what is there but ... foulness? Phew!”\n\nI turned away with disgust; I was no longer reasoning coldly. I began\nto feel myself what I was saying and warmed to the subject. I was\nalready longing to expound the cherished ideas I had brooded over in my\ncorner. Something suddenly flared up in me. An object had appeared\nbefore me.\n\n“Never mind my being here, I am not an example for you. I am, perhaps,\nworse than you are. I was drunk when I came here, though,” I hastened,\nhowever, to say in self-defence. “Besides, a man is no example for a\nwoman. It’s a different thing. I may degrade and defile myself, but I\nam not anyone’s slave. I come and go, and that’s an end of it. I shake\nit off, and I am a different man. But you are a slave from the start.\nYes, a slave! You give up everything, your whole freedom. If you want\nto break your chains afterwards, you won’t be able to; you will be more\nand more fast in the snares. It is an accursed bondage. I know it. I\nwon’t speak of anything else, maybe you won’t understand, but tell me:\nno doubt you are in debt to your madam? There, you see,” I added,\nthough she made no answer, but only listened in silence, entirely\nabsorbed, “that’s a bondage for you! You will never buy your freedom.\nThey will see to that. It’s like selling your soul to the devil.... And\nbesides ... perhaps, I too, am just as unlucky—how do you know—and\nwallow in the mud on purpose, out of misery? You know, men take to\ndrink from grief; well, maybe I am here from grief. Come, tell me, what\nis there good here? Here you and I ... came together ... just now and\ndid not say one word to one another all the time, and it was only\nafterwards you began staring at me like a wild creature, and I at you.\nIs that loving? Is that how one human being should meet another? It’s\nhideous, that’s what it is!”\n\n“Yes!” she assented sharply and hurriedly.\n\nI was positively astounded by the promptitude of this “Yes.” So the\nsame thought may have been straying through her mind when she was\nstaring at me just before. So she, too, was capable of certain\nthoughts? “Damn it all, this was interesting, this was a point of\nlikeness!” I thought, almost rubbing my hands. And indeed it’s easy to\nturn a young soul like that!\n\nIt was the exercise of my power that attracted me most.\n\nShe turned her head nearer to me, and it seemed to me in the darkness\nthat she propped herself on her arm. Perhaps she was scrutinising me.\nHow I regretted that I could not see her eyes. I heard her deep\nbreathing.\n\n“Why have you come here?” I asked her, with a note of authority already\nin my voice.\n\n“Oh, I don’t know.”\n\n“But how nice it would be to be living in your father’s house! It’s\nwarm and free; you have a home of your own.”\n\n“But what if it’s worse than this?”\n\n“I must take the right tone,” flashed through my mind. “I may not get\nfar with sentimentality.” But it was only a momentary thought. I swear\nshe really did interest me. Besides, I was exhausted and moody. And\ncunning so easily goes hand-in-hand with feeling.\n\n“Who denies it!” I hastened to answer. “Anything may happen. I am\nconvinced that someone has wronged you, and that you are more sinned\nagainst than sinning. Of course, I know nothing of your story, but it’s\nnot likely a girl like you has come here of her own inclination....”\n\n“A girl like me?” she whispered, hardly audibly; but I heard it.\n\nDamn it all, I was flattering her. That was horrid. But perhaps it was\na good thing.... She was silent.\n\n“See, Liza, I will tell you about myself. If I had had a home from\nchildhood, I shouldn’t be what I am now. I often think that. However\nbad it may be at home, anyway they are your father and mother, and not\nenemies, strangers. Once a year at least, they’ll show their love of\nyou. Anyway, you know you are at home. I grew up without a home; and\nperhaps that’s why I’ve turned so ... unfeeling.”\n\nI waited again. “Perhaps she doesn’t understand,” I thought, “and,\nindeed, it is absurd—it’s moralising.”\n\n“If I were a father and had a daughter, I believe I should love my\ndaughter more than my sons, really,” I began indirectly, as though\ntalking of something else, to distract her attention. I must confess I\nblushed.\n\n“Why so?” she asked.\n\nAh! so she was listening!\n\n“I don’t know, Liza. I knew a father who was a stern, austere man, but\nused to go down on his knees to his daughter, used to kiss her hands,\nher feet, he couldn’t make enough of her, really. When she danced at\nparties he used to stand for five hours at a stretch, gazing at her. He\nwas mad over her: I understand that! She would fall asleep tired at\nnight, and he would wake to kiss her in her sleep and make the sign of\nthe cross over her. He would go about in a dirty old coat, he was\nstingy to everyone else, but would spend his last penny for her, giving\nher expensive presents, and it was his greatest delight when she was\npleased with what he gave her. Fathers always love their daughters more\nthan the mothers do. Some girls live happily at home! And I believe I\nshould never let my daughters marry.”\n\n“What next?” she said, with a faint smile.\n\n“I should be jealous, I really should. To think that she should kiss\nanyone else! That she should love a stranger more than her father! It’s\npainful to imagine it. Of course, that’s all nonsense, of course every\nfather would be reasonable at last. But I believe before I should let\nher marry, I should worry myself to death; I should find fault with all\nher suitors. But I should end by letting her marry whom she herself\nloved. The one whom the daughter loves always seems the worst to the\nfather, you know. That is always so. So many family troubles come from\nthat.”\n\n“Some are glad to sell their daughters, rather than marrying them\nhonourably.”\n\nAh, so that was it!\n\n“Such a thing, Liza, happens in those accursed families in which there\nis neither love nor God,” I retorted warmly, “and where there is no\nlove, there is no sense either. There are such families, it’s true, but\nI am not speaking of them. You must have seen wickedness in your own\nfamily, if you talk like that. Truly, you must have been unlucky. H’m!\n... that sort of thing mostly comes about through poverty.”\n\n“And is it any better with the gentry? Even among the poor, honest\npeople who live happily?”\n\n“H’m ... yes. Perhaps. Another thing, Liza, man is fond of reckoning up\nhis troubles, but does not count his joys. If he counted them up as he\nought, he would see that every lot has enough happiness provided for\nit. And what if all goes well with the family, if the blessing of God\nis upon it, if the husband is a good one, loves you, cherishes you,\nnever leaves you! There is happiness in such a family! Even sometimes\nthere is happiness in the midst of sorrow; and indeed sorrow is\neverywhere. If you marry _you will find out for yourself_. But think of\nthe first years of married life with one you love: what happiness, what\nhappiness there sometimes is in it! And indeed it’s the ordinary thing.\nIn those early days even quarrels with one’s husband end happily. Some\nwomen get up quarrels with their husbands just because they love them.\nIndeed, I knew a woman like that: she seemed to say that because she\nloved him, she would torment him and make him feel it. You know that\nyou may torment a man on purpose through love. Women are particularly\ngiven to that, thinking to themselves ‘I will love him so, I will make\nso much of him afterwards, that it’s no sin to torment him a little\nnow.’ And all in the house rejoice in the sight of you, and you are\nhappy and gay and peaceful and honourable.... Then there are some women\nwho are jealous. If he went off anywhere—I knew one such woman, she\ncouldn’t restrain herself, but would jump up at night and run off on\nthe sly to find out where he was, whether he was with some other woman.\nThat’s a pity. And the woman knows herself it’s wrong, and her heart\nfails her and she suffers, but she loves—it’s all through love. And how\nsweet it is to make up after quarrels, to own herself in the wrong or\nto forgive him! And they both are so happy all at once—as though they\nhad met anew, been married over again; as though their love had begun\nafresh. And no one, no one should know what passes between husband and\nwife if they love one another. And whatever quarrels there may be\nbetween them they ought not to call in their own mother to judge\nbetween them and tell tales of one another. They are their own judges.\nLove is a holy mystery and ought to be hidden from all other eyes,\nwhatever happens. That makes it holier and better. They respect one\nanother more, and much is built on respect. And if once there has been\nlove, if they have been married for love, why should love pass away?\nSurely one can keep it! It is rare that one cannot keep it. And if the\nhusband is kind and straightforward, why should not love last? The\nfirst phase of married love will pass, it is true, but then there will\ncome a love that is better still. Then there will be the union of\nsouls, they will have everything in common, there will be no secrets\nbetween them. And once they have children, the most difficult times\nwill seem to them happy, so long as there is love and courage. Even\ntoil will be a joy, you may deny yourself bread for your children and\neven that will be a joy, They will love you for it afterwards; so you\nare laying by for your future. As the children grow up you feel that\nyou are an example, a support for them; that even after you die your\nchildren will always keep your thoughts and feelings, because they have\nreceived them from you, they will take on your semblance and likeness.\nSo you see this is a great duty. How can it fail to draw the father and\nmother nearer? People say it’s a trial to have children. Who says that?\nIt is heavenly happiness! Are you fond of little children, Liza? I am\nawfully fond of them. You know—a little rosy baby boy at your bosom,\nand what husband’s heart is not touched, seeing his wife nursing his\nchild! A plump little rosy baby, sprawling and snuggling, chubby little\nhands and feet, clean tiny little nails, so tiny that it makes one\nlaugh to look at them; eyes that look as if they understand everything.\nAnd while it sucks it clutches at your bosom with its little hand,\nplays. When its father comes up, the child tears itself away from the\nbosom, flings itself back, looks at its father, laughs, as though it\nwere fearfully funny, and falls to sucking again. Or it will bite its\nmother’s breast when its little teeth are coming, while it looks\nsideways at her with its little eyes as though to say, ‘Look, I am\nbiting!’ Is not all that happiness when they are the three together,\nhusband, wife and child? One can forgive a great deal for the sake of\nsuch moments. Yes, Liza, one must first learn to live oneself before\none blames others!”\n\n“It’s by pictures, pictures like that one must get at you,” I thought\nto myself, though I did speak with real feeling, and all at once I\nflushed crimson. “What if she were suddenly to burst out laughing, what\nshould I do then?” That idea drove me to fury. Towards the end of my\nspeech I really was excited, and now my vanity was somehow wounded. The\nsilence continued. I almost nudged her.\n\n“Why are you—” she began and stopped. But I understood: there was a\nquiver of something different in her voice, not abrupt, harsh and\nunyielding as before, but something soft and shamefaced, so shamefaced\nthat I suddenly felt ashamed and guilty.\n\n“What?” I asked, with tender curiosity.\n\n“Why, you...”\n\n“What?”\n\n“Why, you ... speak somehow like a book,” she said, and again there was\na note of irony in her voice.\n\nThat remark sent a pang to my heart. It was not what I was expecting.\n\nI did not understand that she was hiding her feelings under irony, that\nthis is usually the last refuge of modest and chaste-souled people when\nthe privacy of their soul is coarsely and intrusively invaded, and that\ntheir pride makes them refuse to surrender till the last moment and\nshrink from giving expression to their feelings before you. I ought to\nhave guessed the truth from the timidity with which she had repeatedly\napproached her sarcasm, only bringing herself to utter it at last with\nan effort. But I did not guess, and an evil feeling took possession of\nme.\n\n“Wait a bit!” I thought.\n\n\n\n\nVII\n\n\n“Oh, hush, Liza! How can you talk about being like a book, when it\nmakes even me, an outsider, feel sick? Though I don’t look at it as an\noutsider, for, indeed, it touches me to the heart.... Is it possible,\nis it possible that you do not feel sick at being here yourself?\nEvidently habit does wonders! God knows what habit can do with anyone.\nCan you seriously think that you will never grow old, that you will\nalways be good-looking, and that they will keep you here for ever and\never? I say nothing of the loathsomeness of the life here.... Though\nlet me tell you this about it—about your present life, I mean; here\nthough you are young now, attractive, nice, with soul and feeling, yet\nyou know as soon as I came to myself just now I felt at once sick at\nbeing here with you! One can only come here when one is drunk. But if\nyou were anywhere else, living as good people live, I should perhaps be\nmore than attracted by you, should fall in love with you, should be\nglad of a look from you, let alone a word; I should hang about your\ndoor, should go down on my knees to you, should look upon you as my\nbetrothed and think it an honour to be allowed to. I should not dare to\nhave an impure thought about you. But here, you see, I know that I have\nonly to whistle and you have to come with me whether you like it or\nnot. I don’t consult your wishes, but you mine. The lowest labourer\nhires himself as a workman, but he doesn’t make a slave of himself\naltogether; besides, he knows that he will be free again presently. But\nwhen are you free? Only think what you are giving up here? What is it\nyou are making a slave of? It is your soul, together with your body;\nyou are selling your soul which you have no right to dispose of! You\ngive your love to be outraged by every drunkard! Love! But that’s\neverything, you know, it’s a priceless diamond, it’s a maiden’s\ntreasure, love—why, a man would be ready to give his soul, to face\ndeath to gain that love. But how much is your love worth now? You are\nsold, all of you, body and soul, and there is no need to strive for\nlove when you can have everything without love. And you know there is\nno greater insult to a girl than that, do you understand? To be sure, I\nhave heard that they comfort you, poor fools, they let you have lovers\nof your own here. But you know that’s simply a farce, that’s simply a\nsham, it’s just laughing at you, and you are taken in by it! Why, do\nyou suppose he really loves you, that lover of yours? I don’t believe\nit. How can he love you when he knows you may be called away from him\nany minute? He would be a low fellow if he did! Will he have a grain of\nrespect for you? What have you in common with him? He laughs at you and\nrobs you—that is all his love amounts to! You are lucky if he does not\nbeat you. Very likely he does beat you, too. Ask him, if you have got\none, whether he will marry you. He will laugh in your face, if he\ndoesn’t spit in it or give you a blow—though maybe he is not worth a\nbad halfpenny himself. And for what have you ruined your life, if you\ncome to think of it? For the coffee they give you to drink and the\nplentiful meals? But with what object are they feeding you up? An\nhonest girl couldn’t swallow the food, for she would know what she was\nbeing fed for. You are in debt here, and, of course, you will always be\nin debt, and you will go on in debt to the end, till the visitors here\nbegin to scorn you. And that will soon happen, don’t rely upon your\nyouth—all that flies by express train here, you know. You will be\nkicked out. And not simply kicked out; long before that she’ll begin\nnagging at you, scolding you, abusing you, as though you had not\nsacrificed your health for her, had not thrown away your youth and your\nsoul for her benefit, but as though you had ruined her, beggared her,\nrobbed her. And don’t expect anyone to take your part: the others, your\ncompanions, will attack you, too, win her favour, for all are in\nslavery here, and have lost all conscience and pity here long ago. They\nhave become utterly vile, and nothing on earth is viler, more\nloathsome, and more insulting than their abuse. And you are laying down\neverything here, unconditionally, youth and health and beauty and hope,\nand at twenty-two you will look like a woman of five-and-thirty, and\nyou will be lucky if you are not diseased, pray to God for that! No\ndoubt you are thinking now that you have a gay time and no work to do!\nYet there is no work harder or more dreadful in the world or ever has\nbeen. One would think that the heart alone would be worn out with\ntears. And you won’t dare to say a word, not half a word when they\ndrive you away from here; you will go away as though you were to blame.\nYou will change to another house, then to a third, then somewhere else,\ntill you come down at last to the Haymarket. There you will be beaten\nat every turn; that is good manners there, the visitors don’t know how\nto be friendly without beating you. You don’t believe that it is so\nhateful there? Go and look for yourself some time, you can see with\nyour own eyes. Once, one New Year’s Day, I saw a woman at a door. They\nhad turned her out as a joke, to give her a taste of the frost because\nshe had been crying so much, and they shut the door behind her. At nine\no’clock in the morning she was already quite drunk, dishevelled,\nhalf-naked, covered with bruises, her face was powdered, but she had a\nblack-eye, blood was trickling from her nose and her teeth; some cabman\nhad just given her a drubbing. She was sitting on the stone steps, a\nsalt fish of some sort was in her hand; she was crying, wailing\nsomething about her luck and beating with the fish on the steps, and\ncabmen and drunken soldiers were crowding in the doorway taunting her.\nYou don’t believe that you will ever be like that? I should be sorry to\nbelieve it, too, but how do you know; maybe ten years, eight years ago\nthat very woman with the salt fish came here fresh as a cherub,\ninnocent, pure, knowing no evil, blushing at every word. Perhaps she\nwas like you, proud, ready to take offence, not like the others;\nperhaps she looked like a queen, and knew what happiness was in store\nfor the man who should love her and whom she should love. Do you see\nhow it ended? And what if at that very minute when she was beating on\nthe filthy steps with that fish, drunken and dishevelled—what if at\nthat very minute she recalled the pure early days in her father’s\nhouse, when she used to go to school and the neighbour’s son watched\nfor her on the way, declaring that he would love her as long as he\nlived, that he would devote his life to her, and when they vowed to\nlove one another for ever and be married as soon as they were grown up!\nNo, Liza, it would be happy for you if you were to die soon of\nconsumption in some corner, in some cellar like that woman just now. In\nthe hospital, do you say? You will be lucky if they take you, but what\nif you are still of use to the madam here? Consumption is a queer\ndisease, it is not like fever. The patient goes on hoping till the last\nminute and says he is all right. He deludes himself And that just suits\nyour madam. Don’t doubt it, that’s how it is; you have sold your soul,\nand what is more you owe money, so you daren’t say a word. But when you\nare dying, all will abandon you, all will turn away from you, for then\nthere will be nothing to get from you. What’s more, they will reproach\nyou for cumbering the place, for being so long over dying. However you\nbeg you won’t get a drink of water without abuse: ‘Whenever are you\ngoing off, you nasty hussy, you won’t let us sleep with your moaning,\nyou make the gentlemen sick.’ That’s true, I have heard such things\nsaid myself. They will thrust you dying into the filthiest corner in\nthe cellar—in the damp and darkness; what will your thoughts be, lying\nthere alone? When you die, strange hands will lay you out, with\ngrumbling and impatience; no one will bless you, no one will sigh for\nyou, they only want to get rid of you as soon as may be; they will buy\na coffin, take you to the grave as they did that poor woman today, and\ncelebrate your memory at the tavern. In the grave, sleet, filth, wet\nsnow—no need to put themselves out for you—‘Let her down, Vanuha; it’s\njust like her luck—even here, she is head-foremost, the hussy. Shorten\nthe cord, you rascal.’ ‘It’s all right as it is.’ ‘All right, is it?\nWhy, she’s on her side! She was a fellow-creature, after all! But,\nnever mind, throw the earth on her.’ And they won’t care to waste much\ntime quarrelling over you. They will scatter the wet blue clay as quick\nas they can and go off to the tavern ... and there your memory on earth\nwill end; other women have children to go to their graves, fathers,\nhusbands. While for you neither tear, nor sigh, nor remembrance; no one\nin the whole world will ever come to you, your name will vanish from\nthe face of the earth—as though you had never existed, never been born\nat all! Nothing but filth and mud, however you knock at your coffin lid\nat night, when the dead arise, however you cry: ‘Let me out, kind\npeople, to live in the light of day! My life was no life at all; my\nlife has been thrown away like a dish-clout; it was drunk away in the\ntavern at the Haymarket; let me out, kind people, to live in the world\nagain.’”\n\nAnd I worked myself up to such a pitch that I began to have a lump in\nmy throat myself, and ... and all at once I stopped, sat up in dismay\nand, bending over apprehensively, began to listen with a beating heart.\nI had reason to be troubled.\n\nI had felt for some time that I was turning her soul upside down and\nrending her heart, and—and the more I was convinced of it, the more\neagerly I desired to gain my object as quickly and as effectually as\npossible. It was the exercise of my skill that carried me away; yet it\nwas not merely sport....\n\nI knew I was speaking stiffly, artificially, even bookishly, in fact, I\ncould not speak except “like a book.” But that did not trouble me: I\nknew, I felt that I should be understood and that this very bookishness\nmight be an assistance. But now, having attained my effect, I was\nsuddenly panic-stricken. Never before had I witnessed such despair! She\nwas lying on her face, thrusting her face into the pillow and clutching\nit in both hands. Her heart was being torn. Her youthful body was\nshuddering all over as though in convulsions. Suppressed sobs rent her\nbosom and suddenly burst out in weeping and wailing, then she pressed\ncloser into the pillow: she did not want anyone here, not a living\nsoul, to know of her anguish and her tears. She bit the pillow, bit her\nhand till it bled (I saw that afterwards), or, thrusting her fingers\ninto her dishevelled hair, seemed rigid with the effort of restraint,\nholding her breath and clenching her teeth. I began saying something,\nbegging her to calm herself, but felt that I did not dare; and all at\nonce, in a sort of cold shiver, almost in terror, began fumbling in the\ndark, trying hurriedly to get dressed to go. It was dark; though I\ntried my best I could not finish dressing quickly. Suddenly I felt a\nbox of matches and a candlestick with a whole candle in it. As soon as\nthe room was lighted up, Liza sprang up, sat up in bed, and with a\ncontorted face, with a half insane smile, looked at me almost\nsenselessly. I sat down beside her and took her hands; she came to\nherself, made an impulsive movement towards me, would have caught hold\nof me, but did not dare, and slowly bowed her head before me.\n\n“Liza, my dear, I was wrong ... forgive me, my dear,” I began, but she\nsqueezed my hand in her fingers so tightly that I felt I was saying the\nwrong thing and stopped.\n\n“This is my address, Liza, come to me.”\n\n“I will come,” she answered resolutely, her head still bowed.\n\n“But now I am going, good-bye ... till we meet again.”\n\nI got up; she, too, stood up and suddenly flushed all over, gave a\nshudder, snatched up a shawl that was lying on a chair and muffled\nherself in it to her chin. As she did this she gave another sickly\nsmile, blushed and looked at me strangely. I felt wretched; I was in\nhaste to get away—to disappear.\n\n“Wait a minute,” she said suddenly, in the passage just at the doorway,\nstopping me with her hand on my overcoat. She put down the candle in\nhot haste and ran off; evidently she had thought of something or wanted\nto show me something. As she ran away she flushed, her eyes shone, and\nthere was a smile on her lips—what was the meaning of it? Against my\nwill I waited: she came back a minute later with an expression that\nseemed to ask forgiveness for something. In fact, it was not the same\nface, not the same look as the evening before: sullen, mistrustful and\nobstinate. Her eyes now were imploring, soft, and at the same time\ntrustful, caressing, timid. The expression with which children look at\npeople they are very fond of, of whom they are asking a favour. Her\neyes were a light hazel, they were lovely eyes, full of life, and\ncapable of expressing love as well as sullen hatred.\n\nMaking no explanation, as though I, as a sort of higher being, must\nunderstand everything without explanations, she held out a piece of\npaper to me. Her whole face was positively beaming at that instant with\nnaive, almost childish, triumph. I unfolded it. It was a letter to her\nfrom a medical student or someone of that sort—a very high-flown and\nflowery, but extremely respectful, love-letter. I don’t recall the\nwords now, but I remember well that through the high-flown phrases\nthere was apparent a genuine feeling, which cannot be feigned. When I\nhad finished reading it I met her glowing, questioning, and childishly\nimpatient eyes fixed upon me. She fastened her eyes upon my face and\nwaited impatiently for what I should say. In a few words, hurriedly,\nbut with a sort of joy and pride, she explained to me that she had been\nto a dance somewhere in a private house, a family of “very nice people,\n_who knew nothing_, absolutely nothing, for she had only come here so\nlately and it had all happened ... and she hadn’t made up her mind to\nstay and was certainly going away as soon as she had paid her debt...”\nand at that party there had been the student who had danced with her\nall the evening. He had talked to her, and it turned out that he had\nknown her in old days at Riga when he was a child, they had played\ntogether, but a very long time ago—and he knew her parents, but _about\nthis_ he knew nothing, nothing whatever, and had no suspicion! And the\nday after the dance (three days ago) he had sent her that letter\nthrough the friend with whom she had gone to the party ... and ...\nwell, that was all.\n\nShe dropped her shining eyes with a sort of bashfulness as she\nfinished.\n\nThe poor girl was keeping that student’s letter as a precious treasure,\nand had run to fetch it, her only treasure, because she did not want me\nto go away without knowing that she, too, was honestly and genuinely\nloved; that she, too, was addressed respectfully. No doubt that letter\nwas destined to lie in her box and lead to nothing. But none the less,\nI am certain that she would keep it all her life as a precious\ntreasure, as her pride and justification, and now at such a minute she\nhad thought of that letter and brought it with naive pride to raise\nherself in my eyes that I might see, that I, too, might think well of\nher. I said nothing, pressed her hand and went out. I so longed to get\naway ... I walked all the way home, in spite of the fact that the\nmelting snow was still falling in heavy flakes. I was exhausted,\nshattered, in bewilderment. But behind the bewilderment the truth was\nalready gleaming. The loathsome truth.\n\n\n\n\nVIII\n\n\nIt was some time, however, before I consented to recognise that truth.\nWaking up in the morning after some hours of heavy, leaden sleep, and\nimmediately realising all that had happened on the previous day, I was\npositively amazed at my last night’s _sentimentality_ with Liza, at all\nthose “outcries of horror and pity.” “To think of having such an attack\nof womanish hysteria, pah!” I concluded. And what did I thrust my\naddress upon her for? What if she comes? Let her come, though; it\ndoesn’t matter.... But _obviously_, that was not now the chief and the\nmost important matter: I had to make haste and at all costs save my\nreputation in the eyes of Zverkov and Simonov as quickly as possible;\nthat was the chief business. And I was so taken up that morning that I\nactually forgot all about Liza.\n\nFirst of all I had at once to repay what I had borrowed the day before\nfrom Simonov. I resolved on a desperate measure: to borrow fifteen\nroubles straight off from Anton Antonitch. As luck would have it he was\nin the best of humours that morning, and gave it to me at once, on the\nfirst asking. I was so delighted at this that, as I signed the IOU with\na swaggering air, I told him casually that the night before “I had been\nkeeping it up with some friends at the Hôtel de Paris; we were giving a\nfarewell party to a comrade, in fact, I might say a friend of my\nchildhood, and you know—a desperate rake, fearfully spoilt—of course,\nhe belongs to a good family, and has considerable means, a brilliant\ncareer; he is witty, charming, a regular Lovelace, you understand; we\ndrank an extra ‘half-dozen’ and ...”\n\nAnd it went off all right; all this was uttered very easily,\nunconstrainedly and complacently.\n\nOn reaching home I promptly wrote to Simonov.\n\nTo this hour I am lost in admiration when I recall the truly\ngentlemanly, good-humoured, candid tone of my letter. With tact and\ngood-breeding, and, above all, entirely without superfluous words, I\nblamed myself for all that had happened. I defended myself, “if I\nreally may be allowed to defend myself,” by alleging that being utterly\nunaccustomed to wine, I had been intoxicated with the first glass,\nwhich I said, I had drunk before they arrived, while I was waiting for\nthem at the Hôtel de Paris between five and six o’clock. I begged\nSimonov’s pardon especially; I asked him to convey my explanations to\nall the others, especially to Zverkov, whom “I seemed to remember as\nthough in a dream” I had insulted. I added that I would have called\nupon all of them myself, but my head ached, and besides I had not the\nface to. I was particularly pleased with a certain lightness, almost\ncarelessness (strictly within the bounds of politeness, however), which\nwas apparent in my style, and better than any possible arguments, gave\nthem at once to understand that I took rather an independent view of\n“all that unpleasantness last night”; that I was by no means so utterly\ncrushed as you, my friends, probably imagine; but on the contrary,\nlooked upon it as a gentleman serenely respecting himself should look\nupon it. “On a young hero’s past no censure is cast!”\n\n“There is actually an aristocratic playfulness about it!” I thought\nadmiringly, as I read over the letter. “And it’s all because I am an\nintellectual and cultivated man! Another man in my place would not have\nknown how to extricate himself, but here I have got out of it and am as\njolly as ever again, and all because I am ‘a cultivated and educated\nman of our day.’ And, indeed, perhaps, everything was due to the wine\nyesterday. H’m!” ... No, it was not the wine. I did not drink anything\nat all between five and six when I was waiting for them. I had lied to\nSimonov; I had lied shamelessly; and indeed I wasn’t ashamed now....\nHang it all though, the great thing was that I was rid of it.\n\nI put six roubles in the letter, sealed it up, and asked Apollon to\ntake it to Simonov. When he learned that there was money in the letter,\nApollon became more respectful and agreed to take it. Towards evening I\nwent out for a walk. My head was still aching and giddy after\nyesterday. But as evening came on and the twilight grew denser, my\nimpressions and, following them, my thoughts, grew more and more\ndifferent and confused. Something was not dead within me, in the depths\nof my heart and conscience it would not die, and it showed itself in\nacute depression. For the most part I jostled my way through the most\ncrowded business streets, along Myeshtchansky Street, along Sadovy\nStreet and in Yusupov Garden. I always liked particularly sauntering\nalong these streets in the dusk, just when there were crowds of working\npeople of all sorts going home from their daily work, with faces\nlooking cross with anxiety. What I liked was just that cheap bustle,\nthat bare prose. On this occasion the jostling of the streets irritated\nme more than ever, I could not make out what was wrong with me, I could\nnot find the clue, something seemed rising up continually in my soul,\npainfully, and refusing to be appeased. I returned home completely\nupset, it was just as though some crime were lying on my conscience.\n\nThe thought that Liza was coming worried me continually. It seemed\nqueer to me that of all my recollections of yesterday this tormented\nme, as it were, especially, as it were, quite separately. Everything\nelse I had quite succeeded in forgetting by the evening; I dismissed it\nall and was still perfectly satisfied with my letter to Simonov. But on\nthis point I was not satisfied at all. It was as though I were worried\nonly by Liza. “What if she comes,” I thought incessantly, “well, it\ndoesn’t matter, let her come! H’m! it’s horrid that she should see, for\ninstance, how I live. Yesterday I seemed such a hero to her, while now,\nh’m! It’s horrid, though, that I have let myself go so, the room looks\nlike a beggar’s. And I brought myself to go out to dinner in such a\nsuit! And my American leather sofa with the stuffing sticking out. And\nmy dressing-gown, which will not cover me, such tatters, and she will\nsee all this and she will see Apollon. That beast is certain to insult\nher. He will fasten upon her in order to be rude to me. And I, of\ncourse, shall be panic-stricken as usual, I shall begin bowing and\nscraping before her and pulling my dressing-gown round me, I shall\nbegin smiling, telling lies. Oh, the beastliness! And it isn’t the\nbeastliness of it that matters most! There is something more important,\nmore loathsome, viler! Yes, viler! And to put on that dishonest lying\nmask again! ...”\n\nWhen I reached that thought I fired up all at once.\n\n“Why dishonest? How dishonest? I was speaking sincerely last night. I\nremember there was real feeling in me, too. What I wanted was to excite\nan honourable feeling in her.... Her crying was a good thing, it will\nhave a good effect.”\n\nYet I could not feel at ease. All that evening, even when I had come\nback home, even after nine o’clock, when I calculated that Liza could\nnot possibly come, still she haunted me, and what was worse, she came\nback to my mind always in the same position. One moment out of all that\nhad happened last night stood vividly before my imagination; the moment\nwhen I struck a match and saw her pale, distorted face, with its look\nof torture. And what a pitiful, what an unnatural, what a distorted\nsmile she had at that moment! But I did not know then, that fifteen\nyears later I should still in my imagination see Liza, always with the\npitiful, distorted, inappropriate smile which was on her face at that\nminute.\n\nNext day I was ready again to look upon it all as nonsense, due to\nover-excited nerves, and, above all, as _exaggerated_. I was always\nconscious of that weak point of mine, and sometimes very much afraid of\nit. “I exaggerate everything, that is where I go wrong,” I repeated to\nmyself every hour. But, however, “Liza will very likely come all the\nsame,” was the refrain with which all my reflections ended. I was so\nuneasy that I sometimes flew into a fury: “She’ll come, she is certain\nto come!” I cried, running about the room, “if not today, she will come\ntomorrow; she’ll find me out! The damnable romanticism of these pure\nhearts! Oh, the vileness—oh, the silliness—oh, the stupidity of these\n‘wretched sentimental souls!’ Why, how fail to understand? How could\none fail to understand? ...”\n\nBut at this point I stopped short, and in great confusion, indeed.\n\nAnd how few, how few words, I thought, in passing, were needed; how\nlittle of the idyllic (and affectedly, bookishly, artificially idyllic\ntoo) had sufficed to turn a whole human life at once according to my\nwill. That’s virginity, to be sure! Freshness of soil!\n\nAt times a thought occurred to me, to go to her, “to tell her all,” and\nbeg her not to come to me. But this thought stirred such wrath in me\nthat I believed I should have crushed that “damned” Liza if she had\nchanced to be near me at the time. I should have insulted her, have\nspat at her, have turned her out, have struck her!\n\nOne day passed, however, another and another; she did not come and I\nbegan to grow calmer. I felt particularly bold and cheerful after nine\no’clock, I even sometimes began dreaming, and rather sweetly: I, for\ninstance, became the salvation of Liza, simply through her coming to me\nand my talking to her.... I develop her, educate her. Finally, I notice\nthat she loves me, loves me passionately. I pretend not to understand\n(I don’t know, however, why I pretend, just for effect, perhaps). At\nlast all confusion, transfigured, trembling and sobbing, she flings\nherself at my feet and says that I am her saviour, and that she loves\nme better than anything in the world. I am amazed, but.... “Liza,” I\nsay, “can you imagine that I have not noticed your love? I saw it all,\nI divined it, but I did not dare to approach you first, because I had\nan influence over you and was afraid that you would force yourself,\nfrom gratitude, to respond to my love, would try to rouse in your heart\na feeling which was perhaps absent, and I did not wish that ... because\nit would be tyranny ... it would be indelicate (in short, I launch off\nat that point into European, inexplicably lofty subtleties a la George\nSand), but now, now you are mine, you are my creation, you are pure,\nyou are good, you are my noble wife.\n\n‘Into my house come bold and free,\nIts rightful mistress there to be’.”\n\n\nThen we begin living together, go abroad and so on, and so on. In fact,\nin the end it seemed vulgar to me myself, and I began putting out my\ntongue at myself.\n\nBesides, they won’t let her out, “the hussy!” I thought. They don’t let\nthem go out very readily, especially in the evening (for some reason I\nfancied she would come in the evening, and at seven o’clock precisely).\nThough she did say she was not altogether a slave there yet, and had\ncertain rights; so, h’m! Damn it all, she will come, she is sure to\ncome!\n\nIt was a good thing, in fact, that Apollon distracted my attention at\nthat time by his rudeness. He drove me beyond all patience! He was the\nbane of my life, the curse laid upon me by Providence. We had been\nsquabbling continually for years, and I hated him. My God, how I hated\nhim! I believe I had never hated anyone in my life as I hated him,\nespecially at some moments. He was an elderly, dignified man, who\nworked part of his time as a tailor. But for some unknown reason he\ndespised me beyond all measure, and looked down upon me insufferably.\nThough, indeed, he looked down upon everyone. Simply to glance at that\nflaxen, smoothly brushed head, at the tuft of hair he combed up on his\nforehead and oiled with sunflower oil, at that dignified mouth,\ncompressed into the shape of the letter V, made one feel one was\nconfronting a man who never doubted of himself. He was a pedant, to the\nmost extreme point, the greatest pedant I had met on earth, and with\nthat had a vanity only befitting Alexander of Macedon. He was in love\nwith every button on his coat, every nail on his fingers—absolutely in\nlove with them, and he looked it! In his behaviour to me he was a\nperfect tyrant, he spoke very little to me, and if he chanced to glance\nat me he gave me a firm, majestically self-confident and invariably\nironical look that drove me sometimes to fury. He did his work with the\nair of doing me the greatest favour, though he did scarcely anything\nfor me, and did not, indeed, consider himself bound to do anything.\nThere could be no doubt that he looked upon me as the greatest fool on\nearth, and that “he did not get rid of me” was simply that he could get\nwages from me every month. He consented to do nothing for me for seven\nroubles a month. Many sins should be forgiven me for what I suffered\nfrom him. My hatred reached such a point that sometimes his very step\nalmost threw me into convulsions. What I loathed particularly was his\nlisp. His tongue must have been a little too long or something of that\nsort, for he continually lisped, and seemed to be very proud of it,\nimagining that it greatly added to his dignity. He spoke in a slow,\nmeasured tone, with his hands behind his back and his eyes fixed on the\nground. He maddened me particularly when he read aloud the psalms to\nhimself behind his partition. Many a battle I waged over that reading!\nBut he was awfully fond of reading aloud in the evenings, in a slow,\neven, sing-song voice, as though over the dead. It is interesting that\nthat is how he has ended: he hires himself out to read the psalms over\nthe dead, and at the same time he kills rats and makes blacking. But at\nthat time I could not get rid of him, it was as though he were\nchemically combined with my existence. Besides, nothing would have\ninduced him to consent to leave me. I could not live in furnished\nlodgings: my lodging was my private solitude, my shell, my cave, in\nwhich I concealed myself from all mankind, and Apollon seemed to me,\nfor some reason, an integral part of that flat, and for seven years I\ncould not turn him away.\n\nTo be two or three days behind with his wages, for instance, was\nimpossible. He would have made such a fuss, I should not have known\nwhere to hide my head. But I was so exasperated with everyone during\nthose days, that I made up my mind for some reason and with some object\nto _punish_ Apollon and not to pay him for a fortnight the wages that\nwere owing him. I had for a long time—for the last two years—been\nintending to do this, simply in order to teach him not to give himself\nairs with me, and to show him that if I liked I could withhold his\nwages. I purposed to say nothing to him about it, and was purposely\nsilent indeed, in order to score off his pride and force him to be the\nfirst to speak of his wages. Then I would take the seven roubles out of\na drawer, show him I have the money put aside on purpose, but that I\nwon’t, I won’t, I simply won’t pay him his wages, I won’t just because\nthat is “what I wish,” because “I am master, and it is for me to\ndecide,” because he has been disrespectful, because he has been rude;\nbut if he were to ask respectfully I might be softened and give it to\nhim, otherwise he might wait another fortnight, another three weeks, a\nwhole month....\n\nBut angry as I was, yet he got the better of me. I could not hold out\nfor four days. He began as he always did begin in such cases, for there\nhad been such cases already, there had been attempts (and it may be\nobserved I knew all this beforehand, I knew his nasty tactics by\nheart). He would begin by fixing upon me an exceedingly severe stare,\nkeeping it up for several minutes at a time, particularly on meeting me\nor seeing me out of the house. If I held out and pretended not to\nnotice these stares, he would, still in silence, proceed to further\ntortures. All at once, _à propos_ of nothing, he would walk softly and\nsmoothly into my room, when I was pacing up and down or reading, stand\nat the door, one hand behind his back and one foot behind the other,\nand fix upon me a stare more than severe, utterly contemptuous. If I\nsuddenly asked him what he wanted, he would make me no answer, but\ncontinue staring at me persistently for some seconds, then, with a\npeculiar compression of his lips and a most significant air,\ndeliberately turn round and deliberately go back to his room. Two hours\nlater he would come out again and again present himself before me in\nthe same way. It had happened that in my fury I did not even ask him\nwhat he wanted, but simply raised my head sharply and imperiously and\nbegan staring back at him. So we stared at one another for two minutes;\nat last he turned with deliberation and dignity and went back again for\ntwo hours.\n\nIf I were still not brought to reason by all this, but persisted in my\nrevolt, he would suddenly begin sighing while he looked at me, long,\ndeep sighs as though measuring by them the depths of my moral\ndegradation, and, of course, it ended at last by his triumphing\ncompletely: I raged and shouted, but still was forced to do what he\nwanted.\n\nThis time the usual staring manoeuvres had scarcely begun when I lost\nmy temper and flew at him in a fury. I was irritated beyond endurance\napart from him.\n\n“Stay,” I cried, in a frenzy, as he was slowly and silently turning,\nwith one hand behind his back, to go to his room. “Stay! Come back,\ncome back, I tell you!” and I must have bawled so unnaturally, that he\nturned round and even looked at me with some wonder. However, he\npersisted in saying nothing, and that infuriated me.\n\n“How dare you come and look at me like that without being sent for?\nAnswer!”\n\nAfter looking at me calmly for half a minute, he began turning round\nagain.\n\n“Stay!” I roared, running up to him, “don’t stir! There. Answer, now:\nwhat did you come in to look at?”\n\n“If you have any order to give me it’s my duty to carry it out,” he\nanswered, after another silent pause, with a slow, measured lisp,\nraising his eyebrows and calmly twisting his head from one side to\nanother, all this with exasperating composure.\n\n“That’s not what I am asking you about, you torturer!” I shouted,\nturning crimson with anger. “I’ll tell you why you came here myself:\nyou see, I don’t give you your wages, you are so proud you don’t want\nto bow down and ask for it, and so you come to punish me with your\nstupid stares, to worry me and you have no sus-pic-ion how stupid it\nis—stupid, stupid, stupid, stupid! ...”\n\nHe would have turned round again without a word, but I seized him.\n\n“Listen,” I shouted to him. “Here’s the money, do you see, here it is,”\n(I took it out of the table drawer); “here’s the seven roubles\ncomplete, but you are not going to have it, you ... are ... not ...\ngoing ... to ... have it until you come respectfully with bowed head to\nbeg my pardon. Do you hear?”\n\n“That cannot be,” he answered, with the most unnatural self-confidence.\n\n“It shall be so,” I said, “I give you my word of honour, it shall be!”\n\n“And there’s nothing for me to beg your pardon for,” he went on, as\nthough he had not noticed my exclamations at all. “Why, besides, you\ncalled me a ‘torturer,’ for which I can summon you at the\npolice-station at any time for insulting behaviour.”\n\n“Go, summon me,” I roared, “go at once, this very minute, this very\nsecond! You are a torturer all the same! a torturer!”\n\nBut he merely looked at me, then turned, and regardless of my loud\ncalls to him, he walked to his room with an even step and without\nlooking round.\n\n“If it had not been for Liza nothing of this would have happened,” I\ndecided inwardly. Then, after waiting a minute, I went myself behind\nhis screen with a dignified and solemn air, though my heart was beating\nslowly and violently.\n\n“Apollon,” I said quietly and emphatically, though I was breathless,\n“go at once without a minute’s delay and fetch the police-officer.”\n\nHe had meanwhile settled himself at his table, put on his spectacles\nand taken up some sewing. But, hearing my order, he burst into a\nguffaw.\n\n“At once, go this minute! Go on, or else you can’t imagine what will\nhappen.”\n\n“You are certainly out of your mind,” he observed, without even raising\nhis head, lisping as deliberately as ever and threading his needle.\n“Whoever heard of a man sending for the police against himself? And as\nfor being frightened—you are upsetting yourself about nothing, for\nnothing will come of it.”\n\n“Go!” I shrieked, clutching him by the shoulder. I felt I should strike\nhim in a minute.\n\nBut I did not notice the door from the passage softly and slowly open\nat that instant and a figure come in, stop short, and begin staring at\nus in perplexity I glanced, nearly swooned with shame, and rushed back\nto my room. There, clutching at my hair with both hands, I leaned my\nhead against the wall and stood motionless in that position.\n\nTwo minutes later I heard Apollon’s deliberate footsteps. “There is\nsome woman asking for you,” he said, looking at me with peculiar\nseverity. Then he stood aside and let in Liza. He would not go away,\nbut stared at us sarcastically.\n\n“Go away, go away,” I commanded in desperation. At that moment my clock\nbegan whirring and wheezing and struck seven.\n\n\n\n\nIX\n\n\n“Into my house come bold and free,\nIts rightful mistress there to be.”\n\n\nI stood before her crushed, crestfallen, revoltingly confused, and I\nbelieve I smiled as I did my utmost to wrap myself in the skirts of my\nragged wadded dressing-gown—exactly as I had imagined the scene not\nlong before in a fit of depression. After standing over us for a couple\nof minutes Apollon went away, but that did not make me more at ease.\nWhat made it worse was that she, too, was overwhelmed with confusion,\nmore so, in fact, than I should have expected. At the sight of me, of\ncourse.\n\n“Sit down,” I said mechanically, moving a chair up to the table, and I\nsat down on the sofa. She obediently sat down at once and gazed at me\nopen-eyed, evidently expecting something from me at once. This naïveté\nof expectation drove me to fury, but I restrained myself.\n\nShe ought to have tried not to notice, as though everything had been as\nusual, while instead of that, she ... and I dimly felt that I should\nmake her pay dearly for _all this_.\n\n“You have found me in a strange position, Liza,” I began, stammering\nand knowing that this was the wrong way to begin. “No, no, don’t\nimagine anything,” I cried, seeing that she had suddenly flushed. “I am\nnot ashamed of my poverty.... On the contrary, I look with pride on my\npoverty. I am poor but honourable.... One can be poor and honourable,”\nI muttered. “However ... would you like tea?....”\n\n“No,” she was beginning.\n\n“Wait a minute.”\n\nI leapt up and ran to Apollon. I had to get out of the room somehow.\n\n“Apollon,” I whispered in feverish haste, flinging down before him the\nseven roubles which had remained all the time in my clenched fist,\n“here are your wages, you see I give them to you; but for that you must\ncome to my rescue: bring me tea and a dozen rusks from the restaurant.\nIf you won’t go, you’ll make me a miserable man! You don’t know what\nthis woman is.... This is—everything! You may be imagining\nsomething.... But you don’t know what that woman is! ...”\n\nApollon, who had already sat down to his work and put on his spectacles\nagain, at first glanced askance at the money without speaking or\nputting down his needle; then, without paying the slightest attention\nto me or making any answer, he went on busying himself with his needle,\nwhich he had not yet threaded. I waited before him for three minutes\nwith my arms crossed _à la Napoléon_. My temples were moist with sweat.\nI was pale, I felt it. But, thank God, he must have been moved to pity,\nlooking at me. Having threaded his needle he deliberately got up from\nhis seat, deliberately moved back his chair, deliberately took off his\nspectacles, deliberately counted the money, and finally asking me over\nhis shoulder: “Shall I get a whole portion?” deliberately walked out of\nthe room. As I was going back to Liza, the thought occurred to me on\nthe way: shouldn’t I run away just as I was in my dressing-gown, no\nmatter where, and then let happen what would?\n\nI sat down again. She looked at me uneasily. For some minutes we were\nsilent.\n\n“I will kill him,” I shouted suddenly, striking the table with my fist\nso that the ink spurted out of the inkstand.\n\n“What are you saying!” she cried, starting.\n\n“I will kill him! kill him!” I shrieked, suddenly striking the table in\nabsolute frenzy, and at the same time fully understanding how stupid it\nwas to be in such a frenzy. “You don’t know, Liza, what that torturer\nis to me. He is my torturer.... He has gone now to fetch some rusks; he\n...”\n\nAnd suddenly I burst into tears. It was an hysterical attack. How\nashamed I felt in the midst of my sobs; but still I could not restrain\nthem.\n\nShe was frightened.\n\n“What is the matter? What is wrong?” she cried, fussing about me.\n\n“Water, give me water, over there!” I muttered in a faint voice, though\nI was inwardly conscious that I could have got on very well without\nwater and without muttering in a faint voice. But I was, what is\ncalled, _putting it on_, to save appearances, though the attack was a\ngenuine one.\n\nShe gave me water, looking at me in bewilderment. At that moment\nApollon brought in the tea. It suddenly seemed to me that this\ncommonplace, prosaic tea was horribly undignified and paltry after all\nthat had happened, and I blushed crimson. Liza looked at Apollon with\npositive alarm. He went out without a glance at either of us.\n\n“Liza, do you despise me?” I asked, looking at her fixedly, trembling\nwith impatience to know what she was thinking.\n\nShe was confused, and did not know what to answer.\n\n“Drink your tea,” I said to her angrily. I was angry with myself, but,\nof course, it was she who would have to pay for it. A horrible spite\nagainst her suddenly surged up in my heart; I believe I could have\nkilled her. To revenge myself on her I swore inwardly not to say a word\nto her all the time. “She is the cause of it all,” I thought.\n\nOur silence lasted for five minutes. The tea stood on the table; we did\nnot touch it. I had got to the point of purposely refraining from\nbeginning in order to embarrass her further; it was awkward for her to\nbegin alone. Several times she glanced at me with mournful perplexity.\nI was obstinately silent. I was, of course, myself the chief sufferer,\nbecause I was fully conscious of the disgusting meanness of my spiteful\nstupidity, and yet at the same time I could not restrain myself.\n\n“I want to... get away ... from there altogether,” she began, to break\nthe silence in some way, but, poor girl, that was just what she ought\nnot to have spoken about at such a stupid moment to a man so stupid as\nI was. My heart positively ached with pity for her tactless and\nunnecessary straightforwardness. But something hideous at once stifled\nall compassion in me; it even provoked me to greater venom. I did not\ncare what happened. Another five minutes passed.\n\n“Perhaps I am in your way,” she began timidly, hardly audibly, and was\ngetting up.\n\nBut as soon as I saw this first impulse of wounded dignity I positively\ntrembled with spite, and at once burst out.\n\n“Why have you come to me, tell me that, please?” I began, gasping for\nbreath and regardless of logical connection in my words. I longed to\nhave it all out at once, at one burst; I did not even trouble how to\nbegin. “Why have you come? Answer, answer,” I cried, hardly knowing\nwhat I was doing. “I’ll tell you, my good girl, why you have come.\nYou’ve come because I talked sentimental stuff to you then. So now you\nare soft as butter and longing for fine sentiments again. So you may as\nwell know that I was laughing at you then. And I am laughing at you\nnow. Why are you shuddering? Yes, I was laughing at you! I had been\ninsulted just before, at dinner, by the fellows who came that evening\nbefore me. I came to you, meaning to thrash one of them, an officer;\nbut I didn’t succeed, I didn’t find him; I had to avenge the insult on\nsomeone to get back my own again; you turned up, I vented my spleen on\nyou and laughed at you. I had been humiliated, so I wanted to\nhumiliate; I had been treated like a rag, so I wanted to show my\npower.... That’s what it was, and you imagined I had come there on\npurpose to save you. Yes? You imagined that? You imagined that?”\n\nI knew that she would perhaps be muddled and not take it all in\nexactly, but I knew, too, that she would grasp the gist of it, very\nwell indeed. And so, indeed, she did. She turned white as a\nhandkerchief, tried to say something, and her lips worked painfully;\nbut she sank on a chair as though she had been felled by an axe. And\nall the time afterwards she listened to me with her lips parted and her\neyes wide open, shuddering with awful terror. The cynicism, the\ncynicism of my words overwhelmed her....\n\n“Save you!” I went on, jumping up from my chair and running up and down\nthe room before her. “Save you from what? But perhaps I am worse than\nyou myself. Why didn’t you throw it in my teeth when I was giving you\nthat sermon: ‘But what did you come here yourself for? was it to read\nus a sermon?’ Power, power was what I wanted then, sport was what I\nwanted, I wanted to wring out your tears, your humiliation, your\nhysteria—that was what I wanted then! Of course, I couldn’t keep it up\nthen, because I am a wretched creature, I was frightened, and, the\ndevil knows why, gave you my address in my folly. Afterwards, before I\ngot home, I was cursing and swearing at you because of that address, I\nhated you already because of the lies I had told you. Because I only\nlike playing with words, only dreaming, but, do you know, what I really\nwant is that you should all go to hell. That is what I want. I want\npeace; yes, I’d sell the whole world for a farthing, straight off, so\nlong as I was left in peace. Is the world to go to pot, or am I to go\nwithout my tea? I say that the world may go to pot for me so long as I\nalways get my tea. Did you know that, or not? Well, anyway, I know that\nI am a blackguard, a scoundrel, an egoist, a sluggard. Here I have been\nshuddering for the last three days at the thought of your coming. And\ndo you know what has worried me particularly for these three days? That\nI posed as such a hero to you, and now you would see me in a wretched\ntorn dressing-gown, beggarly, loathsome. I told you just now that I was\nnot ashamed of my poverty; so you may as well know that I am ashamed of\nit; I am more ashamed of it than of anything, more afraid of it than of\nbeing found out if I were a thief, because I am as vain as though I had\nbeen skinned and the very air blowing on me hurt. Surely by now you\nmust realise that I shall never forgive you for having found me in this\nwretched dressing-gown, just as I was flying at Apollon like a spiteful\ncur. The saviour, the former hero, was flying like a mangy, unkempt\nsheep-dog at his lackey, and the lackey was jeering at him! And I shall\nnever forgive you for the tears I could not help shedding before you\njust now, like some silly woman put to shame! And for what I am\nconfessing to you now, I shall never forgive you either! Yes—you must\nanswer for it all because you turned up like this, because I am a\nblackguard, because I am the nastiest, stupidest, absurdest and most\nenvious of all the worms on earth, who are not a bit better than I am,\nbut, the devil knows why, are never put to confusion; while I shall\nalways be insulted by every louse, that is my doom! And what is it to\nme that you don’t understand a word of this! And what do I care, what\ndo I care about you, and whether you go to ruin there or not? Do you\nunderstand? How I shall hate you now after saying this, for having been\nhere and listening. Why, it’s not once in a lifetime a man speaks out\nlike this, and then it is in hysterics! ... What more do you want? Why\ndo you still stand confronting me, after all this? Why are you worrying\nme? Why don’t you go?”\n\nBut at this point a strange thing happened. I was so accustomed to\nthink and imagine everything from books, and to picture everything in\nthe world to myself just as I had made it up in my dreams beforehand,\nthat I could not all at once take in this strange circumstance. What\nhappened was this: Liza, insulted and crushed by me, understood a great\ndeal more than I imagined. She understood from all this what a woman\nunderstands first of all, if she feels genuine love, that is, that I\nwas myself unhappy.\n\nThe frightened and wounded expression on her face was followed first by\na look of sorrowful perplexity. When I began calling myself a scoundrel\nand a blackguard and my tears flowed (the tirade was accompanied\nthroughout by tears) her whole face worked convulsively. She was on the\npoint of getting up and stopping me; when I finished she took no notice\nof my shouting: “Why are you here, why don’t you go away?” but realised\nonly that it must have been very bitter to me to say all this. Besides,\nshe was so crushed, poor girl; she considered herself infinitely\nbeneath me; how could she feel anger or resentment? She suddenly leapt\nup from her chair with an irresistible impulse and held out her hands,\nyearning towards me, though still timid and not daring to stir.... At\nthis point there was a revulsion in my heart too. Then she suddenly\nrushed to me, threw her arms round me and burst into tears. I, too,\ncould not restrain myself, and sobbed as I never had before.\n\n“They won’t let me ... I can’t be good!” I managed to articulate; then\nI went to the sofa, fell on it face downwards, and sobbed on it for a\nquarter of an hour in genuine hysterics. She came close to me, put her\narms round me and stayed motionless in that position. But the trouble\nwas that the hysterics could not go on for ever, and (I am writing the\nloathsome truth) lying face downwards on the sofa with my face thrust\ninto my nasty leather pillow, I began by degrees to be aware of a\nfar-away, involuntary but irresistible feeling that it would be awkward\nnow for me to raise my head and look Liza straight in the face. Why was\nI ashamed? I don’t know, but I was ashamed. The thought, too, came into\nmy overwrought brain that our parts now were completely changed, that\nshe was now the heroine, while I was just a crushed and humiliated\ncreature as she had been before me that night—four days before.... And\nall this came into my mind during the minutes I was lying on my face on\nthe sofa.\n\nMy God! surely I was not envious of her then.\n\nI don’t know, to this day I cannot decide, and at the time, of course,\nI was still less able to understand what I was feeling than now. I\ncannot get on without domineering and tyrannising over someone, but ...\nthere is no explaining anything by reasoning and so it is useless to\nreason.\n\nI conquered myself, however, and raised my head; I had to do so sooner\nor later ... and I am convinced to this day that it was just because I\nwas ashamed to look at her that another feeling was suddenly kindled\nand flamed up in my heart ... a feeling of mastery and possession. My\neyes gleamed with passion, and I gripped her hands tightly. How I hated\nher and how I was drawn to her at that minute! The one feeling\nintensified the other. It was almost like an act of vengeance. At first\nthere was a look of amazement, even of terror on her face, but only for\none instant. She warmly and rapturously embraced me.\n\n\n\n\nX\n\n\nA quarter of an hour later I was rushing up and down the room in\nfrenzied impatience, from minute to minute I went up to the screen and\npeeped through the crack at Liza. She was sitting on the ground with\nher head leaning against the bed, and must have been crying. But she\ndid not go away, and that irritated me. This time she understood it\nall. I had insulted her finally, but ... there’s no need to describe\nit. She realised that my outburst of passion had been simply revenge, a\nfresh humiliation, and that to my earlier, almost causeless hatred was\nadded now a _personal hatred_, born of envy.... Though I do not\nmaintain positively that she understood all this distinctly; but she\ncertainly did fully understand that I was a despicable man, and what\nwas worse, incapable of loving her.\n\nI know I shall be told that this is incredible—but it is incredible to\nbe as spiteful and stupid as I was; it may be added that it was strange\nI should not love her, or at any rate, appreciate her love. Why is it\nstrange? In the first place, by then I was incapable of love, for I\nrepeat, with me loving meant tyrannising and showing my moral\nsuperiority. I have never in my life been able to imagine any other\nsort of love, and have nowadays come to the point of sometimes thinking\nthat love really consists in the right—freely given by the beloved\nobject—to tyrannise over her.\n\nEven in my underground dreams I did not imagine love except as a\nstruggle. I began it always with hatred and ended it with moral\nsubjugation, and afterwards I never knew what to do with the subjugated\nobject. And what is there to wonder at in that, since I had succeeded\nin so corrupting myself, since I was so out of touch with “real life,”\nas to have actually thought of reproaching her, and putting her to\nshame for having come to me to hear “fine sentiments”; and did not even\nguess that she had come not to hear fine sentiments, but to love me,\nbecause to a woman all reformation, all salvation from any sort of\nruin, and all moral renewal is included in love and can only show\nitself in that form.\n\nI did not hate her so much, however, when I was running about the room\nand peeping through the crack in the screen. I was only insufferably\noppressed by her being here. I wanted her to disappear. I wanted\n“peace,” to be left alone in my underground world. Real life oppressed\nme with its novelty so much that I could hardly breathe.\n\nBut several minutes passed and she still remained, without stirring, as\nthough she were unconscious. I had the shamelessness to tap softly at\nthe screen as though to remind her.... She started, sprang up, and flew\nto seek her kerchief, her hat, her coat, as though making her escape\nfrom me.... Two minutes later she came from behind the screen and\nlooked with heavy eyes at me. I gave a spiteful grin, which was forced,\nhowever, to _keep up appearances_, and I turned away from her eyes.\n\n“Good-bye,” she said, going towards the door.\n\nI ran up to her, seized her hand, opened it, thrust something in it and\nclosed it again. Then I turned at once and dashed away in haste to the\nother corner of the room to avoid seeing, anyway....\n\nI did mean a moment since to tell a lie—to write that I did this\naccidentally, not knowing what I was doing through foolishness, through\nlosing my head. But I don’t want to lie, and so I will say straight out\nthat I opened her hand and put the money in it ... from spite. It came\ninto my head to do this while I was running up and down the room and\nshe was sitting behind the screen. But this I can say for certain:\nthough I did that cruel thing purposely, it was not an impulse from the\nheart, but came from my evil brain. This cruelty was so affected, so\npurposely made up, so completely a product of the brain, of books, that\nI could not even keep it up a minute—first I dashed away to avoid\nseeing her, and then in shame and despair rushed after Liza. I opened\nthe door in the passage and began listening.\n\n“Liza! Liza!” I cried on the stairs, but in a low voice, not boldly.\nThere was no answer, but I fancied I heard her footsteps, lower down on\nthe stairs.\n\n“Liza!” I cried, more loudly.\n\nNo answer. But at that minute I heard the stiff outer glass door open\nheavily with a creak and slam violently; the sound echoed up the\nstairs.\n\nShe had gone. I went back to my room in hesitation. I felt horribly\noppressed.\n\nI stood still at the table, beside the chair on which she had sat and\nlooked aimlessly before me. A minute passed, suddenly I started;\nstraight before me on the table I saw.... In short, I saw a crumpled\nblue five-rouble note, the one I had thrust into her hand a minute\nbefore. It was the same note; it could be no other, there was no other\nin the flat. So she had managed to fling it from her hand on the table\nat the moment when I had dashed into the further corner.\n\nWell! I might have expected that she would do that. Might I have\nexpected it? No, I was such an egoist, I was so lacking in respect for\nmy fellow-creatures that I could not even imagine she would do so. I\ncould not endure it. A minute later I flew like a madman to dress,\nflinging on what I could at random and ran headlong after her. She\ncould not have got two hundred paces away when I ran out into the\nstreet.\n\nIt was a still night and the snow was coming down in masses and falling\nalmost perpendicularly, covering the pavement and the empty street as\nthough with a pillow. There was no one in the street, no sound was to\nbe heard. The street lamps gave a disconsolate and useless glimmer. I\nran two hundred paces to the cross-roads and stopped short.\n\nWhere had she gone? And why was I running after her?\n\nWhy? To fall down before her, to sob with remorse, to kiss her feet, to\nentreat her forgiveness! I longed for that, my whole breast was being\nrent to pieces, and never, never shall I recall that minute with\nindifference. But—what for? I thought. Should I not begin to hate her,\nperhaps, even tomorrow, just because I had kissed her feet today?\nShould I give her happiness? Had I not recognised that day, for the\nhundredth time, what I was worth? Should I not torture her?\n\nI stood in the snow, gazing into the troubled darkness and pondered\nthis.\n\n“And will it not be better?” I mused fantastically, afterwards at home,\nstifling the living pang of my heart with fantastic dreams. “Will it\nnot be better that she should keep the resentment of the insult for\never? Resentment—why, it is purification; it is a most stinging and\npainful consciousness! Tomorrow I should have defiled her soul and have\nexhausted her heart, while now the feeling of insult will never die in\nher heart, and however loathsome the filth awaiting her—the feeling of\ninsult will elevate and purify her ... by hatred ... h’m! ... perhaps,\ntoo, by forgiveness.... Will all that make things easier for her\nthough? ...”\n\nAnd, indeed, I will ask on my own account here, an idle question: which\nis better—cheap happiness or exalted sufferings? Well, which is better?\n\nSo I dreamed as I sat at home that evening, almost dead with the pain\nin my soul. Never had I endured such suffering and remorse, yet could\nthere have been the faintest doubt when I ran out from my lodging that\nI should turn back half-way? I never met Liza again and I have heard\nnothing of her. I will add, too, that I remained for a long time\nafterwards pleased with the phrase about the benefit from resentment\nand hatred in spite of the fact that I almost fell ill from misery.\n\n\nEven now, so many years later, all this is somehow a very evil memory.\nI have many evil memories now, but ... hadn’t I better end my “Notes”\nhere? I believe I made a mistake in beginning to write them, anyway I\nhave felt ashamed all the time I’ve been writing this story; so it’s\nhardly literature so much as a corrective punishment. Why, to tell long\nstories, showing how I have spoiled my life through morally rotting in\nmy corner, through lack of fitting environment, through divorce from\nreal life, and rankling spite in my underground world, would certainly\nnot be interesting; a novel needs a hero, and all the traits for an\nanti-hero are _expressly_ gathered together here, and what matters\nmost, it all produces an unpleasant impression, for we are all divorced\nfrom life, we are all cripples, every one of us, more or less. We are\nso divorced from it that we feel at once a sort of loathing for real\nlife, and so cannot bear to be reminded of it. Why, we have come almost\nto looking upon real life as an effort, almost as hard work, and we are\nall privately agreed that it is better in books. And why do we fuss and\nfume sometimes? Why are we perverse and ask for something else? We\ndon’t know what ourselves. It would be the worse for us if our petulant\nprayers were answered. Come, try, give any one of us, for instance, a\nlittle more independence, untie our hands, widen the spheres of our\nactivity, relax the control and we ... yes, I assure you ... we should\nbe begging to be under control again at once. I know that you will very\nlikely be angry with me for that, and will begin shouting and stamping.\nSpeak for yourself, you will say, and for your miseries in your\nunderground holes, and don’t dare to say all of us—excuse me,\ngentlemen, I am not justifying myself with that “all of us.” As for\nwhat concerns me in particular I have only in my life carried to an\nextreme what you have not dared to carry halfway, and what’s more, you\nhave taken your cowardice for good sense, and have found comfort in\ndeceiving yourselves. So that perhaps, after all, there is more life in\nme than in you. Look into it more carefully! Why, we don’t even know\nwhat living means now, what it is, and what it is called? Leave us\nalone without books and we shall be lost and in confusion at once. We\nshall not know what to join on to, what to cling to, what to love and\nwhat to hate, what to respect and what to despise. We are oppressed at\nbeing men—men with a real individual body and blood, we are ashamed of\nit, we think it a disgrace and try to contrive to be some sort of\nimpossible generalised man. We are stillborn, and for generations past\nhave been begotten, not by living fathers, and that suits us better and\nbetter. We are developing a taste for it. Soon we shall contrive to be\nborn somehow from an idea. But enough; I don’t want to write more from\n“Underground.”\n\n[The notes of this paradoxalist do not end here, however. He could not\nrefrain from going on with them, but it seems to us that we may stop\nhere.]\n\n\n\n\n*** END OF THE PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND ***\n\nUpdated editions will replace the previous one--the old editions will\nbe renamed.\n\nCreating the works from print editions not protected by U.S. copyright\nlaw means that no one owns a United States copyright in these works,\nso the Foundation (and you!) can copy and distribute it in the\nUnited States without permission and without paying copyright\nroyalties. Special rules, set forth in the General Terms of Use part\nof this license, apply to copying and distributing Project\nGutenberg-tm electronic works to protect the PROJECT GUTENBERG-tm\nconcept and trademark. Project Gutenberg is a registered trademark,\nand may not be used if you charge for an eBook, except by following\nthe terms of the trademark license, including paying royalties for use\nof the Project Gutenberg trademark. If you do not charge anything for\ncopies of this eBook, complying with the trademark license is very\neasy. You may use this eBook for nearly any purpose such as creation\nof derivative works, reports, performances and research. Project\nGutenberg eBooks may be modified and printed and given away--you may\ndo practically ANYTHING in the United States with eBooks not protected\nby U.S. copyright law. Redistribution is subject to the trademark\nlicense, especially commercial redistribution.\n\nSTART: FULL LICENSE\n\nTHE FULL PROJECT GUTENBERG LICENSE\nPLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK\n\nTo protect the Project Gutenberg-tm mission of promoting the free\ndistribution of electronic works, by using or distributing this work\n(or any other work associated in any way with the phrase \"Project\nGutenberg\"), you agree to comply with all the terms of the Full\nProject Gutenberg-tm License available with this file or online at\nwww.gutenberg.org/license.\n\nSection 1. General Terms of Use and Redistributing Project\nGutenberg-tm electronic works\n\n1.A. By reading or using any part of this Project Gutenberg-tm\nelectronic work, you indicate that you have read, understand, agree to\nand accept all the terms of this license and intellectual property\n(trademark/copyright) agreement. If you do not agree to abide by all\nthe terms of this agreement, you must cease using and return or\ndestroy all copies of Project Gutenberg-tm electronic works in your\npossession. If you paid a fee for obtaining a copy of or access to a\nProject Gutenberg-tm electronic work and you do not agree to be bound\nby the terms of this agreement, you may obtain a refund from the\nperson or entity to whom you paid the fee as set forth in paragraph\n1.E.8.\n\n1.B. \"Project Gutenberg\" is a registered trademark. It may only be\nused on or associated in any way with an electronic work by people who\nagree to be bound by the terms of this agreement. There are a few\nthings that you can do with most Project Gutenberg-tm electronic works\neven without complying with the full terms of this agreement. See\nparagraph 1.C below. There are a lot of things you can do with Project\nGutenberg-tm electronic works if you follow the terms of this\nagreement and help preserve free future access to Project Gutenberg-tm\nelectronic works. See paragraph 1.E below.\n\n1.C. The Project Gutenberg Literary Archive Foundation (\"the\nFoundation\" or PGLAF), owns a compilation copyright in the collection\nof Project Gutenberg-tm electronic works. Nearly all the individual\nworks in the collection are in the public domain in the United\nStates. If an individual work is unprotected by copyright law in the\nUnited States and you are located in the United States, we do not\nclaim a right to prevent you from copying, distributing, performing,\ndisplaying or creating derivative works based on the work as long as\nall references to Project Gutenberg are removed. Of course, we hope\nthat you will support the Project Gutenberg-tm mission of promoting\nfree access to electronic works by freely sharing Project Gutenberg-tm\nworks in compliance with the terms of this agreement for keeping the\nProject Gutenberg-tm name associated with the work. You can easily\ncomply with the terms of this agreement by keeping this work in the\nsame format with its attached full Project Gutenberg-tm License when\nyou share it without charge with others.\n\n1.D. The copyright laws of the place where you are located also govern\nwhat you can do with this work. Copyright laws in most countries are\nin a constant state of change. If you are outside the United States,\ncheck the laws of your country in addition to the terms of this\nagreement before downloading, copying, displaying, performing,\ndistributing or creating derivative works based on this work or any\nother Project Gutenberg-tm work. The Foundation makes no\nrepresentations concerning the copyright status of any work in any\ncountry other than the United States.\n\n1.E. Unless you have removed all references to Project Gutenberg:\n\n1.E.1. The following sentence, with active links to, or other\nimmediate access to, the full Project Gutenberg-tm License must appear\nprominently whenever any copy of a Project Gutenberg-tm work (any work\non which the phrase \"Project Gutenberg\" appears, or with which the\nphrase \"Project Gutenberg\" is associated) is accessed, displayed,\nperformed, viewed, copied or distributed:\n\n  This eBook is for the use of anyone anywhere in the United States and\n  most other parts of the world at no cost and with almost no\n  restrictions whatsoever. You may copy it, give it away or re-use it\n  under the terms of the Project Gutenberg License included with this\n  eBook or online at www.gutenberg.org. If you are not located in the\n  United States, you will have to check the laws of the country where\n  you are located before using this eBook.\n\n1.E.2. If an individual Project Gutenberg-tm electronic work is\nderived from texts not protected by U.S. copyright law (does not\ncontain a notice indicating that it is posted with permission of the\ncopyright holder), the work can be copied and distributed to anyone in\nthe United States without paying any fees or charges. If you are\nredistributing or providing access to a work with the phrase \"Project\nGutenberg\" associated with or appearing on the work, you must comply\neither with the requirements of paragraphs 1.E.1 through 1.E.7 or\nobtain permission for the use of the work and the Project Gutenberg-tm\ntrademark as set forth in paragraphs 1.E.8 or 1.E.9.\n\n1.E.3. If an individual Project Gutenberg-tm electronic work is posted\nwith the permission of the copyright holder, your use and distribution\nmust comply with both paragraphs 1.E.1 through 1.E.7 and any\nadditional terms imposed by the copyright holder. Additional terms\nwill be linked to the Project Gutenberg-tm License for all works\nposted with the permission of the copyright holder found at the\nbeginning of this work.\n\n1.E.4. Do not unlink or detach or remove the full Project Gutenberg-tm\nLicense terms from this work, or any files containing a part of this\nwork or any other work associated with Project Gutenberg-tm.\n\n1.E.5. Do not copy, display, perform, distribute or redistribute this\nelectronic work, or any part of this electronic work, without\nprominently displaying the sentence set forth in paragraph 1.E.1 with\nactive links or immediate access to the full terms of the Project\nGutenberg-tm License.\n\n1.E.6. You may convert to and distribute this work in any binary,\ncompressed, marked up, nonproprietary or proprietary form, including\nany word processing or hypertext form. However, if you provide access\nto or distribute copies of a Project Gutenberg-tm work in a format\nother than \"Plain Vanilla ASCII\" or other format used in the official\nversion posted on the official Project Gutenberg-tm website\n(www.gutenberg.org), you must, at no additional cost, fee or expense\nto the user, provide a copy, a means of exporting a copy, or a means\nof obtaining a copy upon request, of the work in its original \"Plain\nVanilla ASCII\" or other form. Any alternate format must include the\nfull Project Gutenberg-tm License as specified in paragraph 1.E.1.\n\n1.E.7. Do not charge a fee for access to, viewing, displaying,\nperforming, copying or distributing any Project Gutenberg-tm works\nunless you comply with paragraph 1.E.8 or 1.E.9.\n\n1.E.8. You may charge a reasonable fee for copies of or providing\naccess to or distributing Project Gutenberg-tm electronic works\nprovided that:\n\n* You pay a royalty fee of 20% of the gross profits you derive from\n  the use of Project Gutenberg-tm works calculated using the method\n  you already use to calculate your applicable taxes. The fee is owed\n  to the owner of the Project Gutenberg-tm trademark, but he has\n  agreed to donate royalties under this paragraph to the Project\n  Gutenberg Literary Archive Foundation. Royalty payments must be paid\n  within 60 days following each date on which you prepare (or are\n  legally required to prepare) your periodic tax returns. Royalty\n  payments should be clearly marked as such and sent to the Project\n  Gutenberg Literary Archive Foundation at the address specified in\n  Section 4, \"Information about donations to the Project Gutenberg\n  Literary Archive Foundation.\"\n\n* You provide a full refund of any money paid by a user who notifies\n  you in writing (or by e-mail) within 30 days of receipt that s/he\n  does not agree to the terms of the full Project Gutenberg-tm\n  License. You must require such a user to return or destroy all\n  copies of the works possessed in a physical medium and discontinue\n  all use of and all access to other copies of Project Gutenberg-tm\n  works.\n\n* You provide, in accordance with paragraph 1.F.3, a full refund of\n  any money paid for a work or a replacement copy, if a defect in the\n  electronic work is discovered and reported to you within 90 days of\n  receipt of the work.\n\n* You comply with all other terms of this agreement for free\n  distribution of Project Gutenberg-tm works.\n\n1.E.9. If you wish to charge a fee or distribute a Project\nGutenberg-tm electronic work or group of works on different terms than\nare set forth in this agreement, you must obtain permission in writing\nfrom the Project Gutenberg Literary Archive Foundation, the manager of\nthe Project Gutenberg-tm trademark. Contact the Foundation as set\nforth in Section 3 below.\n\n1.F.\n\n1.F.1. Project Gutenberg volunteers and employees expend considerable\neffort to identify, do copyright research on, transcribe and proofread\nworks not protected by U.S. copyright law in creating the Project\nGutenberg-tm collection. Despite these efforts, Project Gutenberg-tm\nelectronic works, and the medium on which they may be stored, may\ncontain \"Defects,\" such as, but not limited to, incomplete, inaccurate\nor corrupt data, transcription errors, a copyright or other\nintellectual property infringement, a defective or damaged disk or\nother medium, a computer virus, or computer codes that damage or\ncannot be read by your equipment.\n\n1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the \"Right\nof Replacement or Refund\" described in paragraph 1.F.3, the Project\nGutenberg Literary Archive Foundation, the owner of the Project\nGutenberg-tm trademark, and any other party distributing a Project\nGutenberg-tm electronic work under this agreement, disclaim all\nliability to you for damages, costs and expenses, including legal\nfees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT\nLIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE\nPROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE\nTRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE\nLIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR\nINCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH\nDAMAGE.\n\n1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a\ndefect in this electronic work within 90 days of receiving it, you can\nreceive a refund of the money (if any) you paid for it by sending a\nwritten explanation to the person you received the work from. If you\nreceived the work on a physical medium, you must return the medium\nwith your written explanation. The person or entity that provided you\nwith the defective work may elect to provide a replacement copy in\nlieu of a refund. If you received the work electronically, the person\nor entity providing it to you may choose to give you a second\nopportunity to receive the work electronically in lieu of a refund. If\nthe second copy is also defective, you may demand a refund in writing\nwithout further opportunities to fix the problem.\n\n1.F.4. Except for the limited right of replacement or refund set forth\nin paragraph 1.F.3, this work is provided to you 'AS-IS', WITH NO\nOTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT\nLIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.\n\n1.F.5. Some states do not allow disclaimers of certain implied\nwarranties or the exclusion or limitation of certain types of\ndamages. If any disclaimer or limitation set forth in this agreement\nviolates the law of the state applicable to this agreement, the\nagreement shall be interpreted to make the maximum disclaimer or\nlimitation permitted by the applicable state law. The invalidity or\nunenforceability of any provision of this agreement shall not void the\nremaining provisions.\n\n1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the\ntrademark owner, any agent or employee of the Foundation, anyone\nproviding copies of Project Gutenberg-tm electronic works in\naccordance with this agreement, and any volunteers associated with the\nproduction, promotion and distribution of Project Gutenberg-tm\nelectronic works, harmless from all liability, costs and expenses,\nincluding legal fees, that arise directly or indirectly from any of\nthe following which you do or cause to occur: (a) distribution of this\nor any Project Gutenberg-tm work, (b) alteration, modification, or\nadditions or deletions to any Project Gutenberg-tm work, and (c) any\nDefect you cause.\n\nSection 2. Information about the Mission of Project Gutenberg-tm\n\nProject Gutenberg-tm is synonymous with the free distribution of\nelectronic works in formats readable by the widest variety of\ncomputers including obsolete, old, middle-aged and new computers. It\nexists because of the efforts of hundreds of volunteers and donations\nfrom people in all walks of life.\n\nVolunteers and financial support to provide volunteers with the\nassistance they need are critical to reaching Project Gutenberg-tm's\ngoals and ensuring that the Project Gutenberg-tm collection will\nremain freely available for generations to come. In 2001, the Project\nGutenberg Literary Archive Foundation was created to provide a secure\nand permanent future for Project Gutenberg-tm and future\ngenerations. To learn more about the Project Gutenberg Literary\nArchive Foundation and how your efforts and donations can help, see\nSections 3 and 4 and the Foundation information page at\nwww.gutenberg.org\n\nSection 3. Information about the Project Gutenberg Literary\nArchive Foundation\n\nThe Project Gutenberg Literary Archive Foundation is a non-profit\n501(c)(3) educational corporation organized under the laws of the\nstate of Mississippi and granted tax exempt status by the Internal\nRevenue Service. The Foundation's EIN or federal tax identification\nnumber is 64-6221541. Contributions to the Project Gutenberg Literary\nArchive Foundation are tax deductible to the full extent permitted by\nU.S. federal laws and your state's laws.\n\nThe Foundation's business office is located at 809 North 1500 West,\nSalt Lake City, UT 84116, (801) 596-1887. Email contact links and up\nto date contact information can be found at the Foundation's website\nand official page at www.gutenberg.org/contact\n\nSection 4. Information about Donations to the Project Gutenberg\nLiterary Archive Foundation\n\nProject Gutenberg-tm depends upon and cannot survive without\nwidespread public support and donations to carry out its mission of\nincreasing the number of public domain and licensed works that can be\nfreely distributed in machine-readable form accessible by the widest\narray of equipment including outdated equipment. Many small donations\n($1 to $5,000) are particularly important to maintaining tax exempt\nstatus with the IRS.\n\nThe Foundation is committed to complying with the laws regulating\ncharities and charitable donations in all 50 states of the United\nStates. Compliance requirements are not uniform and it takes a\nconsiderable effort, much paperwork and many fees to meet and keep up\nwith these requirements. We do not solicit donations in locations\nwhere we have not received written confirmation of compliance. To SEND\nDONATIONS or determine the status of compliance for any particular\nstate visit www.gutenberg.org/donate\n\nWhile we cannot and do not solicit contributions from states where we\nhave not met the solicitation requirements, we know of no prohibition\nagainst accepting unsolicited donations from donors in such states who\napproach us with offers to donate.\n\nInternational donations are gratefully accepted, but we cannot make\nany statements concerning tax treatment of donations received from\noutside the United States. U.S. laws alone swamp our small staff.\n\nPlease check the Project Gutenberg web pages for current donation\nmethods and addresses. Donations are accepted in a number of other\nways including checks, online payments and credit card donations. To\ndonate, please visit: www.gutenberg.org/donate\n\nSection 5. General Information About Project Gutenberg-tm electronic works\n\nProfessor Michael S. Hart was the originator of the Project\nGutenberg-tm concept of a library of electronic works that could be\nfreely shared with anyone. For forty years, he produced and\ndistributed Project Gutenberg-tm eBooks with only a loose network of\nvolunteer support.\n\nProject Gutenberg-tm eBooks are often created from several printed\neditions, all of which are confirmed as not protected by copyright in\nthe U.S. unless a copyright notice is included. Thus, we do not\nnecessarily keep eBooks in compliance with any particular paper\nedition.\n\nMost people start at our website which has the main PG search\nfacility: www.gutenberg.org\n\nThis website includes information about Project Gutenberg-tm,\nincluding how to make donations to the Project Gutenberg Literary\nArchive Foundation, how to help produce our new eBooks, and how to\nsubscribe to our email newsletter to hear about new eBooks."
  },
  {
    "objectID": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#preprocess-text",
    "href": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#preprocess-text",
    "title": "Text summarizer in Python, Notes from underground",
    "section": "Preprocess text",
    "text": "Preprocess text\nWe use regular expression to do text preprocessing. We will: 1. replace reference number with empty space, if any… 2. replace one or more spaces with single space.\n\ntext = raw_text\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # replace reference number i.e. [1], [10], [20] with empty space, if any..\ntext = re.sub(r'\\s+',' ',text) # replace one or more spaces with single space\nprint(text)\n\n﻿The Project Gutenberg eBook of Notes from the Underground, by Fyodor Dostoyevsky This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Notes from the Underground Author: Fyodor Dostoyevsky Translator: Constance Garnett Release Date: July, 1996 [eBook #600] [Most recently updated: December 26, 2021] Language: English Produced by: Judith Boss. HTML version by Al Haines *** START OF THE PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** Notes from the Underground by Fyodor Dostoyevsky Contents NOTES FROM THE UNDERGROUND PART I Underground I II III IV V VI VII VIII IX X XI PART II À Propos of the Wet Snow I II III IV V VI VII VIII IX X NOTES FROM THE UNDERGROUND[*] A NOVEL * The author of the diary and the diary itself are, of course, imaginary. Nevertheless it is clear that such persons as the writer of these notes not only may, but positively must, exist in our society, when we consider the circumstances in the midst of which our society is formed. I have tried to expose to the view of the public more distinctly than is commonly done, one of the characters of the recent past. He is one of the representatives of a generation still living. In this fragment, entitled “Underground,” this person introduces himself and his views, and, as it were, tries to explain the causes owing to which he has made his appearance and was bound to make his appearance in our midst. In the second fragment there are added the actual notes of this person concerning certain events in his life.—AUTHOR’S NOTE. PART I Underground I I am a sick man.... I am a spiteful man. I am an unattractive man. I believe my liver is diseased. However, I know nothing at all about my disease, and do not know for certain what ails me. I don’t consult a doctor for it, and never have, though I have a respect for medicine and doctors. Besides, I am extremely superstitious, sufficiently so to respect medicine, anyway (I am well-educated enough not to be superstitious, but I am superstitious). No, I refuse to consult a doctor from spite. That you probably will not understand. Well, I understand it, though. Of course, I can’t explain who it is precisely that I am mortifying in this case by my spite: I am perfectly well aware that I cannot “pay out” the doctors by not consulting them; I know better than anyone that by all this I am only injuring myself and no one else. But still, if I don’t consult a doctor it is from spite. My liver is bad, well—let it get worse! I have been going on like that for a long time—twenty years. Now I am forty. I used to be in the government service, but am no longer. I was a spiteful official. I was rude and took pleasure in being so. I did not take bribes, you see, so I was bound to find a recompense in that, at least. (A poor jest, but I will not scratch it out. I wrote it thinking it would sound very witty; but now that I have seen myself that I only wanted to show off in a despicable way, I will not scratch it out on purpose!) When petitioners used to come for information to the table at which I sat, I used to grind my teeth at them, and felt intense enjoyment when I succeeded in making anybody unhappy. I almost did succeed. For the most part they were all timid people—of course, they were petitioners. But of the uppish ones there was one officer in particular I could not endure. He simply would not be humble, and clanked his sword in a disgusting way. I carried on a feud with him for eighteen months over that sword. At last I got the better of him. He left off clanking it. That happened in my youth, though. But do you know, gentlemen, what was the chief point about my spite? Why, the whole point, the real sting of it lay in the fact that continually, even in the moment of the acutest spleen, I was inwardly conscious with shame that I was not only not a spiteful but not even an embittered man, that I was simply scaring sparrows at random and amusing myself by it. I might foam at the mouth, but bring me a doll to play with, give me a cup of tea with sugar in it, and maybe I should be appeased. I might even be genuinely touched, though probably I should grind my teeth at myself afterwards and lie awake at night with shame for months after. That was my way. I was lying when I said just now that I was a spiteful official. I was lying from spite. I was simply amusing myself with the petitioners and with the officer, and in reality I never could become spiteful. I was conscious every moment in myself of many, very many elements absolutely opposite to that. I felt them positively swarming in me, these opposite elements. I knew that they had been swarming in me all my life and craving some outlet from me, but I would not let them, would not let them, purposely would not let them come out. They tormented me till I was ashamed: they drove me to convulsions and—sickened me, at last, how they sickened me! Now, are not you fancying, gentlemen, that I am expressing remorse for something now, that I am asking your forgiveness for something? I am sure you are fancying that ... However, I assure you I do not care if you are.... It was not only that I could not become spiteful, I did not know how to become anything; neither spiteful nor kind, neither a rascal nor an honest man, neither a hero nor an insect. Now, I am living out my life in my corner, taunting myself with the spiteful and useless consolation that an intelligent man cannot become anything seriously, and it is only the fool who becomes anything. Yes, a man in the nineteenth century must and morally ought to be pre-eminently a characterless creature; a man of character, an active man is pre-eminently a limited creature. That is my conviction of forty years. I am forty years old now, and you know forty years is a whole lifetime; you know it is extreme old age. To live longer than forty years is bad manners, is vulgar, immoral. Who does live beyond forty? Answer that, sincerely and honestly I will tell you who do: fools and worthless fellows. I tell all old men that to their face, all these venerable old men, all these silver-haired and reverend seniors! I tell the whole world that to its face! I have a right to say so, for I shall go on living to sixty myself. To seventy! To eighty! ... Stay, let me take breath ... You imagine no doubt, gentlemen, that I want to amuse you. You are mistaken in that, too. I am by no means such a mirthful person as you imagine, or as you may imagine; however, irritated by all this babble (and I feel that you are irritated) you think fit to ask me who I am—then my answer is, I am a collegiate assessor. I was in the service that I might have something to eat (and solely for that reason), and when last year a distant relation left me six thousand roubles in his will I immediately retired from the service and settled down in my corner. I used to live in this corner before, but now I have settled down in it. My room is a wretched, horrid one in the outskirts of the town. My servant is an old country-woman, ill-natured from stupidity, and, moreover, there is always a nasty smell about her. I am told that the Petersburg climate is bad for me, and that with my small means it is very expensive to live in Petersburg. I know all that better than all these sage and experienced counsellors and monitors.... But I am remaining in Petersburg; I am not going away from Petersburg! I am not going away because ... ech! Why, it is absolutely no matter whether I am going away or not going away. But what can a decent man speak of with most pleasure? Answer: Of himself. Well, so I will talk about myself. II I want now to tell you, gentlemen, whether you care to hear it or not, why I could not even become an insect. I tell you solemnly, that I have many times tried to become an insect. But I was not equal even to that. I swear, gentlemen, that to be too conscious is an illness—a real thorough-going illness. For man’s everyday needs, it would have been quite enough to have the ordinary human consciousness, that is, half or a quarter of the amount which falls to the lot of a cultivated man of our unhappy nineteenth century, especially one who has the fatal ill-luck to inhabit Petersburg, the most theoretical and intentional town on the whole terrestrial globe. (There are intentional and unintentional towns.) It would have been quite enough, for instance, to have the consciousness by which all so-called direct persons and men of action live. I bet you think I am writing all this from affectation, to be witty at the expense of men of action; and what is more, that from ill-bred affectation, I am clanking a sword like my officer. But, gentlemen, whoever can pride himself on his diseases and even swagger over them? Though, after all, everyone does do that; people do pride themselves on their diseases, and I do, may be, more than anyone. We will not dispute it; my contention was absurd. But yet I am firmly persuaded that a great deal of consciousness, every sort of consciousness, in fact, is a disease. I stick to that. Let us leave that, too, for a minute. Tell me this: why does it happen that at the very, yes, at the very moments when I am most capable of feeling every refinement of all that is “sublime and beautiful,” as they used to say at one time, it would, as though of design, happen to me not only to feel but to do such ugly things, such that ... Well, in short, actions that all, perhaps, commit; but which, as though purposely, occurred to me at the very time when I was most conscious that they ought not to be committed. The more conscious I was of goodness and of all that was “sublime and beautiful,” the more deeply I sank into my mire and the more ready I was to sink in it altogether. But the chief point was that all this was, as it were, not accidental in me, but as though it were bound to be so. It was as though it were my most normal condition, and not in the least disease or depravity, so that at last all desire in me to struggle against this depravity passed. It ended by my almost believing (perhaps actually believing) that this was perhaps my normal condition. But at first, in the beginning, what agonies I endured in that struggle! I did not believe it was the same with other people, and all my life I hid this fact about myself as a secret. I was ashamed (even now, perhaps, I am ashamed): I got to the point of feeling a sort of secret abnormal, despicable enjoyment in returning home to my corner on some disgusting Petersburg night, acutely conscious that that day I had committed a loathsome action again, that what was done could never be undone, and secretly, inwardly gnawing, gnawing at myself for it, tearing and consuming myself till at last the bitterness turned into a sort of shameful accursed sweetness, and at last—into positive real enjoyment! Yes, into enjoyment, into enjoyment! I insist upon that. I have spoken of this because I keep wanting to know for a fact whether other people feel such enjoyment? I will explain; the enjoyment was just from the too intense consciousness of one’s own degradation; it was from feeling oneself that one had reached the last barrier, that it was horrible, but that it could not be otherwise; that there was no escape for you; that you never could become a different man; that even if time and faith were still left you to change into something different you would most likely not wish to change; or if you did wish to, even then you would do nothing; because perhaps in reality there was nothing for you to change into. And the worst of it was, and the root of it all, that it was all in accord with the normal fundamental laws of over-acute consciousness, and with the inertia that was the direct result of those laws, and that consequently one was not only unable to change but could do absolutely nothing. Thus it would follow, as the result of acute consciousness, that one is not to blame in being a scoundrel; as though that were any consolation to the scoundrel once he has come to realise that he actually is a scoundrel. But enough.... Ech, I have talked a lot of nonsense, but what have I explained? How is enjoyment in this to be explained? But I will explain it. I will get to the bottom of it! That is why I have taken up my pen.... I, for instance, have a great deal of _amour propre_. I am as suspicious and prone to take offence as a humpback or a dwarf. But upon my word I sometimes have had moments when if I had happened to be slapped in the face I should, perhaps, have been positively glad of it. I say, in earnest, that I should probably have been able to discover even in that a peculiar sort of enjoyment—the enjoyment, of course, of despair; but in despair there are the most intense enjoyments, especially when one is very acutely conscious of the hopelessness of one’s position. And when one is slapped in the face—why then the consciousness of being rubbed into a pulp would positively overwhelm one. The worst of it is, look at it which way one will, it still turns out that I was always the most to blame in everything. And what is most humiliating of all, to blame for no fault of my own but, so to say, through the laws of nature. In the first place, to blame because I am cleverer than any of the people surrounding me. (I have always considered myself cleverer than any of the people surrounding me, and sometimes, would you believe it, have been positively ashamed of it. At any rate, I have all my life, as it were, turned my eyes away and never could look people straight in the face.) To blame, finally, because even if I had had magnanimity, I should only have had more suffering from the sense of its uselessness. I should certainly have never been able to do anything from being magnanimous—neither to forgive, for my assailant would perhaps have slapped me from the laws of nature, and one cannot forgive the laws of nature; nor to forget, for even if it were owing to the laws of nature, it is insulting all the same. Finally, even if I had wanted to be anything but magnanimous, had desired on the contrary to revenge myself on my assailant, I could not have revenged myself on any one for anything because I should certainly never have made up my mind to do anything, even if I had been able to. Why should I not have made up my mind? About that in particular I want to say a few words. III With people who know how to revenge themselves and to stand up for themselves in general, how is it done? Why, when they are possessed, let us suppose, by the feeling of revenge, then for the time there is nothing else but that feeling left in their whole being. Such a gentleman simply dashes straight for his object like an infuriated bull with its horns down, and nothing but a wall will stop him. (By the way: facing the wall, such gentlemen—that is, the “direct” persons and men of action—are genuinely nonplussed. For them a wall is not an evasion, as for us people who think and consequently do nothing; it is not an excuse for turning aside, an excuse for which we are always very glad, though we scarcely believe in it ourselves, as a rule. No, they are nonplussed in all sincerity. The wall has for them something tranquillising, morally soothing, final—maybe even something mysterious ... but of the wall later.) Well, such a direct person I regard as the real normal man, as his tender mother nature wished to see him when she graciously brought him into being on the earth. I envy such a man till I am green in the face. He is stupid. I am not disputing that, but perhaps the normal man should be stupid, how do you know? Perhaps it is very beautiful, in fact. And I am the more persuaded of that suspicion, if one can call it so, by the fact that if you take, for instance, the antithesis of the normal man, that is, the man of acute consciousness, who has come, of course, not out of the lap of nature but out of a retort (this is almost mysticism, gentlemen, but I suspect this, too), this retort-made man is sometimes so nonplussed in the presence of his antithesis that with all his exaggerated consciousness he genuinely thinks of himself as a mouse and not a man. It may be an acutely conscious mouse, yet it is a mouse, while the other is a man, and therefore, et caetera, et caetera. And the worst of it is, he himself, his very own self, looks on himself as a mouse; no one asks him to do so; and that is an important point. Now let us look at this mouse in action. Let us suppose, for instance, that it feels insulted, too (and it almost always does feel insulted), and wants to revenge itself, too. There may even be a greater accumulation of spite in it than in _l’homme de la nature et de la vérité_. The base and nasty desire to vent that spite on its assailant rankles perhaps even more nastily in it than in _l’homme de la nature et de la vérité_. For through his innate stupidity the latter looks upon his revenge as justice pure and simple; while in consequence of his acute consciousness the mouse does not believe in the justice of it. To come at last to the deed itself, to the very act of revenge. Apart from the one fundamental nastiness the luckless mouse succeeds in creating around it so many other nastinesses in the form of doubts and questions, adds to the one question so many unsettled questions that there inevitably works up around it a sort of fatal brew, a stinking mess, made up of its doubts, emotions, and of the contempt spat upon it by the direct men of action who stand solemnly about it as judges and arbitrators, laughing at it till their healthy sides ache. Of course the only thing left for it is to dismiss all that with a wave of its paw, and, with a smile of assumed contempt in which it does not even itself believe, creep ignominiously into its mouse-hole. There in its nasty, stinking, underground home our insulted, crushed and ridiculed mouse promptly becomes absorbed in cold, malignant and, above all, everlasting spite. For forty years together it will remember its injury down to the smallest, most ignominious details, and every time will add, of itself, details still more ignominious, spitefully teasing and tormenting itself with its own imagination. It will itself be ashamed of its imaginings, but yet it will recall it all, it will go over and over every detail, it will invent unheard of things against itself, pretending that those things might happen, and will forgive nothing. Maybe it will begin to revenge itself, too, but, as it were, piecemeal, in trivial ways, from behind the stove, incognito, without believing either in its own right to vengeance, or in the success of its revenge, knowing that from all its efforts at revenge it will suffer a hundred times more than he on whom it revenges itself, while he, I daresay, will not even scratch himself. On its deathbed it will recall it all over again, with interest accumulated over all the years and ... But it is just in that cold, abominable half despair, half belief, in that conscious burying oneself alive for grief in the underworld for forty years, in that acutely recognised and yet partly doubtful hopelessness of one’s position, in that hell of unsatisfied desires turned inward, in that fever of oscillations, of resolutions determined for ever and repented of again a minute later—that the savour of that strange enjoyment of which I have spoken lies. It is so subtle, so difficult of analysis, that persons who are a little limited, or even simply persons of strong nerves, will not understand a single atom of it. “Possibly,” you will add on your own account with a grin, “people will not understand it either who have never received a slap in the face,” and in that way you will politely hint to me that I, too, perhaps, have had the experience of a slap in the face in my life, and so I speak as one who knows. I bet that you are thinking that. But set your minds at rest, gentlemen, I have not received a slap in the face, though it is absolutely a matter of indifference to me what you may think about it. Possibly, I even regret, myself, that I have given so few slaps in the face during my life. But enough ... not another word on that subject of such extreme interest to you. I will continue calmly concerning persons with strong nerves who do not understand a certain refinement of enjoyment. Though in certain circumstances these gentlemen bellow their loudest like bulls, though this, let us suppose, does them the greatest credit, yet, as I have said already, confronted with the impossible they subside at once. The impossible means the stone wall! What stone wall? Why, of course, the laws of nature, the deductions of natural science, mathematics. As soon as they prove to you, for instance, that you are descended from a monkey, then it is no use scowling, accept it for a fact. When they prove to you that in reality one drop of your own fat must be dearer to you than a hundred thousand of your fellow-creatures, and that this conclusion is the final solution of all so-called virtues and duties and all such prejudices and fancies, then you have just to accept it, there is no help for it, for twice two is a law of mathematics. Just try refuting it. “Upon my word, they will shout at you, it is no use protesting: it is a case of twice two makes four! Nature does not ask your permission, she has nothing to do with your wishes, and whether you like her laws or dislike them, you are bound to accept her as she is, and consequently all her conclusions. A wall, you see, is a wall ... and so on, and so on.” Merciful Heavens! but what do I care for the laws of nature and arithmetic, when, for some reason I dislike those laws and the fact that twice two makes four? Of course I cannot break through the wall by battering my head against it if I really have not the strength to knock it down, but I am not going to be reconciled to it simply because it is a stone wall and I have not the strength. As though such a stone wall really were a consolation, and really did contain some word of conciliation, simply because it is as true as twice two makes four. Oh, absurdity of absurdities! How much better it is to understand it all, to recognise it all, all the impossibilities and the stone wall; not to be reconciled to one of those impossibilities and stone walls if it disgusts you to be reconciled to it; by the way of the most inevitable, logical combinations to reach the most revolting conclusions on the everlasting theme, that even for the stone wall you are yourself somehow to blame, though again it is as clear as day you are not to blame in the least, and therefore grinding your teeth in silent impotence to sink into luxurious inertia, brooding on the fact that there is no one even for you to feel vindictive against, that you have not, and perhaps never will have, an object for your spite, that it is a sleight of hand, a bit of juggling, a card-sharper’s trick, that it is simply a mess, no knowing what and no knowing who, but in spite of all these uncertainties and jugglings, still there is an ache in you, and the more you do not know, the worse the ache. IV “Ha, ha, ha! You will be finding enjoyment in toothache next,” you cry, with a laugh. “Well, even in toothache there is enjoyment,” I answer. I had toothache for a whole month and I know there is. In that case, of course, people are not spiteful in silence, but moan; but they are not candid moans, they are malignant moans, and the malignancy is the whole point. The enjoyment of the sufferer finds expression in those moans; if he did not feel enjoyment in them he would not moan. It is a good example, gentlemen, and I will develop it. Those moans express in the first place all the aimlessness of your pain, which is so humiliating to your consciousness; the whole legal system of nature on which you spit disdainfully, of course, but from which you suffer all the same while she does not. They express the consciousness that you have no enemy to punish, but that you have pain; the consciousness that in spite of all possible Wagenheims you are in complete slavery to your teeth; that if someone wishes it, your teeth will leave off aching, and if he does not, they will go on aching another three months; and that finally if you are still contumacious and still protest, all that is left you for your own gratification is to thrash yourself or beat your wall with your fist as hard as you can, and absolutely nothing more. Well, these mortal insults, these jeers on the part of someone unknown, end at last in an enjoyment which sometimes reaches the highest degree of voluptuousness. I ask you, gentlemen, listen sometimes to the moans of an educated man of the nineteenth century suffering from toothache, on the second or third day of the attack, when he is beginning to moan, not as he moaned on the first day, that is, not simply because he has toothache, not just as any coarse peasant, but as a man affected by progress and European civilisation, a man who is “divorced from the soil and the national elements,” as they express it now-a-days. His moans become nasty, disgustingly malignant, and go on for whole days and nights. And of course he knows himself that he is doing himself no sort of good with his moans; he knows better than anyone that he is only lacerating and harassing himself and others for nothing; he knows that even the audience before whom he is making his efforts, and his whole family, listen to him with loathing, do not put a ha’porth of faith in him, and inwardly understand that he might moan differently, more simply, without trills and flourishes, and that he is only amusing himself like that from ill-humour, from malignancy. Well, in all these recognitions and disgraces it is that there lies a voluptuous pleasure. As though he would say: “I am worrying you, I am lacerating your hearts, I am keeping everyone in the house awake. Well, stay awake then, you, too, feel every minute that I have toothache. I am not a hero to you now, as I tried to seem before, but simply a nasty person, an impostor. Well, so be it, then! I am very glad that you see through me. It is nasty for you to hear my despicable moans: well, let it be nasty; here I will let you have a nastier flourish in a minute....” You do not understand even now, gentlemen? No, it seems our development and our consciousness must go further to understand all the intricacies of this pleasure. You laugh? Delighted. My jests, gentlemen, are of course in bad taste, jerky, involved, lacking self-confidence. But of course that is because I do not respect myself. Can a man of perception respect himself at all? V Come, can a man who attempts to find enjoyment in the very feeling of his own degradation possibly have a spark of respect for himself? I am not saying this now from any mawkish kind of remorse. And, indeed, I could never endure saying, “Forgive me, Papa, I won’t do it again,” not because I am incapable of saying that—on the contrary, perhaps just because I have been too capable of it, and in what a way, too. As though of design I used to get into trouble in cases when I was not to blame in any way. That was the nastiest part of it. At the same time I was genuinely touched and penitent, I used to shed tears and, of course, deceived myself, though I was not acting in the least and there was a sick feeling in my heart at the time.... For that one could not blame even the laws of nature, though the laws of nature have continually all my life offended me more than anything. It is loathsome to remember it all, but it was loathsome even then. Of course, a minute or so later I would realise wrathfully that it was all a lie, a revolting lie, an affected lie, that is, all this penitence, this emotion, these vows of reform. You will ask why did I worry myself with such antics: answer, because it was very dull to sit with one’s hands folded, and so one began cutting capers. That is really it. Observe yourselves more carefully, gentlemen, then you will understand that it is so. I invented adventures for myself and made up a life, so as at least to live in some way. How many times it has happened to me—well, for instance, to take offence simply on purpose, for nothing; and one knows oneself, of course, that one is offended at nothing; that one is putting it on, but yet one brings oneself at last to the point of being really offended. All my life I have had an impulse to play such pranks, so that in the end I could not control it in myself. Another time, twice, in fact, I tried hard to be in love. I suffered, too, gentlemen, I assure you. In the depth of my heart there was no faith in my suffering, only a faint stir of mockery, but yet I did suffer, and in the real, orthodox way; I was jealous, beside myself ... and it was all from _ennui_, gentlemen, all from _ennui;_ inertia overcame me. You know the direct, legitimate fruit of consciousness is inertia, that is, conscious sitting-with-the-hands-folded. I have referred to this already. I repeat, I repeat with emphasis: all “direct” persons and men of action are active just because they are stupid and limited. How explain that? I will tell you: in consequence of their limitation they take immediate and secondary causes for primary ones, and in that way persuade themselves more quickly and easily than other people do that they have found an infallible foundation for their activity, and their minds are at ease and you know that is the chief thing. To begin to act, you know, you must first have your mind completely at ease and no trace of doubt left in it. Why, how am I, for example, to set my mind at rest? Where are the primary causes on which I am to build? Where are my foundations? Where am I to get them from? I exercise myself in reflection, and consequently with me every primary cause at once draws after itself another still more primary, and so on to infinity. That is just the essence of every sort of consciousness and reflection. It must be a case of the laws of nature again. What is the result of it in the end? Why, just the same. Remember I spoke just now of vengeance. (I am sure you did not take it in.) I said that a man revenges himself because he sees justice in it. Therefore he has found a primary cause, that is, justice. And so he is at rest on all sides, and consequently he carries out his revenge calmly and successfully, being persuaded that he is doing a just and honest thing. But I see no justice in it, I find no sort of virtue in it either, and consequently if I attempt to revenge myself, it is only out of spite. Spite, of course, might overcome everything, all my doubts, and so might serve quite successfully in place of a primary cause, precisely because it is not a cause. But what is to be done if I have not even spite (I began with that just now, you know). In consequence again of those accursed laws of consciousness, anger in me is subject to chemical disintegration. You look into it, the object flies off into air, your reasons evaporate, the criminal is not to be found, the wrong becomes not a wrong but a phantom, something like the toothache, for which no one is to blame, and consequently there is only the same outlet left again—that is, to beat the wall as hard as you can. So you give it up with a wave of the hand because you have not found a fundamental cause. And try letting yourself be carried away by your feelings, blindly, without reflection, without a primary cause, repelling consciousness at least for a time; hate or love, if only not to sit with your hands folded. The day after tomorrow, at the latest, you will begin despising yourself for having knowingly deceived yourself. Result: a soap-bubble and inertia. Oh, gentlemen, do you know, perhaps I consider myself an intelligent man, only because all my life I have been able neither to begin nor to finish anything. Granted I am a babbler, a harmless vexatious babbler, like all of us. But what is to be done if the direct and sole vocation of every intelligent man is babble, that is, the intentional pouring of water through a sieve? VI Oh, if I had done nothing simply from laziness! Heavens, how I should have respected myself, then. I should have respected myself because I should at least have been capable of being lazy; there would at least have been one quality, as it were, positive in me, in which I could have believed myself. Question: What is he? Answer: A sluggard; how very pleasant it would have been to hear that of oneself! It would mean that I was positively defined, it would mean that there was something to say about me. “Sluggard”—why, it is a calling and vocation, it is a career. Do not jest, it is so. I should then be a member of the best club by right, and should find my occupation in continually respecting myself. I knew a gentleman who prided himself all his life on being a connoisseur of Lafitte. He considered this as his positive virtue, and never doubted himself. He died, not simply with a tranquil, but with a triumphant conscience, and he was quite right, too. Then I should have chosen a career for myself, I should have been a sluggard and a glutton, not a simple one, but, for instance, one with sympathies for everything sublime and beautiful. How do you like that? I have long had visions of it. That “sublime and beautiful” weighs heavily on my mind at forty But that is at forty; then—oh, then it would have been different! I should have found for myself a form of activity in keeping with it, to be precise, drinking to the health of everything “sublime and beautiful.” I should have snatched at every opportunity to drop a tear into my glass and then to drain it to all that is “sublime and beautiful.” I should then have turned everything into the sublime and the beautiful; in the nastiest, unquestionable trash, I should have sought out the sublime and the beautiful. I should have exuded tears like a wet sponge. An artist, for instance, paints a picture worthy of Gay. At once I drink to the health of the artist who painted the picture worthy of Gay, because I love all that is “sublime and beautiful.” An author has written _As you will:_ at once I drink to the health of “anyone you will” because I love all that is “sublime and beautiful.” I should claim respect for doing so. I should persecute anyone who would not show me respect. I should live at ease, I should die with dignity, why, it is charming, perfectly charming! And what a good round belly I should have grown, what a treble chin I should have established, what a ruby nose I should have coloured for myself, so that everyone would have said, looking at me: “Here is an asset! Here is something real and solid!” And, say what you like, it is very agreeable to hear such remarks about oneself in this negative age. VII But these are all golden dreams. Oh, tell me, who was it first announced, who was it first proclaimed, that man only does nasty things because he does not know his own interests; and that if he were enlightened, if his eyes were opened to his real normal interests, man would at once cease to do nasty things, would at once become good and noble because, being enlightened and understanding his real advantage, he would see his own advantage in the good and nothing else, and we all know that not one man can, consciously, act against his own interests, consequently, so to say, through necessity, he would begin doing good? Oh, the babe! Oh, the pure, innocent child! Why, in the first place, when in all these thousands of years has there been a time when man has acted only from his own interest? What is to be done with the millions of facts that bear witness that men, _consciously_, that is fully understanding their real interests, have left them in the background and have rushed headlong on another path, to meet peril and danger, compelled to this course by nobody and by nothing, but, as it were, simply disliking the beaten track, and have obstinately, wilfully, struck out another difficult, absurd way, seeking it almost in the darkness. So, I suppose, this obstinacy and perversity were pleasanter to them than any advantage.... Advantage! What is advantage? And will you take it upon yourself to define with perfect accuracy in what the advantage of man consists? And what if it so happens that a man’s advantage, _sometimes_, not only may, but even must, consist in his desiring in certain cases what is harmful to himself and not advantageous. And if so, if there can be such a case, the whole principle falls into dust. What do you think—are there such cases? You laugh; laugh away, gentlemen, but only answer me: have man’s advantages been reckoned up with perfect certainty? Are there not some which not only have not been included but cannot possibly be included under any classification? You see, you gentlemen have, to the best of my knowledge, taken your whole register of human advantages from the averages of statistical figures and politico-economical formulas. Your advantages are prosperity, wealth, freedom, peace—and so on, and so on. So that the man who should, for instance, go openly and knowingly in opposition to all that list would to your thinking, and indeed mine, too, of course, be an obscurantist or an absolute madman: would not he? But, you know, this is what is surprising: why does it so happen that all these statisticians, sages and lovers of humanity, when they reckon up human advantages invariably leave out one? They don’t even take it into their reckoning in the form in which it should be taken, and the whole reckoning depends upon that. It would be no greater matter, they would simply have to take it, this advantage, and add it to the list. But the trouble is, that this strange advantage does not fall under any classification and is not in place in any list. I have a friend for instance ... Ech! gentlemen, but of course he is your friend, too; and indeed there is no one, no one to whom he is not a friend! When he prepares for any undertaking this gentleman immediately explains to you, elegantly and clearly, exactly how he must act in accordance with the laws of reason and truth. What is more, he will talk to you with excitement and passion of the true normal interests of man; with irony he will upbraid the short-sighted fools who do not understand their own interests, nor the true significance of virtue; and, within a quarter of an hour, without any sudden outside provocation, but simply through something inside him which is stronger than all his interests, he will go off on quite a different tack—that is, act in direct opposition to what he has just been saying about himself, in opposition to the laws of reason, in opposition to his own advantage, in fact in opposition to everything ... I warn you that my friend is a compound personality and therefore it is difficult to blame him as an individual. The fact is, gentlemen, it seems there must really exist something that is dearer to almost every man than his greatest advantages, or (not to be illogical) there is a most advantageous advantage (the very one omitted of which we spoke just now) which is more important and more advantageous than all other advantages, for the sake of which a man if necessary is ready to act in opposition to all laws; that is, in opposition to reason, honour, peace, prosperity—in fact, in opposition to all those excellent and useful things if only he can attain that fundamental, most advantageous advantage which is dearer to him than all. “Yes, but it’s advantage all the same,” you will retort. But excuse me, I’ll make the point clear, and it is not a case of playing upon words. What matters is, that this advantage is remarkable from the very fact that it breaks down all our classifications, and continually shatters every system constructed by lovers of mankind for the benefit of mankind. In fact, it upsets everything. But before I mention this advantage to you, I want to compromise myself personally, and therefore I boldly declare that all these fine systems, all these theories for explaining to mankind their real normal interests, in order that inevitably striving to pursue these interests they may at once become good and noble—are, in my opinion, so far, mere logical exercises! Yes, logical exercises. Why, to maintain this theory of the regeneration of mankind by means of the pursuit of his own advantage is to my mind almost the same thing ... as to affirm, for instance, following Buckle, that through civilisation mankind becomes softer, and consequently less bloodthirsty and less fitted for warfare. Logically it does seem to follow from his arguments. But man has such a predilection for systems and abstract deductions that he is ready to distort the truth intentionally, he is ready to deny the evidence of his senses only to justify his logic. I take this example because it is the most glaring instance of it. Only look about you: blood is being spilt in streams, and in the merriest way, as though it were champagne. Take the whole of the nineteenth century in which Buckle lived. Take Napoleon—the Great and also the present one. Take North America—the eternal union. Take the farce of Schleswig-Holstein.... And what is it that civilisation softens in us? The only gain of civilisation for mankind is the greater capacity for variety of sensations—and absolutely nothing more. And through the development of this many-sidedness man may come to finding enjoyment in bloodshed. In fact, this has already happened to him. Have you noticed that it is the most civilised gentlemen who have been the subtlest slaughterers, to whom the Attilas and Stenka Razins could not hold a candle, and if they are not so conspicuous as the Attilas and Stenka Razins it is simply because they are so often met with, are so ordinary and have become so familiar to us. In any case civilisation has made mankind if not more bloodthirsty, at least more vilely, more loathsomely bloodthirsty. In old days he saw justice in bloodshed and with his conscience at peace exterminated those he thought proper. Now we do think bloodshed abominable and yet we engage in this abomination, and with more energy than ever. Which is worse? Decide that for yourselves. They say that Cleopatra (excuse an instance from Roman history) was fond of sticking gold pins into her slave-girls’ breasts and derived gratification from their screams and writhings. You will say that that was in the comparatively barbarous times; that these are barbarous times too, because also, comparatively speaking, pins are stuck in even now; that though man has now learned to see more clearly than in barbarous ages, he is still far from having learnt to act as reason and science would dictate. But yet you are fully convinced that he will be sure to learn when he gets rid of certain old bad habits, and when common sense and science have completely re-educated human nature and turned it in a normal direction. You are confident that then man will cease from _intentional_ error and will, so to say, be compelled not to want to set his will against his normal interests. That is not all; then, you say, science itself will teach man (though to my mind it’s a superfluous luxury) that he never has really had any caprice or will of his own, and that he himself is something of the nature of a piano-key or the stop of an organ, and that there are, besides, things called the laws of nature; so that everything he does is not done by his willing it, but is done of itself, by the laws of nature. Consequently we have only to discover these laws of nature, and man will no longer have to answer for his actions and life will become exceedingly easy for him. All human actions will then, of course, be tabulated according to these laws, mathematically, like tables of logarithms up to 108,000, and entered in an index; or, better still, there would be published certain edifying works of the nature of encyclopaedic lexicons, in which everything will be so clearly calculated and explained that there will be no more incidents or adventures in the world. Then—this is all what you say—new economic relations will be established, all ready-made and worked out with mathematical exactitude, so that every possible question will vanish in the twinkling of an eye, simply because every possible answer to it will be provided. Then the “Palace of Crystal” will be built. Then ... In fact, those will be halcyon days. Of course there is no guaranteeing (this is my comment) that it will not be, for instance, frightfully dull then (for what will one have to do when everything will be calculated and tabulated), but on the other hand everything will be extraordinarily rational. Of course boredom may lead you to anything. It is boredom sets one sticking golden pins into people, but all that would not matter. What is bad (this is my comment again) is that I dare say people will be thankful for the gold pins then. Man is stupid, you know, phenomenally stupid; or rather he is not at all stupid, but he is so ungrateful that you could not find another like him in all creation. I, for instance, would not be in the least surprised if all of a sudden, _à propos_ of nothing, in the midst of general prosperity a gentleman with an ignoble, or rather with a reactionary and ironical, countenance were to arise and, putting his arms akimbo, say to us all: “I say, gentleman, hadn’t we better kick over the whole show and scatter rationalism to the winds, simply to send these logarithms to the devil, and to enable us to live once more at our own sweet foolish will!” That again would not matter, but what is annoying is that he would be sure to find followers—such is the nature of man. And all that for the most foolish reason, which, one would think, was hardly worth mentioning: that is, that man everywhere and at all times, whoever he may be, has preferred to act as he chose and not in the least as his reason and advantage dictated. And one may choose what is contrary to one’s own interests, and sometimes one _positively ought_ (that is my idea). One’s own free unfettered choice, one’s own caprice, however wild it may be, one’s own fancy worked up at times to frenzy—is that very “most advantageous advantage” which we have overlooked, which comes under no classification and against which all systems and theories are continually being shattered to atoms. And how do these wiseacres know that man wants a normal, a virtuous choice? What has made them conceive that man must want a rationally advantageous choice? What man wants is simply _independent_ choice, whatever that independence may cost and wherever it may lead. And choice, of course, the devil only knows what choice. VIII “Ha! ha! ha! But you know there is no such thing as choice in reality, say what you like,” you will interpose with a chuckle. “Science has succeeded in so far analysing man that we know already that choice and what is called freedom of will is nothing else than—” Stay, gentlemen, I meant to begin with that myself I confess, I was rather frightened. I was just going to say that the devil only knows what choice depends on, and that perhaps that was a very good thing, but I remembered the teaching of science ... and pulled myself up. And here you have begun upon it. Indeed, if there really is some day discovered a formula for all our desires and caprices—that is, an explanation of what they depend upon, by what laws they arise, how they develop, what they are aiming at in one case and in another and so on, that is a real mathematical formula—then, most likely, man will at once cease to feel desire, indeed, he will be certain to. For who would want to choose by rule? Besides, he will at once be transformed from a human being into an organ-stop or something of the sort; for what is a man without desires, without free will and without choice, if not a stop in an organ? What do you think? Let us reckon the chances—can such a thing happen or not? “H’m!” you decide. “Our choice is usually mistaken from a false view of our advantage. We sometimes choose absolute nonsense because in our foolishness we see in that nonsense the easiest means for attaining a supposed advantage. But when all that is explained and worked out on paper (which is perfectly possible, for it is contemptible and senseless to suppose that some laws of nature man will never understand), then certainly so-called desires will no longer exist. For if a desire should come into conflict with reason we shall then reason and not desire, because it will be impossible retaining our reason to be _senseless_ in our desires, and in that way knowingly act against reason and desire to injure ourselves. And as all choice and reasoning can be really calculated—because there will some day be discovered the laws of our so-called free will—so, joking apart, there may one day be something like a table constructed of them, so that we really shall choose in accordance with it. If, for instance, some day they calculate and prove to me that I made a long nose at someone because I could not help making a long nose at him and that I had to do it in that particular way, what _freedom_ is left me, especially if I am a learned man and have taken my degree somewhere? Then I should be able to calculate my whole life for thirty years beforehand. In short, if this could be arranged there would be nothing left for us to do; anyway, we should have to understand that. And, in fact, we ought unwearyingly to repeat to ourselves that at such and such a time and in such and such circumstances nature does not ask our leave; that we have got to take her as she is and not fashion her to suit our fancy, and if we really aspire to formulas and tables of rules, and well, even ... to the chemical retort, there’s no help for it, we must accept the retort too, or else it will be accepted without our consent....” Yes, but here I come to a stop! Gentlemen, you must excuse me for being over-philosophical; it’s the result of forty years underground! Allow me to indulge my fancy. You see, gentlemen, reason is an excellent thing, there’s no disputing that, but reason is nothing but reason and satisfies only the rational side of man’s nature, while will is a manifestation of the whole life, that is, of the whole human life including reason and all the impulses. And although our life, in this manifestation of it, is often worthless, yet it is life and not simply extracting square roots. Here I, for instance, quite naturally want to live, in order to satisfy all my capacities for life, and not simply my capacity for reasoning, that is, not simply one twentieth of my capacity for life. What does reason know? Reason only knows what it has succeeded in learning (some things, perhaps, it will never learn; this is a poor comfort, but why not say so frankly?) and human nature acts as a whole, with everything that is in it, consciously or unconsciously, and, even if it goes wrong, it lives. I suspect, gentlemen, that you are looking at me with compassion; you tell me again that an enlightened and developed man, such, in short, as the future man will be, cannot consciously desire anything disadvantageous to himself, that that can be proved mathematically. I thoroughly agree, it can—by mathematics. But I repeat for the hundredth time, there is one case, one only, when man may consciously, purposely, desire what is injurious to himself, what is stupid, very stupid—simply in order to have the right to desire for himself even what is very stupid and not to be bound by an obligation to desire only what is sensible. Of course, this very stupid thing, this caprice of ours, may be in reality, gentlemen, more advantageous for us than anything else on earth, especially in certain cases. And in particular it may be more advantageous than any advantage even when it does us obvious harm, and contradicts the soundest conclusions of our reason concerning our advantage—for in any circumstances it preserves for us what is most precious and most important—that is, our personality, our individuality. Some, you see, maintain that this really is the most precious thing for mankind; choice can, of course, if it chooses, be in agreement with reason; and especially if this be not abused but kept within bounds. It is profitable and sometimes even praiseworthy. But very often, and even most often, choice is utterly and stubbornly opposed to reason ... and ... and ... do you know that that, too, is profitable, sometimes even praiseworthy? Gentlemen, let us suppose that man is not stupid. (Indeed one cannot refuse to suppose that, if only from the one consideration, that, if man is stupid, then who is wise?) But if he is not stupid, he is monstrously ungrateful! Phenomenally ungrateful. In fact, I believe that the best definition of man is the ungrateful biped. But that is not all, that is not his worst defect; his worst defect is his perpetual moral obliquity, perpetual—from the days of the Flood to the Schleswig-Holstein period. Moral obliquity and consequently lack of good sense; for it has long been accepted that lack of good sense is due to no other cause than moral obliquity. Put it to the test and cast your eyes upon the history of mankind. What will you see? Is it a grand spectacle? Grand, if you like. Take the Colossus of Rhodes, for instance, that’s worth something. With good reason Mr. Anaevsky testifies of it that some say that it is the work of man’s hands, while others maintain that it has been created by nature herself. Is it many-coloured? May be it is many-coloured, too: if one takes the dress uniforms, military and civilian, of all peoples in all ages—that alone is worth something, and if you take the undress uniforms you will never get to the end of it; no historian would be equal to the job. Is it monotonous? May be it’s monotonous too: it’s fighting and fighting; they are fighting now, they fought first and they fought last—you will admit, that it is almost too monotonous. In short, one may say anything about the history of the world—anything that might enter the most disordered imagination. The only thing one can’t say is that it’s rational. The very word sticks in one’s throat. And, indeed, this is the odd thing that is continually happening: there are continually turning up in life moral and rational persons, sages and lovers of humanity who make it their object to live all their lives as morally and rationally as possible, to be, so to speak, a light to their neighbours simply in order to show them that it is possible to live morally and rationally in this world. And yet we all know that those very people sooner or later have been false to themselves, playing some queer trick, often a most unseemly one. Now I ask you: what can be expected of man since he is a being endowed with strange qualities? Shower upon him every earthly blessing, drown him in a sea of happiness, so that nothing but bubbles of bliss can be seen on the surface; give him economic prosperity, such that he should have nothing else to do but sleep, eat cakes and busy himself with the continuation of his species, and even then out of sheer ingratitude, sheer spite, man would play you some nasty trick. He would even risk his cakes and would deliberately desire the most fatal rubbish, the most uneconomical absurdity, simply to introduce into all this positive good sense his fatal fantastic element. It is just his fantastic dreams, his vulgar folly that he will desire to retain, simply in order to prove to himself—as though that were so necessary—that men still are men and not the keys of a piano, which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar. And that is not all: even if man really were nothing but a piano-key, even if this were proved to him by natural science and mathematics, even then he would not become reasonable, but would purposely do something perverse out of simple ingratitude, simply to gain his point. And if he does not find means he will contrive destruction and chaos, will contrive sufferings of all sorts, only to gain his point! He will launch a curse upon the world, and as only man can curse (it is his privilege, the primary distinction between him and other animals), may be by his curse alone he will attain his object—that is, convince himself that he is a man and not a piano-key! If you say that all this, too, can be calculated and tabulated—chaos and darkness and curses, so that the mere possibility of calculating it all beforehand would stop it all, and reason would reassert itself, then man would purposely go mad in order to be rid of reason and gain his point! I believe in it, I answer for it, for the whole work of man really seems to consist in nothing but proving to himself every minute that he is a man and not a piano-key! It may be at the cost of his skin, it may be by cannibalism! And this being so, can one help being tempted to rejoice that it has not yet come off, and that desire still depends on something we don’t know? You will scream at me (that is, if you condescend to do so) that no one is touching my free will, that all they are concerned with is that my will should of itself, of its own free will, coincide with my own normal interests, with the laws of nature and arithmetic. Good heavens, gentlemen, what sort of free will is left when we come to tabulation and arithmetic, when it will all be a case of twice two make four? Twice two makes four without my will. As if free will meant that! IX Gentlemen, I am joking, and I know myself that my jokes are not brilliant, but you know one can take everything as a joke. I am, perhaps, jesting against the grain. Gentlemen, I am tormented by questions; answer them for me. You, for instance, want to cure men of their old habits and reform their will in accordance with science and good sense. But how do you know, not only that it is possible, but also that it is _desirable_ to reform man in that way? And what leads you to the conclusion that man’s inclinations _need_ reforming? In short, how do you know that such a reformation will be a benefit to man? And to go to the root of the matter, why are you so positively convinced that not to act against his real normal interests guaranteed by the conclusions of reason and arithmetic is certainly always advantageous for man and must always be a law for mankind? So far, you know, this is only your supposition. It may be the law of logic, but not the law of humanity. You think, gentlemen, perhaps that I am mad? Allow me to defend myself. I agree that man is pre-eminently a creative animal, predestined to strive consciously for an object and to engage in engineering—that is, incessantly and eternally to make new roads, _wherever they may lead_. But the reason why he wants sometimes to go off at a tangent may just be that he is _predestined_ to make the road, and perhaps, too, that however stupid the “direct” practical man may be, the thought sometimes will occur to him that the road almost always does lead _somewhere_, and that the destination it leads to is less important than the process of making it, and that the chief thing is to save the well-conducted child from despising engineering, and so giving way to the fatal idleness, which, as we all know, is the mother of all the vices. Man likes to make roads and to create, that is a fact beyond dispute. But why has he such a passionate love for destruction and chaos also? Tell me that! But on that point I want to say a couple of words myself. May it not be that he loves chaos and destruction (there can be no disputing that he does sometimes love it) because he is instinctively afraid of attaining his object and completing the edifice he is constructing? Who knows, perhaps he only loves that edifice from a distance, and is by no means in love with it at close quarters; perhaps he only loves building it and does not want to live in it, but will leave it, when completed, for the use of _les animaux domestiques_—such as the ants, the sheep, and so on. Now the ants have quite a different taste. They have a marvellous edifice of that pattern which endures for ever—the ant-heap. With the ant-heap the respectable race of ants began and with the ant-heap they will probably end, which does the greatest credit to their perseverance and good sense. But man is a frivolous and incongruous creature, and perhaps, like a chess player, loves the process of the game, not the end of it. And who knows (there is no saying with certainty), perhaps the only goal on earth to which mankind is striving lies in this incessant process of attaining, in other words, in life itself, and not in the thing to be attained, which must always be expressed as a formula, as positive as twice two makes four, and such positiveness is not life, gentlemen, but is the beginning of death. Anyway, man has always been afraid of this mathematical certainty, and I am afraid of it now. Granted that man does nothing but seek that mathematical certainty, he traverses oceans, sacrifices his life in the quest, but to succeed, really to find it, dreads, I assure you. He feels that when he has found it there will be nothing for him to look for. When workmen have finished their work they do at least receive their pay, they go to the tavern, then they are taken to the police-station—and there is occupation for a week. But where can man go? Anyway, one can observe a certain awkwardness about him when he has attained such objects. He loves the process of attaining, but does not quite like to have attained, and that, of course, is very absurd. In fact, man is a comical creature; there seems to be a kind of jest in it all. But yet mathematical certainty is after all, something insufferable. Twice two makes four seems to me simply a piece of insolence. Twice two makes four is a pert coxcomb who stands with arms akimbo barring your path and spitting. I admit that twice two makes four is an excellent thing, but if we are to give everything its due, twice two makes five is sometimes a very charming thing too. And why are you so firmly, so triumphantly, convinced that only the normal and the positive—in other words, only what is conducive to welfare—is for the advantage of man? Is not reason in error as regards advantage? Does not man, perhaps, love something besides well-being? Perhaps he is just as fond of suffering? Perhaps suffering is just as great a benefit to him as well-being? Man is sometimes extraordinarily, passionately, in love with suffering, and that is a fact. There is no need to appeal to universal history to prove that; only ask yourself, if you are a man and have lived at all. As far as my personal opinion is concerned, to care only for well-being seems to me positively ill-bred. Whether it’s good or bad, it is sometimes very pleasant, too, to smash things. I hold no brief for suffering nor for well-being either. I am standing for ... my caprice, and for its being guaranteed to me when necessary. Suffering would be out of place in vaudevilles, for instance; I know that. In the “Palace of Crystal” it is unthinkable; suffering means doubt, negation, and what would be the good of a “palace of crystal” if there could be any doubt about it? And yet I think man will never renounce real suffering, that is, destruction and chaos. Why, suffering is the sole origin of consciousness. Though I did lay it down at the beginning that consciousness is the greatest misfortune for man, yet I know man prizes it and would not give it up for any satisfaction. Consciousness, for instance, is infinitely superior to twice two makes four. Once you have mathematical certainty there is nothing left to do or to understand. There will be nothing left but to bottle up your five senses and plunge into contemplation. While if you stick to consciousness, even though the same result is attained, you can at least flog yourself at times, and that will, at any rate, liven you up. Reactionary as it is, corporal punishment is better than nothing. X You believe in a palace of crystal that can never be destroyed—a palace at which one will not be able to put out one’s tongue or make a long nose on the sly. And perhaps that is just why I am afraid of this edifice, that it is of crystal and can never be destroyed and that one cannot put one’s tongue out at it even on the sly. You see, if it were not a palace, but a hen-house, I might creep into it to avoid getting wet, and yet I would not call the hen-house a palace out of gratitude to it for keeping me dry. You laugh and say that in such circumstances a hen-house is as good as a mansion. Yes, I answer, if one had to live simply to keep out of the rain. But what is to be done if I have taken it into my head that that is not the only object in life, and that if one must live one had better live in a mansion? That is my choice, my desire. You will only eradicate it when you have changed my preference. Well, do change it, allure me with something else, give me another ideal. But meanwhile I will not take a hen-house for a mansion. The palace of crystal may be an idle dream, it may be that it is inconsistent with the laws of nature and that I have invented it only through my own stupidity, through the old-fashioned irrational habits of my generation. But what does it matter to me that it is inconsistent? That makes no difference since it exists in my desires, or rather exists as long as my desires exist. Perhaps you are laughing again? Laugh away; I will put up with any mockery rather than pretend that I am satisfied when I am hungry. I know, anyway, that I will not be put off with a compromise, with a recurring zero, simply because it is consistent with the laws of nature and actually exists. I will not accept as the crown of my desires a block of buildings with tenements for the poor on a lease of a thousand years, and perhaps with a sign-board of a dentist hanging out. Destroy my desires, eradicate my ideals, show me something better, and I will follow you. You will say, perhaps, that it is not worth your trouble; but in that case I can give you the same answer. We are discussing things seriously; but if you won’t deign to give me your attention, I will drop your acquaintance. I can retreat into my underground hole. But while I am alive and have desires I would rather my hand were withered off than bring one brick to such a building! Don’t remind me that I have just rejected the palace of crystal for the sole reason that one cannot put out one’s tongue at it. I did not say because I am so fond of putting my tongue out. Perhaps the thing I resented was, that of all your edifices there has not been one at which one could not put out one’s tongue. On the contrary, I would let my tongue be cut off out of gratitude if things could be so arranged that I should lose all desire to put it out. It is not my fault that things cannot be so arranged, and that one must be satisfied with model flats. Then why am I made with such desires? Can I have been constructed simply in order to come to the conclusion that all my construction is a cheat? Can this be my whole purpose? I do not believe it. But do you know what: I am convinced that we underground folk ought to be kept on a curb. Though we may sit forty years underground without speaking, when we do come out into the light of day and break out we talk and talk and talk.... XI The long and the short of it is, gentlemen, that it is better to do nothing! Better conscious inertia! And so hurrah for underground! Though I have said that I envy the normal man to the last drop of my bile, yet I should not care to be in his place such as he is now (though I shall not cease envying him). No, no; anyway the underground life is more advantageous. There, at any rate, one can ... Oh, but even now I am lying! I am lying because I know myself that it is not underground that is better, but something different, quite different, for which I am thirsting, but which I cannot find! Damn underground! I will tell you another thing that would be better, and that is, if I myself believed in anything of what I have just written. I swear to you, gentlemen, there is not one thing, not one word of what I have written that I really believe. That is, I believe it, perhaps, but at the same time I feel and suspect that I am lying like a cobbler. “Then why have you written all this?” you will say to me. “I ought to put you underground for forty years without anything to do and then come to you in your cellar, to find out what stage you have reached! How can a man be left with nothing to do for forty years?” “Isn’t that shameful, isn’t that humiliating?” you will say, perhaps, wagging your heads contemptuously. “You thirst for life and try to settle the problems of life by a logical tangle. And how persistent, how insolent are your sallies, and at the same time what a scare you are in! You talk nonsense and are pleased with it; you say impudent things and are in continual alarm and apologising for them. You declare that you are afraid of nothing and at the same time try to ingratiate yourself in our good opinion. You declare that you are gnashing your teeth and at the same time you try to be witty so as to amuse us. You know that your witticisms are not witty, but you are evidently well satisfied with their literary value. You may, perhaps, have really suffered, but you have no respect for your own suffering. You may have sincerity, but you have no modesty; out of the pettiest vanity you expose your sincerity to publicity and ignominy. You doubtlessly mean to say something, but hide your last word through fear, because you have not the resolution to utter it, and only have a cowardly impudence. You boast of consciousness, but you are not sure of your ground, for though your mind works, yet your heart is darkened and corrupt, and you cannot have a full, genuine consciousness without a pure heart. And how intrusive you are, how you insist and grimace! Lies, lies, lies!” Of course I have myself made up all the things you say. That, too, is from underground. I have been for forty years listening to you through a crack under the floor. I have invented them myself, there was nothing else I could invent. It is no wonder that I have learned it by heart and it has taken a literary form.... But can you really be so credulous as to think that I will print all this and give it to you to read too? And another problem: why do I call you “gentlemen,” why do I address you as though you really were my readers? Such confessions as I intend to make are never printed nor given to other people to read. Anyway, I am not strong-minded enough for that, and I don’t see why I should be. But you see a fancy has occurred to me and I want to realise it at all costs. Let me explain. Every man has reminiscences which he would not tell to everyone, but only to his friends. He has other matters in his mind which he would not reveal even to his friends, but only to himself, and that in secret. But there are other things which a man is afraid to tell even to himself, and every decent man has a number of such things stored away in his mind. The more decent he is, the greater the number of such things in his mind. Anyway, I have only lately determined to remember some of my early adventures. Till now I have always avoided them, even with a certain uneasiness. Now, when I am not only recalling them, but have actually decided to write an account of them, I want to try the experiment whether one can, even with oneself, be perfectly open and not take fright at the whole truth. I will observe, in parenthesis, that Heine says that a true autobiography is almost an impossibility, and that man is bound to lie about himself. He considers that Rousseau certainly told lies about himself in his confessions, and even intentionally lied, out of vanity. I am convinced that Heine is right; I quite understand how sometimes one may, out of sheer vanity, attribute regular crimes to oneself, and indeed I can very well conceive that kind of vanity. But Heine judged of people who made their confessions to the public. I write only for myself, and I wish to declare once and for all that if I write as though I were addressing readers, that is simply because it is easier for me to write in that form. It is a form, an empty form—I shall never have readers. I have made this plain already ... I don’t wish to be hampered by any restrictions in the compilation of my notes. I shall not attempt any system or method. I will jot things down as I remember them. But here, perhaps, someone will catch at the word and ask me: if you really don’t reckon on readers, why do you make such compacts with yourself—and on paper too—that is, that you won’t attempt any system or method, that you jot things down as you remember them, and so on, and so on? Why are you explaining? Why do you apologise? Well, there it is, I answer. There is a whole psychology in all this, though. Perhaps it is simply that I am a coward. And perhaps that I purposely imagine an audience before me in order that I may be more dignified while I write. There are perhaps thousands of reasons. Again, what is my object precisely in writing? If it is not for the benefit of the public why should I not simply recall these incidents in my own mind without putting them on paper? Quite so; but yet it is more imposing on paper. There is something more impressive in it; I shall be better able to criticise myself and improve my style. Besides, I shall perhaps obtain actual relief from writing. Today, for instance, I am particularly oppressed by one memory of a distant past. It came back vividly to my mind a few days ago, and has remained haunting me like an annoying tune that one cannot get rid of. And yet I must get rid of it somehow. I have hundreds of such reminiscences; but at times some one stands out from the hundred and oppresses me. For some reason I believe that if I write it down I should get rid of it. Why not try? Besides, I am bored, and I never have anything to do. Writing will be a sort of work. They say work makes man kind-hearted and honest. Well, here is a chance for me, anyway. Snow is falling today, yellow and dingy. It fell yesterday, too, and a few days ago. I fancy it is the wet snow that has reminded me of that incident which I cannot shake off now. And so let it be a story _à propos_ of the falling snow. PART II À Propos of the Wet Snow When from dark error’s subjugation My words of passionate exhortation Had wrenched thy fainting spirit free; And writhing prone in thine affliction Thou didst recall with malediction The vice that had encompassed thee: And when thy slumbering conscience, fretting By recollection’s torturing flame, Thou didst reveal the hideous setting Of thy life’s current ere I came: When suddenly I saw thee sicken, And weeping, hide thine anguished face, Revolted, maddened, horror-stricken, At memories of foul disgrace. NEKRASSOV (_translated by Juliet Soskice_). I At that time I was only twenty-four. My life was even then gloomy, ill-regulated, and as solitary as that of a savage. I made friends with no one and positively avoided talking, and buried myself more and more in my hole. At work in the office I never looked at anyone, and was perfectly well aware that my companions looked upon me, not only as a queer fellow, but even looked upon me—I always fancied this—with a sort of loathing. I sometimes wondered why it was that nobody except me fancied that he was looked upon with aversion? One of the clerks had a most repulsive, pock-marked face, which looked positively villainous. I believe I should not have dared to look at anyone with such an unsightly countenance. Another had such a very dirty old uniform that there was an unpleasant odour in his proximity. Yet not one of these gentlemen showed the slightest self-consciousness—either about their clothes or their countenance or their character in any way. Neither of them ever imagined that they were looked at with repulsion; if they had imagined it they would not have minded—so long as their superiors did not look at them in that way. It is clear to me now that, owing to my unbounded vanity and to the high standard I set for myself, I often looked at myself with furious discontent, which verged on loathing, and so I inwardly attributed the same feeling to everyone. I hated my face, for instance: I thought it disgusting, and even suspected that there was something base in my expression, and so every day when I turned up at the office I tried to behave as independently as possible, and to assume a lofty expression, so that I might not be suspected of being abject. “My face may be ugly,” I thought, “but let it be lofty, expressive, and, above all, _extremely_ intelligent.” But I was positively and painfully certain that it was impossible for my countenance ever to express those qualities. And what was worst of all, I thought it actually stupid looking, and I would have been quite satisfied if I could have looked intelligent. In fact, I would even have put up with looking base if, at the same time, my face could have been thought strikingly intelligent. Of course, I hated my fellow clerks one and all, and I despised them all, yet at the same time I was, as it were, afraid of them. In fact, it happened at times that I thought more highly of them than of myself. It somehow happened quite suddenly that I alternated between despising them and thinking them superior to myself. A cultivated and decent man cannot be vain without setting a fearfully high standard for himself, and without despising and almost hating himself at certain moments. But whether I despised them or thought them superior I dropped my eyes almost every time I met anyone. I even made experiments whether I could face so and so’s looking at me, and I was always the first to drop my eyes. This worried me to distraction. I had a sickly dread, too, of being ridiculous, and so had a slavish passion for the conventional in everything external. I loved to fall into the common rut, and had a whole-hearted terror of any kind of eccentricity in myself. But how could I live up to it? I was morbidly sensitive as a man of our age should be. They were all stupid, and as like one another as so many sheep. Perhaps I was the only one in the office who fancied that I was a coward and a slave, and I fancied it just because I was more highly developed. But it was not only that I fancied it, it really was so. I was a coward and a slave. I say this without the slightest embarrassment. Every decent man of our age must be a coward and a slave. That is his normal condition. Of that I am firmly persuaded. He is made and constructed to that very end. And not only at the present time owing to some casual circumstances, but always, at all times, a decent man is bound to be a coward and a slave. It is the law of nature for all decent people all over the earth. If anyone of them happens to be valiant about something, he need not be comforted nor carried away by that; he would show the white feather just the same before something else. That is how it invariably and inevitably ends. Only donkeys and mules are valiant, and they only till they are pushed up to the wall. It is not worth while to pay attention to them for they really are of no consequence. Another circumstance, too, worried me in those days: that there was no one like me and I was unlike anyone else. “I am alone and they are _everyone_,” I thought—and pondered. From that it is evident that I was still a youngster. The very opposite sometimes happened. It was loathsome sometimes to go to the office; things reached such a point that I often came home ill. But all at once, _à propos_ of nothing, there would come a phase of scepticism and indifference (everything happened in phases to me), and I would laugh myself at my intolerance and fastidiousness, I would reproach myself with being _romantic_. At one time I was unwilling to speak to anyone, while at other times I would not only talk, but go to the length of contemplating making friends with them. All my fastidiousness would suddenly, for no rhyme or reason, vanish. Who knows, perhaps I never had really had it, and it had simply been affected, and got out of books. I have not decided that question even now. Once I quite made friends with them, visited their homes, played preference, drank vodka, talked of promotions.... But here let me make a digression. We Russians, speaking generally, have never had those foolish transcendental “romantics”—German, and still more French—on whom nothing produces any effect; if there were an earthquake, if all France perished at the barricades, they would still be the same, they would not even have the decency to affect a change, but would still go on singing their transcendental songs to the hour of their death, because they are fools. We, in Russia, have no fools; that is well known. That is what distinguishes us from foreign lands. Consequently these transcendental natures are not found amongst us in their pure form. The idea that they are is due to our “realistic” journalists and critics of that day, always on the look out for Kostanzhoglos and Uncle Pyotr Ivanitchs and foolishly accepting them as our ideal; they have slandered our romantics, taking them for the same transcendental sort as in Germany or France. On the contrary, the characteristics of our “romantics” are absolutely and directly opposed to the transcendental European type, and no European standard can be applied to them. (Allow me to make use of this word “romantic”—an old-fashioned and much respected word which has done good service and is familiar to all.) The characteristics of our romantic are to understand everything, _to see everything and to see it often incomparably more clearly than our most realistic minds see it;_ to refuse to accept anyone or anything, but at the same time not to despise anything; to give way, to yield, from policy; never to lose sight of a useful practical object (such as rent-free quarters at the government expense, pensions, decorations), to keep their eye on that object through all the enthusiasms and volumes of lyrical poems, and at the same time to preserve “the sublime and the beautiful” inviolate within them to the hour of their death, and to preserve themselves also, incidentally, like some precious jewel wrapped in cotton wool if only for the benefit of “the sublime and the beautiful.” Our “romantic” is a man of great breadth and the greatest rogue of all our rogues, I assure you.... I can assure you from experience, indeed. Of course, that is, if he is intelligent. But what am I saying! The romantic is always intelligent, and I only meant to observe that although we have had foolish romantics they don’t count, and they were only so because in the flower of their youth they degenerated into Germans, and to preserve their precious jewel more comfortably, settled somewhere out there—by preference in Weimar or the Black Forest. I, for instance, genuinely despised my official work and did not openly abuse it simply because I was in it myself and got a salary for it. Anyway, take note, I did not openly abuse it. Our romantic would rather go out of his mind—a thing, however, which very rarely happens—than take to open abuse, unless he had some other career in view; and he is never kicked out. At most, they would take him to the lunatic asylum as “the King of Spain” if he should go very mad. But it is only the thin, fair people who go out of their minds in Russia. Innumerable “romantics” attain later in life to considerable rank in the service. Their many-sidedness is remarkable! And what a faculty they have for the most contradictory sensations! I was comforted by this thought even in those days, and I am of the same opinion now. That is why there are so many “broad natures” among us who never lose their ideal even in the depths of degradation; and though they never stir a finger for their ideal, though they are arrant thieves and knaves, yet they tearfully cherish their first ideal and are extraordinarily honest at heart. Yes, it is only among us that the most incorrigible rogue can be absolutely and loftily honest at heart without in the least ceasing to be a rogue. I repeat, our romantics, frequently, become such accomplished rascals (I use the term “rascals” affectionately), suddenly display such a sense of reality and practical knowledge that their bewildered superiors and the public generally can only ejaculate in amazement. Their many-sidedness is really amazing, and goodness knows what it may develop into later on, and what the future has in store for us. It is not a poor material! I do not say this from any foolish or boastful patriotism. But I feel sure that you are again imagining that I am joking. Or perhaps it’s just the contrary and you are convinced that I really think so. Anyway, gentlemen, I shall welcome both views as an honour and a special favour. And do forgive my digression. I did not, of course, maintain friendly relations with my comrades and soon was at loggerheads with them, and in my youth and inexperience I even gave up bowing to them, as though I had cut off all relations. That, however, only happened to me once. As a rule, I was always alone. In the first place I spent most of my time at home, reading. I tried to stifle all that was continually seething within me by means of external impressions. And the only external means I had was reading. Reading, of course, was a great help—exciting me, giving me pleasure and pain. But at times it bored me fearfully. One longed for movement in spite of everything, and I plunged all at once into dark, underground, loathsome vice of the pettiest kind. My wretched passions were acute, smarting, from my continual, sickly irritability I had hysterical impulses, with tears and convulsions. I had no resource except reading, that is, there was nothing in my surroundings which I could respect and which attracted me. I was overwhelmed with depression, too; I had an hysterical craving for incongruity and for contrast, and so I took to vice. I have not said all this to justify myself.... But, no! I am lying. I did want to justify myself. I make that little observation for my own benefit, gentlemen. I don’t want to lie. I vowed to myself I would not. And so, furtively, timidly, in solitude, at night, I indulged in filthy vice, with a feeling of shame which never deserted me, even at the most loathsome moments, and which at such moments nearly made me curse. Already even then I had my underground world in my soul. I was fearfully afraid of being seen, of being met, of being recognised. I visited various obscure haunts. One night as I was passing a tavern I saw through a lighted window some gentlemen fighting with billiard cues, and saw one of them thrown out of the window. At other times I should have felt very much disgusted, but I was in such a mood at the time, that I actually envied the gentleman thrown out of the window—and I envied him so much that I even went into the tavern and into the billiard-room. “Perhaps,” I thought, “I’ll have a fight, too, and they’ll throw me out of the window.” I was not drunk—but what is one to do—depression will drive a man to such a pitch of hysteria? But nothing happened. It seemed that I was not even equal to being thrown out of the window and I went away without having my fight. An officer put me in my place from the first moment. I was standing by the billiard-table and in my ignorance blocking up the way, and he wanted to pass; he took me by the shoulders and without a word—without a warning or explanation—moved me from where I was standing to another spot and passed by as though he had not noticed me. I could have forgiven blows, but I could not forgive his having moved me without noticing me. Devil knows what I would have given for a real regular quarrel—a more decent, a more _literary_ one, so to speak. I had been treated like a fly. This officer was over six foot, while I was a spindly little fellow. But the quarrel was in my hands. I had only to protest and I certainly would have been thrown out of the window. But I changed my mind and preferred to beat a resentful retreat. I went out of the tavern straight home, confused and troubled, and the next night I went out again with the same lewd intentions, still more furtively, abjectly and miserably than before, as it were, with tears in my eyes—but still I did go out again. Don’t imagine, though, it was cowardice made me slink away from the officer; I never have been a coward at heart, though I have always been a coward in action. Don’t be in a hurry to laugh—I assure you I can explain it all. Oh, if only that officer had been one of the sort who would consent to fight a duel! But no, he was one of those gentlemen (alas, long extinct!) who preferred fighting with cues or, like Gogol’s Lieutenant Pirogov, appealing to the police. They did not fight duels and would have thought a duel with a civilian like me an utterly unseemly procedure in any case—and they looked upon the duel altogether as something impossible, something free-thinking and French. But they were quite ready to bully, especially when they were over six foot. I did not slink away through cowardice, but through an unbounded vanity. I was afraid not of his six foot, not of getting a sound thrashing and being thrown out of the window; I should have had physical courage enough, I assure you; but I had not the moral courage. What I was afraid of was that everyone present, from the insolent marker down to the lowest little stinking, pimply clerk in a greasy collar, would jeer at me and fail to understand when I began to protest and to address them in literary language. For of the point of honour—not of honour, but of the point of honour (_point d’honneur_)—one cannot speak among us except in literary language. You can’t allude to the “point of honour” in ordinary language. I was fully convinced (the sense of reality, in spite of all my romanticism!) that they would all simply split their sides with laughter, and that the officer would not simply beat me, that is, without insulting me, but would certainly prod me in the back with his knee, kick me round the billiard-table, and only then perhaps have pity and drop me out of the window. Of course, this trivial incident could not with me end in that. I often met that officer afterwards in the street and noticed him very carefully. I am not quite sure whether he recognised me, I imagine not; I judge from certain signs. But I—I stared at him with spite and hatred and so it went on ... for several years! My resentment grew even deeper with years. At first I began making stealthy inquiries about this officer. It was difficult for me to do so, for I knew no one. But one day I heard someone shout his surname in the street as I was following him at a distance, as though I were tied to him—and so I learnt his surname. Another time I followed him to his flat, and for ten kopecks learned from the porter where he lived, on which storey, whether he lived alone or with others, and so on—in fact, everything one could learn from a porter. One morning, though I had never tried my hand with the pen, it suddenly occurred to me to write a satire on this officer in the form of a novel which would unmask his villainy. I wrote the novel with relish. I did unmask his villainy, I even exaggerated it; at first I so altered his surname that it could easily be recognised, but on second thoughts I changed it, and sent the story to the _Otetchestvenniya Zapiski_. But at that time such attacks were not the fashion and my story was not printed. That was a great vexation to me. Sometimes I was positively choked with resentment. At last I determined to challenge my enemy to a duel. I composed a splendid, charming letter to him, imploring him to apologise to me, and hinting rather plainly at a duel in case of refusal. The letter was so composed that if the officer had had the least understanding of the sublime and the beautiful he would certainly have flung himself on my neck and have offered me his friendship. And how fine that would have been! How we should have got on together! “He could have shielded me with his higher rank, while I could have improved his mind with my culture, and, well ... my ideas, and all sorts of things might have happened.” Only fancy, this was two years after his insult to me, and my challenge would have been a ridiculous anachronism, in spite of all the ingenuity of my letter in disguising and explaining away the anachronism. But, thank God (to this day I thank the Almighty with tears in my eyes) I did not send the letter to him. Cold shivers run down my back when I think of what might have happened if I had sent it. And all at once I revenged myself in the simplest way, by a stroke of genius! A brilliant thought suddenly dawned upon me. Sometimes on holidays I used to stroll along the sunny side of the Nevsky about four o’clock in the afternoon. Though it was hardly a stroll so much as a series of innumerable miseries, humiliations and resentments; but no doubt that was just what I wanted. I used to wriggle along in a most unseemly fashion, like an eel, continually moving aside to make way for generals, for officers of the guards and the hussars, or for ladies. At such minutes there used to be a convulsive twinge at my heart, and I used to feel hot all down my back at the mere thought of the wretchedness of my attire, of the wretchedness and abjectness of my little scurrying figure. This was a regular martyrdom, a continual, intolerable humiliation at the thought, which passed into an incessant and direct sensation, that I was a mere fly in the eyes of all this world, a nasty, disgusting fly—more intelligent, more highly developed, more refined in feeling than any of them, of course—but a fly that was continually making way for everyone, insulted and injured by everyone. Why I inflicted this torture upon myself, why I went to the Nevsky, I don’t know. I felt simply drawn there at every possible opportunity. Already then I began to experience a rush of the enjoyment of which I spoke in the first chapter. After my affair with the officer I felt even more drawn there than before: it was on the Nevsky that I met him most frequently, there I could admire him. He, too, went there chiefly on holidays, He, too, turned out of his path for generals and persons of high rank, and he too, wriggled between them like an eel; but people, like me, or even better dressed than me, he simply walked over; he made straight for them as though there was nothing but empty space before him, and never, under any circumstances, turned aside. I gloated over my resentment watching him and ... always resentfully made way for him. It exasperated me that even in the street I could not be on an even footing with him. “Why must you invariably be the first to move aside?” I kept asking myself in hysterical rage, waking up sometimes at three o’clock in the morning. “Why is it you and not he? There’s no regulation about it; there’s no written law. Let the making way be equal as it usually is when refined people meet; he moves half-way and you move half-way; you pass with mutual respect.” But that never happened, and I always moved aside, while he did not even notice my making way for him. And lo and behold a bright idea dawned upon me! “What,” I thought, “if I meet him and don’t move on one side? What if I don’t move aside on purpose, even if I knock up against him? How would that be?” This audacious idea took such a hold on me that it gave me no peace. I was dreaming of it continually, horribly, and I purposely went more frequently to the Nevsky in order to picture more vividly how I should do it when I did do it. I was delighted. This intention seemed to me more and more practical and possible. “Of course I shall not really push him,” I thought, already more good-natured in my joy. “I will simply not turn aside, will run up against him, not very violently, but just shouldering each other—just as much as decency permits. I will push against him just as much as he pushes against me.” At last I made up my mind completely. But my preparations took a great deal of time. To begin with, when I carried out my plan I should need to be looking rather more decent, and so I had to think of my get-up. “In case of emergency, if, for instance, there were any sort of public scandal (and the public there is of the most _recherché:_ the Countess walks there; Prince D. walks there; all the literary world is there), I must be well dressed; that inspires respect and of itself puts us on an equal footing in the eyes of the society.” With this object I asked for some of my salary in advance, and bought at Tchurkin’s a pair of black gloves and a decent hat. Black gloves seemed to me both more dignified and _bon ton_ than the lemon-coloured ones which I had contemplated at first. “The colour is too gaudy, it looks as though one were trying to be conspicuous,” and I did not take the lemon-coloured ones. I had got ready long beforehand a good shirt, with white bone studs; my overcoat was the only thing that held me back. The coat in itself was a very good one, it kept me warm; but it was wadded and it had a raccoon collar which was the height of vulgarity. I had to change the collar at any sacrifice, and to have a beaver one like an officer’s. For this purpose I began visiting the Gostiny Dvor and after several attempts I pitched upon a piece of cheap German beaver. Though these German beavers soon grow shabby and look wretched, yet at first they look exceedingly well, and I only needed it for the occasion. I asked the price; even so, it was too expensive. After thinking it over thoroughly I decided to sell my raccoon collar. The rest of the money—a considerable sum for me, I decided to borrow from Anton Antonitch Syetotchkin, my immediate superior, an unassuming person, though grave and judicious. He never lent money to anyone, but I had, on entering the service, been specially recommended to him by an important personage who had got me my berth. I was horribly worried. To borrow from Anton Antonitch seemed to me monstrous and shameful. I did not sleep for two or three nights. Indeed, I did not sleep well at that time, I was in a fever; I had a vague sinking at my heart or else a sudden throbbing, throbbing, throbbing! Anton Antonitch was surprised at first, then he frowned, then he reflected, and did after all lend me the money, receiving from me a written authorisation to take from my salary a fortnight later the sum that he had lent me. In this way everything was at last ready. The handsome beaver replaced the mean-looking raccoon, and I began by degrees to get to work. It would never have done to act offhand, at random; the plan had to be carried out skilfully, by degrees. But I must confess that after many efforts I began to despair: we simply could not run into each other. I made every preparation, I was quite determined—it seemed as though we should run into one another directly—and before I knew what I was doing I had stepped aside for him again and he had passed without noticing me. I even prayed as I approached him that God would grant me determination. One time I had made up my mind thoroughly, but it ended in my stumbling and falling at his feet because at the very last instant when I was six inches from him my courage failed me. He very calmly stepped over me, while I flew on one side like a ball. That night I was ill again, feverish and delirious. And suddenly it ended most happily. The night before I had made up my mind not to carry out my fatal plan and to abandon it all, and with that object I went to the Nevsky for the last time, just to see how I would abandon it all. Suddenly, three paces from my enemy, I unexpectedly made up my mind—I closed my eyes, and we ran full tilt, shoulder to shoulder, against one another! I did not budge an inch and passed him on a perfectly equal footing! He did not even look round and pretended not to notice it; but he was only pretending, I am convinced of that. I am convinced of that to this day! Of course, I got the worst of it—he was stronger, but that was not the point. The point was that I had attained my object, I had kept up my dignity, I had not yielded a step, and had put myself publicly on an equal social footing with him. I returned home feeling that I was fully avenged for everything. I was delighted. I was triumphant and sang Italian arias. Of course, I will not describe to you what happened to me three days later; if you have read my first chapter you can guess for yourself. The officer was afterwards transferred; I have not seen him now for fourteen years. What is the dear fellow doing now? Whom is he walking over? II But the period of my dissipation would end and I always felt very sick afterwards. It was followed by remorse—I tried to drive it away; I felt too sick. By degrees, however, I grew used to that too. I grew used to everything, or rather I voluntarily resigned myself to enduring it. But I had a means of escape that reconciled everything—that was to find refuge in “the sublime and the beautiful,” in dreams, of course. I was a terrible dreamer, I would dream for three months on end, tucked away in my corner, and you may believe me that at those moments I had no resemblance to the gentleman who, in the perturbation of his chicken heart, put a collar of German beaver on his great-coat. I suddenly became a hero. I would not have admitted my six-foot lieutenant even if he had called on me. I could not even picture him before me then. What were my dreams and how I could satisfy myself with them—it is hard to say now, but at the time I was satisfied with them. Though, indeed, even now, I am to some extent satisfied with them. Dreams were particularly sweet and vivid after a spell of dissipation; they came with remorse and with tears, with curses and transports. There were moments of such positive intoxication, of such happiness, that there was not the faintest trace of irony within me, on my honour. I had faith, hope, love. I believed blindly at such times that by some miracle, by some external circumstance, all this would suddenly open out, expand; that suddenly a vista of suitable activity—beneficent, good, and, above all, _ready made_ (what sort of activity I had no idea, but the great thing was that it should be all ready for me)—would rise up before me—and I should come out into the light of day, almost riding a white horse and crowned with laurel. Anything but the foremost place I could not conceive for myself, and for that very reason I quite contentedly occupied the lowest in reality. Either to be a hero or to grovel in the mud—there was nothing between. That was my ruin, for when I was in the mud I comforted myself with the thought that at other times I was a hero, and the hero was a cloak for the mud: for an ordinary man it was shameful to defile himself, but a hero was too lofty to be utterly defiled, and so he might defile himself. It is worth noting that these attacks of the “sublime and the beautiful” visited me even during the period of dissipation and just at the times when I was touching the bottom. They came in separate spurts, as though reminding me of themselves, but did not banish the dissipation by their appearance. On the contrary, they seemed to add a zest to it by contrast, and were only sufficiently present to serve as an appetising sauce. That sauce was made up of contradictions and sufferings, of agonising inward analysis, and all these pangs and pin-pricks gave a certain piquancy, even a significance to my dissipation—in fact, completely answered the purpose of an appetising sauce. There was a certain depth of meaning in it. And I could hardly have resigned myself to the simple, vulgar, direct debauchery of a clerk and have endured all the filthiness of it. What could have allured me about it then and have drawn me at night into the street? No, I had a lofty way of getting out of it all. And what loving-kindness, oh Lord, what loving-kindness I felt at times in those dreams of mine! in those “flights into the sublime and the beautiful”; though it was fantastic love, though it was never applied to anything human in reality, yet there was so much of this love that one did not feel afterwards even the impulse to apply it in reality; that would have been superfluous. Everything, however, passed satisfactorily by a lazy and fascinating transition into the sphere of art, that is, into the beautiful forms of life, lying ready, largely stolen from the poets and novelists and adapted to all sorts of needs and uses. I, for instance, was triumphant over everyone; everyone, of course, was in dust and ashes, and was forced spontaneously to recognise my superiority, and I forgave them all. I was a poet and a grand gentleman, I fell in love; I came in for countless millions and immediately devoted them to humanity, and at the same time I confessed before all the people my shameful deeds, which, of course, were not merely shameful, but had in them much that was “sublime and beautiful” something in the Manfred style. Everyone would kiss me and weep (what idiots they would be if they did not), while I should go barefoot and hungry preaching new ideas and fighting a victorious Austerlitz against the obscurantists. Then the band would play a march, an amnesty would be declared, the Pope would agree to retire from Rome to Brazil; then there would be a ball for the whole of Italy at the Villa Borghese on the shores of Lake Como, Lake Como being for that purpose transferred to the neighbourhood of Rome; then would come a scene in the bushes, and so on, and so on—as though you did not know all about it? You will say that it is vulgar and contemptible to drag all this into public after all the tears and transports which I have myself confessed. But why is it contemptible? Can you imagine that I am ashamed of it all, and that it was stupider than anything in your life, gentlemen? And I can assure you that some of these fancies were by no means badly composed.... It did not all happen on the shores of Lake Como. And yet you are right—it really is vulgar and contemptible. And most contemptible of all it is that now I am attempting to justify myself to you. And even more contemptible than that is my making this remark now. But that’s enough, or there will be no end to it; each step will be more contemptible than the last.... I could never stand more than three months of dreaming at a time without feeling an irresistible desire to plunge into society. To plunge into society meant to visit my superior at the office, Anton Antonitch Syetotchkin. He was the only permanent acquaintance I have had in my life, and I wonder at the fact myself now. But I only went to see him when that phase came over me, and when my dreams had reached such a point of bliss that it became essential at once to embrace my fellows and all mankind; and for that purpose I needed, at least, one human being, actually existing. I had to call on Anton Antonitch, however, on Tuesday—his at-home day; so I had always to time my passionate desire to embrace humanity so that it might fall on a Tuesday. This Anton Antonitch lived on the fourth storey in a house in Five Corners, in four low-pitched rooms, one smaller than the other, of a particularly frugal and sallow appearance. He had two daughters and their aunt, who used to pour out the tea. Of the daughters one was thirteen and another fourteen, they both had snub noses, and I was awfully shy of them because they were always whispering and giggling together. The master of the house usually sat in his study on a leather couch in front of the table with some grey-headed gentleman, usually a colleague from our office or some other department. I never saw more than two or three visitors there, always the same. They talked about the excise duty; about business in the senate, about salaries, about promotions, about His Excellency, and the best means of pleasing him, and so on. I had the patience to sit like a fool beside these people for four hours at a stretch, listening to them without knowing what to say to them or venturing to say a word. I became stupefied, several times I felt myself perspiring, I was overcome by a sort of paralysis; but this was pleasant and good for me. On returning home I deferred for a time my desire to embrace all mankind. I had however one other acquaintance of a sort, Simonov, who was an old schoolfellow. I had a number of schoolfellows, indeed, in Petersburg, but I did not associate with them and had even given up nodding to them in the street. I believe I had transferred into the department I was in simply to avoid their company and to cut off all connection with my hateful childhood. Curses on that school and all those terrible years of penal servitude! In short, I parted from my schoolfellows as soon as I got out into the world. There were two or three left to whom I nodded in the street. One of them was Simonov, who had in no way been distinguished at school, was of a quiet and equable disposition; but I discovered in him a certain independence of character and even honesty I don’t even suppose that he was particularly stupid. I had at one time spent some rather soulful moments with him, but these had not lasted long and had somehow been suddenly clouded over. He was evidently uncomfortable at these reminiscences, and was, I fancy, always afraid that I might take up the same tone again. I suspected that he had an aversion for me, but still I went on going to see him, not being quite certain of it. And so on one occasion, unable to endure my solitude and knowing that as it was Thursday Anton Antonitch’s door would be closed, I thought of Simonov. Climbing up to his fourth storey I was thinking that the man disliked me and that it was a mistake to go and see him. But as it always happened that such reflections impelled me, as though purposely, to put myself into a false position, I went in. It was almost a year since I had last seen Simonov. III I found two of my old schoolfellows with him. They seemed to be discussing an important matter. All of them took scarcely any notice of my entrance, which was strange, for I had not met them for years. Evidently they looked upon me as something on the level of a common fly. I had not been treated like that even at school, though they all hated me. I knew, of course, that they must despise me now for my lack of success in the service, and for my having let myself sink so low, going about badly dressed and so on—which seemed to them a sign of my incapacity and insignificance. But I had not expected such contempt. Simonov was positively surprised at my turning up. Even in old days he had always seemed surprised at my coming. All this disconcerted me: I sat down, feeling rather miserable, and began listening to what they were saying. They were engaged in warm and earnest conversation about a farewell dinner which they wanted to arrange for the next day to a comrade of theirs called Zverkov, an officer in the army, who was going away to a distant province. This Zverkov had been all the time at school with me too. I had begun to hate him particularly in the upper forms. In the lower forms he had simply been a pretty, playful boy whom everybody liked. I had hated him, however, even in the lower forms, just because he was a pretty and playful boy. He was always bad at his lessons and got worse and worse as he went on; however, he left with a good certificate, as he had powerful interests. During his last year at school he came in for an estate of two hundred serfs, and as almost all of us were poor he took up a swaggering tone among us. He was vulgar in the extreme, but at the same time he was a good-natured fellow, even in his swaggering. In spite of superficial, fantastic and sham notions of honour and dignity, all but very few of us positively grovelled before Zverkov, and the more so the more he swaggered. And it was not from any interested motive that they grovelled, but simply because he had been favoured by the gifts of nature. Moreover, it was, as it were, an accepted idea among us that Zverkov was a specialist in regard to tact and the social graces. This last fact particularly infuriated me. I hated the abrupt self-confident tone of his voice, his admiration of his own witticisms, which were often frightfully stupid, though he was bold in his language; I hated his handsome, but stupid face (for which I would, however, have gladly exchanged my intelligent one), and the free-and-easy military manners in fashion in the “’forties.” I hated the way in which he used to talk of his future conquests of women (he did not venture to begin his attack upon women until he had the epaulettes of an officer, and was looking forward to them with impatience), and boasted of the duels he would constantly be fighting. I remember how I, invariably so taciturn, suddenly fastened upon Zverkov, when one day talking at a leisure moment with his schoolfellows of his future relations with the fair sex, and growing as sportive as a puppy in the sun, he all at once declared that he would not leave a single village girl on his estate unnoticed, that that was his _droit de seigneur_, and that if the peasants dared to protest he would have them all flogged and double the tax on them, the bearded rascals. Our servile rabble applauded, but I attacked him, not from compassion for the girls and their fathers, but simply because they were applauding such an insect. I got the better of him on that occasion, but though Zverkov was stupid he was lively and impudent, and so laughed it off, and in such a way that my victory was not really complete; the laugh was on his side. He got the better of me on several occasions afterwards, but without malice, jestingly, casually. I remained angrily and contemptuously silent and would not answer him. When we left school he made advances to me; I did not rebuff them, for I was flattered, but we soon parted and quite naturally. Afterwards I heard of his barrack-room success as a lieutenant, and of the fast life he was leading. Then there came other rumours—of his successes in the service. By then he had taken to cutting me in the street, and I suspected that he was afraid of compromising himself by greeting a personage as insignificant as me. I saw him once in the theatre, in the third tier of boxes. By then he was wearing shoulder-straps. He was twisting and twirling about, ingratiating himself with the daughters of an ancient General. In three years he had gone off considerably, though he was still rather handsome and adroit. One could see that by the time he was thirty he would be corpulent. So it was to this Zverkov that my schoolfellows were going to give a dinner on his departure. They had kept up with him for those three years, though privately they did not consider themselves on an equal footing with him, I am convinced of that. Of Simonov’s two visitors, one was Ferfitchkin, a Russianised German—a little fellow with the face of a monkey, a blockhead who was always deriding everyone, a very bitter enemy of mine from our days in the lower forms—a vulgar, impudent, swaggering fellow, who affected a most sensitive feeling of personal honour, though, of course, he was a wretched little coward at heart. He was one of those worshippers of Zverkov who made up to the latter from interested motives, and often borrowed money from him. Simonov’s other visitor, Trudolyubov, was a person in no way remarkable—a tall young fellow, in the army, with a cold face, fairly honest, though he worshipped success of every sort, and was only capable of thinking of promotion. He was some sort of distant relation of Zverkov’s, and this, foolish as it seems, gave him a certain importance among us. He always thought me of no consequence whatever; his behaviour to me, though not quite courteous, was tolerable. “Well, with seven roubles each,” said Trudolyubov, “twenty-one roubles between the three of us, we ought to be able to get a good dinner. Zverkov, of course, won’t pay.” “Of course not, since we are inviting him,” Simonov decided. “Can you imagine,” Ferfitchkin interrupted hotly and conceitedly, like some insolent flunkey boasting of his master the General’s decorations, “can you imagine that Zverkov will let us pay alone? He will accept from delicacy, but he will order half a dozen bottles of champagne.” “Do we want half a dozen for the four of us?” observed Trudolyubov, taking notice only of the half dozen. “So the three of us, with Zverkov for the fourth, twenty-one roubles, at the Hôtel de Paris at five o’clock tomorrow,” Simonov, who had been asked to make the arrangements, concluded finally. “How twenty-one roubles?” I asked in some agitation, with a show of being offended; “if you count me it will not be twenty-one, but twenty-eight roubles.” It seemed to me that to invite myself so suddenly and unexpectedly would be positively graceful, and that they would all be conquered at once and would look at me with respect. “Do you want to join, too?” Simonov observed, with no appearance of pleasure, seeming to avoid looking at me. He knew me through and through. It infuriated me that he knew me so thoroughly. “Why not? I am an old schoolfellow of his, too, I believe, and I must own I feel hurt that you have left me out,” I said, boiling over again. “And where were we to find you?” Ferfitchkin put in roughly. “You never were on good terms with Zverkov,” Trudolyubov added, frowning. But I had already clutched at the idea and would not give it up. “It seems to me that no one has a right to form an opinion upon that,” I retorted in a shaking voice, as though something tremendous had happened. “Perhaps that is just my reason for wishing it now, that I have not always been on good terms with him.” “Oh, there’s no making you out ... with these refinements,” Trudolyubov jeered. “We’ll put your name down,” Simonov decided, addressing me. “Tomorrow at five-o’clock at the Hôtel de Paris.” “What about the money?” Ferfitchkin began in an undertone, indicating me to Simonov, but he broke off, for even Simonov was embarrassed. “That will do,” said Trudolyubov, getting up. “If he wants to come so much, let him.” “But it’s a private thing, between us friends,” Ferfitchkin said crossly, as he, too, picked up his hat. “It’s not an official gathering.” “We do not want at all, perhaps ...” They went away. Ferfitchkin did not greet me in any way as he went out, Trudolyubov barely nodded. Simonov, with whom I was left _tête-à-tête_, was in a state of vexation and perplexity, and looked at me queerly. He did not sit down and did not ask me to. “H’m ... yes ... tomorrow, then. Will you pay your subscription now? I just ask so as to know,” he muttered in embarrassment. I flushed crimson, as I did so I remembered that I had owed Simonov fifteen roubles for ages—which I had, indeed, never forgotten, though I had not paid it. “You will understand, Simonov, that I could have no idea when I came here.... I am very much vexed that I have forgotten....” “All right, all right, that doesn’t matter. You can pay tomorrow after the dinner. I simply wanted to know.... Please don’t...” He broke off and began pacing the room still more vexed. As he walked he began to stamp with his heels. “Am I keeping you?” I asked, after two minutes of silence. “Oh!” he said, starting, “that is—to be truthful—yes. I have to go and see someone ... not far from here,” he added in an apologetic voice, somewhat abashed. “My goodness, why didn’t you say so?” I cried, seizing my cap, with an astonishingly free-and-easy air, which was the last thing I should have expected of myself. “It’s close by ... not two paces away,” Simonov repeated, accompanying me to the front door with a fussy air which did not suit him at all. “So five o’clock, punctually, tomorrow,” he called down the stairs after me. He was very glad to get rid of me. I was in a fury. “What possessed me, what possessed me to force myself upon them?” I wondered, grinding my teeth as I strode along the street, “for a scoundrel, a pig like that Zverkov! Of course I had better not go; of course, I must just snap my fingers at them. I am not bound in any way. I’ll send Simonov a note by tomorrow’s post....” But what made me furious was that I knew for certain that I should go, that I should make a point of going; and the more tactless, the more unseemly my going would be, the more certainly I would go. And there was a positive obstacle to my going: I had no money. All I had was nine roubles, I had to give seven of that to my servant, Apollon, for his monthly wages. That was all I paid him—he had to keep himself. Not to pay him was impossible, considering his character. But I will talk about that fellow, about that plague of mine, another time. However, I knew I should go and should not pay him his wages. That night I had the most hideous dreams. No wonder; all the evening I had been oppressed by memories of my miserable days at school, and I could not shake them off. I was sent to the school by distant relations, upon whom I was dependent and of whom I have heard nothing since—they sent me there a forlorn, silent boy, already crushed by their reproaches, already troubled by doubt, and looking with savage distrust at everyone. My schoolfellows met me with spiteful and merciless jibes because I was not like any of them. But I could not endure their taunts; I could not give in to them with the ignoble readiness with which they gave in to one another. I hated them from the first, and shut myself away from everyone in timid, wounded and disproportionate pride. Their coarseness revolted me. They laughed cynically at my face, at my clumsy figure; and yet what stupid faces they had themselves. In our school the boys’ faces seemed in a special way to degenerate and grow stupider. How many fine-looking boys came to us! In a few years they became repulsive. Even at sixteen I wondered at them morosely; even then I was struck by the pettiness of their thoughts, the stupidity of their pursuits, their games, their conversations. They had no understanding of such essential things, they took no interest in such striking, impressive subjects, that I could not help considering them inferior to myself. It was not wounded vanity that drove me to it, and for God’s sake do not thrust upon me your hackneyed remarks, repeated to nausea, that “I was only a dreamer,” while they even then had an understanding of life. They understood nothing, they had no idea of real life, and I swear that that was what made me most indignant with them. On the contrary, the most obvious, striking reality they accepted with fantastic stupidity and even at that time were accustomed to respect success. Everything that was just, but oppressed and looked down upon, they laughed at heartlessly and shamefully. They took rank for intelligence; even at sixteen they were already talking about a snug berth. Of course, a great deal of it was due to their stupidity, to the bad examples with which they had always been surrounded in their childhood and boyhood. They were monstrously depraved. Of course a great deal of that, too, was superficial and an assumption of cynicism; of course there were glimpses of youth and freshness even in their depravity; but even that freshness was not attractive, and showed itself in a certain rakishness. I hated them horribly, though perhaps I was worse than any of them. They repaid me in the same way, and did not conceal their aversion for me. But by then I did not desire their affection: on the contrary, I continually longed for their humiliation. To escape from their derision I purposely began to make all the progress I could with my studies and forced my way to the very top. This impressed them. Moreover, they all began by degrees to grasp that I had already read books none of them could read, and understood things (not forming part of our school curriculum) of which they had not even heard. They took a savage and sarcastic view of it, but were morally impressed, especially as the teachers began to notice me on those grounds. The mockery ceased, but the hostility remained, and cold and strained relations became permanent between us. In the end I could not put up with it: with years a craving for society, for friends, developed in me. I attempted to get on friendly terms with some of my schoolfellows; but somehow or other my intimacy with them was always strained and soon ended of itself. Once, indeed, I did have a friend. But I was already a tyrant at heart; I wanted to exercise unbounded sway over him; I tried to instil into him a contempt for his surroundings; I required of him a disdainful and complete break with those surroundings. I frightened him with my passionate affection; I reduced him to tears, to hysterics. He was a simple and devoted soul; but when he devoted himself to me entirely I began to hate him immediately and repulsed him—as though all I needed him for was to win a victory over him, to subjugate him and nothing else. But I could not subjugate all of them; my friend was not at all like them either, he was, in fact, a rare exception. The first thing I did on leaving school was to give up the special job for which I had been destined so as to break all ties, to curse my past and shake the dust from off my feet.... And goodness knows why, after all that, I should go trudging off to Simonov’s! Early next morning I roused myself and jumped out of bed with excitement, as though it were all about to happen at once. But I believed that some radical change in my life was coming, and would inevitably come that day. Owing to its rarity, perhaps, any external event, however trivial, always made me feel as though some radical change in my life were at hand. I went to the office, however, as usual, but sneaked away home two hours earlier to get ready. The great thing, I thought, is not to be the first to arrive, or they will think I am overjoyed at coming. But there were thousands of such great points to consider, and they all agitated and overwhelmed me. I polished my boots a second time with my own hands; nothing in the world would have induced Apollon to clean them twice a day, as he considered that it was more than his duties required of him. I stole the brushes to clean them from the passage, being careful he should not detect it, for fear of his contempt. Then I minutely examined my clothes and thought that everything looked old, worn and threadbare. I had let myself get too slovenly. My uniform, perhaps, was tidy, but I could not go out to dinner in my uniform. The worst of it was that on the knee of my trousers was a big yellow stain. I had a foreboding that that stain would deprive me of nine-tenths of my personal dignity. I knew, too, that it was very poor to think so. “But this is no time for thinking: now I am in for the real thing,” I thought, and my heart sank. I knew, too, perfectly well even then, that I was monstrously exaggerating the facts. But how could I help it? I could not control myself and was already shaking with fever. With despair I pictured to myself how coldly and disdainfully that “scoundrel” Zverkov would meet me; with what dull-witted, invincible contempt the blockhead Trudolyubov would look at me; with what impudent rudeness the insect Ferfitchkin would snigger at me in order to curry favour with Zverkov; how completely Simonov would take it all in, and how he would despise me for the abjectness of my vanity and lack of spirit—and, worst of all, how paltry, _unliterary_, commonplace it would all be. Of course, the best thing would be not to go at all. But that was most impossible of all: if I feel impelled to do anything, I seem to be pitchforked into it. I should have jeered at myself ever afterwards: “So you funked it, you funked it, you funked the _real thing!_” On the contrary, I passionately longed to show all that “rabble” that I was by no means such a spiritless creature as I seemed to myself. What is more, even in the acutest paroxysm of this cowardly fever, I dreamed of getting the upper hand, of dominating them, carrying them away, making them like me—if only for my “elevation of thought and unmistakable wit.” They would abandon Zverkov, he would sit on one side, silent and ashamed, while I should crush him. Then, perhaps, we would be reconciled and drink to our everlasting friendship; but what was most bitter and humiliating for me was that I knew even then, knew fully and for certain, that I needed nothing of all this really, that I did not really want to crush, to subdue, to attract them, and that I did not care a straw really for the result, even if I did achieve it. Oh, how I prayed for the day to pass quickly! In unutterable anguish I went to the window, opened the movable pane and looked out into the troubled darkness of the thickly falling wet snow. At last my wretched little clock hissed out five. I seized my hat and, trying not to look at Apollon, who had been all day expecting his month’s wages, but in his foolishness was unwilling to be the first to speak about it, I slipped between him and the door and, jumping into a high-class sledge, on which I spent my last half rouble, I drove up in grand style to the Hôtel de Paris. IV I had been certain the day before that I should be the first to arrive. But it was not a question of being the first to arrive. Not only were they not there, but I had difficulty in finding our room. The table was not laid even. What did it mean? After a good many questions I elicited from the waiters that the dinner had been ordered not for five, but for six o’clock. This was confirmed at the buffet too. I felt really ashamed to go on questioning them. It was only twenty-five minutes past five. If they changed the dinner hour they ought at least to have let me know—that is what the post is for, and not to have put me in an absurd position in my own eyes and ... and even before the waiters. I sat down; the servant began laying the table; I felt even more humiliated when he was present. Towards six o’clock they brought in candles, though there were lamps burning in the room. It had not occurred to the waiter, however, to bring them in at once when I arrived. In the next room two gloomy, angry-looking persons were eating their dinners in silence at two different tables. There was a great deal of noise, even shouting, in a room further away; one could hear the laughter of a crowd of people, and nasty little shrieks in French: there were ladies at the dinner. It was sickening, in fact. I rarely passed more unpleasant moments, so much so that when they did arrive all together punctually at six I was overjoyed to see them, as though they were my deliverers, and even forgot that it was incumbent upon me to show resentment. Zverkov walked in at the head of them; evidently he was the leading spirit. He and all of them were laughing; but, seeing me, Zverkov drew himself up a little, walked up to me deliberately with a slight, rather jaunty bend from the waist. He shook hands with me in a friendly, but not over-friendly, fashion, with a sort of circumspect courtesy like that of a General, as though in giving me his hand he were warding off something. I had imagined, on the contrary, that on coming in he would at once break into his habitual thin, shrill laugh and fall to making his insipid jokes and witticisms. I had been preparing for them ever since the previous day, but I had not expected such condescension, such high-official courtesy. So, then, he felt himself ineffably superior to me in every respect! If he only meant to insult me by that high-official tone, it would not matter, I thought—I could pay him back for it one way or another. But what if, in reality, without the least desire to be offensive, that sheepshead had a notion in earnest that he was superior to me and could only look at me in a patronising way? The very supposition made me gasp. “I was surprised to hear of your desire to join us,” he began, lisping and drawling, which was something new. “You and I seem to have seen nothing of one another. You fight shy of us. You shouldn’t. We are not such terrible people as you think. Well, anyway, I am glad to renew our acquaintance.” And he turned carelessly to put down his hat on the window. “Have you been waiting long?” Trudolyubov inquired. “I arrived at five o’clock as you told me yesterday,” I answered aloud, with an irritability that threatened an explosion. “Didn’t you let him know that we had changed the hour?” said Trudolyubov to Simonov. “No, I didn’t. I forgot,” the latter replied, with no sign of regret, and without even apologising to me he went off to order the _hors d’œuvres_. “So you’ve been here a whole hour? Oh, poor fellow!” Zverkov cried ironically, for to his notions this was bound to be extremely funny. That rascal Ferfitchkin followed with his nasty little snigger like a puppy yapping. My position struck him, too, as exquisitely ludicrous and embarrassing. “It isn’t funny at all!” I cried to Ferfitchkin, more and more irritated. “It wasn’t my fault, but other people’s. They neglected to let me know. It was ... it was ... it was simply absurd.” “It’s not only absurd, but something else as well,” muttered Trudolyubov, naively taking my part. “You are not hard enough upon it. It was simply rudeness—unintentional, of course. And how could Simonov ... h’m!” “If a trick like that had been played on me,” observed Ferfitchkin, “I should ...” “But you should have ordered something for yourself,” Zverkov interrupted, “or simply asked for dinner without waiting for us.” “You will allow that I might have done that without your permission,” I rapped out. “If I waited, it was ...” “Let us sit down, gentlemen,” cried Simonov, coming in. “Everything is ready; I can answer for the champagne; it is capitally frozen.... You see, I did not know your address, where was I to look for you?” he suddenly turned to me, but again he seemed to avoid looking at me. Evidently he had something against me. It must have been what happened yesterday. All sat down; I did the same. It was a round table. Trudolyubov was on my left, Simonov on my right, Zverkov was sitting opposite, Ferfitchkin next to him, between him and Trudolyubov. “Tell me, are you ... in a government office?” Zverkov went on attending to me. Seeing that I was embarrassed he seriously thought that he ought to be friendly to me, and, so to speak, cheer me up. “Does he want me to throw a bottle at his head?” I thought, in a fury. In my novel surroundings I was unnaturally ready to be irritated. “In the N—— office,” I answered jerkily, with my eyes on my plate. “And ha-ave you a go-od berth? I say, what ma-a-de you leave your original job?” “What ma-a-de me was that I wanted to leave my original job,” I drawled more than he, hardly able to control myself. Ferfitchkin went off into a guffaw. Simonov looked at me ironically. Trudolyubov left off eating and began looking at me with curiosity. Zverkov winced, but he tried not to notice it. “And the remuneration?” “What remuneration?” “I mean, your sa-a-lary?” “Why are you cross-examining me?” However, I told him at once what my salary was. I turned horribly red. “It is not very handsome,” Zverkov observed majestically. “Yes, you can’t afford to dine at cafés on that,” Ferfitchkin added insolently. “To my thinking it’s very poor,” Trudolyubov observed gravely. “And how thin you have grown! How you have changed!” added Zverkov, with a shade of venom in his voice, scanning me and my attire with a sort of insolent compassion. “Oh, spare his blushes,” cried Ferfitchkin, sniggering. “My dear sir, allow me to tell you I am not blushing,” I broke out at last; “do you hear? I am dining here, at this cafe, at my own expense, not at other people’s—note that, Mr. Ferfitchkin.” “Wha-at? Isn’t every one here dining at his own expense? You would seem to be ...” Ferfitchkin flew out at me, turning as red as a lobster, and looking me in the face with fury. “Tha-at,” I answered, feeling I had gone too far, “and I imagine it would be better to talk of something more intelligent.” “You intend to show off your intelligence, I suppose?” “Don’t disturb yourself, that would be quite out of place here.” “Why are you clacking away like that, my good sir, eh? Have you gone out of your wits in your office?” “Enough, gentlemen, enough!” Zverkov cried, authoritatively. “How stupid it is!” muttered Simonov. “It really is stupid. We have met here, a company of friends, for a farewell dinner to a comrade and you carry on an altercation,” said Trudolyubov, rudely addressing himself to me alone. “You invited yourself to join us, so don’t disturb the general harmony.” “Enough, enough!” cried Zverkov. “Give over, gentlemen, it’s out of place. Better let me tell you how I nearly got married the day before yesterday....” And then followed a burlesque narrative of how this gentleman had almost been married two days before. There was not a word about the marriage, however, but the story was adorned with generals, colonels and kammer-junkers, while Zverkov almost took the lead among them. It was greeted with approving laughter; Ferfitchkin positively squealed. No one paid any attention to me, and I sat crushed and humiliated. “Good Heavens, these are not the people for me!” I thought. “And what a fool I have made of myself before them! I let Ferfitchkin go too far, though. The brutes imagine they are doing me an honour in letting me sit down with them. They don’t understand that it’s an honour to them and not to me! I’ve grown thinner! My clothes! Oh, damn my trousers! Zverkov noticed the yellow stain on the knee as soon as he came in.... But what’s the use! I must get up at once, this very minute, take my hat and simply go without a word ... with contempt! And tomorrow I can send a challenge. The scoundrels! As though I cared about the seven roubles. They may think.... Damn it! I don’t care about the seven roubles. I’ll go this minute!” Of course I remained. I drank sherry and Lafitte by the glassful in my discomfiture. Being unaccustomed to it, I was quickly affected. My annoyance increased as the wine went to my head. I longed all at once to insult them all in a most flagrant manner and then go away. To seize the moment and show what I could do, so that they would say, “He’s clever, though he is absurd,” and ... and ... in fact, damn them all! I scanned them all insolently with my drowsy eyes. But they seemed to have forgotten me altogether. They were noisy, vociferous, cheerful. Zverkov was talking all the time. I began listening. Zverkov was talking of some exuberant lady whom he had at last led on to declaring her love (of course, he was lying like a horse), and how he had been helped in this affair by an intimate friend of his, a Prince Kolya, an officer in the hussars, who had three thousand serfs. “And yet this Kolya, who has three thousand serfs, has not put in an appearance here tonight to see you off,” I cut in suddenly. For one minute every one was silent. “You are drunk already.” Trudolyubov deigned to notice me at last, glancing contemptuously in my direction. Zverkov, without a word, examined me as though I were an insect. I dropped my eyes. Simonov made haste to fill up the glasses with champagne. Trudolyubov raised his glass, as did everyone else but me. “Your health and good luck on the journey!” he cried to Zverkov. “To old times, to our future, hurrah!” They all tossed off their glasses, and crowded round Zverkov to kiss him. I did not move; my full glass stood untouched before me. “Why, aren’t you going to drink it?” roared Trudolyubov, losing patience and turning menacingly to me. “I want to make a speech separately, on my own account ... and then I’ll drink it, Mr. Trudolyubov.” “Spiteful brute!” muttered Simonov. I drew myself up in my chair and feverishly seized my glass, prepared for something extraordinary, though I did not know myself precisely what I was going to say. “_Silence!_” cried Ferfitchkin. “Now for a display of wit!” Zverkov waited very gravely, knowing what was coming. “Mr. Lieutenant Zverkov,” I began, “let me tell you that I hate phrases, phrasemongers and men in corsets ... that’s the first point, and there is a second one to follow it.” There was a general stir. “The second point is: I hate ribaldry and ribald talkers. Especially ribald talkers! The third point: I love justice, truth and honesty.” I went on almost mechanically, for I was beginning to shiver with horror myself and had no idea how I came to be talking like this. “I love thought, Monsieur Zverkov; I love true comradeship, on an equal footing and not ... H’m ... I love ... But, however, why not? I will drink your health, too, Mr. Zverkov. Seduce the Circassian girls, shoot the enemies of the fatherland and ... and ... to your health, Monsieur Zverkov!” Zverkov got up from his seat, bowed to me and said: “I am very much obliged to you.” He was frightfully offended and turned pale. “Damn the fellow!” roared Trudolyubov, bringing his fist down on the table. “Well, he wants a punch in the face for that,” squealed Ferfitchkin. “We ought to turn him out,” muttered Simonov. “Not a word, gentlemen, not a movement!” cried Zverkov solemnly, checking the general indignation. “I thank you all, but I can show him for myself how much value I attach to his words.” “Mr. Ferfitchkin, you will give me satisfaction tomorrow for your words just now!” I said aloud, turning with dignity to Ferfitchkin. “A duel, you mean? Certainly,” he answered. But probably I was so ridiculous as I challenged him and it was so out of keeping with my appearance that everyone including Ferfitchkin was prostrate with laughter. “Yes, let him alone, of course! He is quite drunk,” Trudolyubov said with disgust. “I shall never forgive myself for letting him join us,” Simonov muttered again. “Now is the time to throw a bottle at their heads,” I thought to myself. I picked up the bottle ... and filled my glass.... “No, I’d better sit on to the end,” I went on thinking; “you would be pleased, my friends, if I went away. Nothing will induce me to go. I’ll go on sitting here and drinking to the end, on purpose, as a sign that I don’t think you of the slightest consequence. I will go on sitting and drinking, because this is a public-house and I paid my entrance money. I’ll sit here and drink, for I look upon you as so many pawns, as inanimate pawns. I’ll sit here and drink ... and sing if I want to, yes, sing, for I have the right to ... to sing ... H’m!” But I did not sing. I simply tried not to look at any of them. I assumed most unconcerned attitudes and waited with impatience for them to speak _first_. But alas, they did not address me! And oh, how I wished, how I wished at that moment to be reconciled to them! It struck eight, at last nine. They moved from the table to the sofa. Zverkov stretched himself on a lounge and put one foot on a round table. Wine was brought there. He did, as a fact, order three bottles on his own account. I, of course, was not invited to join them. They all sat round him on the sofa. They listened to him, almost with reverence. It was evident that they were fond of him. “What for? What for?” I wondered. From time to time they were moved to drunken enthusiasm and kissed each other. They talked of the Caucasus, of the nature of true passion, of snug berths in the service, of the income of an hussar called Podharzhevsky, whom none of them knew personally, and rejoiced in the largeness of it, of the extraordinary grace and beauty of a Princess D., whom none of them had ever seen; then it came to Shakespeare’s being immortal. I smiled contemptuously and walked up and down the other side of the room, opposite the sofa, from the table to the stove and back again. I tried my very utmost to show them that I could do without them, and yet I purposely made a noise with my boots, thumping with my heels. But it was all in vain. They paid no attention. I had the patience to walk up and down in front of them from eight o’clock till eleven, in the same place, from the table to the stove and back again. “I walk up and down to please myself and no one can prevent me.” The waiter who came into the room stopped, from time to time, to look at me. I was somewhat giddy from turning round so often; at moments it seemed to me that I was in delirium. During those three hours I was three times soaked with sweat and dry again. At times, with an intense, acute pang I was stabbed to the heart by the thought that ten years, twenty years, forty years would pass, and that even in forty years I would remember with loathing and humiliation those filthiest, most ludicrous, and most awful moments of my life. No one could have gone out of his way to degrade himself more shamelessly, and I fully realised it, fully, and yet I went on pacing up and down from the table to the stove. “Oh, if you only knew what thoughts and feelings I am capable of, how cultured I am!” I thought at moments, mentally addressing the sofa on which my enemies were sitting. But my enemies behaved as though I were not in the room. Once—only once—they turned towards me, just when Zverkov was talking about Shakespeare, and I suddenly gave a contemptuous laugh. I laughed in such an affected and disgusting way that they all at once broke off their conversation, and silently and gravely for two minutes watched me walking up and down from the table to the stove, _taking no notice of them_. But nothing came of it: they said nothing, and two minutes later they ceased to notice me again. It struck eleven. “Friends,” cried Zverkov getting up from the sofa, “let us all be off now, _there!_” “Of course, of course,” the others assented. I turned sharply to Zverkov. I was so harassed, so exhausted, that I would have cut my throat to put an end to it. I was in a fever; my hair, soaked with perspiration, stuck to my forehead and temples. “Zverkov, I beg your pardon,” I said abruptly and resolutely. “Ferfitchkin, yours too, and everyone’s, everyone’s: I have insulted you all!” “Aha! A duel is not in your line, old man,” Ferfitchkin hissed venomously. It sent a sharp pang to my heart. “No, it’s not the duel I am afraid of, Ferfitchkin! I am ready to fight you tomorrow, after we are reconciled. I insist upon it, in fact, and you cannot refuse. I want to show you that I am not afraid of a duel. You shall fire first and I shall fire into the air.” “He is comforting himself,” said Simonov. “He’s simply raving,” said Trudolyubov. “But let us pass. Why are you barring our way? What do you want?” Zverkov answered disdainfully. They were all flushed, their eyes were bright: they had been drinking heavily. “I ask for your friendship, Zverkov; I insulted you, but ...” “Insulted? _You_ insulted _me?_ Understand, sir, that you never, under any circumstances, could possibly insult _me_.” “And that’s enough for you. Out of the way!” concluded Trudolyubov. “Olympia is mine, friends, that’s agreed!” cried Zverkov. “We won’t dispute your right, we won’t dispute your right,” the others answered, laughing. I stood as though spat upon. The party went noisily out of the room. Trudolyubov struck up some stupid song. Simonov remained behind for a moment to tip the waiters. I suddenly went up to him. “Simonov! give me six roubles!” I said, with desperate resolution. He looked at me in extreme amazement, with vacant eyes. He, too, was drunk. “You don’t mean you are coming with us?” “Yes.” “I’ve no money,” he snapped out, and with a scornful laugh he went out of the room. I clutched at his overcoat. It was a nightmare. “Simonov, I saw you had money. Why do you refuse me? Am I a scoundrel? Beware of refusing me: if you knew, if you knew why I am asking! My whole future, my whole plans depend upon it!” Simonov pulled out the money and almost flung it at me. “Take it, if you have no sense of shame!” he pronounced pitilessly, and ran to overtake them. I was left for a moment alone. Disorder, the remains of dinner, a broken wine-glass on the floor, spilt wine, cigarette ends, fumes of drink and delirium in my brain, an agonising misery in my heart and finally the waiter, who had seen and heard all and was looking inquisitively into my face. “I am going there!” I cried. “Either they shall all go down on their knees to beg for my friendship, or I will give Zverkov a slap in the face!” V “So this is it, this is it at last—contact with real life,” I muttered as I ran headlong downstairs. “This is very different from the Pope’s leaving Rome and going to Brazil, very different from the ball on Lake Como!” “You are a scoundrel,” a thought flashed through my mind, “if you laugh at this now.” “No matter!” I cried, answering myself. “Now everything is lost!” There was no trace to be seen of them, but that made no difference—I knew where they had gone. At the steps was standing a solitary night sledge-driver in a rough peasant coat, powdered over with the still falling, wet, and as it were warm, snow. It was hot and steamy. The little shaggy piebald horse was also covered with snow and coughing, I remember that very well. I made a rush for the roughly made sledge; but as soon as I raised my foot to get into it, the recollection of how Simonov had just given me six roubles seemed to double me up and I tumbled into the sledge like a sack. “No, I must do a great deal to make up for all that,” I cried. “But I will make up for it or perish on the spot this very night. Start!” We set off. There was a perfect whirl in my head. “They won’t go down on their knees to beg for my friendship. That is a mirage, cheap mirage, revolting, romantic and fantastical—that’s another ball on Lake Como. And so I am bound to slap Zverkov’s face! It is my duty to. And so it is settled; I am flying to give him a slap in the face. Hurry up!” The driver tugged at the reins. “As soon as I go in I’ll give it him. Ought I before giving him the slap to say a few words by way of preface? No. I’ll simply go in and give it him. They will all be sitting in the drawing-room, and he with Olympia on the sofa. That damned Olympia! She laughed at my looks on one occasion and refused me. I’ll pull Olympia’s hair, pull Zverkov’s ears! No, better one ear, and pull him by it round the room. Maybe they will all begin beating me and will kick me out. That’s most likely, indeed. No matter! Anyway, I shall first slap him; the initiative will be mine; and by the laws of honour that is everything: he will be branded and cannot wipe off the slap by any blows, by nothing but a duel. He will be forced to fight. And let them beat me now. Let them, the ungrateful wretches! Trudolyubov will beat me hardest, he is so strong; Ferfitchkin will be sure to catch hold sideways and tug at my hair. But no matter, no matter! That’s what I am going for. The blockheads will be forced at last to see the tragedy of it all! When they drag me to the door I shall call out to them that in reality they are not worth my little finger. Get on, driver, get on!” I cried to the driver. He started and flicked his whip, I shouted so savagely. “We shall fight at daybreak, that’s a settled thing. I’ve done with the office. Ferfitchkin made a joke about it just now. But where can I get pistols? Nonsense! I’ll get my salary in advance and buy them. And powder, and bullets? That’s the second’s business. And how can it all be done by daybreak? and where am I to get a second? I have no friends. Nonsense!” I cried, lashing myself up more and more. “It’s of no consequence! The first person I meet in the street is bound to be my second, just as he would be bound to pull a drowning man out of water. The most eccentric things may happen. Even if I were to ask the director himself to be my second tomorrow, he would be bound to consent, if only from a feeling of chivalry, and to keep the secret! Anton Antonitch....” The fact is, that at that very minute the disgusting absurdity of my plan and the other side of the question was clearer and more vivid to my imagination than it could be to anyone on earth. But .... “Get on, driver, get on, you rascal, get on!” “Ugh, sir!” said the son of toil. Cold shivers suddenly ran down me. Wouldn’t it be better ... to go straight home? My God, my God! Why did I invite myself to this dinner yesterday? But no, it’s impossible. And my walking up and down for three hours from the table to the stove? No, they, they and no one else must pay for my walking up and down! They must wipe out this dishonour! Drive on! And what if they give me into custody? They won’t dare! They’ll be afraid of the scandal. And what if Zverkov is so contemptuous that he refuses to fight a duel? He is sure to; but in that case I’ll show them ... I will turn up at the posting station when he’s setting off tomorrow, I’ll catch him by the leg, I’ll pull off his coat when he gets into the carriage. I’ll get my teeth into his hand, I’ll bite him. “See what lengths you can drive a desperate man to!” He may hit me on the head and they may belabour me from behind. I will shout to the assembled multitude: “Look at this young puppy who is driving off to captivate the Circassian girls after letting me spit in his face!” Of course, after that everything will be over! The office will have vanished off the face of the earth. I shall be arrested, I shall be tried, I shall be dismissed from the service, thrown in prison, sent to Siberia. Never mind! In fifteen years when they let me out of prison I will trudge off to him, a beggar, in rags. I shall find him in some provincial town. He will be married and happy. He will have a grown-up daughter.... I shall say to him: “Look, monster, at my hollow cheeks and my rags! I’ve lost everything—my career, my happiness, art, science, _the woman I loved_, and all through you. Here are pistols. I have come to discharge my pistol and ... and I ... forgive you. Then I shall fire into the air and he will hear nothing more of me....” I was actually on the point of tears, though I knew perfectly well at that moment that all this was out of Pushkin’s _Silvio_ and Lermontov’s _Masquerade_. And all at once I felt horribly ashamed, so ashamed that I stopped the horse, got out of the sledge, and stood still in the snow in the middle of the street. The driver gazed at me, sighing and astonished. What was I to do? I could not go on there—it was evidently stupid, and I could not leave things as they were, because that would seem as though ... Heavens, how could I leave things! And after such insults! “No!” I cried, throwing myself into the sledge again. “It is ordained! It is fate! Drive on, drive on!” And in my impatience I punched the sledge-driver on the back of the neck. “What are you up to? What are you hitting me for?” the peasant shouted, but he whipped up his nag so that it began kicking. The wet snow was falling in big flakes; I unbuttoned myself, regardless of it. I forgot everything else, for I had finally decided on the slap, and felt with horror that it was going to happen _now, at once_, and that _no force could stop it_. The deserted street lamps gleamed sullenly in the snowy darkness like torches at a funeral. The snow drifted under my great-coat, under my coat, under my cravat, and melted there. I did not wrap myself up—all was lost, anyway. At last we arrived. I jumped out, almost unconscious, ran up the steps and began knocking and kicking at the door. I felt fearfully weak, particularly in my legs and knees. The door was opened quickly as though they knew I was coming. As a fact, Simonov had warned them that perhaps another gentleman would arrive, and this was a place in which one had to give notice and to observe certain precautions. It was one of those “millinery establishments” which were abolished by the police a good time ago. By day it really was a shop; but at night, if one had an introduction, one might visit it for other purposes. I walked rapidly through the dark shop into the familiar drawing-room, where there was only one candle burning, and stood still in amazement: there was no one there. “Where are they?” I asked somebody. But by now, of course, they had separated. Before me was standing a person with a stupid smile, the “madam” herself, who had seen me before. A minute later a door opened and another person came in. Taking no notice of anything I strode about the room, and, I believe, I talked to myself. I felt as though I had been saved from death and was conscious of this, joyfully, all over: I should have given that slap, I should certainly, certainly have given it! But now they were not here and ... everything had vanished and changed! I looked round. I could not realise my condition yet. I looked mechanically at the girl who had come in: and had a glimpse of a fresh, young, rather pale face, with straight, dark eyebrows, and with grave, as it were wondering, eyes that attracted me at once; I should have hated her if she had been smiling. I began looking at her more intently and, as it were, with effort. I had not fully collected my thoughts. There was something simple and good-natured in her face, but something strangely grave. I am sure that this stood in her way here, and no one of those fools had noticed her. She could not, however, have been called a beauty, though she was tall, strong-looking, and well built. She was very simply dressed. Something loathsome stirred within me. I went straight up to her. I chanced to look into the glass. My harassed face struck me as revolting in the extreme, pale, angry, abject, with dishevelled hair. “No matter, I am glad of it,” I thought; “I am glad that I shall seem repulsive to her; I like that.” VI ... Somewhere behind a screen a clock began wheezing, as though oppressed by something, as though someone were strangling it. After an unnaturally prolonged wheezing there followed a shrill, nasty, and as it were unexpectedly rapid, chime—as though someone were suddenly jumping forward. It struck two. I woke up, though I had indeed not been asleep but lying half-conscious. It was almost completely dark in the narrow, cramped, low-pitched room, cumbered up with an enormous wardrobe and piles of cardboard boxes and all sorts of frippery and litter. The candle end that had been burning on the table was going out and gave a faint flicker from time to time. In a few minutes there would be complete darkness. I was not long in coming to myself; everything came back to my mind at once, without an effort, as though it had been in ambush to pounce upon me again. And, indeed, even while I was unconscious a point seemed continually to remain in my memory unforgotten, and round it my dreams moved drearily. But strange to say, everything that had happened to me in that day seemed to me now, on waking, to be in the far, far away past, as though I had long, long ago lived all that down. My head was full of fumes. Something seemed to be hovering over me, rousing me, exciting me, and making me restless. Misery and spite seemed surging up in me again and seeking an outlet. Suddenly I saw beside me two wide open eyes scrutinising me curiously and persistently. The look in those eyes was coldly detached, sullen, as it were utterly remote; it weighed upon me. A grim idea came into my brain and passed all over my body, as a horrible sensation, such as one feels when one goes into a damp and mouldy cellar. There was something unnatural in those two eyes, beginning to look at me only now. I recalled, too, that during those two hours I had not said a single word to this creature, and had, in fact, considered it utterly superfluous; in fact, the silence had for some reason gratified me. Now I suddenly realised vividly the hideous idea—revolting as a spider—of vice, which, without love, grossly and shamelessly begins with that in which true love finds its consummation. For a long time we gazed at each other like that, but she did not drop her eyes before mine and her expression did not change, so that at last I felt uncomfortable. “What is your name?” I asked abruptly, to put an end to it. “Liza,” she answered almost in a whisper, but somehow far from graciously, and she turned her eyes away. I was silent. “What weather! The snow ... it’s disgusting!” I said, almost to myself, putting my arm under my head despondently, and gazing at the ceiling. She made no answer. This was horrible. “Have you always lived in Petersburg?” I asked a minute later, almost angrily, turning my head slightly towards her. “No.” “Where do you come from?” “From Riga,” she answered reluctantly. “Are you a German?” “No, Russian.” “Have you been here long?” “Where?” “In this house?” “A fortnight.” She spoke more and more jerkily. The candle went out; I could no longer distinguish her face. “Have you a father and mother?” “Yes ... no ... I have.” “Where are they?” “There ... in Riga.” “What are they?” “Oh, nothing.” “Nothing? Why, what class are they?” “Tradespeople.” “Have you always lived with them?” “Yes.” “How old are you?” “Twenty.” “Why did you leave them?” “Oh, for no reason.” That answer meant “Let me alone; I feel sick, sad.” We were silent. God knows why I did not go away. I felt myself more and more sick and dreary. The images of the previous day began of themselves, apart from my will, flitting through my memory in confusion. I suddenly recalled something I had seen that morning when, full of anxious thoughts, I was hurrying to the office. “I saw them carrying a coffin out yesterday and they nearly dropped it,” I suddenly said aloud, not that I desired to open the conversation, but as it were by accident. “A coffin?” “Yes, in the Haymarket; they were bringing it up out of a cellar.” “From a cellar?” “Not from a cellar, but a basement. Oh, you know ... down below ... from a house of ill-fame. It was filthy all round ... Egg-shells, litter ... a stench. It was loathsome.” Silence. “A nasty day to be buried,” I began, simply to avoid being silent. “Nasty, in what way?” “The snow, the wet.” (I yawned.) “It makes no difference,” she said suddenly, after a brief silence. “No, it’s horrid.” (I yawned again). “The gravediggers must have sworn at getting drenched by the snow. And there must have been water in the grave.” “Why water in the grave?” she asked, with a sort of curiosity, but speaking even more harshly and abruptly than before. I suddenly began to feel provoked. “Why, there must have been water at the bottom a foot deep. You can’t dig a dry grave in Volkovo Cemetery.” “Why?” “Why? Why, the place is waterlogged. It’s a regular marsh. So they bury them in water. I’ve seen it myself ... many times.” (I had never seen it once, indeed I had never been in Volkovo, and had only heard stories of it.) “Do you mean to say, you don’t mind how you die?” “But why should I die?” she answered, as though defending herself. “Why, some day you will die, and you will die just the same as that dead woman. She was ... a girl like you. She died of consumption.” “A wench would have died in hospital ...” (She knows all about it already: she said “wench,” not “girl.”) “She was in debt to her madam,” I retorted, more and more provoked by the discussion; “and went on earning money for her up to the end, though she was in consumption. Some sledge-drivers standing by were talking about her to some soldiers and telling them so. No doubt they knew her. They were laughing. They were going to meet in a pot-house to drink to her memory.” A great deal of this was my invention. Silence followed, profound silence. She did not stir. “And is it better to die in a hospital?” “Isn’t it just the same? Besides, why should I die?” she added irritably. “If not now, a little later.” “Why a little later?” “Why, indeed? Now you are young, pretty, fresh, you fetch a high price. But after another year of this life you will be very different—you will go off.” “In a year?” “Anyway, in a year you will be worth less,” I continued malignantly. “You will go from here to something lower, another house; a year later—to a third, lower and lower, and in seven years you will come to a basement in the Haymarket. That will be if you were lucky. But it would be much worse if you got some disease, consumption, say ... and caught a chill, or something or other. It’s not easy to get over an illness in your way of life. If you catch anything you may not get rid of it. And so you would die.” “Oh, well, then I shall die,” she answered, quite vindictively, and she made a quick movement. “But one is sorry.” “Sorry for whom?” “Sorry for life.” Silence. “Have you been engaged to be married? Eh?” “What’s that to you?” “Oh, I am not cross-examining you. It’s nothing to me. Why are you so cross? Of course you may have had your own troubles. What is it to me? It’s simply that I felt sorry.” “Sorry for whom?” “Sorry for you.” “No need,” she whispered hardly audibly, and again made a faint movement. That incensed me at once. What! I was so gentle with her, and she.... “Why, do you think that you are on the right path?” “I don’t think anything.” “That’s what’s wrong, that you don’t think. Realise it while there is still time. There still is time. You are still young, good-looking; you might love, be married, be happy....” “Not all married women are happy,” she snapped out in the rude abrupt tone she had used at first. “Not all, of course, but anyway it is much better than the life here. Infinitely better. Besides, with love one can live even without happiness. Even in sorrow life is sweet; life is sweet, however one lives. But here what is there but ... foulness? Phew!” I turned away with disgust; I was no longer reasoning coldly. I began to feel myself what I was saying and warmed to the subject. I was already longing to expound the cherished ideas I had brooded over in my corner. Something suddenly flared up in me. An object had appeared before me. “Never mind my being here, I am not an example for you. I am, perhaps, worse than you are. I was drunk when I came here, though,” I hastened, however, to say in self-defence. “Besides, a man is no example for a woman. It’s a different thing. I may degrade and defile myself, but I am not anyone’s slave. I come and go, and that’s an end of it. I shake it off, and I am a different man. But you are a slave from the start. Yes, a slave! You give up everything, your whole freedom. If you want to break your chains afterwards, you won’t be able to; you will be more and more fast in the snares. It is an accursed bondage. I know it. I won’t speak of anything else, maybe you won’t understand, but tell me: no doubt you are in debt to your madam? There, you see,” I added, though she made no answer, but only listened in silence, entirely absorbed, “that’s a bondage for you! You will never buy your freedom. They will see to that. It’s like selling your soul to the devil.... And besides ... perhaps, I too, am just as unlucky—how do you know—and wallow in the mud on purpose, out of misery? You know, men take to drink from grief; well, maybe I am here from grief. Come, tell me, what is there good here? Here you and I ... came together ... just now and did not say one word to one another all the time, and it was only afterwards you began staring at me like a wild creature, and I at you. Is that loving? Is that how one human being should meet another? It’s hideous, that’s what it is!” “Yes!” she assented sharply and hurriedly. I was positively astounded by the promptitude of this “Yes.” So the same thought may have been straying through her mind when she was staring at me just before. So she, too, was capable of certain thoughts? “Damn it all, this was interesting, this was a point of likeness!” I thought, almost rubbing my hands. And indeed it’s easy to turn a young soul like that! It was the exercise of my power that attracted me most. She turned her head nearer to me, and it seemed to me in the darkness that she propped herself on her arm. Perhaps she was scrutinising me. How I regretted that I could not see her eyes. I heard her deep breathing. “Why have you come here?” I asked her, with a note of authority already in my voice. “Oh, I don’t know.” “But how nice it would be to be living in your father’s house! It’s warm and free; you have a home of your own.” “But what if it’s worse than this?” “I must take the right tone,” flashed through my mind. “I may not get far with sentimentality.” But it was only a momentary thought. I swear she really did interest me. Besides, I was exhausted and moody. And cunning so easily goes hand-in-hand with feeling. “Who denies it!” I hastened to answer. “Anything may happen. I am convinced that someone has wronged you, and that you are more sinned against than sinning. Of course, I know nothing of your story, but it’s not likely a girl like you has come here of her own inclination....” “A girl like me?” she whispered, hardly audibly; but I heard it. Damn it all, I was flattering her. That was horrid. But perhaps it was a good thing.... She was silent. “See, Liza, I will tell you about myself. If I had had a home from childhood, I shouldn’t be what I am now. I often think that. However bad it may be at home, anyway they are your father and mother, and not enemies, strangers. Once a year at least, they’ll show their love of you. Anyway, you know you are at home. I grew up without a home; and perhaps that’s why I’ve turned so ... unfeeling.” I waited again. “Perhaps she doesn’t understand,” I thought, “and, indeed, it is absurd—it’s moralising.” “If I were a father and had a daughter, I believe I should love my daughter more than my sons, really,” I began indirectly, as though talking of something else, to distract her attention. I must confess I blushed. “Why so?” she asked. Ah! so she was listening! “I don’t know, Liza. I knew a father who was a stern, austere man, but used to go down on his knees to his daughter, used to kiss her hands, her feet, he couldn’t make enough of her, really. When she danced at parties he used to stand for five hours at a stretch, gazing at her. He was mad over her: I understand that! She would fall asleep tired at night, and he would wake to kiss her in her sleep and make the sign of the cross over her. He would go about in a dirty old coat, he was stingy to everyone else, but would spend his last penny for her, giving her expensive presents, and it was his greatest delight when she was pleased with what he gave her. Fathers always love their daughters more than the mothers do. Some girls live happily at home! And I believe I should never let my daughters marry.” “What next?” she said, with a faint smile. “I should be jealous, I really should. To think that she should kiss anyone else! That she should love a stranger more than her father! It’s painful to imagine it. Of course, that’s all nonsense, of course every father would be reasonable at last. But I believe before I should let her marry, I should worry myself to death; I should find fault with all her suitors. But I should end by letting her marry whom she herself loved. The one whom the daughter loves always seems the worst to the father, you know. That is always so. So many family troubles come from that.” “Some are glad to sell their daughters, rather than marrying them honourably.” Ah, so that was it! “Such a thing, Liza, happens in those accursed families in which there is neither love nor God,” I retorted warmly, “and where there is no love, there is no sense either. There are such families, it’s true, but I am not speaking of them. You must have seen wickedness in your own family, if you talk like that. Truly, you must have been unlucky. H’m! ... that sort of thing mostly comes about through poverty.” “And is it any better with the gentry? Even among the poor, honest people who live happily?” “H’m ... yes. Perhaps. Another thing, Liza, man is fond of reckoning up his troubles, but does not count his joys. If he counted them up as he ought, he would see that every lot has enough happiness provided for it. And what if all goes well with the family, if the blessing of God is upon it, if the husband is a good one, loves you, cherishes you, never leaves you! There is happiness in such a family! Even sometimes there is happiness in the midst of sorrow; and indeed sorrow is everywhere. If you marry _you will find out for yourself_. But think of the first years of married life with one you love: what happiness, what happiness there sometimes is in it! And indeed it’s the ordinary thing. In those early days even quarrels with one’s husband end happily. Some women get up quarrels with their husbands just because they love them. Indeed, I knew a woman like that: she seemed to say that because she loved him, she would torment him and make him feel it. You know that you may torment a man on purpose through love. Women are particularly given to that, thinking to themselves ‘I will love him so, I will make so much of him afterwards, that it’s no sin to torment him a little now.’ And all in the house rejoice in the sight of you, and you are happy and gay and peaceful and honourable.... Then there are some women who are jealous. If he went off anywhere—I knew one such woman, she couldn’t restrain herself, but would jump up at night and run off on the sly to find out where he was, whether he was with some other woman. That’s a pity. And the woman knows herself it’s wrong, and her heart fails her and she suffers, but she loves—it’s all through love. And how sweet it is to make up after quarrels, to own herself in the wrong or to forgive him! And they both are so happy all at once—as though they had met anew, been married over again; as though their love had begun afresh. And no one, no one should know what passes between husband and wife if they love one another. And whatever quarrels there may be between them they ought not to call in their own mother to judge between them and tell tales of one another. They are their own judges. Love is a holy mystery and ought to be hidden from all other eyes, whatever happens. That makes it holier and better. They respect one another more, and much is built on respect. And if once there has been love, if they have been married for love, why should love pass away? Surely one can keep it! It is rare that one cannot keep it. And if the husband is kind and straightforward, why should not love last? The first phase of married love will pass, it is true, but then there will come a love that is better still. Then there will be the union of souls, they will have everything in common, there will be no secrets between them. And once they have children, the most difficult times will seem to them happy, so long as there is love and courage. Even toil will be a joy, you may deny yourself bread for your children and even that will be a joy, They will love you for it afterwards; so you are laying by for your future. As the children grow up you feel that you are an example, a support for them; that even after you die your children will always keep your thoughts and feelings, because they have received them from you, they will take on your semblance and likeness. So you see this is a great duty. How can it fail to draw the father and mother nearer? People say it’s a trial to have children. Who says that? It is heavenly happiness! Are you fond of little children, Liza? I am awfully fond of them. You know—a little rosy baby boy at your bosom, and what husband’s heart is not touched, seeing his wife nursing his child! A plump little rosy baby, sprawling and snuggling, chubby little hands and feet, clean tiny little nails, so tiny that it makes one laugh to look at them; eyes that look as if they understand everything. And while it sucks it clutches at your bosom with its little hand, plays. When its father comes up, the child tears itself away from the bosom, flings itself back, looks at its father, laughs, as though it were fearfully funny, and falls to sucking again. Or it will bite its mother’s breast when its little teeth are coming, while it looks sideways at her with its little eyes as though to say, ‘Look, I am biting!’ Is not all that happiness when they are the three together, husband, wife and child? One can forgive a great deal for the sake of such moments. Yes, Liza, one must first learn to live oneself before one blames others!” “It’s by pictures, pictures like that one must get at you,” I thought to myself, though I did speak with real feeling, and all at once I flushed crimson. “What if she were suddenly to burst out laughing, what should I do then?” That idea drove me to fury. Towards the end of my speech I really was excited, and now my vanity was somehow wounded. The silence continued. I almost nudged her. “Why are you—” she began and stopped. But I understood: there was a quiver of something different in her voice, not abrupt, harsh and unyielding as before, but something soft and shamefaced, so shamefaced that I suddenly felt ashamed and guilty. “What?” I asked, with tender curiosity. “Why, you...” “What?” “Why, you ... speak somehow like a book,” she said, and again there was a note of irony in her voice. That remark sent a pang to my heart. It was not what I was expecting. I did not understand that she was hiding her feelings under irony, that this is usually the last refuge of modest and chaste-souled people when the privacy of their soul is coarsely and intrusively invaded, and that their pride makes them refuse to surrender till the last moment and shrink from giving expression to their feelings before you. I ought to have guessed the truth from the timidity with which she had repeatedly approached her sarcasm, only bringing herself to utter it at last with an effort. But I did not guess, and an evil feeling took possession of me. “Wait a bit!” I thought. VII “Oh, hush, Liza! How can you talk about being like a book, when it makes even me, an outsider, feel sick? Though I don’t look at it as an outsider, for, indeed, it touches me to the heart.... Is it possible, is it possible that you do not feel sick at being here yourself? Evidently habit does wonders! God knows what habit can do with anyone. Can you seriously think that you will never grow old, that you will always be good-looking, and that they will keep you here for ever and ever? I say nothing of the loathsomeness of the life here.... Though let me tell you this about it—about your present life, I mean; here though you are young now, attractive, nice, with soul and feeling, yet you know as soon as I came to myself just now I felt at once sick at being here with you! One can only come here when one is drunk. But if you were anywhere else, living as good people live, I should perhaps be more than attracted by you, should fall in love with you, should be glad of a look from you, let alone a word; I should hang about your door, should go down on my knees to you, should look upon you as my betrothed and think it an honour to be allowed to. I should not dare to have an impure thought about you. But here, you see, I know that I have only to whistle and you have to come with me whether you like it or not. I don’t consult your wishes, but you mine. The lowest labourer hires himself as a workman, but he doesn’t make a slave of himself altogether; besides, he knows that he will be free again presently. But when are you free? Only think what you are giving up here? What is it you are making a slave of? It is your soul, together with your body; you are selling your soul which you have no right to dispose of! You give your love to be outraged by every drunkard! Love! But that’s everything, you know, it’s a priceless diamond, it’s a maiden’s treasure, love—why, a man would be ready to give his soul, to face death to gain that love. But how much is your love worth now? You are sold, all of you, body and soul, and there is no need to strive for love when you can have everything without love. And you know there is no greater insult to a girl than that, do you understand? To be sure, I have heard that they comfort you, poor fools, they let you have lovers of your own here. But you know that’s simply a farce, that’s simply a sham, it’s just laughing at you, and you are taken in by it! Why, do you suppose he really loves you, that lover of yours? I don’t believe it. How can he love you when he knows you may be called away from him any minute? He would be a low fellow if he did! Will he have a grain of respect for you? What have you in common with him? He laughs at you and robs you—that is all his love amounts to! You are lucky if he does not beat you. Very likely he does beat you, too. Ask him, if you have got one, whether he will marry you. He will laugh in your face, if he doesn’t spit in it or give you a blow—though maybe he is not worth a bad halfpenny himself. And for what have you ruined your life, if you come to think of it? For the coffee they give you to drink and the plentiful meals? But with what object are they feeding you up? An honest girl couldn’t swallow the food, for she would know what she was being fed for. You are in debt here, and, of course, you will always be in debt, and you will go on in debt to the end, till the visitors here begin to scorn you. And that will soon happen, don’t rely upon your youth—all that flies by express train here, you know. You will be kicked out. And not simply kicked out; long before that she’ll begin nagging at you, scolding you, abusing you, as though you had not sacrificed your health for her, had not thrown away your youth and your soul for her benefit, but as though you had ruined her, beggared her, robbed her. And don’t expect anyone to take your part: the others, your companions, will attack you, too, win her favour, for all are in slavery here, and have lost all conscience and pity here long ago. They have become utterly vile, and nothing on earth is viler, more loathsome, and more insulting than their abuse. And you are laying down everything here, unconditionally, youth and health and beauty and hope, and at twenty-two you will look like a woman of five-and-thirty, and you will be lucky if you are not diseased, pray to God for that! No doubt you are thinking now that you have a gay time and no work to do! Yet there is no work harder or more dreadful in the world or ever has been. One would think that the heart alone would be worn out with tears. And you won’t dare to say a word, not half a word when they drive you away from here; you will go away as though you were to blame. You will change to another house, then to a third, then somewhere else, till you come down at last to the Haymarket. There you will be beaten at every turn; that is good manners there, the visitors don’t know how to be friendly without beating you. You don’t believe that it is so hateful there? Go and look for yourself some time, you can see with your own eyes. Once, one New Year’s Day, I saw a woman at a door. They had turned her out as a joke, to give her a taste of the frost because she had been crying so much, and they shut the door behind her. At nine o’clock in the morning she was already quite drunk, dishevelled, half-naked, covered with bruises, her face was powdered, but she had a black-eye, blood was trickling from her nose and her teeth; some cabman had just given her a drubbing. She was sitting on the stone steps, a salt fish of some sort was in her hand; she was crying, wailing something about her luck and beating with the fish on the steps, and cabmen and drunken soldiers were crowding in the doorway taunting her. You don’t believe that you will ever be like that? I should be sorry to believe it, too, but how do you know; maybe ten years, eight years ago that very woman with the salt fish came here fresh as a cherub, innocent, pure, knowing no evil, blushing at every word. Perhaps she was like you, proud, ready to take offence, not like the others; perhaps she looked like a queen, and knew what happiness was in store for the man who should love her and whom she should love. Do you see how it ended? And what if at that very minute when she was beating on the filthy steps with that fish, drunken and dishevelled—what if at that very minute she recalled the pure early days in her father’s house, when she used to go to school and the neighbour’s son watched for her on the way, declaring that he would love her as long as he lived, that he would devote his life to her, and when they vowed to love one another for ever and be married as soon as they were grown up! No, Liza, it would be happy for you if you were to die soon of consumption in some corner, in some cellar like that woman just now. In the hospital, do you say? You will be lucky if they take you, but what if you are still of use to the madam here? Consumption is a queer disease, it is not like fever. The patient goes on hoping till the last minute and says he is all right. He deludes himself And that just suits your madam. Don’t doubt it, that’s how it is; you have sold your soul, and what is more you owe money, so you daren’t say a word. But when you are dying, all will abandon you, all will turn away from you, for then there will be nothing to get from you. What’s more, they will reproach you for cumbering the place, for being so long over dying. However you beg you won’t get a drink of water without abuse: ‘Whenever are you going off, you nasty hussy, you won’t let us sleep with your moaning, you make the gentlemen sick.’ That’s true, I have heard such things said myself. They will thrust you dying into the filthiest corner in the cellar—in the damp and darkness; what will your thoughts be, lying there alone? When you die, strange hands will lay you out, with grumbling and impatience; no one will bless you, no one will sigh for you, they only want to get rid of you as soon as may be; they will buy a coffin, take you to the grave as they did that poor woman today, and celebrate your memory at the tavern. In the grave, sleet, filth, wet snow—no need to put themselves out for you—‘Let her down, Vanuha; it’s just like her luck—even here, she is head-foremost, the hussy. Shorten the cord, you rascal.’ ‘It’s all right as it is.’ ‘All right, is it? Why, she’s on her side! She was a fellow-creature, after all! But, never mind, throw the earth on her.’ And they won’t care to waste much time quarrelling over you. They will scatter the wet blue clay as quick as they can and go off to the tavern ... and there your memory on earth will end; other women have children to go to their graves, fathers, husbands. While for you neither tear, nor sigh, nor remembrance; no one in the whole world will ever come to you, your name will vanish from the face of the earth—as though you had never existed, never been born at all! Nothing but filth and mud, however you knock at your coffin lid at night, when the dead arise, however you cry: ‘Let me out, kind people, to live in the light of day! My life was no life at all; my life has been thrown away like a dish-clout; it was drunk away in the tavern at the Haymarket; let me out, kind people, to live in the world again.’” And I worked myself up to such a pitch that I began to have a lump in my throat myself, and ... and all at once I stopped, sat up in dismay and, bending over apprehensively, began to listen with a beating heart. I had reason to be troubled. I had felt for some time that I was turning her soul upside down and rending her heart, and—and the more I was convinced of it, the more eagerly I desired to gain my object as quickly and as effectually as possible. It was the exercise of my skill that carried me away; yet it was not merely sport.... I knew I was speaking stiffly, artificially, even bookishly, in fact, I could not speak except “like a book.” But that did not trouble me: I knew, I felt that I should be understood and that this very bookishness might be an assistance. But now, having attained my effect, I was suddenly panic-stricken. Never before had I witnessed such despair! She was lying on her face, thrusting her face into the pillow and clutching it in both hands. Her heart was being torn. Her youthful body was shuddering all over as though in convulsions. Suppressed sobs rent her bosom and suddenly burst out in weeping and wailing, then she pressed closer into the pillow: she did not want anyone here, not a living soul, to know of her anguish and her tears. She bit the pillow, bit her hand till it bled (I saw that afterwards), or, thrusting her fingers into her dishevelled hair, seemed rigid with the effort of restraint, holding her breath and clenching her teeth. I began saying something, begging her to calm herself, but felt that I did not dare; and all at once, in a sort of cold shiver, almost in terror, began fumbling in the dark, trying hurriedly to get dressed to go. It was dark; though I tried my best I could not finish dressing quickly. Suddenly I felt a box of matches and a candlestick with a whole candle in it. As soon as the room was lighted up, Liza sprang up, sat up in bed, and with a contorted face, with a half insane smile, looked at me almost senselessly. I sat down beside her and took her hands; she came to herself, made an impulsive movement towards me, would have caught hold of me, but did not dare, and slowly bowed her head before me. “Liza, my dear, I was wrong ... forgive me, my dear,” I began, but she squeezed my hand in her fingers so tightly that I felt I was saying the wrong thing and stopped. “This is my address, Liza, come to me.” “I will come,” she answered resolutely, her head still bowed. “But now I am going, good-bye ... till we meet again.” I got up; she, too, stood up and suddenly flushed all over, gave a shudder, snatched up a shawl that was lying on a chair and muffled herself in it to her chin. As she did this she gave another sickly smile, blushed and looked at me strangely. I felt wretched; I was in haste to get away—to disappear. “Wait a minute,” she said suddenly, in the passage just at the doorway, stopping me with her hand on my overcoat. She put down the candle in hot haste and ran off; evidently she had thought of something or wanted to show me something. As she ran away she flushed, her eyes shone, and there was a smile on her lips—what was the meaning of it? Against my will I waited: she came back a minute later with an expression that seemed to ask forgiveness for something. In fact, it was not the same face, not the same look as the evening before: sullen, mistrustful and obstinate. Her eyes now were imploring, soft, and at the same time trustful, caressing, timid. The expression with which children look at people they are very fond of, of whom they are asking a favour. Her eyes were a light hazel, they were lovely eyes, full of life, and capable of expressing love as well as sullen hatred. Making no explanation, as though I, as a sort of higher being, must understand everything without explanations, she held out a piece of paper to me. Her whole face was positively beaming at that instant with naive, almost childish, triumph. I unfolded it. It was a letter to her from a medical student or someone of that sort—a very high-flown and flowery, but extremely respectful, love-letter. I don’t recall the words now, but I remember well that through the high-flown phrases there was apparent a genuine feeling, which cannot be feigned. When I had finished reading it I met her glowing, questioning, and childishly impatient eyes fixed upon me. She fastened her eyes upon my face and waited impatiently for what I should say. In a few words, hurriedly, but with a sort of joy and pride, she explained to me that she had been to a dance somewhere in a private house, a family of “very nice people, _who knew nothing_, absolutely nothing, for she had only come here so lately and it had all happened ... and she hadn’t made up her mind to stay and was certainly going away as soon as she had paid her debt...” and at that party there had been the student who had danced with her all the evening. He had talked to her, and it turned out that he had known her in old days at Riga when he was a child, they had played together, but a very long time ago—and he knew her parents, but _about this_ he knew nothing, nothing whatever, and had no suspicion! And the day after the dance (three days ago) he had sent her that letter through the friend with whom she had gone to the party ... and ... well, that was all. She dropped her shining eyes with a sort of bashfulness as she finished. The poor girl was keeping that student’s letter as a precious treasure, and had run to fetch it, her only treasure, because she did not want me to go away without knowing that she, too, was honestly and genuinely loved; that she, too, was addressed respectfully. No doubt that letter was destined to lie in her box and lead to nothing. But none the less, I am certain that she would keep it all her life as a precious treasure, as her pride and justification, and now at such a minute she had thought of that letter and brought it with naive pride to raise herself in my eyes that I might see, that I, too, might think well of her. I said nothing, pressed her hand and went out. I so longed to get away ... I walked all the way home, in spite of the fact that the melting snow was still falling in heavy flakes. I was exhausted, shattered, in bewilderment. But behind the bewilderment the truth was already gleaming. The loathsome truth. VIII It was some time, however, before I consented to recognise that truth. Waking up in the morning after some hours of heavy, leaden sleep, and immediately realising all that had happened on the previous day, I was positively amazed at my last night’s _sentimentality_ with Liza, at all those “outcries of horror and pity.” “To think of having such an attack of womanish hysteria, pah!” I concluded. And what did I thrust my address upon her for? What if she comes? Let her come, though; it doesn’t matter.... But _obviously_, that was not now the chief and the most important matter: I had to make haste and at all costs save my reputation in the eyes of Zverkov and Simonov as quickly as possible; that was the chief business. And I was so taken up that morning that I actually forgot all about Liza. First of all I had at once to repay what I had borrowed the day before from Simonov. I resolved on a desperate measure: to borrow fifteen roubles straight off from Anton Antonitch. As luck would have it he was in the best of humours that morning, and gave it to me at once, on the first asking. I was so delighted at this that, as I signed the IOU with a swaggering air, I told him casually that the night before “I had been keeping it up with some friends at the Hôtel de Paris; we were giving a farewell party to a comrade, in fact, I might say a friend of my childhood, and you know—a desperate rake, fearfully spoilt—of course, he belongs to a good family, and has considerable means, a brilliant career; he is witty, charming, a regular Lovelace, you understand; we drank an extra ‘half-dozen’ and ...” And it went off all right; all this was uttered very easily, unconstrainedly and complacently. On reaching home I promptly wrote to Simonov. To this hour I am lost in admiration when I recall the truly gentlemanly, good-humoured, candid tone of my letter. With tact and good-breeding, and, above all, entirely without superfluous words, I blamed myself for all that had happened. I defended myself, “if I really may be allowed to defend myself,” by alleging that being utterly unaccustomed to wine, I had been intoxicated with the first glass, which I said, I had drunk before they arrived, while I was waiting for them at the Hôtel de Paris between five and six o’clock. I begged Simonov’s pardon especially; I asked him to convey my explanations to all the others, especially to Zverkov, whom “I seemed to remember as though in a dream” I had insulted. I added that I would have called upon all of them myself, but my head ached, and besides I had not the face to. I was particularly pleased with a certain lightness, almost carelessness (strictly within the bounds of politeness, however), which was apparent in my style, and better than any possible arguments, gave them at once to understand that I took rather an independent view of “all that unpleasantness last night”; that I was by no means so utterly crushed as you, my friends, probably imagine; but on the contrary, looked upon it as a gentleman serenely respecting himself should look upon it. “On a young hero’s past no censure is cast!” “There is actually an aristocratic playfulness about it!” I thought admiringly, as I read over the letter. “And it’s all because I am an intellectual and cultivated man! Another man in my place would not have known how to extricate himself, but here I have got out of it and am as jolly as ever again, and all because I am ‘a cultivated and educated man of our day.’ And, indeed, perhaps, everything was due to the wine yesterday. H’m!” ... No, it was not the wine. I did not drink anything at all between five and six when I was waiting for them. I had lied to Simonov; I had lied shamelessly; and indeed I wasn’t ashamed now.... Hang it all though, the great thing was that I was rid of it. I put six roubles in the letter, sealed it up, and asked Apollon to take it to Simonov. When he learned that there was money in the letter, Apollon became more respectful and agreed to take it. Towards evening I went out for a walk. My head was still aching and giddy after yesterday. But as evening came on and the twilight grew denser, my impressions and, following them, my thoughts, grew more and more different and confused. Something was not dead within me, in the depths of my heart and conscience it would not die, and it showed itself in acute depression. For the most part I jostled my way through the most crowded business streets, along Myeshtchansky Street, along Sadovy Street and in Yusupov Garden. I always liked particularly sauntering along these streets in the dusk, just when there were crowds of working people of all sorts going home from their daily work, with faces looking cross with anxiety. What I liked was just that cheap bustle, that bare prose. On this occasion the jostling of the streets irritated me more than ever, I could not make out what was wrong with me, I could not find the clue, something seemed rising up continually in my soul, painfully, and refusing to be appeased. I returned home completely upset, it was just as though some crime were lying on my conscience. The thought that Liza was coming worried me continually. It seemed queer to me that of all my recollections of yesterday this tormented me, as it were, especially, as it were, quite separately. Everything else I had quite succeeded in forgetting by the evening; I dismissed it all and was still perfectly satisfied with my letter to Simonov. But on this point I was not satisfied at all. It was as though I were worried only by Liza. “What if she comes,” I thought incessantly, “well, it doesn’t matter, let her come! H’m! it’s horrid that she should see, for instance, how I live. Yesterday I seemed such a hero to her, while now, h’m! It’s horrid, though, that I have let myself go so, the room looks like a beggar’s. And I brought myself to go out to dinner in such a suit! And my American leather sofa with the stuffing sticking out. And my dressing-gown, which will not cover me, such tatters, and she will see all this and she will see Apollon. That beast is certain to insult her. He will fasten upon her in order to be rude to me. And I, of course, shall be panic-stricken as usual, I shall begin bowing and scraping before her and pulling my dressing-gown round me, I shall begin smiling, telling lies. Oh, the beastliness! And it isn’t the beastliness of it that matters most! There is something more important, more loathsome, viler! Yes, viler! And to put on that dishonest lying mask again! ...” When I reached that thought I fired up all at once. “Why dishonest? How dishonest? I was speaking sincerely last night. I remember there was real feeling in me, too. What I wanted was to excite an honourable feeling in her.... Her crying was a good thing, it will have a good effect.” Yet I could not feel at ease. All that evening, even when I had come back home, even after nine o’clock, when I calculated that Liza could not possibly come, still she haunted me, and what was worse, she came back to my mind always in the same position. One moment out of all that had happened last night stood vividly before my imagination; the moment when I struck a match and saw her pale, distorted face, with its look of torture. And what a pitiful, what an unnatural, what a distorted smile she had at that moment! But I did not know then, that fifteen years later I should still in my imagination see Liza, always with the pitiful, distorted, inappropriate smile which was on her face at that minute. Next day I was ready again to look upon it all as nonsense, due to over-excited nerves, and, above all, as _exaggerated_. I was always conscious of that weak point of mine, and sometimes very much afraid of it. “I exaggerate everything, that is where I go wrong,” I repeated to myself every hour. But, however, “Liza will very likely come all the same,” was the refrain with which all my reflections ended. I was so uneasy that I sometimes flew into a fury: “She’ll come, she is certain to come!” I cried, running about the room, “if not today, she will come tomorrow; she’ll find me out! The damnable romanticism of these pure hearts! Oh, the vileness—oh, the silliness—oh, the stupidity of these ‘wretched sentimental souls!’ Why, how fail to understand? How could one fail to understand? ...” But at this point I stopped short, and in great confusion, indeed. And how few, how few words, I thought, in passing, were needed; how little of the idyllic (and affectedly, bookishly, artificially idyllic too) had sufficed to turn a whole human life at once according to my will. That’s virginity, to be sure! Freshness of soil! At times a thought occurred to me, to go to her, “to tell her all,” and beg her not to come to me. But this thought stirred such wrath in me that I believed I should have crushed that “damned” Liza if she had chanced to be near me at the time. I should have insulted her, have spat at her, have turned her out, have struck her! One day passed, however, another and another; she did not come and I began to grow calmer. I felt particularly bold and cheerful after nine o’clock, I even sometimes began dreaming, and rather sweetly: I, for instance, became the salvation of Liza, simply through her coming to me and my talking to her.... I develop her, educate her. Finally, I notice that she loves me, loves me passionately. I pretend not to understand (I don’t know, however, why I pretend, just for effect, perhaps). At last all confusion, transfigured, trembling and sobbing, she flings herself at my feet and says that I am her saviour, and that she loves me better than anything in the world. I am amazed, but.... “Liza,” I say, “can you imagine that I have not noticed your love? I saw it all, I divined it, but I did not dare to approach you first, because I had an influence over you and was afraid that you would force yourself, from gratitude, to respond to my love, would try to rouse in your heart a feeling which was perhaps absent, and I did not wish that ... because it would be tyranny ... it would be indelicate (in short, I launch off at that point into European, inexplicably lofty subtleties a la George Sand), but now, now you are mine, you are my creation, you are pure, you are good, you are my noble wife. ‘Into my house come bold and free, Its rightful mistress there to be’.” Then we begin living together, go abroad and so on, and so on. In fact, in the end it seemed vulgar to me myself, and I began putting out my tongue at myself. Besides, they won’t let her out, “the hussy!” I thought. They don’t let them go out very readily, especially in the evening (for some reason I fancied she would come in the evening, and at seven o’clock precisely). Though she did say she was not altogether a slave there yet, and had certain rights; so, h’m! Damn it all, she will come, she is sure to come! It was a good thing, in fact, that Apollon distracted my attention at that time by his rudeness. He drove me beyond all patience! He was the bane of my life, the curse laid upon me by Providence. We had been squabbling continually for years, and I hated him. My God, how I hated him! I believe I had never hated anyone in my life as I hated him, especially at some moments. He was an elderly, dignified man, who worked part of his time as a tailor. But for some unknown reason he despised me beyond all measure, and looked down upon me insufferably. Though, indeed, he looked down upon everyone. Simply to glance at that flaxen, smoothly brushed head, at the tuft of hair he combed up on his forehead and oiled with sunflower oil, at that dignified mouth, compressed into the shape of the letter V, made one feel one was confronting a man who never doubted of himself. He was a pedant, to the most extreme point, the greatest pedant I had met on earth, and with that had a vanity only befitting Alexander of Macedon. He was in love with every button on his coat, every nail on his fingers—absolutely in love with them, and he looked it! In his behaviour to me he was a perfect tyrant, he spoke very little to me, and if he chanced to glance at me he gave me a firm, majestically self-confident and invariably ironical look that drove me sometimes to fury. He did his work with the air of doing me the greatest favour, though he did scarcely anything for me, and did not, indeed, consider himself bound to do anything. There could be no doubt that he looked upon me as the greatest fool on earth, and that “he did not get rid of me” was simply that he could get wages from me every month. He consented to do nothing for me for seven roubles a month. Many sins should be forgiven me for what I suffered from him. My hatred reached such a point that sometimes his very step almost threw me into convulsions. What I loathed particularly was his lisp. His tongue must have been a little too long or something of that sort, for he continually lisped, and seemed to be very proud of it, imagining that it greatly added to his dignity. He spoke in a slow, measured tone, with his hands behind his back and his eyes fixed on the ground. He maddened me particularly when he read aloud the psalms to himself behind his partition. Many a battle I waged over that reading! But he was awfully fond of reading aloud in the evenings, in a slow, even, sing-song voice, as though over the dead. It is interesting that that is how he has ended: he hires himself out to read the psalms over the dead, and at the same time he kills rats and makes blacking. But at that time I could not get rid of him, it was as though he were chemically combined with my existence. Besides, nothing would have induced him to consent to leave me. I could not live in furnished lodgings: my lodging was my private solitude, my shell, my cave, in which I concealed myself from all mankind, and Apollon seemed to me, for some reason, an integral part of that flat, and for seven years I could not turn him away. To be two or three days behind with his wages, for instance, was impossible. He would have made such a fuss, I should not have known where to hide my head. But I was so exasperated with everyone during those days, that I made up my mind for some reason and with some object to _punish_ Apollon and not to pay him for a fortnight the wages that were owing him. I had for a long time—for the last two years—been intending to do this, simply in order to teach him not to give himself airs with me, and to show him that if I liked I could withhold his wages. I purposed to say nothing to him about it, and was purposely silent indeed, in order to score off his pride and force him to be the first to speak of his wages. Then I would take the seven roubles out of a drawer, show him I have the money put aside on purpose, but that I won’t, I won’t, I simply won’t pay him his wages, I won’t just because that is “what I wish,” because “I am master, and it is for me to decide,” because he has been disrespectful, because he has been rude; but if he were to ask respectfully I might be softened and give it to him, otherwise he might wait another fortnight, another three weeks, a whole month.... But angry as I was, yet he got the better of me. I could not hold out for four days. He began as he always did begin in such cases, for there had been such cases already, there had been attempts (and it may be observed I knew all this beforehand, I knew his nasty tactics by heart). He would begin by fixing upon me an exceedingly severe stare, keeping it up for several minutes at a time, particularly on meeting me or seeing me out of the house. If I held out and pretended not to notice these stares, he would, still in silence, proceed to further tortures. All at once, _à propos_ of nothing, he would walk softly and smoothly into my room, when I was pacing up and down or reading, stand at the door, one hand behind his back and one foot behind the other, and fix upon me a stare more than severe, utterly contemptuous. If I suddenly asked him what he wanted, he would make me no answer, but continue staring at me persistently for some seconds, then, with a peculiar compression of his lips and a most significant air, deliberately turn round and deliberately go back to his room. Two hours later he would come out again and again present himself before me in the same way. It had happened that in my fury I did not even ask him what he wanted, but simply raised my head sharply and imperiously and began staring back at him. So we stared at one another for two minutes; at last he turned with deliberation and dignity and went back again for two hours. If I were still not brought to reason by all this, but persisted in my revolt, he would suddenly begin sighing while he looked at me, long, deep sighs as though measuring by them the depths of my moral degradation, and, of course, it ended at last by his triumphing completely: I raged and shouted, but still was forced to do what he wanted. This time the usual staring manoeuvres had scarcely begun when I lost my temper and flew at him in a fury. I was irritated beyond endurance apart from him. “Stay,” I cried, in a frenzy, as he was slowly and silently turning, with one hand behind his back, to go to his room. “Stay! Come back, come back, I tell you!” and I must have bawled so unnaturally, that he turned round and even looked at me with some wonder. However, he persisted in saying nothing, and that infuriated me. “How dare you come and look at me like that without being sent for? Answer!” After looking at me calmly for half a minute, he began turning round again. “Stay!” I roared, running up to him, “don’t stir! There. Answer, now: what did you come in to look at?” “If you have any order to give me it’s my duty to carry it out,” he answered, after another silent pause, with a slow, measured lisp, raising his eyebrows and calmly twisting his head from one side to another, all this with exasperating composure. “That’s not what I am asking you about, you torturer!” I shouted, turning crimson with anger. “I’ll tell you why you came here myself: you see, I don’t give you your wages, you are so proud you don’t want to bow down and ask for it, and so you come to punish me with your stupid stares, to worry me and you have no sus-pic-ion how stupid it is—stupid, stupid, stupid, stupid! ...” He would have turned round again without a word, but I seized him. “Listen,” I shouted to him. “Here’s the money, do you see, here it is,” (I took it out of the table drawer); “here’s the seven roubles complete, but you are not going to have it, you ... are ... not ... going ... to ... have it until you come respectfully with bowed head to beg my pardon. Do you hear?” “That cannot be,” he answered, with the most unnatural self-confidence. “It shall be so,” I said, “I give you my word of honour, it shall be!” “And there’s nothing for me to beg your pardon for,” he went on, as though he had not noticed my exclamations at all. “Why, besides, you called me a ‘torturer,’ for which I can summon you at the police-station at any time for insulting behaviour.” “Go, summon me,” I roared, “go at once, this very minute, this very second! You are a torturer all the same! a torturer!” But he merely looked at me, then turned, and regardless of my loud calls to him, he walked to his room with an even step and without looking round. “If it had not been for Liza nothing of this would have happened,” I decided inwardly. Then, after waiting a minute, I went myself behind his screen with a dignified and solemn air, though my heart was beating slowly and violently. “Apollon,” I said quietly and emphatically, though I was breathless, “go at once without a minute’s delay and fetch the police-officer.” He had meanwhile settled himself at his table, put on his spectacles and taken up some sewing. But, hearing my order, he burst into a guffaw. “At once, go this minute! Go on, or else you can’t imagine what will happen.” “You are certainly out of your mind,” he observed, without even raising his head, lisping as deliberately as ever and threading his needle. “Whoever heard of a man sending for the police against himself? And as for being frightened—you are upsetting yourself about nothing, for nothing will come of it.” “Go!” I shrieked, clutching him by the shoulder. I felt I should strike him in a minute. But I did not notice the door from the passage softly and slowly open at that instant and a figure come in, stop short, and begin staring at us in perplexity I glanced, nearly swooned with shame, and rushed back to my room. There, clutching at my hair with both hands, I leaned my head against the wall and stood motionless in that position. Two minutes later I heard Apollon’s deliberate footsteps. “There is some woman asking for you,” he said, looking at me with peculiar severity. Then he stood aside and let in Liza. He would not go away, but stared at us sarcastically. “Go away, go away,” I commanded in desperation. At that moment my clock began whirring and wheezing and struck seven. IX “Into my house come bold and free, Its rightful mistress there to be.” I stood before her crushed, crestfallen, revoltingly confused, and I believe I smiled as I did my utmost to wrap myself in the skirts of my ragged wadded dressing-gown—exactly as I had imagined the scene not long before in a fit of depression. After standing over us for a couple of minutes Apollon went away, but that did not make me more at ease. What made it worse was that she, too, was overwhelmed with confusion, more so, in fact, than I should have expected. At the sight of me, of course. “Sit down,” I said mechanically, moving a chair up to the table, and I sat down on the sofa. She obediently sat down at once and gazed at me open-eyed, evidently expecting something from me at once. This naïveté of expectation drove me to fury, but I restrained myself. She ought to have tried not to notice, as though everything had been as usual, while instead of that, she ... and I dimly felt that I should make her pay dearly for _all this_. “You have found me in a strange position, Liza,” I began, stammering and knowing that this was the wrong way to begin. “No, no, don’t imagine anything,” I cried, seeing that she had suddenly flushed. “I am not ashamed of my poverty.... On the contrary, I look with pride on my poverty. I am poor but honourable.... One can be poor and honourable,” I muttered. “However ... would you like tea?....” “No,” she was beginning. “Wait a minute.” I leapt up and ran to Apollon. I had to get out of the room somehow. “Apollon,” I whispered in feverish haste, flinging down before him the seven roubles which had remained all the time in my clenched fist, “here are your wages, you see I give them to you; but for that you must come to my rescue: bring me tea and a dozen rusks from the restaurant. If you won’t go, you’ll make me a miserable man! You don’t know what this woman is.... This is—everything! You may be imagining something.... But you don’t know what that woman is! ...” Apollon, who had already sat down to his work and put on his spectacles again, at first glanced askance at the money without speaking or putting down his needle; then, without paying the slightest attention to me or making any answer, he went on busying himself with his needle, which he had not yet threaded. I waited before him for three minutes with my arms crossed _à la Napoléon_. My temples were moist with sweat. I was pale, I felt it. But, thank God, he must have been moved to pity, looking at me. Having threaded his needle he deliberately got up from his seat, deliberately moved back his chair, deliberately took off his spectacles, deliberately counted the money, and finally asking me over his shoulder: “Shall I get a whole portion?” deliberately walked out of the room. As I was going back to Liza, the thought occurred to me on the way: shouldn’t I run away just as I was in my dressing-gown, no matter where, and then let happen what would? I sat down again. She looked at me uneasily. For some minutes we were silent. “I will kill him,” I shouted suddenly, striking the table with my fist so that the ink spurted out of the inkstand. “What are you saying!” she cried, starting. “I will kill him! kill him!” I shrieked, suddenly striking the table in absolute frenzy, and at the same time fully understanding how stupid it was to be in such a frenzy. “You don’t know, Liza, what that torturer is to me. He is my torturer.... He has gone now to fetch some rusks; he ...” And suddenly I burst into tears. It was an hysterical attack. How ashamed I felt in the midst of my sobs; but still I could not restrain them. She was frightened. “What is the matter? What is wrong?” she cried, fussing about me. “Water, give me water, over there!” I muttered in a faint voice, though I was inwardly conscious that I could have got on very well without water and without muttering in a faint voice. But I was, what is called, _putting it on_, to save appearances, though the attack was a genuine one. She gave me water, looking at me in bewilderment. At that moment Apollon brought in the tea. It suddenly seemed to me that this commonplace, prosaic tea was horribly undignified and paltry after all that had happened, and I blushed crimson. Liza looked at Apollon with positive alarm. He went out without a glance at either of us. “Liza, do you despise me?” I asked, looking at her fixedly, trembling with impatience to know what she was thinking. She was confused, and did not know what to answer. “Drink your tea,” I said to her angrily. I was angry with myself, but, of course, it was she who would have to pay for it. A horrible spite against her suddenly surged up in my heart; I believe I could have killed her. To revenge myself on her I swore inwardly not to say a word to her all the time. “She is the cause of it all,” I thought. Our silence lasted for five minutes. The tea stood on the table; we did not touch it. I had got to the point of purposely refraining from beginning in order to embarrass her further; it was awkward for her to begin alone. Several times she glanced at me with mournful perplexity. I was obstinately silent. I was, of course, myself the chief sufferer, because I was fully conscious of the disgusting meanness of my spiteful stupidity, and yet at the same time I could not restrain myself. “I want to... get away ... from there altogether,” she began, to break the silence in some way, but, poor girl, that was just what she ought not to have spoken about at such a stupid moment to a man so stupid as I was. My heart positively ached with pity for her tactless and unnecessary straightforwardness. But something hideous at once stifled all compassion in me; it even provoked me to greater venom. I did not care what happened. Another five minutes passed. “Perhaps I am in your way,” she began timidly, hardly audibly, and was getting up. But as soon as I saw this first impulse of wounded dignity I positively trembled with spite, and at once burst out. “Why have you come to me, tell me that, please?” I began, gasping for breath and regardless of logical connection in my words. I longed to have it all out at once, at one burst; I did not even trouble how to begin. “Why have you come? Answer, answer,” I cried, hardly knowing what I was doing. “I’ll tell you, my good girl, why you have come. You’ve come because I talked sentimental stuff to you then. So now you are soft as butter and longing for fine sentiments again. So you may as well know that I was laughing at you then. And I am laughing at you now. Why are you shuddering? Yes, I was laughing at you! I had been insulted just before, at dinner, by the fellows who came that evening before me. I came to you, meaning to thrash one of them, an officer; but I didn’t succeed, I didn’t find him; I had to avenge the insult on someone to get back my own again; you turned up, I vented my spleen on you and laughed at you. I had been humiliated, so I wanted to humiliate; I had been treated like a rag, so I wanted to show my power.... That’s what it was, and you imagined I had come there on purpose to save you. Yes? You imagined that? You imagined that?” I knew that she would perhaps be muddled and not take it all in exactly, but I knew, too, that she would grasp the gist of it, very well indeed. And so, indeed, she did. She turned white as a handkerchief, tried to say something, and her lips worked painfully; but she sank on a chair as though she had been felled by an axe. And all the time afterwards she listened to me with her lips parted and her eyes wide open, shuddering with awful terror. The cynicism, the cynicism of my words overwhelmed her.... “Save you!” I went on, jumping up from my chair and running up and down the room before her. “Save you from what? But perhaps I am worse than you myself. Why didn’t you throw it in my teeth when I was giving you that sermon: ‘But what did you come here yourself for? was it to read us a sermon?’ Power, power was what I wanted then, sport was what I wanted, I wanted to wring out your tears, your humiliation, your hysteria—that was what I wanted then! Of course, I couldn’t keep it up then, because I am a wretched creature, I was frightened, and, the devil knows why, gave you my address in my folly. Afterwards, before I got home, I was cursing and swearing at you because of that address, I hated you already because of the lies I had told you. Because I only like playing with words, only dreaming, but, do you know, what I really want is that you should all go to hell. That is what I want. I want peace; yes, I’d sell the whole world for a farthing, straight off, so long as I was left in peace. Is the world to go to pot, or am I to go without my tea? I say that the world may go to pot for me so long as I always get my tea. Did you know that, or not? Well, anyway, I know that I am a blackguard, a scoundrel, an egoist, a sluggard. Here I have been shuddering for the last three days at the thought of your coming. And do you know what has worried me particularly for these three days? That I posed as such a hero to you, and now you would see me in a wretched torn dressing-gown, beggarly, loathsome. I told you just now that I was not ashamed of my poverty; so you may as well know that I am ashamed of it; I am more ashamed of it than of anything, more afraid of it than of being found out if I were a thief, because I am as vain as though I had been skinned and the very air blowing on me hurt. Surely by now you must realise that I shall never forgive you for having found me in this wretched dressing-gown, just as I was flying at Apollon like a spiteful cur. The saviour, the former hero, was flying like a mangy, unkempt sheep-dog at his lackey, and the lackey was jeering at him! And I shall never forgive you for the tears I could not help shedding before you just now, like some silly woman put to shame! And for what I am confessing to you now, I shall never forgive you either! Yes—you must answer for it all because you turned up like this, because I am a blackguard, because I am the nastiest, stupidest, absurdest and most envious of all the worms on earth, who are not a bit better than I am, but, the devil knows why, are never put to confusion; while I shall always be insulted by every louse, that is my doom! And what is it to me that you don’t understand a word of this! And what do I care, what do I care about you, and whether you go to ruin there or not? Do you understand? How I shall hate you now after saying this, for having been here and listening. Why, it’s not once in a lifetime a man speaks out like this, and then it is in hysterics! ... What more do you want? Why do you still stand confronting me, after all this? Why are you worrying me? Why don’t you go?” But at this point a strange thing happened. I was so accustomed to think and imagine everything from books, and to picture everything in the world to myself just as I had made it up in my dreams beforehand, that I could not all at once take in this strange circumstance. What happened was this: Liza, insulted and crushed by me, understood a great deal more than I imagined. She understood from all this what a woman understands first of all, if she feels genuine love, that is, that I was myself unhappy. The frightened and wounded expression on her face was followed first by a look of sorrowful perplexity. When I began calling myself a scoundrel and a blackguard and my tears flowed (the tirade was accompanied throughout by tears) her whole face worked convulsively. She was on the point of getting up and stopping me; when I finished she took no notice of my shouting: “Why are you here, why don’t you go away?” but realised only that it must have been very bitter to me to say all this. Besides, she was so crushed, poor girl; she considered herself infinitely beneath me; how could she feel anger or resentment? She suddenly leapt up from her chair with an irresistible impulse and held out her hands, yearning towards me, though still timid and not daring to stir.... At this point there was a revulsion in my heart too. Then she suddenly rushed to me, threw her arms round me and burst into tears. I, too, could not restrain myself, and sobbed as I never had before. “They won’t let me ... I can’t be good!” I managed to articulate; then I went to the sofa, fell on it face downwards, and sobbed on it for a quarter of an hour in genuine hysterics. She came close to me, put her arms round me and stayed motionless in that position. But the trouble was that the hysterics could not go on for ever, and (I am writing the loathsome truth) lying face downwards on the sofa with my face thrust into my nasty leather pillow, I began by degrees to be aware of a far-away, involuntary but irresistible feeling that it would be awkward now for me to raise my head and look Liza straight in the face. Why was I ashamed? I don’t know, but I was ashamed. The thought, too, came into my overwrought brain that our parts now were completely changed, that she was now the heroine, while I was just a crushed and humiliated creature as she had been before me that night—four days before.... And all this came into my mind during the minutes I was lying on my face on the sofa. My God! surely I was not envious of her then. I don’t know, to this day I cannot decide, and at the time, of course, I was still less able to understand what I was feeling than now. I cannot get on without domineering and tyrannising over someone, but ... there is no explaining anything by reasoning and so it is useless to reason. I conquered myself, however, and raised my head; I had to do so sooner or later ... and I am convinced to this day that it was just because I was ashamed to look at her that another feeling was suddenly kindled and flamed up in my heart ... a feeling of mastery and possession. My eyes gleamed with passion, and I gripped her hands tightly. How I hated her and how I was drawn to her at that minute! The one feeling intensified the other. It was almost like an act of vengeance. At first there was a look of amazement, even of terror on her face, but only for one instant. She warmly and rapturously embraced me. X A quarter of an hour later I was rushing up and down the room in frenzied impatience, from minute to minute I went up to the screen and peeped through the crack at Liza. She was sitting on the ground with her head leaning against the bed, and must have been crying. But she did not go away, and that irritated me. This time she understood it all. I had insulted her finally, but ... there’s no need to describe it. She realised that my outburst of passion had been simply revenge, a fresh humiliation, and that to my earlier, almost causeless hatred was added now a _personal hatred_, born of envy.... Though I do not maintain positively that she understood all this distinctly; but she certainly did fully understand that I was a despicable man, and what was worse, incapable of loving her. I know I shall be told that this is incredible—but it is incredible to be as spiteful and stupid as I was; it may be added that it was strange I should not love her, or at any rate, appreciate her love. Why is it strange? In the first place, by then I was incapable of love, for I repeat, with me loving meant tyrannising and showing my moral superiority. I have never in my life been able to imagine any other sort of love, and have nowadays come to the point of sometimes thinking that love really consists in the right—freely given by the beloved object—to tyrannise over her. Even in my underground dreams I did not imagine love except as a struggle. I began it always with hatred and ended it with moral subjugation, and afterwards I never knew what to do with the subjugated object. And what is there to wonder at in that, since I had succeeded in so corrupting myself, since I was so out of touch with “real life,” as to have actually thought of reproaching her, and putting her to shame for having come to me to hear “fine sentiments”; and did not even guess that she had come not to hear fine sentiments, but to love me, because to a woman all reformation, all salvation from any sort of ruin, and all moral renewal is included in love and can only show itself in that form. I did not hate her so much, however, when I was running about the room and peeping through the crack in the screen. I was only insufferably oppressed by her being here. I wanted her to disappear. I wanted “peace,” to be left alone in my underground world. Real life oppressed me with its novelty so much that I could hardly breathe. But several minutes passed and she still remained, without stirring, as though she were unconscious. I had the shamelessness to tap softly at the screen as though to remind her.... She started, sprang up, and flew to seek her kerchief, her hat, her coat, as though making her escape from me.... Two minutes later she came from behind the screen and looked with heavy eyes at me. I gave a spiteful grin, which was forced, however, to _keep up appearances_, and I turned away from her eyes. “Good-bye,” she said, going towards the door. I ran up to her, seized her hand, opened it, thrust something in it and closed it again. Then I turned at once and dashed away in haste to the other corner of the room to avoid seeing, anyway.... I did mean a moment since to tell a lie—to write that I did this accidentally, not knowing what I was doing through foolishness, through losing my head. But I don’t want to lie, and so I will say straight out that I opened her hand and put the money in it ... from spite. It came into my head to do this while I was running up and down the room and she was sitting behind the screen. But this I can say for certain: though I did that cruel thing purposely, it was not an impulse from the heart, but came from my evil brain. This cruelty was so affected, so purposely made up, so completely a product of the brain, of books, that I could not even keep it up a minute—first I dashed away to avoid seeing her, and then in shame and despair rushed after Liza. I opened the door in the passage and began listening. “Liza! Liza!” I cried on the stairs, but in a low voice, not boldly. There was no answer, but I fancied I heard her footsteps, lower down on the stairs. “Liza!” I cried, more loudly. No answer. But at that minute I heard the stiff outer glass door open heavily with a creak and slam violently; the sound echoed up the stairs. She had gone. I went back to my room in hesitation. I felt horribly oppressed. I stood still at the table, beside the chair on which she had sat and looked aimlessly before me. A minute passed, suddenly I started; straight before me on the table I saw.... In short, I saw a crumpled blue five-rouble note, the one I had thrust into her hand a minute before. It was the same note; it could be no other, there was no other in the flat. So she had managed to fling it from her hand on the table at the moment when I had dashed into the further corner. Well! I might have expected that she would do that. Might I have expected it? No, I was such an egoist, I was so lacking in respect for my fellow-creatures that I could not even imagine she would do so. I could not endure it. A minute later I flew like a madman to dress, flinging on what I could at random and ran headlong after her. She could not have got two hundred paces away when I ran out into the street. It was a still night and the snow was coming down in masses and falling almost perpendicularly, covering the pavement and the empty street as though with a pillow. There was no one in the street, no sound was to be heard. The street lamps gave a disconsolate and useless glimmer. I ran two hundred paces to the cross-roads and stopped short. Where had she gone? And why was I running after her? Why? To fall down before her, to sob with remorse, to kiss her feet, to entreat her forgiveness! I longed for that, my whole breast was being rent to pieces, and never, never shall I recall that minute with indifference. But—what for? I thought. Should I not begin to hate her, perhaps, even tomorrow, just because I had kissed her feet today? Should I give her happiness? Had I not recognised that day, for the hundredth time, what I was worth? Should I not torture her? I stood in the snow, gazing into the troubled darkness and pondered this. “And will it not be better?” I mused fantastically, afterwards at home, stifling the living pang of my heart with fantastic dreams. “Will it not be better that she should keep the resentment of the insult for ever? Resentment—why, it is purification; it is a most stinging and painful consciousness! Tomorrow I should have defiled her soul and have exhausted her heart, while now the feeling of insult will never die in her heart, and however loathsome the filth awaiting her—the feeling of insult will elevate and purify her ... by hatred ... h’m! ... perhaps, too, by forgiveness.... Will all that make things easier for her though? ...” And, indeed, I will ask on my own account here, an idle question: which is better—cheap happiness or exalted sufferings? Well, which is better? So I dreamed as I sat at home that evening, almost dead with the pain in my soul. Never had I endured such suffering and remorse, yet could there have been the faintest doubt when I ran out from my lodging that I should turn back half-way? I never met Liza again and I have heard nothing of her. I will add, too, that I remained for a long time afterwards pleased with the phrase about the benefit from resentment and hatred in spite of the fact that I almost fell ill from misery. Even now, so many years later, all this is somehow a very evil memory. I have many evil memories now, but ... hadn’t I better end my “Notes” here? I believe I made a mistake in beginning to write them, anyway I have felt ashamed all the time I’ve been writing this story; so it’s hardly literature so much as a corrective punishment. Why, to tell long stories, showing how I have spoiled my life through morally rotting in my corner, through lack of fitting environment, through divorce from real life, and rankling spite in my underground world, would certainly not be interesting; a novel needs a hero, and all the traits for an anti-hero are _expressly_ gathered together here, and what matters most, it all produces an unpleasant impression, for we are all divorced from life, we are all cripples, every one of us, more or less. We are so divorced from it that we feel at once a sort of loathing for real life, and so cannot bear to be reminded of it. Why, we have come almost to looking upon real life as an effort, almost as hard work, and we are all privately agreed that it is better in books. And why do we fuss and fume sometimes? Why are we perverse and ask for something else? We don’t know what ourselves. It would be the worse for us if our petulant prayers were answered. Come, try, give any one of us, for instance, a little more independence, untie our hands, widen the spheres of our activity, relax the control and we ... yes, I assure you ... we should be begging to be under control again at once. I know that you will very likely be angry with me for that, and will begin shouting and stamping. Speak for yourself, you will say, and for your miseries in your underground holes, and don’t dare to say all of us—excuse me, gentlemen, I am not justifying myself with that “all of us.” As for what concerns me in particular I have only in my life carried to an extreme what you have not dared to carry halfway, and what’s more, you have taken your cowardice for good sense, and have found comfort in deceiving yourselves. So that perhaps, after all, there is more life in me than in you. Look into it more carefully! Why, we don’t even know what living means now, what it is, and what it is called? Leave us alone without books and we shall be lost and in confusion at once. We shall not know what to join on to, what to cling to, what to love and what to hate, what to respect and what to despise. We are oppressed at being men—men with a real individual body and blood, we are ashamed of it, we think it a disgrace and try to contrive to be some sort of impossible generalised man. We are stillborn, and for generations past have been begotten, not by living fathers, and that suits us better and better. We are developing a taste for it. Soon we shall contrive to be born somehow from an idea. But enough; I don’t want to write more from “Underground.” [The notes of this paradoxalist do not end here, however. He could not refrain from going on with them, but it seems to us that we may stop here.] *** END OF THE PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** Updated editions will replace the previous one--the old editions will be renamed. Creating the works from print editions not protected by U.S. copyright law means that no one owns a United States copyright in these works, so the Foundation (and you!) can copy and distribute it in the United States without permission and without paying copyright royalties. Special rules, set forth in the General Terms of Use part of this license, apply to copying and distributing Project Gutenberg-tm electronic works to protect the PROJECT GUTENBERG-tm concept and trademark. Project Gutenberg is a registered trademark, and may not be used if you charge for an eBook, except by following the terms of the trademark license, including paying royalties for use of the Project Gutenberg trademark. If you do not charge anything for copies of this eBook, complying with the trademark license is very easy. You may use this eBook for nearly any purpose such as creation of derivative works, reports, performances and research. Project Gutenberg eBooks may be modified and printed and given away--you may do practically ANYTHING in the United States with eBooks not protected by U.S. copyright law. Redistribution is subject to the trademark license, especially commercial redistribution. START: FULL LICENSE THE FULL PROJECT GUTENBERG LICENSE PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK To protect the Project Gutenberg-tm mission of promoting the free distribution of electronic works, by using or distributing this work (or any other work associated in any way with the phrase \"Project Gutenberg\"), you agree to comply with all the terms of the Full Project Gutenberg-tm License available with this file or online at www.gutenberg.org/license. Section 1. General Terms of Use and Redistributing Project Gutenberg-tm electronic works 1.A. By reading or using any part of this Project Gutenberg-tm electronic work, you indicate that you have read, understand, agree to and accept all the terms of this license and intellectual property (trademark/copyright) agreement. If you do not agree to abide by all the terms of this agreement, you must cease using and return or destroy all copies of Project Gutenberg-tm electronic works in your possession. If you paid a fee for obtaining a copy of or access to a Project Gutenberg-tm electronic work and you do not agree to be bound by the terms of this agreement, you may obtain a refund from the person or entity to whom you paid the fee as set forth in paragraph 1.E.8. 1.B. \"Project Gutenberg\" is a registered trademark. It may only be used on or associated in any way with an electronic work by people who agree to be bound by the terms of this agreement. There are a few things that you can do with most Project Gutenberg-tm electronic works even without complying with the full terms of this agreement. See paragraph 1.C below. There are a lot of things you can do with Project Gutenberg-tm electronic works if you follow the terms of this agreement and help preserve free future access to Project Gutenberg-tm electronic works. See paragraph 1.E below. 1.C. The Project Gutenberg Literary Archive Foundation (\"the Foundation\" or PGLAF), owns a compilation copyright in the collection of Project Gutenberg-tm electronic works. Nearly all the individual works in the collection are in the public domain in the United States. If an individual work is unprotected by copyright law in the United States and you are located in the United States, we do not claim a right to prevent you from copying, distributing, performing, displaying or creating derivative works based on the work as long as all references to Project Gutenberg are removed. Of course, we hope that you will support the Project Gutenberg-tm mission of promoting free access to electronic works by freely sharing Project Gutenberg-tm works in compliance with the terms of this agreement for keeping the Project Gutenberg-tm name associated with the work. You can easily comply with the terms of this agreement by keeping this work in the same format with its attached full Project Gutenberg-tm License when you share it without charge with others. 1.D. The copyright laws of the place where you are located also govern what you can do with this work. Copyright laws in most countries are in a constant state of change. If you are outside the United States, check the laws of your country in addition to the terms of this agreement before downloading, copying, displaying, performing, distributing or creating derivative works based on this work or any other Project Gutenberg-tm work. The Foundation makes no representations concerning the copyright status of any work in any country other than the United States. 1.E. Unless you have removed all references to Project Gutenberg: 1.E.1. The following sentence, with active links to, or other immediate access to, the full Project Gutenberg-tm License must appear prominently whenever any copy of a Project Gutenberg-tm work (any work on which the phrase \"Project Gutenberg\" appears, or with which the phrase \"Project Gutenberg\" is associated) is accessed, displayed, performed, viewed, copied or distributed: This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. 1.E.2. If an individual Project Gutenberg-tm electronic work is derived from texts not protected by U.S. copyright law (does not contain a notice indicating that it is posted with permission of the copyright holder), the work can be copied and distributed to anyone in the United States without paying any fees or charges. If you are redistributing or providing access to a work with the phrase \"Project Gutenberg\" associated with or appearing on the work, you must comply either with the requirements of paragraphs 1.E.1 through 1.E.7 or obtain permission for the use of the work and the Project Gutenberg-tm trademark as set forth in paragraphs 1.E.8 or 1.E.9. 1.E.3. If an individual Project Gutenberg-tm electronic work is posted with the permission of the copyright holder, your use and distribution must comply with both paragraphs 1.E.1 through 1.E.7 and any additional terms imposed by the copyright holder. Additional terms will be linked to the Project Gutenberg-tm License for all works posted with the permission of the copyright holder found at the beginning of this work. 1.E.4. Do not unlink or detach or remove the full Project Gutenberg-tm License terms from this work, or any files containing a part of this work or any other work associated with Project Gutenberg-tm. 1.E.5. Do not copy, display, perform, distribute or redistribute this electronic work, or any part of this electronic work, without prominently displaying the sentence set forth in paragraph 1.E.1 with active links or immediate access to the full terms of the Project Gutenberg-tm License. 1.E.6. You may convert to and distribute this work in any binary, compressed, marked up, nonproprietary or proprietary form, including any word processing or hypertext form. However, if you provide access to or distribute copies of a Project Gutenberg-tm work in a format other than \"Plain Vanilla ASCII\" or other format used in the official version posted on the official Project Gutenberg-tm website (www.gutenberg.org), you must, at no additional cost, fee or expense to the user, provide a copy, a means of exporting a copy, or a means of obtaining a copy upon request, of the work in its original \"Plain Vanilla ASCII\" or other form. Any alternate format must include the full Project Gutenberg-tm License as specified in paragraph 1.E.1. 1.E.7. Do not charge a fee for access to, viewing, displaying, performing, copying or distributing any Project Gutenberg-tm works unless you comply with paragraph 1.E.8 or 1.E.9. 1.E.8. You may charge a reasonable fee for copies of or providing access to or distributing Project Gutenberg-tm electronic works provided that: * You pay a royalty fee of 20% of the gross profits you derive from the use of Project Gutenberg-tm works calculated using the method you already use to calculate your applicable taxes. The fee is owed to the owner of the Project Gutenberg-tm trademark, but he has agreed to donate royalties under this paragraph to the Project Gutenberg Literary Archive Foundation. Royalty payments must be paid within 60 days following each date on which you prepare (or are legally required to prepare) your periodic tax returns. Royalty payments should be clearly marked as such and sent to the Project Gutenberg Literary Archive Foundation at the address specified in Section 4, \"Information about donations to the Project Gutenberg Literary Archive Foundation.\" * You provide a full refund of any money paid by a user who notifies you in writing (or by e-mail) within 30 days of receipt that s/he does not agree to the terms of the full Project Gutenberg-tm License. You must require such a user to return or destroy all copies of the works possessed in a physical medium and discontinue all use of and all access to other copies of Project Gutenberg-tm works. * You provide, in accordance with paragraph 1.F.3, a full refund of any money paid for a work or a replacement copy, if a defect in the electronic work is discovered and reported to you within 90 days of receipt of the work. * You comply with all other terms of this agreement for free distribution of Project Gutenberg-tm works. 1.E.9. If you wish to charge a fee or distribute a Project Gutenberg-tm electronic work or group of works on different terms than are set forth in this agreement, you must obtain permission in writing from the Project Gutenberg Literary Archive Foundation, the manager of the Project Gutenberg-tm trademark. Contact the Foundation as set forth in Section 3 below. 1.F. 1.F.1. Project Gutenberg volunteers and employees expend considerable effort to identify, do copyright research on, transcribe and proofread works not protected by U.S. copyright law in creating the Project Gutenberg-tm collection. Despite these efforts, Project Gutenberg-tm electronic works, and the medium on which they may be stored, may contain \"Defects,\" such as, but not limited to, incomplete, inaccurate or corrupt data, transcription errors, a copyright or other intellectual property infringement, a defective or damaged disk or other medium, a computer virus, or computer codes that damage or cannot be read by your equipment. 1.F.2. LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the \"Right of Replacement or Refund\" described in paragraph 1.F.3, the Project Gutenberg Literary Archive Foundation, the owner of the Project Gutenberg-tm trademark, and any other party distributing a Project Gutenberg-tm electronic work under this agreement, disclaim all liability to you for damages, costs and expenses, including legal fees. YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE PROVIDED IN PARAGRAPH 1.F.3. YOU AGREE THAT THE FOUNDATION, THE TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH DAMAGE. 1.F.3. LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a defect in this electronic work within 90 days of receiving it, you can receive a refund of the money (if any) you paid for it by sending a written explanation to the person you received the work from. If you received the work on a physical medium, you must return the medium with your written explanation. The person or entity that provided you with the defective work may elect to provide a replacement copy in lieu of a refund. If you received the work electronically, the person or entity providing it to you may choose to give you a second opportunity to receive the work electronically in lieu of a refund. If the second copy is also defective, you may demand a refund in writing without further opportunities to fix the problem. 1.F.4. Except for the limited right of replacement or refund set forth in paragraph 1.F.3, this work is provided to you 'AS-IS', WITH NO OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE. 1.F.5. Some states do not allow disclaimers of certain implied warranties or the exclusion or limitation of certain types of damages. If any disclaimer or limitation set forth in this agreement violates the law of the state applicable to this agreement, the agreement shall be interpreted to make the maximum disclaimer or limitation permitted by the applicable state law. The invalidity or unenforceability of any provision of this agreement shall not void the remaining provisions. 1.F.6. INDEMNITY - You agree to indemnify and hold the Foundation, the trademark owner, any agent or employee of the Foundation, anyone providing copies of Project Gutenberg-tm electronic works in accordance with this agreement, and any volunteers associated with the production, promotion and distribution of Project Gutenberg-tm electronic works, harmless from all liability, costs and expenses, including legal fees, that arise directly or indirectly from any of the following which you do or cause to occur: (a) distribution of this or any Project Gutenberg-tm work, (b) alteration, modification, or additions or deletions to any Project Gutenberg-tm work, and (c) any Defect you cause. Section 2. Information about the Mission of Project Gutenberg-tm Project Gutenberg-tm is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete, old, middle-aged and new computers. It exists because of the efforts of hundreds of volunteers and donations from people in all walks of life. Volunteers and financial support to provide volunteers with the assistance they need are critical to reaching Project Gutenberg-tm's goals and ensuring that the Project Gutenberg-tm collection will remain freely available for generations to come. In 2001, the Project Gutenberg Literary Archive Foundation was created to provide a secure and permanent future for Project Gutenberg-tm and future generations. To learn more about the Project Gutenberg Literary Archive Foundation and how your efforts and donations can help, see Sections 3 and 4 and the Foundation information page at www.gutenberg.org Section 3. Information about the Project Gutenberg Literary Archive Foundation The Project Gutenberg Literary Archive Foundation is a non-profit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the Internal Revenue Service. The Foundation's EIN or federal tax identification number is 64-6221541. Contributions to the Project Gutenberg Literary Archive Foundation are tax deductible to the full extent permitted by U.S. federal laws and your state's laws. The Foundation's business office is located at 809 North 1500 West, Salt Lake City, UT 84116, (801) 596-1887. Email contact links and up to date contact information can be found at the Foundation's website and official page at www.gutenberg.org/contact Section 4. Information about Donations to the Project Gutenberg Literary Archive Foundation Project Gutenberg-tm depends upon and cannot survive without widespread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine-readable form accessible by the widest array of equipment including outdated equipment. Many small donations ($1 to $5,000) are particularly important to maintaining tax exempt status with the IRS. The Foundation is committed to complying with the laws regulating charities and charitable donations in all 50 states of the United States. Compliance requirements are not uniform and it takes a considerable effort, much paperwork and many fees to meet and keep up with these requirements. We do not solicit donations in locations where we have not received written confirmation of compliance. To SEND DONATIONS or determine the status of compliance for any particular state visit www.gutenberg.org/donate While we cannot and do not solicit contributions from states where we have not met the solicitation requirements, we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate. International donations are gratefully accepted, but we cannot make any statements concerning tax treatment of donations received from outside the United States. U.S. laws alone swamp our small staff. Please check the Project Gutenberg web pages for current donation methods and addresses. Donations are accepted in a number of other ways including checks, online payments and credit card donations. To donate, please visit: www.gutenberg.org/donate Section 5. General Information About Project Gutenberg-tm electronic works Professor Michael S. Hart was the originator of the Project Gutenberg-tm concept of a library of electronic works that could be freely shared with anyone. For forty years, he produced and distributed Project Gutenberg-tm eBooks with only a loose network of volunteer support. Project Gutenberg-tm eBooks are often created from several printed editions, all of which are confirmed as not protected by copyright in the U.S. unless a copyright notice is included. Thus, we do not necessarily keep eBooks in compliance with any particular paper edition. Most people start at our website which has the main PG search facility: www.gutenberg.org This website includes information about Project Gutenberg-tm, including how to make donations to the Project Gutenberg Literary Archive Foundation, how to help produce our new eBooks, and how to subscribe to our email newsletter to hear about new eBooks. \n\n\nNext, we form a clean text with lower case (without special characters, digits and extra spaces) and split it into individual word, for word score computation and formation of the word histogram.\nThe reason to form a clean text is so that the algorithm won’t treat, i.e. “understanding” and understanding, as two different words.\n\n# generate clean text\nclean_text = text.lower() # convert all uppercase characters into lowercase characters\nclean_text = re.sub(r'\\W',' ',clean_text) # replace character other than [a-zA-Z0-9] with empty space\nclean_text = re.sub(r'\\d',' ',clean_text) # replace digit with empty space\nclean_text = re.sub(r'\\s+',' ',clean_text) # replace one or more spaces with a single space\n\nprint(clean_text)\n\n the project gutenberg ebook of notes from the underground by fyodor dostoyevsky this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www gutenberg org if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook title notes from the underground author fyodor dostoyevsky translator constance garnett release date july ebook most recently updated december language english produced by judith boss html version by al haines start of the project gutenberg ebook notes from the underground notes from the underground by fyodor dostoyevsky contents notes from the underground part i underground i ii iii iv v vi vii viii ix x xi part ii à propos of the wet snow i ii iii iv v vi vii viii ix x notes from the underground a novel the author of the diary and the diary itself are of course imaginary nevertheless it is clear that such persons as the writer of these notes not only may but positively must exist in our society when we consider the circumstances in the midst of which our society is formed i have tried to expose to the view of the public more distinctly than is commonly done one of the characters of the recent past he is one of the representatives of a generation still living in this fragment entitled underground this person introduces himself and his views and as it were tries to explain the causes owing to which he has made his appearance and was bound to make his appearance in our midst in the second fragment there are added the actual notes of this person concerning certain events in his life author s note part i underground i i am a sick man i am a spiteful man i am an unattractive man i believe my liver is diseased however i know nothing at all about my disease and do not know for certain what ails me i don t consult a doctor for it and never have though i have a respect for medicine and doctors besides i am extremely superstitious sufficiently so to respect medicine anyway i am well educated enough not to be superstitious but i am superstitious no i refuse to consult a doctor from spite that you probably will not understand well i understand it though of course i can t explain who it is precisely that i am mortifying in this case by my spite i am perfectly well aware that i cannot pay out the doctors by not consulting them i know better than anyone that by all this i am only injuring myself and no one else but still if i don t consult a doctor it is from spite my liver is bad well let it get worse i have been going on like that for a long time twenty years now i am forty i used to be in the government service but am no longer i was a spiteful official i was rude and took pleasure in being so i did not take bribes you see so i was bound to find a recompense in that at least a poor jest but i will not scratch it out i wrote it thinking it would sound very witty but now that i have seen myself that i only wanted to show off in a despicable way i will not scratch it out on purpose when petitioners used to come for information to the table at which i sat i used to grind my teeth at them and felt intense enjoyment when i succeeded in making anybody unhappy i almost did succeed for the most part they were all timid people of course they were petitioners but of the uppish ones there was one officer in particular i could not endure he simply would not be humble and clanked his sword in a disgusting way i carried on a feud with him for eighteen months over that sword at last i got the better of him he left off clanking it that happened in my youth though but do you know gentlemen what was the chief point about my spite why the whole point the real sting of it lay in the fact that continually even in the moment of the acutest spleen i was inwardly conscious with shame that i was not only not a spiteful but not even an embittered man that i was simply scaring sparrows at random and amusing myself by it i might foam at the mouth but bring me a doll to play with give me a cup of tea with sugar in it and maybe i should be appeased i might even be genuinely touched though probably i should grind my teeth at myself afterwards and lie awake at night with shame for months after that was my way i was lying when i said just now that i was a spiteful official i was lying from spite i was simply amusing myself with the petitioners and with the officer and in reality i never could become spiteful i was conscious every moment in myself of many very many elements absolutely opposite to that i felt them positively swarming in me these opposite elements i knew that they had been swarming in me all my life and craving some outlet from me but i would not let them would not let them purposely would not let them come out they tormented me till i was ashamed they drove me to convulsions and sickened me at last how they sickened me now are not you fancying gentlemen that i am expressing remorse for something now that i am asking your forgiveness for something i am sure you are fancying that however i assure you i do not care if you are it was not only that i could not become spiteful i did not know how to become anything neither spiteful nor kind neither a rascal nor an honest man neither a hero nor an insect now i am living out my life in my corner taunting myself with the spiteful and useless consolation that an intelligent man cannot become anything seriously and it is only the fool who becomes anything yes a man in the nineteenth century must and morally ought to be pre eminently a characterless creature a man of character an active man is pre eminently a limited creature that is my conviction of forty years i am forty years old now and you know forty years is a whole lifetime you know it is extreme old age to live longer than forty years is bad manners is vulgar immoral who does live beyond forty answer that sincerely and honestly i will tell you who do fools and worthless fellows i tell all old men that to their face all these venerable old men all these silver haired and reverend seniors i tell the whole world that to its face i have a right to say so for i shall go on living to sixty myself to seventy to eighty stay let me take breath you imagine no doubt gentlemen that i want to amuse you you are mistaken in that too i am by no means such a mirthful person as you imagine or as you may imagine however irritated by all this babble and i feel that you are irritated you think fit to ask me who i am then my answer is i am a collegiate assessor i was in the service that i might have something to eat and solely for that reason and when last year a distant relation left me six thousand roubles in his will i immediately retired from the service and settled down in my corner i used to live in this corner before but now i have settled down in it my room is a wretched horrid one in the outskirts of the town my servant is an old country woman ill natured from stupidity and moreover there is always a nasty smell about her i am told that the petersburg climate is bad for me and that with my small means it is very expensive to live in petersburg i know all that better than all these sage and experienced counsellors and monitors but i am remaining in petersburg i am not going away from petersburg i am not going away because ech why it is absolutely no matter whether i am going away or not going away but what can a decent man speak of with most pleasure answer of himself well so i will talk about myself ii i want now to tell you gentlemen whether you care to hear it or not why i could not even become an insect i tell you solemnly that i have many times tried to become an insect but i was not equal even to that i swear gentlemen that to be too conscious is an illness a real thorough going illness for man s everyday needs it would have been quite enough to have the ordinary human consciousness that is half or a quarter of the amount which falls to the lot of a cultivated man of our unhappy nineteenth century especially one who has the fatal ill luck to inhabit petersburg the most theoretical and intentional town on the whole terrestrial globe there are intentional and unintentional towns it would have been quite enough for instance to have the consciousness by which all so called direct persons and men of action live i bet you think i am writing all this from affectation to be witty at the expense of men of action and what is more that from ill bred affectation i am clanking a sword like my officer but gentlemen whoever can pride himself on his diseases and even swagger over them though after all everyone does do that people do pride themselves on their diseases and i do may be more than anyone we will not dispute it my contention was absurd but yet i am firmly persuaded that a great deal of consciousness every sort of consciousness in fact is a disease i stick to that let us leave that too for a minute tell me this why does it happen that at the very yes at the very moments when i am most capable of feeling every refinement of all that is sublime and beautiful as they used to say at one time it would as though of design happen to me not only to feel but to do such ugly things such that well in short actions that all perhaps commit but which as though purposely occurred to me at the very time when i was most conscious that they ought not to be committed the more conscious i was of goodness and of all that was sublime and beautiful the more deeply i sank into my mire and the more ready i was to sink in it altogether but the chief point was that all this was as it were not accidental in me but as though it were bound to be so it was as though it were my most normal condition and not in the least disease or depravity so that at last all desire in me to struggle against this depravity passed it ended by my almost believing perhaps actually believing that this was perhaps my normal condition but at first in the beginning what agonies i endured in that struggle i did not believe it was the same with other people and all my life i hid this fact about myself as a secret i was ashamed even now perhaps i am ashamed i got to the point of feeling a sort of secret abnormal despicable enjoyment in returning home to my corner on some disgusting petersburg night acutely conscious that that day i had committed a loathsome action again that what was done could never be undone and secretly inwardly gnawing gnawing at myself for it tearing and consuming myself till at last the bitterness turned into a sort of shameful accursed sweetness and at last into positive real enjoyment yes into enjoyment into enjoyment i insist upon that i have spoken of this because i keep wanting to know for a fact whether other people feel such enjoyment i will explain the enjoyment was just from the too intense consciousness of one s own degradation it was from feeling oneself that one had reached the last barrier that it was horrible but that it could not be otherwise that there was no escape for you that you never could become a different man that even if time and faith were still left you to change into something different you would most likely not wish to change or if you did wish to even then you would do nothing because perhaps in reality there was nothing for you to change into and the worst of it was and the root of it all that it was all in accord with the normal fundamental laws of over acute consciousness and with the inertia that was the direct result of those laws and that consequently one was not only unable to change but could do absolutely nothing thus it would follow as the result of acute consciousness that one is not to blame in being a scoundrel as though that were any consolation to the scoundrel once he has come to realise that he actually is a scoundrel but enough ech i have talked a lot of nonsense but what have i explained how is enjoyment in this to be explained but i will explain it i will get to the bottom of it that is why i have taken up my pen i for instance have a great deal of _amour propre_ i am as suspicious and prone to take offence as a humpback or a dwarf but upon my word i sometimes have had moments when if i had happened to be slapped in the face i should perhaps have been positively glad of it i say in earnest that i should probably have been able to discover even in that a peculiar sort of enjoyment the enjoyment of course of despair but in despair there are the most intense enjoyments especially when one is very acutely conscious of the hopelessness of one s position and when one is slapped in the face why then the consciousness of being rubbed into a pulp would positively overwhelm one the worst of it is look at it which way one will it still turns out that i was always the most to blame in everything and what is most humiliating of all to blame for no fault of my own but so to say through the laws of nature in the first place to blame because i am cleverer than any of the people surrounding me i have always considered myself cleverer than any of the people surrounding me and sometimes would you believe it have been positively ashamed of it at any rate i have all my life as it were turned my eyes away and never could look people straight in the face to blame finally because even if i had had magnanimity i should only have had more suffering from the sense of its uselessness i should certainly have never been able to do anything from being magnanimous neither to forgive for my assailant would perhaps have slapped me from the laws of nature and one cannot forgive the laws of nature nor to forget for even if it were owing to the laws of nature it is insulting all the same finally even if i had wanted to be anything but magnanimous had desired on the contrary to revenge myself on my assailant i could not have revenged myself on any one for anything because i should certainly never have made up my mind to do anything even if i had been able to why should i not have made up my mind about that in particular i want to say a few words iii with people who know how to revenge themselves and to stand up for themselves in general how is it done why when they are possessed let us suppose by the feeling of revenge then for the time there is nothing else but that feeling left in their whole being such a gentleman simply dashes straight for his object like an infuriated bull with its horns down and nothing but a wall will stop him by the way facing the wall such gentlemen that is the direct persons and men of action are genuinely nonplussed for them a wall is not an evasion as for us people who think and consequently do nothing it is not an excuse for turning aside an excuse for which we are always very glad though we scarcely believe in it ourselves as a rule no they are nonplussed in all sincerity the wall has for them something tranquillising morally soothing final maybe even something mysterious but of the wall later well such a direct person i regard as the real normal man as his tender mother nature wished to see him when she graciously brought him into being on the earth i envy such a man till i am green in the face he is stupid i am not disputing that but perhaps the normal man should be stupid how do you know perhaps it is very beautiful in fact and i am the more persuaded of that suspicion if one can call it so by the fact that if you take for instance the antithesis of the normal man that is the man of acute consciousness who has come of course not out of the lap of nature but out of a retort this is almost mysticism gentlemen but i suspect this too this retort made man is sometimes so nonplussed in the presence of his antithesis that with all his exaggerated consciousness he genuinely thinks of himself as a mouse and not a man it may be an acutely conscious mouse yet it is a mouse while the other is a man and therefore et caetera et caetera and the worst of it is he himself his very own self looks on himself as a mouse no one asks him to do so and that is an important point now let us look at this mouse in action let us suppose for instance that it feels insulted too and it almost always does feel insulted and wants to revenge itself too there may even be a greater accumulation of spite in it than in _l homme de la nature et de la vérité_ the base and nasty desire to vent that spite on its assailant rankles perhaps even more nastily in it than in _l homme de la nature et de la vérité_ for through his innate stupidity the latter looks upon his revenge as justice pure and simple while in consequence of his acute consciousness the mouse does not believe in the justice of it to come at last to the deed itself to the very act of revenge apart from the one fundamental nastiness the luckless mouse succeeds in creating around it so many other nastinesses in the form of doubts and questions adds to the one question so many unsettled questions that there inevitably works up around it a sort of fatal brew a stinking mess made up of its doubts emotions and of the contempt spat upon it by the direct men of action who stand solemnly about it as judges and arbitrators laughing at it till their healthy sides ache of course the only thing left for it is to dismiss all that with a wave of its paw and with a smile of assumed contempt in which it does not even itself believe creep ignominiously into its mouse hole there in its nasty stinking underground home our insulted crushed and ridiculed mouse promptly becomes absorbed in cold malignant and above all everlasting spite for forty years together it will remember its injury down to the smallest most ignominious details and every time will add of itself details still more ignominious spitefully teasing and tormenting itself with its own imagination it will itself be ashamed of its imaginings but yet it will recall it all it will go over and over every detail it will invent unheard of things against itself pretending that those things might happen and will forgive nothing maybe it will begin to revenge itself too but as it were piecemeal in trivial ways from behind the stove incognito without believing either in its own right to vengeance or in the success of its revenge knowing that from all its efforts at revenge it will suffer a hundred times more than he on whom it revenges itself while he i daresay will not even scratch himself on its deathbed it will recall it all over again with interest accumulated over all the years and but it is just in that cold abominable half despair half belief in that conscious burying oneself alive for grief in the underworld for forty years in that acutely recognised and yet partly doubtful hopelessness of one s position in that hell of unsatisfied desires turned inward in that fever of oscillations of resolutions determined for ever and repented of again a minute later that the savour of that strange enjoyment of which i have spoken lies it is so subtle so difficult of analysis that persons who are a little limited or even simply persons of strong nerves will not understand a single atom of it possibly you will add on your own account with a grin people will not understand it either who have never received a slap in the face and in that way you will politely hint to me that i too perhaps have had the experience of a slap in the face in my life and so i speak as one who knows i bet that you are thinking that but set your minds at rest gentlemen i have not received a slap in the face though it is absolutely a matter of indifference to me what you may think about it possibly i even regret myself that i have given so few slaps in the face during my life but enough not another word on that subject of such extreme interest to you i will continue calmly concerning persons with strong nerves who do not understand a certain refinement of enjoyment though in certain circumstances these gentlemen bellow their loudest like bulls though this let us suppose does them the greatest credit yet as i have said already confronted with the impossible they subside at once the impossible means the stone wall what stone wall why of course the laws of nature the deductions of natural science mathematics as soon as they prove to you for instance that you are descended from a monkey then it is no use scowling accept it for a fact when they prove to you that in reality one drop of your own fat must be dearer to you than a hundred thousand of your fellow creatures and that this conclusion is the final solution of all so called virtues and duties and all such prejudices and fancies then you have just to accept it there is no help for it for twice two is a law of mathematics just try refuting it upon my word they will shout at you it is no use protesting it is a case of twice two makes four nature does not ask your permission she has nothing to do with your wishes and whether you like her laws or dislike them you are bound to accept her as she is and consequently all her conclusions a wall you see is a wall and so on and so on merciful heavens but what do i care for the laws of nature and arithmetic when for some reason i dislike those laws and the fact that twice two makes four of course i cannot break through the wall by battering my head against it if i really have not the strength to knock it down but i am not going to be reconciled to it simply because it is a stone wall and i have not the strength as though such a stone wall really were a consolation and really did contain some word of conciliation simply because it is as true as twice two makes four oh absurdity of absurdities how much better it is to understand it all to recognise it all all the impossibilities and the stone wall not to be reconciled to one of those impossibilities and stone walls if it disgusts you to be reconciled to it by the way of the most inevitable logical combinations to reach the most revolting conclusions on the everlasting theme that even for the stone wall you are yourself somehow to blame though again it is as clear as day you are not to blame in the least and therefore grinding your teeth in silent impotence to sink into luxurious inertia brooding on the fact that there is no one even for you to feel vindictive against that you have not and perhaps never will have an object for your spite that it is a sleight of hand a bit of juggling a card sharper s trick that it is simply a mess no knowing what and no knowing who but in spite of all these uncertainties and jugglings still there is an ache in you and the more you do not know the worse the ache iv ha ha ha you will be finding enjoyment in toothache next you cry with a laugh well even in toothache there is enjoyment i answer i had toothache for a whole month and i know there is in that case of course people are not spiteful in silence but moan but they are not candid moans they are malignant moans and the malignancy is the whole point the enjoyment of the sufferer finds expression in those moans if he did not feel enjoyment in them he would not moan it is a good example gentlemen and i will develop it those moans express in the first place all the aimlessness of your pain which is so humiliating to your consciousness the whole legal system of nature on which you spit disdainfully of course but from which you suffer all the same while she does not they express the consciousness that you have no enemy to punish but that you have pain the consciousness that in spite of all possible wagenheims you are in complete slavery to your teeth that if someone wishes it your teeth will leave off aching and if he does not they will go on aching another three months and that finally if you are still contumacious and still protest all that is left you for your own gratification is to thrash yourself or beat your wall with your fist as hard as you can and absolutely nothing more well these mortal insults these jeers on the part of someone unknown end at last in an enjoyment which sometimes reaches the highest degree of voluptuousness i ask you gentlemen listen sometimes to the moans of an educated man of the nineteenth century suffering from toothache on the second or third day of the attack when he is beginning to moan not as he moaned on the first day that is not simply because he has toothache not just as any coarse peasant but as a man affected by progress and european civilisation a man who is divorced from the soil and the national elements as they express it now a days his moans become nasty disgustingly malignant and go on for whole days and nights and of course he knows himself that he is doing himself no sort of good with his moans he knows better than anyone that he is only lacerating and harassing himself and others for nothing he knows that even the audience before whom he is making his efforts and his whole family listen to him with loathing do not put a ha porth of faith in him and inwardly understand that he might moan differently more simply without trills and flourishes and that he is only amusing himself like that from ill humour from malignancy well in all these recognitions and disgraces it is that there lies a voluptuous pleasure as though he would say i am worrying you i am lacerating your hearts i am keeping everyone in the house awake well stay awake then you too feel every minute that i have toothache i am not a hero to you now as i tried to seem before but simply a nasty person an impostor well so be it then i am very glad that you see through me it is nasty for you to hear my despicable moans well let it be nasty here i will let you have a nastier flourish in a minute you do not understand even now gentlemen no it seems our development and our consciousness must go further to understand all the intricacies of this pleasure you laugh delighted my jests gentlemen are of course in bad taste jerky involved lacking self confidence but of course that is because i do not respect myself can a man of perception respect himself at all v come can a man who attempts to find enjoyment in the very feeling of his own degradation possibly have a spark of respect for himself i am not saying this now from any mawkish kind of remorse and indeed i could never endure saying forgive me papa i won t do it again not because i am incapable of saying that on the contrary perhaps just because i have been too capable of it and in what a way too as though of design i used to get into trouble in cases when i was not to blame in any way that was the nastiest part of it at the same time i was genuinely touched and penitent i used to shed tears and of course deceived myself though i was not acting in the least and there was a sick feeling in my heart at the time for that one could not blame even the laws of nature though the laws of nature have continually all my life offended me more than anything it is loathsome to remember it all but it was loathsome even then of course a minute or so later i would realise wrathfully that it was all a lie a revolting lie an affected lie that is all this penitence this emotion these vows of reform you will ask why did i worry myself with such antics answer because it was very dull to sit with one s hands folded and so one began cutting capers that is really it observe yourselves more carefully gentlemen then you will understand that it is so i invented adventures for myself and made up a life so as at least to live in some way how many times it has happened to me well for instance to take offence simply on purpose for nothing and one knows oneself of course that one is offended at nothing that one is putting it on but yet one brings oneself at last to the point of being really offended all my life i have had an impulse to play such pranks so that in the end i could not control it in myself another time twice in fact i tried hard to be in love i suffered too gentlemen i assure you in the depth of my heart there was no faith in my suffering only a faint stir of mockery but yet i did suffer and in the real orthodox way i was jealous beside myself and it was all from _ennui_ gentlemen all from _ennui _ inertia overcame me you know the direct legitimate fruit of consciousness is inertia that is conscious sitting with the hands folded i have referred to this already i repeat i repeat with emphasis all direct persons and men of action are active just because they are stupid and limited how explain that i will tell you in consequence of their limitation they take immediate and secondary causes for primary ones and in that way persuade themselves more quickly and easily than other people do that they have found an infallible foundation for their activity and their minds are at ease and you know that is the chief thing to begin to act you know you must first have your mind completely at ease and no trace of doubt left in it why how am i for example to set my mind at rest where are the primary causes on which i am to build where are my foundations where am i to get them from i exercise myself in reflection and consequently with me every primary cause at once draws after itself another still more primary and so on to infinity that is just the essence of every sort of consciousness and reflection it must be a case of the laws of nature again what is the result of it in the end why just the same remember i spoke just now of vengeance i am sure you did not take it in i said that a man revenges himself because he sees justice in it therefore he has found a primary cause that is justice and so he is at rest on all sides and consequently he carries out his revenge calmly and successfully being persuaded that he is doing a just and honest thing but i see no justice in it i find no sort of virtue in it either and consequently if i attempt to revenge myself it is only out of spite spite of course might overcome everything all my doubts and so might serve quite successfully in place of a primary cause precisely because it is not a cause but what is to be done if i have not even spite i began with that just now you know in consequence again of those accursed laws of consciousness anger in me is subject to chemical disintegration you look into it the object flies off into air your reasons evaporate the criminal is not to be found the wrong becomes not a wrong but a phantom something like the toothache for which no one is to blame and consequently there is only the same outlet left again that is to beat the wall as hard as you can so you give it up with a wave of the hand because you have not found a fundamental cause and try letting yourself be carried away by your feelings blindly without reflection without a primary cause repelling consciousness at least for a time hate or love if only not to sit with your hands folded the day after tomorrow at the latest you will begin despising yourself for having knowingly deceived yourself result a soap bubble and inertia oh gentlemen do you know perhaps i consider myself an intelligent man only because all my life i have been able neither to begin nor to finish anything granted i am a babbler a harmless vexatious babbler like all of us but what is to be done if the direct and sole vocation of every intelligent man is babble that is the intentional pouring of water through a sieve vi oh if i had done nothing simply from laziness heavens how i should have respected myself then i should have respected myself because i should at least have been capable of being lazy there would at least have been one quality as it were positive in me in which i could have believed myself question what is he answer a sluggard how very pleasant it would have been to hear that of oneself it would mean that i was positively defined it would mean that there was something to say about me sluggard why it is a calling and vocation it is a career do not jest it is so i should then be a member of the best club by right and should find my occupation in continually respecting myself i knew a gentleman who prided himself all his life on being a connoisseur of lafitte he considered this as his positive virtue and never doubted himself he died not simply with a tranquil but with a triumphant conscience and he was quite right too then i should have chosen a career for myself i should have been a sluggard and a glutton not a simple one but for instance one with sympathies for everything sublime and beautiful how do you like that i have long had visions of it that sublime and beautiful weighs heavily on my mind at forty but that is at forty then oh then it would have been different i should have found for myself a form of activity in keeping with it to be precise drinking to the health of everything sublime and beautiful i should have snatched at every opportunity to drop a tear into my glass and then to drain it to all that is sublime and beautiful i should then have turned everything into the sublime and the beautiful in the nastiest unquestionable trash i should have sought out the sublime and the beautiful i should have exuded tears like a wet sponge an artist for instance paints a picture worthy of gay at once i drink to the health of the artist who painted the picture worthy of gay because i love all that is sublime and beautiful an author has written _as you will _ at once i drink to the health of anyone you will because i love all that is sublime and beautiful i should claim respect for doing so i should persecute anyone who would not show me respect i should live at ease i should die with dignity why it is charming perfectly charming and what a good round belly i should have grown what a treble chin i should have established what a ruby nose i should have coloured for myself so that everyone would have said looking at me here is an asset here is something real and solid and say what you like it is very agreeable to hear such remarks about oneself in this negative age vii but these are all golden dreams oh tell me who was it first announced who was it first proclaimed that man only does nasty things because he does not know his own interests and that if he were enlightened if his eyes were opened to his real normal interests man would at once cease to do nasty things would at once become good and noble because being enlightened and understanding his real advantage he would see his own advantage in the good and nothing else and we all know that not one man can consciously act against his own interests consequently so to say through necessity he would begin doing good oh the babe oh the pure innocent child why in the first place when in all these thousands of years has there been a time when man has acted only from his own interest what is to be done with the millions of facts that bear witness that men _consciously_ that is fully understanding their real interests have left them in the background and have rushed headlong on another path to meet peril and danger compelled to this course by nobody and by nothing but as it were simply disliking the beaten track and have obstinately wilfully struck out another difficult absurd way seeking it almost in the darkness so i suppose this obstinacy and perversity were pleasanter to them than any advantage advantage what is advantage and will you take it upon yourself to define with perfect accuracy in what the advantage of man consists and what if it so happens that a man s advantage _sometimes_ not only may but even must consist in his desiring in certain cases what is harmful to himself and not advantageous and if so if there can be such a case the whole principle falls into dust what do you think are there such cases you laugh laugh away gentlemen but only answer me have man s advantages been reckoned up with perfect certainty are there not some which not only have not been included but cannot possibly be included under any classification you see you gentlemen have to the best of my knowledge taken your whole register of human advantages from the averages of statistical figures and politico economical formulas your advantages are prosperity wealth freedom peace and so on and so on so that the man who should for instance go openly and knowingly in opposition to all that list would to your thinking and indeed mine too of course be an obscurantist or an absolute madman would not he but you know this is what is surprising why does it so happen that all these statisticians sages and lovers of humanity when they reckon up human advantages invariably leave out one they don t even take it into their reckoning in the form in which it should be taken and the whole reckoning depends upon that it would be no greater matter they would simply have to take it this advantage and add it to the list but the trouble is that this strange advantage does not fall under any classification and is not in place in any list i have a friend for instance ech gentlemen but of course he is your friend too and indeed there is no one no one to whom he is not a friend when he prepares for any undertaking this gentleman immediately explains to you elegantly and clearly exactly how he must act in accordance with the laws of reason and truth what is more he will talk to you with excitement and passion of the true normal interests of man with irony he will upbraid the short sighted fools who do not understand their own interests nor the true significance of virtue and within a quarter of an hour without any sudden outside provocation but simply through something inside him which is stronger than all his interests he will go off on quite a different tack that is act in direct opposition to what he has just been saying about himself in opposition to the laws of reason in opposition to his own advantage in fact in opposition to everything i warn you that my friend is a compound personality and therefore it is difficult to blame him as an individual the fact is gentlemen it seems there must really exist something that is dearer to almost every man than his greatest advantages or not to be illogical there is a most advantageous advantage the very one omitted of which we spoke just now which is more important and more advantageous than all other advantages for the sake of which a man if necessary is ready to act in opposition to all laws that is in opposition to reason honour peace prosperity in fact in opposition to all those excellent and useful things if only he can attain that fundamental most advantageous advantage which is dearer to him than all yes but it s advantage all the same you will retort but excuse me i ll make the point clear and it is not a case of playing upon words what matters is that this advantage is remarkable from the very fact that it breaks down all our classifications and continually shatters every system constructed by lovers of mankind for the benefit of mankind in fact it upsets everything but before i mention this advantage to you i want to compromise myself personally and therefore i boldly declare that all these fine systems all these theories for explaining to mankind their real normal interests in order that inevitably striving to pursue these interests they may at once become good and noble are in my opinion so far mere logical exercises yes logical exercises why to maintain this theory of the regeneration of mankind by means of the pursuit of his own advantage is to my mind almost the same thing as to affirm for instance following buckle that through civilisation mankind becomes softer and consequently less bloodthirsty and less fitted for warfare logically it does seem to follow from his arguments but man has such a predilection for systems and abstract deductions that he is ready to distort the truth intentionally he is ready to deny the evidence of his senses only to justify his logic i take this example because it is the most glaring instance of it only look about you blood is being spilt in streams and in the merriest way as though it were champagne take the whole of the nineteenth century in which buckle lived take napoleon the great and also the present one take north america the eternal union take the farce of schleswig holstein and what is it that civilisation softens in us the only gain of civilisation for mankind is the greater capacity for variety of sensations and absolutely nothing more and through the development of this many sidedness man may come to finding enjoyment in bloodshed in fact this has already happened to him have you noticed that it is the most civilised gentlemen who have been the subtlest slaughterers to whom the attilas and stenka razins could not hold a candle and if they are not so conspicuous as the attilas and stenka razins it is simply because they are so often met with are so ordinary and have become so familiar to us in any case civilisation has made mankind if not more bloodthirsty at least more vilely more loathsomely bloodthirsty in old days he saw justice in bloodshed and with his conscience at peace exterminated those he thought proper now we do think bloodshed abominable and yet we engage in this abomination and with more energy than ever which is worse decide that for yourselves they say that cleopatra excuse an instance from roman history was fond of sticking gold pins into her slave girls breasts and derived gratification from their screams and writhings you will say that that was in the comparatively barbarous times that these are barbarous times too because also comparatively speaking pins are stuck in even now that though man has now learned to see more clearly than in barbarous ages he is still far from having learnt to act as reason and science would dictate but yet you are fully convinced that he will be sure to learn when he gets rid of certain old bad habits and when common sense and science have completely re educated human nature and turned it in a normal direction you are confident that then man will cease from _intentional_ error and will so to say be compelled not to want to set his will against his normal interests that is not all then you say science itself will teach man though to my mind it s a superfluous luxury that he never has really had any caprice or will of his own and that he himself is something of the nature of a piano key or the stop of an organ and that there are besides things called the laws of nature so that everything he does is not done by his willing it but is done of itself by the laws of nature consequently we have only to discover these laws of nature and man will no longer have to answer for his actions and life will become exceedingly easy for him all human actions will then of course be tabulated according to these laws mathematically like tables of logarithms up to and entered in an index or better still there would be published certain edifying works of the nature of encyclopaedic lexicons in which everything will be so clearly calculated and explained that there will be no more incidents or adventures in the world then this is all what you say new economic relations will be established all ready made and worked out with mathematical exactitude so that every possible question will vanish in the twinkling of an eye simply because every possible answer to it will be provided then the palace of crystal will be built then in fact those will be halcyon days of course there is no guaranteeing this is my comment that it will not be for instance frightfully dull then for what will one have to do when everything will be calculated and tabulated but on the other hand everything will be extraordinarily rational of course boredom may lead you to anything it is boredom sets one sticking golden pins into people but all that would not matter what is bad this is my comment again is that i dare say people will be thankful for the gold pins then man is stupid you know phenomenally stupid or rather he is not at all stupid but he is so ungrateful that you could not find another like him in all creation i for instance would not be in the least surprised if all of a sudden _à propos_ of nothing in the midst of general prosperity a gentleman with an ignoble or rather with a reactionary and ironical countenance were to arise and putting his arms akimbo say to us all i say gentleman hadn t we better kick over the whole show and scatter rationalism to the winds simply to send these logarithms to the devil and to enable us to live once more at our own sweet foolish will that again would not matter but what is annoying is that he would be sure to find followers such is the nature of man and all that for the most foolish reason which one would think was hardly worth mentioning that is that man everywhere and at all times whoever he may be has preferred to act as he chose and not in the least as his reason and advantage dictated and one may choose what is contrary to one s own interests and sometimes one _positively ought_ that is my idea one s own free unfettered choice one s own caprice however wild it may be one s own fancy worked up at times to frenzy is that very most advantageous advantage which we have overlooked which comes under no classification and against which all systems and theories are continually being shattered to atoms and how do these wiseacres know that man wants a normal a virtuous choice what has made them conceive that man must want a rationally advantageous choice what man wants is simply _independent_ choice whatever that independence may cost and wherever it may lead and choice of course the devil only knows what choice viii ha ha ha but you know there is no such thing as choice in reality say what you like you will interpose with a chuckle science has succeeded in so far analysing man that we know already that choice and what is called freedom of will is nothing else than stay gentlemen i meant to begin with that myself i confess i was rather frightened i was just going to say that the devil only knows what choice depends on and that perhaps that was a very good thing but i remembered the teaching of science and pulled myself up and here you have begun upon it indeed if there really is some day discovered a formula for all our desires and caprices that is an explanation of what they depend upon by what laws they arise how they develop what they are aiming at in one case and in another and so on that is a real mathematical formula then most likely man will at once cease to feel desire indeed he will be certain to for who would want to choose by rule besides he will at once be transformed from a human being into an organ stop or something of the sort for what is a man without desires without free will and without choice if not a stop in an organ what do you think let us reckon the chances can such a thing happen or not h m you decide our choice is usually mistaken from a false view of our advantage we sometimes choose absolute nonsense because in our foolishness we see in that nonsense the easiest means for attaining a supposed advantage but when all that is explained and worked out on paper which is perfectly possible for it is contemptible and senseless to suppose that some laws of nature man will never understand then certainly so called desires will no longer exist for if a desire should come into conflict with reason we shall then reason and not desire because it will be impossible retaining our reason to be _senseless_ in our desires and in that way knowingly act against reason and desire to injure ourselves and as all choice and reasoning can be really calculated because there will some day be discovered the laws of our so called free will so joking apart there may one day be something like a table constructed of them so that we really shall choose in accordance with it if for instance some day they calculate and prove to me that i made a long nose at someone because i could not help making a long nose at him and that i had to do it in that particular way what _freedom_ is left me especially if i am a learned man and have taken my degree somewhere then i should be able to calculate my whole life for thirty years beforehand in short if this could be arranged there would be nothing left for us to do anyway we should have to understand that and in fact we ought unwearyingly to repeat to ourselves that at such and such a time and in such and such circumstances nature does not ask our leave that we have got to take her as she is and not fashion her to suit our fancy and if we really aspire to formulas and tables of rules and well even to the chemical retort there s no help for it we must accept the retort too or else it will be accepted without our consent yes but here i come to a stop gentlemen you must excuse me for being over philosophical it s the result of forty years underground allow me to indulge my fancy you see gentlemen reason is an excellent thing there s no disputing that but reason is nothing but reason and satisfies only the rational side of man s nature while will is a manifestation of the whole life that is of the whole human life including reason and all the impulses and although our life in this manifestation of it is often worthless yet it is life and not simply extracting square roots here i for instance quite naturally want to live in order to satisfy all my capacities for life and not simply my capacity for reasoning that is not simply one twentieth of my capacity for life what does reason know reason only knows what it has succeeded in learning some things perhaps it will never learn this is a poor comfort but why not say so frankly and human nature acts as a whole with everything that is in it consciously or unconsciously and even if it goes wrong it lives i suspect gentlemen that you are looking at me with compassion you tell me again that an enlightened and developed man such in short as the future man will be cannot consciously desire anything disadvantageous to himself that that can be proved mathematically i thoroughly agree it can by mathematics but i repeat for the hundredth time there is one case one only when man may consciously purposely desire what is injurious to himself what is stupid very stupid simply in order to have the right to desire for himself even what is very stupid and not to be bound by an obligation to desire only what is sensible of course this very stupid thing this caprice of ours may be in reality gentlemen more advantageous for us than anything else on earth especially in certain cases and in particular it may be more advantageous than any advantage even when it does us obvious harm and contradicts the soundest conclusions of our reason concerning our advantage for in any circumstances it preserves for us what is most precious and most important that is our personality our individuality some you see maintain that this really is the most precious thing for mankind choice can of course if it chooses be in agreement with reason and especially if this be not abused but kept within bounds it is profitable and sometimes even praiseworthy but very often and even most often choice is utterly and stubbornly opposed to reason and and do you know that that too is profitable sometimes even praiseworthy gentlemen let us suppose that man is not stupid indeed one cannot refuse to suppose that if only from the one consideration that if man is stupid then who is wise but if he is not stupid he is monstrously ungrateful phenomenally ungrateful in fact i believe that the best definition of man is the ungrateful biped but that is not all that is not his worst defect his worst defect is his perpetual moral obliquity perpetual from the days of the flood to the schleswig holstein period moral obliquity and consequently lack of good sense for it has long been accepted that lack of good sense is due to no other cause than moral obliquity put it to the test and cast your eyes upon the history of mankind what will you see is it a grand spectacle grand if you like take the colossus of rhodes for instance that s worth something with good reason mr anaevsky testifies of it that some say that it is the work of man s hands while others maintain that it has been created by nature herself is it many coloured may be it is many coloured too if one takes the dress uniforms military and civilian of all peoples in all ages that alone is worth something and if you take the undress uniforms you will never get to the end of it no historian would be equal to the job is it monotonous may be it s monotonous too it s fighting and fighting they are fighting now they fought first and they fought last you will admit that it is almost too monotonous in short one may say anything about the history of the world anything that might enter the most disordered imagination the only thing one can t say is that it s rational the very word sticks in one s throat and indeed this is the odd thing that is continually happening there are continually turning up in life moral and rational persons sages and lovers of humanity who make it their object to live all their lives as morally and rationally as possible to be so to speak a light to their neighbours simply in order to show them that it is possible to live morally and rationally in this world and yet we all know that those very people sooner or later have been false to themselves playing some queer trick often a most unseemly one now i ask you what can be expected of man since he is a being endowed with strange qualities shower upon him every earthly blessing drown him in a sea of happiness so that nothing but bubbles of bliss can be seen on the surface give him economic prosperity such that he should have nothing else to do but sleep eat cakes and busy himself with the continuation of his species and even then out of sheer ingratitude sheer spite man would play you some nasty trick he would even risk his cakes and would deliberately desire the most fatal rubbish the most uneconomical absurdity simply to introduce into all this positive good sense his fatal fantastic element it is just his fantastic dreams his vulgar folly that he will desire to retain simply in order to prove to himself as though that were so necessary that men still are men and not the keys of a piano which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar and that is not all even if man really were nothing but a piano key even if this were proved to him by natural science and mathematics even then he would not become reasonable but would purposely do something perverse out of simple ingratitude simply to gain his point and if he does not find means he will contrive destruction and chaos will contrive sufferings of all sorts only to gain his point he will launch a curse upon the world and as only man can curse it is his privilege the primary distinction between him and other animals may be by his curse alone he will attain his object that is convince himself that he is a man and not a piano key if you say that all this too can be calculated and tabulated chaos and darkness and curses so that the mere possibility of calculating it all beforehand would stop it all and reason would reassert itself then man would purposely go mad in order to be rid of reason and gain his point i believe in it i answer for it for the whole work of man really seems to consist in nothing but proving to himself every minute that he is a man and not a piano key it may be at the cost of his skin it may be by cannibalism and this being so can one help being tempted to rejoice that it has not yet come off and that desire still depends on something we don t know you will scream at me that is if you condescend to do so that no one is touching my free will that all they are concerned with is that my will should of itself of its own free will coincide with my own normal interests with the laws of nature and arithmetic good heavens gentlemen what sort of free will is left when we come to tabulation and arithmetic when it will all be a case of twice two make four twice two makes four without my will as if free will meant that ix gentlemen i am joking and i know myself that my jokes are not brilliant but you know one can take everything as a joke i am perhaps jesting against the grain gentlemen i am tormented by questions answer them for me you for instance want to cure men of their old habits and reform their will in accordance with science and good sense but how do you know not only that it is possible but also that it is _desirable_ to reform man in that way and what leads you to the conclusion that man s inclinations _need_ reforming in short how do you know that such a reformation will be a benefit to man and to go to the root of the matter why are you so positively convinced that not to act against his real normal interests guaranteed by the conclusions of reason and arithmetic is certainly always advantageous for man and must always be a law for mankind so far you know this is only your supposition it may be the law of logic but not the law of humanity you think gentlemen perhaps that i am mad allow me to defend myself i agree that man is pre eminently a creative animal predestined to strive consciously for an object and to engage in engineering that is incessantly and eternally to make new roads _wherever they may lead_ but the reason why he wants sometimes to go off at a tangent may just be that he is _predestined_ to make the road and perhaps too that however stupid the direct practical man may be the thought sometimes will occur to him that the road almost always does lead _somewhere_ and that the destination it leads to is less important than the process of making it and that the chief thing is to save the well conducted child from despising engineering and so giving way to the fatal idleness which as we all know is the mother of all the vices man likes to make roads and to create that is a fact beyond dispute but why has he such a passionate love for destruction and chaos also tell me that but on that point i want to say a couple of words myself may it not be that he loves chaos and destruction there can be no disputing that he does sometimes love it because he is instinctively afraid of attaining his object and completing the edifice he is constructing who knows perhaps he only loves that edifice from a distance and is by no means in love with it at close quarters perhaps he only loves building it and does not want to live in it but will leave it when completed for the use of _les animaux domestiques_ such as the ants the sheep and so on now the ants have quite a different taste they have a marvellous edifice of that pattern which endures for ever the ant heap with the ant heap the respectable race of ants began and with the ant heap they will probably end which does the greatest credit to their perseverance and good sense but man is a frivolous and incongruous creature and perhaps like a chess player loves the process of the game not the end of it and who knows there is no saying with certainty perhaps the only goal on earth to which mankind is striving lies in this incessant process of attaining in other words in life itself and not in the thing to be attained which must always be expressed as a formula as positive as twice two makes four and such positiveness is not life gentlemen but is the beginning of death anyway man has always been afraid of this mathematical certainty and i am afraid of it now granted that man does nothing but seek that mathematical certainty he traverses oceans sacrifices his life in the quest but to succeed really to find it dreads i assure you he feels that when he has found it there will be nothing for him to look for when workmen have finished their work they do at least receive their pay they go to the tavern then they are taken to the police station and there is occupation for a week but where can man go anyway one can observe a certain awkwardness about him when he has attained such objects he loves the process of attaining but does not quite like to have attained and that of course is very absurd in fact man is a comical creature there seems to be a kind of jest in it all but yet mathematical certainty is after all something insufferable twice two makes four seems to me simply a piece of insolence twice two makes four is a pert coxcomb who stands with arms akimbo barring your path and spitting i admit that twice two makes four is an excellent thing but if we are to give everything its due twice two makes five is sometimes a very charming thing too and why are you so firmly so triumphantly convinced that only the normal and the positive in other words only what is conducive to welfare is for the advantage of man is not reason in error as regards advantage does not man perhaps love something besides well being perhaps he is just as fond of suffering perhaps suffering is just as great a benefit to him as well being man is sometimes extraordinarily passionately in love with suffering and that is a fact there is no need to appeal to universal history to prove that only ask yourself if you are a man and have lived at all as far as my personal opinion is concerned to care only for well being seems to me positively ill bred whether it s good or bad it is sometimes very pleasant too to smash things i hold no brief for suffering nor for well being either i am standing for my caprice and for its being guaranteed to me when necessary suffering would be out of place in vaudevilles for instance i know that in the palace of crystal it is unthinkable suffering means doubt negation and what would be the good of a palace of crystal if there could be any doubt about it and yet i think man will never renounce real suffering that is destruction and chaos why suffering is the sole origin of consciousness though i did lay it down at the beginning that consciousness is the greatest misfortune for man yet i know man prizes it and would not give it up for any satisfaction consciousness for instance is infinitely superior to twice two makes four once you have mathematical certainty there is nothing left to do or to understand there will be nothing left but to bottle up your five senses and plunge into contemplation while if you stick to consciousness even though the same result is attained you can at least flog yourself at times and that will at any rate liven you up reactionary as it is corporal punishment is better than nothing x you believe in a palace of crystal that can never be destroyed a palace at which one will not be able to put out one s tongue or make a long nose on the sly and perhaps that is just why i am afraid of this edifice that it is of crystal and can never be destroyed and that one cannot put one s tongue out at it even on the sly you see if it were not a palace but a hen house i might creep into it to avoid getting wet and yet i would not call the hen house a palace out of gratitude to it for keeping me dry you laugh and say that in such circumstances a hen house is as good as a mansion yes i answer if one had to live simply to keep out of the rain but what is to be done if i have taken it into my head that that is not the only object in life and that if one must live one had better live in a mansion that is my choice my desire you will only eradicate it when you have changed my preference well do change it allure me with something else give me another ideal but meanwhile i will not take a hen house for a mansion the palace of crystal may be an idle dream it may be that it is inconsistent with the laws of nature and that i have invented it only through my own stupidity through the old fashioned irrational habits of my generation but what does it matter to me that it is inconsistent that makes no difference since it exists in my desires or rather exists as long as my desires exist perhaps you are laughing again laugh away i will put up with any mockery rather than pretend that i am satisfied when i am hungry i know anyway that i will not be put off with a compromise with a recurring zero simply because it is consistent with the laws of nature and actually exists i will not accept as the crown of my desires a block of buildings with tenements for the poor on a lease of a thousand years and perhaps with a sign board of a dentist hanging out destroy my desires eradicate my ideals show me something better and i will follow you you will say perhaps that it is not worth your trouble but in that case i can give you the same answer we are discussing things seriously but if you won t deign to give me your attention i will drop your acquaintance i can retreat into my underground hole but while i am alive and have desires i would rather my hand were withered off than bring one brick to such a building don t remind me that i have just rejected the palace of crystal for the sole reason that one cannot put out one s tongue at it i did not say because i am so fond of putting my tongue out perhaps the thing i resented was that of all your edifices there has not been one at which one could not put out one s tongue on the contrary i would let my tongue be cut off out of gratitude if things could be so arranged that i should lose all desire to put it out it is not my fault that things cannot be so arranged and that one must be satisfied with model flats then why am i made with such desires can i have been constructed simply in order to come to the conclusion that all my construction is a cheat can this be my whole purpose i do not believe it but do you know what i am convinced that we underground folk ought to be kept on a curb though we may sit forty years underground without speaking when we do come out into the light of day and break out we talk and talk and talk xi the long and the short of it is gentlemen that it is better to do nothing better conscious inertia and so hurrah for underground though i have said that i envy the normal man to the last drop of my bile yet i should not care to be in his place such as he is now though i shall not cease envying him no no anyway the underground life is more advantageous there at any rate one can oh but even now i am lying i am lying because i know myself that it is not underground that is better but something different quite different for which i am thirsting but which i cannot find damn underground i will tell you another thing that would be better and that is if i myself believed in anything of what i have just written i swear to you gentlemen there is not one thing not one word of what i have written that i really believe that is i believe it perhaps but at the same time i feel and suspect that i am lying like a cobbler then why have you written all this you will say to me i ought to put you underground for forty years without anything to do and then come to you in your cellar to find out what stage you have reached how can a man be left with nothing to do for forty years isn t that shameful isn t that humiliating you will say perhaps wagging your heads contemptuously you thirst for life and try to settle the problems of life by a logical tangle and how persistent how insolent are your sallies and at the same time what a scare you are in you talk nonsense and are pleased with it you say impudent things and are in continual alarm and apologising for them you declare that you are afraid of nothing and at the same time try to ingratiate yourself in our good opinion you declare that you are gnashing your teeth and at the same time you try to be witty so as to amuse us you know that your witticisms are not witty but you are evidently well satisfied with their literary value you may perhaps have really suffered but you have no respect for your own suffering you may have sincerity but you have no modesty out of the pettiest vanity you expose your sincerity to publicity and ignominy you doubtlessly mean to say something but hide your last word through fear because you have not the resolution to utter it and only have a cowardly impudence you boast of consciousness but you are not sure of your ground for though your mind works yet your heart is darkened and corrupt and you cannot have a full genuine consciousness without a pure heart and how intrusive you are how you insist and grimace lies lies lies of course i have myself made up all the things you say that too is from underground i have been for forty years listening to you through a crack under the floor i have invented them myself there was nothing else i could invent it is no wonder that i have learned it by heart and it has taken a literary form but can you really be so credulous as to think that i will print all this and give it to you to read too and another problem why do i call you gentlemen why do i address you as though you really were my readers such confessions as i intend to make are never printed nor given to other people to read anyway i am not strong minded enough for that and i don t see why i should be but you see a fancy has occurred to me and i want to realise it at all costs let me explain every man has reminiscences which he would not tell to everyone but only to his friends he has other matters in his mind which he would not reveal even to his friends but only to himself and that in secret but there are other things which a man is afraid to tell even to himself and every decent man has a number of such things stored away in his mind the more decent he is the greater the number of such things in his mind anyway i have only lately determined to remember some of my early adventures till now i have always avoided them even with a certain uneasiness now when i am not only recalling them but have actually decided to write an account of them i want to try the experiment whether one can even with oneself be perfectly open and not take fright at the whole truth i will observe in parenthesis that heine says that a true autobiography is almost an impossibility and that man is bound to lie about himself he considers that rousseau certainly told lies about himself in his confessions and even intentionally lied out of vanity i am convinced that heine is right i quite understand how sometimes one may out of sheer vanity attribute regular crimes to oneself and indeed i can very well conceive that kind of vanity but heine judged of people who made their confessions to the public i write only for myself and i wish to declare once and for all that if i write as though i were addressing readers that is simply because it is easier for me to write in that form it is a form an empty form i shall never have readers i have made this plain already i don t wish to be hampered by any restrictions in the compilation of my notes i shall not attempt any system or method i will jot things down as i remember them but here perhaps someone will catch at the word and ask me if you really don t reckon on readers why do you make such compacts with yourself and on paper too that is that you won t attempt any system or method that you jot things down as you remember them and so on and so on why are you explaining why do you apologise well there it is i answer there is a whole psychology in all this though perhaps it is simply that i am a coward and perhaps that i purposely imagine an audience before me in order that i may be more dignified while i write there are perhaps thousands of reasons again what is my object precisely in writing if it is not for the benefit of the public why should i not simply recall these incidents in my own mind without putting them on paper quite so but yet it is more imposing on paper there is something more impressive in it i shall be better able to criticise myself and improve my style besides i shall perhaps obtain actual relief from writing today for instance i am particularly oppressed by one memory of a distant past it came back vividly to my mind a few days ago and has remained haunting me like an annoying tune that one cannot get rid of and yet i must get rid of it somehow i have hundreds of such reminiscences but at times some one stands out from the hundred and oppresses me for some reason i believe that if i write it down i should get rid of it why not try besides i am bored and i never have anything to do writing will be a sort of work they say work makes man kind hearted and honest well here is a chance for me anyway snow is falling today yellow and dingy it fell yesterday too and a few days ago i fancy it is the wet snow that has reminded me of that incident which i cannot shake off now and so let it be a story _à propos_ of the falling snow part ii à propos of the wet snow when from dark error s subjugation my words of passionate exhortation had wrenched thy fainting spirit free and writhing prone in thine affliction thou didst recall with malediction the vice that had encompassed thee and when thy slumbering conscience fretting by recollection s torturing flame thou didst reveal the hideous setting of thy life s current ere i came when suddenly i saw thee sicken and weeping hide thine anguished face revolted maddened horror stricken at memories of foul disgrace nekrassov _translated by juliet soskice_ i at that time i was only twenty four my life was even then gloomy ill regulated and as solitary as that of a savage i made friends with no one and positively avoided talking and buried myself more and more in my hole at work in the office i never looked at anyone and was perfectly well aware that my companions looked upon me not only as a queer fellow but even looked upon me i always fancied this with a sort of loathing i sometimes wondered why it was that nobody except me fancied that he was looked upon with aversion one of the clerks had a most repulsive pock marked face which looked positively villainous i believe i should not have dared to look at anyone with such an unsightly countenance another had such a very dirty old uniform that there was an unpleasant odour in his proximity yet not one of these gentlemen showed the slightest self consciousness either about their clothes or their countenance or their character in any way neither of them ever imagined that they were looked at with repulsion if they had imagined it they would not have minded so long as their superiors did not look at them in that way it is clear to me now that owing to my unbounded vanity and to the high standard i set for myself i often looked at myself with furious discontent which verged on loathing and so i inwardly attributed the same feeling to everyone i hated my face for instance i thought it disgusting and even suspected that there was something base in my expression and so every day when i turned up at the office i tried to behave as independently as possible and to assume a lofty expression so that i might not be suspected of being abject my face may be ugly i thought but let it be lofty expressive and above all _extremely_ intelligent but i was positively and painfully certain that it was impossible for my countenance ever to express those qualities and what was worst of all i thought it actually stupid looking and i would have been quite satisfied if i could have looked intelligent in fact i would even have put up with looking base if at the same time my face could have been thought strikingly intelligent of course i hated my fellow clerks one and all and i despised them all yet at the same time i was as it were afraid of them in fact it happened at times that i thought more highly of them than of myself it somehow happened quite suddenly that i alternated between despising them and thinking them superior to myself a cultivated and decent man cannot be vain without setting a fearfully high standard for himself and without despising and almost hating himself at certain moments but whether i despised them or thought them superior i dropped my eyes almost every time i met anyone i even made experiments whether i could face so and so s looking at me and i was always the first to drop my eyes this worried me to distraction i had a sickly dread too of being ridiculous and so had a slavish passion for the conventional in everything external i loved to fall into the common rut and had a whole hearted terror of any kind of eccentricity in myself but how could i live up to it i was morbidly sensitive as a man of our age should be they were all stupid and as like one another as so many sheep perhaps i was the only one in the office who fancied that i was a coward and a slave and i fancied it just because i was more highly developed but it was not only that i fancied it it really was so i was a coward and a slave i say this without the slightest embarrassment every decent man of our age must be a coward and a slave that is his normal condition of that i am firmly persuaded he is made and constructed to that very end and not only at the present time owing to some casual circumstances but always at all times a decent man is bound to be a coward and a slave it is the law of nature for all decent people all over the earth if anyone of them happens to be valiant about something he need not be comforted nor carried away by that he would show the white feather just the same before something else that is how it invariably and inevitably ends only donkeys and mules are valiant and they only till they are pushed up to the wall it is not worth while to pay attention to them for they really are of no consequence another circumstance too worried me in those days that there was no one like me and i was unlike anyone else i am alone and they are _everyone_ i thought and pondered from that it is evident that i was still a youngster the very opposite sometimes happened it was loathsome sometimes to go to the office things reached such a point that i often came home ill but all at once _à propos_ of nothing there would come a phase of scepticism and indifference everything happened in phases to me and i would laugh myself at my intolerance and fastidiousness i would reproach myself with being _romantic_ at one time i was unwilling to speak to anyone while at other times i would not only talk but go to the length of contemplating making friends with them all my fastidiousness would suddenly for no rhyme or reason vanish who knows perhaps i never had really had it and it had simply been affected and got out of books i have not decided that question even now once i quite made friends with them visited their homes played preference drank vodka talked of promotions but here let me make a digression we russians speaking generally have never had those foolish transcendental romantics german and still more french on whom nothing produces any effect if there were an earthquake if all france perished at the barricades they would still be the same they would not even have the decency to affect a change but would still go on singing their transcendental songs to the hour of their death because they are fools we in russia have no fools that is well known that is what distinguishes us from foreign lands consequently these transcendental natures are not found amongst us in their pure form the idea that they are is due to our realistic journalists and critics of that day always on the look out for kostanzhoglos and uncle pyotr ivanitchs and foolishly accepting them as our ideal they have slandered our romantics taking them for the same transcendental sort as in germany or france on the contrary the characteristics of our romantics are absolutely and directly opposed to the transcendental european type and no european standard can be applied to them allow me to make use of this word romantic an old fashioned and much respected word which has done good service and is familiar to all the characteristics of our romantic are to understand everything _to see everything and to see it often incomparably more clearly than our most realistic minds see it _ to refuse to accept anyone or anything but at the same time not to despise anything to give way to yield from policy never to lose sight of a useful practical object such as rent free quarters at the government expense pensions decorations to keep their eye on that object through all the enthusiasms and volumes of lyrical poems and at the same time to preserve the sublime and the beautiful inviolate within them to the hour of their death and to preserve themselves also incidentally like some precious jewel wrapped in cotton wool if only for the benefit of the sublime and the beautiful our romantic is a man of great breadth and the greatest rogue of all our rogues i assure you i can assure you from experience indeed of course that is if he is intelligent but what am i saying the romantic is always intelligent and i only meant to observe that although we have had foolish romantics they don t count and they were only so because in the flower of their youth they degenerated into germans and to preserve their precious jewel more comfortably settled somewhere out there by preference in weimar or the black forest i for instance genuinely despised my official work and did not openly abuse it simply because i was in it myself and got a salary for it anyway take note i did not openly abuse it our romantic would rather go out of his mind a thing however which very rarely happens than take to open abuse unless he had some other career in view and he is never kicked out at most they would take him to the lunatic asylum as the king of spain if he should go very mad but it is only the thin fair people who go out of their minds in russia innumerable romantics attain later in life to considerable rank in the service their many sidedness is remarkable and what a faculty they have for the most contradictory sensations i was comforted by this thought even in those days and i am of the same opinion now that is why there are so many broad natures among us who never lose their ideal even in the depths of degradation and though they never stir a finger for their ideal though they are arrant thieves and knaves yet they tearfully cherish their first ideal and are extraordinarily honest at heart yes it is only among us that the most incorrigible rogue can be absolutely and loftily honest at heart without in the least ceasing to be a rogue i repeat our romantics frequently become such accomplished rascals i use the term rascals affectionately suddenly display such a sense of reality and practical knowledge that their bewildered superiors and the public generally can only ejaculate in amazement their many sidedness is really amazing and goodness knows what it may develop into later on and what the future has in store for us it is not a poor material i do not say this from any foolish or boastful patriotism but i feel sure that you are again imagining that i am joking or perhaps it s just the contrary and you are convinced that i really think so anyway gentlemen i shall welcome both views as an honour and a special favour and do forgive my digression i did not of course maintain friendly relations with my comrades and soon was at loggerheads with them and in my youth and inexperience i even gave up bowing to them as though i had cut off all relations that however only happened to me once as a rule i was always alone in the first place i spent most of my time at home reading i tried to stifle all that was continually seething within me by means of external impressions and the only external means i had was reading reading of course was a great help exciting me giving me pleasure and pain but at times it bored me fearfully one longed for movement in spite of everything and i plunged all at once into dark underground loathsome vice of the pettiest kind my wretched passions were acute smarting from my continual sickly irritability i had hysterical impulses with tears and convulsions i had no resource except reading that is there was nothing in my surroundings which i could respect and which attracted me i was overwhelmed with depression too i had an hysterical craving for incongruity and for contrast and so i took to vice i have not said all this to justify myself but no i am lying i did want to justify myself i make that little observation for my own benefit gentlemen i don t want to lie i vowed to myself i would not and so furtively timidly in solitude at night i indulged in filthy vice with a feeling of shame which never deserted me even at the most loathsome moments and which at such moments nearly made me curse already even then i had my underground world in my soul i was fearfully afraid of being seen of being met of being recognised i visited various obscure haunts one night as i was passing a tavern i saw through a lighted window some gentlemen fighting with billiard cues and saw one of them thrown out of the window at other times i should have felt very much disgusted but i was in such a mood at the time that i actually envied the gentleman thrown out of the window and i envied him so much that i even went into the tavern and into the billiard room perhaps i thought i ll have a fight too and they ll throw me out of the window i was not drunk but what is one to do depression will drive a man to such a pitch of hysteria but nothing happened it seemed that i was not even equal to being thrown out of the window and i went away without having my fight an officer put me in my place from the first moment i was standing by the billiard table and in my ignorance blocking up the way and he wanted to pass he took me by the shoulders and without a word without a warning or explanation moved me from where i was standing to another spot and passed by as though he had not noticed me i could have forgiven blows but i could not forgive his having moved me without noticing me devil knows what i would have given for a real regular quarrel a more decent a more _literary_ one so to speak i had been treated like a fly this officer was over six foot while i was a spindly little fellow but the quarrel was in my hands i had only to protest and i certainly would have been thrown out of the window but i changed my mind and preferred to beat a resentful retreat i went out of the tavern straight home confused and troubled and the next night i went out again with the same lewd intentions still more furtively abjectly and miserably than before as it were with tears in my eyes but still i did go out again don t imagine though it was cowardice made me slink away from the officer i never have been a coward at heart though i have always been a coward in action don t be in a hurry to laugh i assure you i can explain it all oh if only that officer had been one of the sort who would consent to fight a duel but no he was one of those gentlemen alas long extinct who preferred fighting with cues or like gogol s lieutenant pirogov appealing to the police they did not fight duels and would have thought a duel with a civilian like me an utterly unseemly procedure in any case and they looked upon the duel altogether as something impossible something free thinking and french but they were quite ready to bully especially when they were over six foot i did not slink away through cowardice but through an unbounded vanity i was afraid not of his six foot not of getting a sound thrashing and being thrown out of the window i should have had physical courage enough i assure you but i had not the moral courage what i was afraid of was that everyone present from the insolent marker down to the lowest little stinking pimply clerk in a greasy collar would jeer at me and fail to understand when i began to protest and to address them in literary language for of the point of honour not of honour but of the point of honour _point d honneur_ one cannot speak among us except in literary language you can t allude to the point of honour in ordinary language i was fully convinced the sense of reality in spite of all my romanticism that they would all simply split their sides with laughter and that the officer would not simply beat me that is without insulting me but would certainly prod me in the back with his knee kick me round the billiard table and only then perhaps have pity and drop me out of the window of course this trivial incident could not with me end in that i often met that officer afterwards in the street and noticed him very carefully i am not quite sure whether he recognised me i imagine not i judge from certain signs but i i stared at him with spite and hatred and so it went on for several years my resentment grew even deeper with years at first i began making stealthy inquiries about this officer it was difficult for me to do so for i knew no one but one day i heard someone shout his surname in the street as i was following him at a distance as though i were tied to him and so i learnt his surname another time i followed him to his flat and for ten kopecks learned from the porter where he lived on which storey whether he lived alone or with others and so on in fact everything one could learn from a porter one morning though i had never tried my hand with the pen it suddenly occurred to me to write a satire on this officer in the form of a novel which would unmask his villainy i wrote the novel with relish i did unmask his villainy i even exaggerated it at first i so altered his surname that it could easily be recognised but on second thoughts i changed it and sent the story to the _otetchestvenniya zapiski_ but at that time such attacks were not the fashion and my story was not printed that was a great vexation to me sometimes i was positively choked with resentment at last i determined to challenge my enemy to a duel i composed a splendid charming letter to him imploring him to apologise to me and hinting rather plainly at a duel in case of refusal the letter was so composed that if the officer had had the least understanding of the sublime and the beautiful he would certainly have flung himself on my neck and have offered me his friendship and how fine that would have been how we should have got on together he could have shielded me with his higher rank while i could have improved his mind with my culture and well my ideas and all sorts of things might have happened only fancy this was two years after his insult to me and my challenge would have been a ridiculous anachronism in spite of all the ingenuity of my letter in disguising and explaining away the anachronism but thank god to this day i thank the almighty with tears in my eyes i did not send the letter to him cold shivers run down my back when i think of what might have happened if i had sent it and all at once i revenged myself in the simplest way by a stroke of genius a brilliant thought suddenly dawned upon me sometimes on holidays i used to stroll along the sunny side of the nevsky about four o clock in the afternoon though it was hardly a stroll so much as a series of innumerable miseries humiliations and resentments but no doubt that was just what i wanted i used to wriggle along in a most unseemly fashion like an eel continually moving aside to make way for generals for officers of the guards and the hussars or for ladies at such minutes there used to be a convulsive twinge at my heart and i used to feel hot all down my back at the mere thought of the wretchedness of my attire of the wretchedness and abjectness of my little scurrying figure this was a regular martyrdom a continual intolerable humiliation at the thought which passed into an incessant and direct sensation that i was a mere fly in the eyes of all this world a nasty disgusting fly more intelligent more highly developed more refined in feeling than any of them of course but a fly that was continually making way for everyone insulted and injured by everyone why i inflicted this torture upon myself why i went to the nevsky i don t know i felt simply drawn there at every possible opportunity already then i began to experience a rush of the enjoyment of which i spoke in the first chapter after my affair with the officer i felt even more drawn there than before it was on the nevsky that i met him most frequently there i could admire him he too went there chiefly on holidays he too turned out of his path for generals and persons of high rank and he too wriggled between them like an eel but people like me or even better dressed than me he simply walked over he made straight for them as though there was nothing but empty space before him and never under any circumstances turned aside i gloated over my resentment watching him and always resentfully made way for him it exasperated me that even in the street i could not be on an even footing with him why must you invariably be the first to move aside i kept asking myself in hysterical rage waking up sometimes at three o clock in the morning why is it you and not he there s no regulation about it there s no written law let the making way be equal as it usually is when refined people meet he moves half way and you move half way you pass with mutual respect but that never happened and i always moved aside while he did not even notice my making way for him and lo and behold a bright idea dawned upon me what i thought if i meet him and don t move on one side what if i don t move aside on purpose even if i knock up against him how would that be this audacious idea took such a hold on me that it gave me no peace i was dreaming of it continually horribly and i purposely went more frequently to the nevsky in order to picture more vividly how i should do it when i did do it i was delighted this intention seemed to me more and more practical and possible of course i shall not really push him i thought already more good natured in my joy i will simply not turn aside will run up against him not very violently but just shouldering each other just as much as decency permits i will push against him just as much as he pushes against me at last i made up my mind completely but my preparations took a great deal of time to begin with when i carried out my plan i should need to be looking rather more decent and so i had to think of my get up in case of emergency if for instance there were any sort of public scandal and the public there is of the most _recherché _ the countess walks there prince d walks there all the literary world is there i must be well dressed that inspires respect and of itself puts us on an equal footing in the eyes of the society with this object i asked for some of my salary in advance and bought at tchurkin s a pair of black gloves and a decent hat black gloves seemed to me both more dignified and _bon ton_ than the lemon coloured ones which i had contemplated at first the colour is too gaudy it looks as though one were trying to be conspicuous and i did not take the lemon coloured ones i had got ready long beforehand a good shirt with white bone studs my overcoat was the only thing that held me back the coat in itself was a very good one it kept me warm but it was wadded and it had a raccoon collar which was the height of vulgarity i had to change the collar at any sacrifice and to have a beaver one like an officer s for this purpose i began visiting the gostiny dvor and after several attempts i pitched upon a piece of cheap german beaver though these german beavers soon grow shabby and look wretched yet at first they look exceedingly well and i only needed it for the occasion i asked the price even so it was too expensive after thinking it over thoroughly i decided to sell my raccoon collar the rest of the money a considerable sum for me i decided to borrow from anton antonitch syetotchkin my immediate superior an unassuming person though grave and judicious he never lent money to anyone but i had on entering the service been specially recommended to him by an important personage who had got me my berth i was horribly worried to borrow from anton antonitch seemed to me monstrous and shameful i did not sleep for two or three nights indeed i did not sleep well at that time i was in a fever i had a vague sinking at my heart or else a sudden throbbing throbbing throbbing anton antonitch was surprised at first then he frowned then he reflected and did after all lend me the money receiving from me a written authorisation to take from my salary a fortnight later the sum that he had lent me in this way everything was at last ready the handsome beaver replaced the mean looking raccoon and i began by degrees to get to work it would never have done to act offhand at random the plan had to be carried out skilfully by degrees but i must confess that after many efforts i began to despair we simply could not run into each other i made every preparation i was quite determined it seemed as though we should run into one another directly and before i knew what i was doing i had stepped aside for him again and he had passed without noticing me i even prayed as i approached him that god would grant me determination one time i had made up my mind thoroughly but it ended in my stumbling and falling at his feet because at the very last instant when i was six inches from him my courage failed me he very calmly stepped over me while i flew on one side like a ball that night i was ill again feverish and delirious and suddenly it ended most happily the night before i had made up my mind not to carry out my fatal plan and to abandon it all and with that object i went to the nevsky for the last time just to see how i would abandon it all suddenly three paces from my enemy i unexpectedly made up my mind i closed my eyes and we ran full tilt shoulder to shoulder against one another i did not budge an inch and passed him on a perfectly equal footing he did not even look round and pretended not to notice it but he was only pretending i am convinced of that i am convinced of that to this day of course i got the worst of it he was stronger but that was not the point the point was that i had attained my object i had kept up my dignity i had not yielded a step and had put myself publicly on an equal social footing with him i returned home feeling that i was fully avenged for everything i was delighted i was triumphant and sang italian arias of course i will not describe to you what happened to me three days later if you have read my first chapter you can guess for yourself the officer was afterwards transferred i have not seen him now for fourteen years what is the dear fellow doing now whom is he walking over ii but the period of my dissipation would end and i always felt very sick afterwards it was followed by remorse i tried to drive it away i felt too sick by degrees however i grew used to that too i grew used to everything or rather i voluntarily resigned myself to enduring it but i had a means of escape that reconciled everything that was to find refuge in the sublime and the beautiful in dreams of course i was a terrible dreamer i would dream for three months on end tucked away in my corner and you may believe me that at those moments i had no resemblance to the gentleman who in the perturbation of his chicken heart put a collar of german beaver on his great coat i suddenly became a hero i would not have admitted my six foot lieutenant even if he had called on me i could not even picture him before me then what were my dreams and how i could satisfy myself with them it is hard to say now but at the time i was satisfied with them though indeed even now i am to some extent satisfied with them dreams were particularly sweet and vivid after a spell of dissipation they came with remorse and with tears with curses and transports there were moments of such positive intoxication of such happiness that there was not the faintest trace of irony within me on my honour i had faith hope love i believed blindly at such times that by some miracle by some external circumstance all this would suddenly open out expand that suddenly a vista of suitable activity beneficent good and above all _ready made_ what sort of activity i had no idea but the great thing was that it should be all ready for me would rise up before me and i should come out into the light of day almost riding a white horse and crowned with laurel anything but the foremost place i could not conceive for myself and for that very reason i quite contentedly occupied the lowest in reality either to be a hero or to grovel in the mud there was nothing between that was my ruin for when i was in the mud i comforted myself with the thought that at other times i was a hero and the hero was a cloak for the mud for an ordinary man it was shameful to defile himself but a hero was too lofty to be utterly defiled and so he might defile himself it is worth noting that these attacks of the sublime and the beautiful visited me even during the period of dissipation and just at the times when i was touching the bottom they came in separate spurts as though reminding me of themselves but did not banish the dissipation by their appearance on the contrary they seemed to add a zest to it by contrast and were only sufficiently present to serve as an appetising sauce that sauce was made up of contradictions and sufferings of agonising inward analysis and all these pangs and pin pricks gave a certain piquancy even a significance to my dissipation in fact completely answered the purpose of an appetising sauce there was a certain depth of meaning in it and i could hardly have resigned myself to the simple vulgar direct debauchery of a clerk and have endured all the filthiness of it what could have allured me about it then and have drawn me at night into the street no i had a lofty way of getting out of it all and what loving kindness oh lord what loving kindness i felt at times in those dreams of mine in those flights into the sublime and the beautiful though it was fantastic love though it was never applied to anything human in reality yet there was so much of this love that one did not feel afterwards even the impulse to apply it in reality that would have been superfluous everything however passed satisfactorily by a lazy and fascinating transition into the sphere of art that is into the beautiful forms of life lying ready largely stolen from the poets and novelists and adapted to all sorts of needs and uses i for instance was triumphant over everyone everyone of course was in dust and ashes and was forced spontaneously to recognise my superiority and i forgave them all i was a poet and a grand gentleman i fell in love i came in for countless millions and immediately devoted them to humanity and at the same time i confessed before all the people my shameful deeds which of course were not merely shameful but had in them much that was sublime and beautiful something in the manfred style everyone would kiss me and weep what idiots they would be if they did not while i should go barefoot and hungry preaching new ideas and fighting a victorious austerlitz against the obscurantists then the band would play a march an amnesty would be declared the pope would agree to retire from rome to brazil then there would be a ball for the whole of italy at the villa borghese on the shores of lake como lake como being for that purpose transferred to the neighbourhood of rome then would come a scene in the bushes and so on and so on as though you did not know all about it you will say that it is vulgar and contemptible to drag all this into public after all the tears and transports which i have myself confessed but why is it contemptible can you imagine that i am ashamed of it all and that it was stupider than anything in your life gentlemen and i can assure you that some of these fancies were by no means badly composed it did not all happen on the shores of lake como and yet you are right it really is vulgar and contemptible and most contemptible of all it is that now i am attempting to justify myself to you and even more contemptible than that is my making this remark now but that s enough or there will be no end to it each step will be more contemptible than the last i could never stand more than three months of dreaming at a time without feeling an irresistible desire to plunge into society to plunge into society meant to visit my superior at the office anton antonitch syetotchkin he was the only permanent acquaintance i have had in my life and i wonder at the fact myself now but i only went to see him when that phase came over me and when my dreams had reached such a point of bliss that it became essential at once to embrace my fellows and all mankind and for that purpose i needed at least one human being actually existing i had to call on anton antonitch however on tuesday his at home day so i had always to time my passionate desire to embrace humanity so that it might fall on a tuesday this anton antonitch lived on the fourth storey in a house in five corners in four low pitched rooms one smaller than the other of a particularly frugal and sallow appearance he had two daughters and their aunt who used to pour out the tea of the daughters one was thirteen and another fourteen they both had snub noses and i was awfully shy of them because they were always whispering and giggling together the master of the house usually sat in his study on a leather couch in front of the table with some grey headed gentleman usually a colleague from our office or some other department i never saw more than two or three visitors there always the same they talked about the excise duty about business in the senate about salaries about promotions about his excellency and the best means of pleasing him and so on i had the patience to sit like a fool beside these people for four hours at a stretch listening to them without knowing what to say to them or venturing to say a word i became stupefied several times i felt myself perspiring i was overcome by a sort of paralysis but this was pleasant and good for me on returning home i deferred for a time my desire to embrace all mankind i had however one other acquaintance of a sort simonov who was an old schoolfellow i had a number of schoolfellows indeed in petersburg but i did not associate with them and had even given up nodding to them in the street i believe i had transferred into the department i was in simply to avoid their company and to cut off all connection with my hateful childhood curses on that school and all those terrible years of penal servitude in short i parted from my schoolfellows as soon as i got out into the world there were two or three left to whom i nodded in the street one of them was simonov who had in no way been distinguished at school was of a quiet and equable disposition but i discovered in him a certain independence of character and even honesty i don t even suppose that he was particularly stupid i had at one time spent some rather soulful moments with him but these had not lasted long and had somehow been suddenly clouded over he was evidently uncomfortable at these reminiscences and was i fancy always afraid that i might take up the same tone again i suspected that he had an aversion for me but still i went on going to see him not being quite certain of it and so on one occasion unable to endure my solitude and knowing that as it was thursday anton antonitch s door would be closed i thought of simonov climbing up to his fourth storey i was thinking that the man disliked me and that it was a mistake to go and see him but as it always happened that such reflections impelled me as though purposely to put myself into a false position i went in it was almost a year since i had last seen simonov iii i found two of my old schoolfellows with him they seemed to be discussing an important matter all of them took scarcely any notice of my entrance which was strange for i had not met them for years evidently they looked upon me as something on the level of a common fly i had not been treated like that even at school though they all hated me i knew of course that they must despise me now for my lack of success in the service and for my having let myself sink so low going about badly dressed and so on which seemed to them a sign of my incapacity and insignificance but i had not expected such contempt simonov was positively surprised at my turning up even in old days he had always seemed surprised at my coming all this disconcerted me i sat down feeling rather miserable and began listening to what they were saying they were engaged in warm and earnest conversation about a farewell dinner which they wanted to arrange for the next day to a comrade of theirs called zverkov an officer in the army who was going away to a distant province this zverkov had been all the time at school with me too i had begun to hate him particularly in the upper forms in the lower forms he had simply been a pretty playful boy whom everybody liked i had hated him however even in the lower forms just because he was a pretty and playful boy he was always bad at his lessons and got worse and worse as he went on however he left with a good certificate as he had powerful interests during his last year at school he came in for an estate of two hundred serfs and as almost all of us were poor he took up a swaggering tone among us he was vulgar in the extreme but at the same time he was a good natured fellow even in his swaggering in spite of superficial fantastic and sham notions of honour and dignity all but very few of us positively grovelled before zverkov and the more so the more he swaggered and it was not from any interested motive that they grovelled but simply because he had been favoured by the gifts of nature moreover it was as it were an accepted idea among us that zverkov was a specialist in regard to tact and the social graces this last fact particularly infuriated me i hated the abrupt self confident tone of his voice his admiration of his own witticisms which were often frightfully stupid though he was bold in his language i hated his handsome but stupid face for which i would however have gladly exchanged my intelligent one and the free and easy military manners in fashion in the forties i hated the way in which he used to talk of his future conquests of women he did not venture to begin his attack upon women until he had the epaulettes of an officer and was looking forward to them with impatience and boasted of the duels he would constantly be fighting i remember how i invariably so taciturn suddenly fastened upon zverkov when one day talking at a leisure moment with his schoolfellows of his future relations with the fair sex and growing as sportive as a puppy in the sun he all at once declared that he would not leave a single village girl on his estate unnoticed that that was his _droit de seigneur_ and that if the peasants dared to protest he would have them all flogged and double the tax on them the bearded rascals our servile rabble applauded but i attacked him not from compassion for the girls and their fathers but simply because they were applauding such an insect i got the better of him on that occasion but though zverkov was stupid he was lively and impudent and so laughed it off and in such a way that my victory was not really complete the laugh was on his side he got the better of me on several occasions afterwards but without malice jestingly casually i remained angrily and contemptuously silent and would not answer him when we left school he made advances to me i did not rebuff them for i was flattered but we soon parted and quite naturally afterwards i heard of his barrack room success as a lieutenant and of the fast life he was leading then there came other rumours of his successes in the service by then he had taken to cutting me in the street and i suspected that he was afraid of compromising himself by greeting a personage as insignificant as me i saw him once in the theatre in the third tier of boxes by then he was wearing shoulder straps he was twisting and twirling about ingratiating himself with the daughters of an ancient general in three years he had gone off considerably though he was still rather handsome and adroit one could see that by the time he was thirty he would be corpulent so it was to this zverkov that my schoolfellows were going to give a dinner on his departure they had kept up with him for those three years though privately they did not consider themselves on an equal footing with him i am convinced of that of simonov s two visitors one was ferfitchkin a russianised german a little fellow with the face of a monkey a blockhead who was always deriding everyone a very bitter enemy of mine from our days in the lower forms a vulgar impudent swaggering fellow who affected a most sensitive feeling of personal honour though of course he was a wretched little coward at heart he was one of those worshippers of zverkov who made up to the latter from interested motives and often borrowed money from him simonov s other visitor trudolyubov was a person in no way remarkable a tall young fellow in the army with a cold face fairly honest though he worshipped success of every sort and was only capable of thinking of promotion he was some sort of distant relation of zverkov s and this foolish as it seems gave him a certain importance among us he always thought me of no consequence whatever his behaviour to me though not quite courteous was tolerable well with seven roubles each said trudolyubov twenty one roubles between the three of us we ought to be able to get a good dinner zverkov of course won t pay of course not since we are inviting him simonov decided can you imagine ferfitchkin interrupted hotly and conceitedly like some insolent flunkey boasting of his master the general s decorations can you imagine that zverkov will let us pay alone he will accept from delicacy but he will order half a dozen bottles of champagne do we want half a dozen for the four of us observed trudolyubov taking notice only of the half dozen so the three of us with zverkov for the fourth twenty one roubles at the hôtel de paris at five o clock tomorrow simonov who had been asked to make the arrangements concluded finally how twenty one roubles i asked in some agitation with a show of being offended if you count me it will not be twenty one but twenty eight roubles it seemed to me that to invite myself so suddenly and unexpectedly would be positively graceful and that they would all be conquered at once and would look at me with respect do you want to join too simonov observed with no appearance of pleasure seeming to avoid looking at me he knew me through and through it infuriated me that he knew me so thoroughly why not i am an old schoolfellow of his too i believe and i must own i feel hurt that you have left me out i said boiling over again and where were we to find you ferfitchkin put in roughly you never were on good terms with zverkov trudolyubov added frowning but i had already clutched at the idea and would not give it up it seems to me that no one has a right to form an opinion upon that i retorted in a shaking voice as though something tremendous had happened perhaps that is just my reason for wishing it now that i have not always been on good terms with him oh there s no making you out with these refinements trudolyubov jeered we ll put your name down simonov decided addressing me tomorrow at five o clock at the hôtel de paris what about the money ferfitchkin began in an undertone indicating me to simonov but he broke off for even simonov was embarrassed that will do said trudolyubov getting up if he wants to come so much let him but it s a private thing between us friends ferfitchkin said crossly as he too picked up his hat it s not an official gathering we do not want at all perhaps they went away ferfitchkin did not greet me in any way as he went out trudolyubov barely nodded simonov with whom i was left _tête à tête_ was in a state of vexation and perplexity and looked at me queerly he did not sit down and did not ask me to h m yes tomorrow then will you pay your subscription now i just ask so as to know he muttered in embarrassment i flushed crimson as i did so i remembered that i had owed simonov fifteen roubles for ages which i had indeed never forgotten though i had not paid it you will understand simonov that i could have no idea when i came here i am very much vexed that i have forgotten all right all right that doesn t matter you can pay tomorrow after the dinner i simply wanted to know please don t he broke off and began pacing the room still more vexed as he walked he began to stamp with his heels am i keeping you i asked after two minutes of silence oh he said starting that is to be truthful yes i have to go and see someone not far from here he added in an apologetic voice somewhat abashed my goodness why didn t you say so i cried seizing my cap with an astonishingly free and easy air which was the last thing i should have expected of myself it s close by not two paces away simonov repeated accompanying me to the front door with a fussy air which did not suit him at all so five o clock punctually tomorrow he called down the stairs after me he was very glad to get rid of me i was in a fury what possessed me what possessed me to force myself upon them i wondered grinding my teeth as i strode along the street for a scoundrel a pig like that zverkov of course i had better not go of course i must just snap my fingers at them i am not bound in any way i ll send simonov a note by tomorrow s post but what made me furious was that i knew for certain that i should go that i should make a point of going and the more tactless the more unseemly my going would be the more certainly i would go and there was a positive obstacle to my going i had no money all i had was nine roubles i had to give seven of that to my servant apollon for his monthly wages that was all i paid him he had to keep himself not to pay him was impossible considering his character but i will talk about that fellow about that plague of mine another time however i knew i should go and should not pay him his wages that night i had the most hideous dreams no wonder all the evening i had been oppressed by memories of my miserable days at school and i could not shake them off i was sent to the school by distant relations upon whom i was dependent and of whom i have heard nothing since they sent me there a forlorn silent boy already crushed by their reproaches already troubled by doubt and looking with savage distrust at everyone my schoolfellows met me with spiteful and merciless jibes because i was not like any of them but i could not endure their taunts i could not give in to them with the ignoble readiness with which they gave in to one another i hated them from the first and shut myself away from everyone in timid wounded and disproportionate pride their coarseness revolted me they laughed cynically at my face at my clumsy figure and yet what stupid faces they had themselves in our school the boys faces seemed in a special way to degenerate and grow stupider how many fine looking boys came to us in a few years they became repulsive even at sixteen i wondered at them morosely even then i was struck by the pettiness of their thoughts the stupidity of their pursuits their games their conversations they had no understanding of such essential things they took no interest in such striking impressive subjects that i could not help considering them inferior to myself it was not wounded vanity that drove me to it and for god s sake do not thrust upon me your hackneyed remarks repeated to nausea that i was only a dreamer while they even then had an understanding of life they understood nothing they had no idea of real life and i swear that that was what made me most indignant with them on the contrary the most obvious striking reality they accepted with fantastic stupidity and even at that time were accustomed to respect success everything that was just but oppressed and looked down upon they laughed at heartlessly and shamefully they took rank for intelligence even at sixteen they were already talking about a snug berth of course a great deal of it was due to their stupidity to the bad examples with which they had always been surrounded in their childhood and boyhood they were monstrously depraved of course a great deal of that too was superficial and an assumption of cynicism of course there were glimpses of youth and freshness even in their depravity but even that freshness was not attractive and showed itself in a certain rakishness i hated them horribly though perhaps i was worse than any of them they repaid me in the same way and did not conceal their aversion for me but by then i did not desire their affection on the contrary i continually longed for their humiliation to escape from their derision i purposely began to make all the progress i could with my studies and forced my way to the very top this impressed them moreover they all began by degrees to grasp that i had already read books none of them could read and understood things not forming part of our school curriculum of which they had not even heard they took a savage and sarcastic view of it but were morally impressed especially as the teachers began to notice me on those grounds the mockery ceased but the hostility remained and cold and strained relations became permanent between us in the end i could not put up with it with years a craving for society for friends developed in me i attempted to get on friendly terms with some of my schoolfellows but somehow or other my intimacy with them was always strained and soon ended of itself once indeed i did have a friend but i was already a tyrant at heart i wanted to exercise unbounded sway over him i tried to instil into him a contempt for his surroundings i required of him a disdainful and complete break with those surroundings i frightened him with my passionate affection i reduced him to tears to hysterics he was a simple and devoted soul but when he devoted himself to me entirely i began to hate him immediately and repulsed him as though all i needed him for was to win a victory over him to subjugate him and nothing else but i could not subjugate all of them my friend was not at all like them either he was in fact a rare exception the first thing i did on leaving school was to give up the special job for which i had been destined so as to break all ties to curse my past and shake the dust from off my feet and goodness knows why after all that i should go trudging off to simonov s early next morning i roused myself and jumped out of bed with excitement as though it were all about to happen at once but i believed that some radical change in my life was coming and would inevitably come that day owing to its rarity perhaps any external event however trivial always made me feel as though some radical change in my life were at hand i went to the office however as usual but sneaked away home two hours earlier to get ready the great thing i thought is not to be the first to arrive or they will think i am overjoyed at coming but there were thousands of such great points to consider and they all agitated and overwhelmed me i polished my boots a second time with my own hands nothing in the world would have induced apollon to clean them twice a day as he considered that it was more than his duties required of him i stole the brushes to clean them from the passage being careful he should not detect it for fear of his contempt then i minutely examined my clothes and thought that everything looked old worn and threadbare i had let myself get too slovenly my uniform perhaps was tidy but i could not go out to dinner in my uniform the worst of it was that on the knee of my trousers was a big yellow stain i had a foreboding that that stain would deprive me of nine tenths of my personal dignity i knew too that it was very poor to think so but this is no time for thinking now i am in for the real thing i thought and my heart sank i knew too perfectly well even then that i was monstrously exaggerating the facts but how could i help it i could not control myself and was already shaking with fever with despair i pictured to myself how coldly and disdainfully that scoundrel zverkov would meet me with what dull witted invincible contempt the blockhead trudolyubov would look at me with what impudent rudeness the insect ferfitchkin would snigger at me in order to curry favour with zverkov how completely simonov would take it all in and how he would despise me for the abjectness of my vanity and lack of spirit and worst of all how paltry _unliterary_ commonplace it would all be of course the best thing would be not to go at all but that was most impossible of all if i feel impelled to do anything i seem to be pitchforked into it i should have jeered at myself ever afterwards so you funked it you funked it you funked the _real thing _ on the contrary i passionately longed to show all that rabble that i was by no means such a spiritless creature as i seemed to myself what is more even in the acutest paroxysm of this cowardly fever i dreamed of getting the upper hand of dominating them carrying them away making them like me if only for my elevation of thought and unmistakable wit they would abandon zverkov he would sit on one side silent and ashamed while i should crush him then perhaps we would be reconciled and drink to our everlasting friendship but what was most bitter and humiliating for me was that i knew even then knew fully and for certain that i needed nothing of all this really that i did not really want to crush to subdue to attract them and that i did not care a straw really for the result even if i did achieve it oh how i prayed for the day to pass quickly in unutterable anguish i went to the window opened the movable pane and looked out into the troubled darkness of the thickly falling wet snow at last my wretched little clock hissed out five i seized my hat and trying not to look at apollon who had been all day expecting his month s wages but in his foolishness was unwilling to be the first to speak about it i slipped between him and the door and jumping into a high class sledge on which i spent my last half rouble i drove up in grand style to the hôtel de paris iv i had been certain the day before that i should be the first to arrive but it was not a question of being the first to arrive not only were they not there but i had difficulty in finding our room the table was not laid even what did it mean after a good many questions i elicited from the waiters that the dinner had been ordered not for five but for six o clock this was confirmed at the buffet too i felt really ashamed to go on questioning them it was only twenty five minutes past five if they changed the dinner hour they ought at least to have let me know that is what the post is for and not to have put me in an absurd position in my own eyes and and even before the waiters i sat down the servant began laying the table i felt even more humiliated when he was present towards six o clock they brought in candles though there were lamps burning in the room it had not occurred to the waiter however to bring them in at once when i arrived in the next room two gloomy angry looking persons were eating their dinners in silence at two different tables there was a great deal of noise even shouting in a room further away one could hear the laughter of a crowd of people and nasty little shrieks in french there were ladies at the dinner it was sickening in fact i rarely passed more unpleasant moments so much so that when they did arrive all together punctually at six i was overjoyed to see them as though they were my deliverers and even forgot that it was incumbent upon me to show resentment zverkov walked in at the head of them evidently he was the leading spirit he and all of them were laughing but seeing me zverkov drew himself up a little walked up to me deliberately with a slight rather jaunty bend from the waist he shook hands with me in a friendly but not over friendly fashion with a sort of circumspect courtesy like that of a general as though in giving me his hand he were warding off something i had imagined on the contrary that on coming in he would at once break into his habitual thin shrill laugh and fall to making his insipid jokes and witticisms i had been preparing for them ever since the previous day but i had not expected such condescension such high official courtesy so then he felt himself ineffably superior to me in every respect if he only meant to insult me by that high official tone it would not matter i thought i could pay him back for it one way or another but what if in reality without the least desire to be offensive that sheepshead had a notion in earnest that he was superior to me and could only look at me in a patronising way the very supposition made me gasp i was surprised to hear of your desire to join us he began lisping and drawling which was something new you and i seem to have seen nothing of one another you fight shy of us you shouldn t we are not such terrible people as you think well anyway i am glad to renew our acquaintance and he turned carelessly to put down his hat on the window have you been waiting long trudolyubov inquired i arrived at five o clock as you told me yesterday i answered aloud with an irritability that threatened an explosion didn t you let him know that we had changed the hour said trudolyubov to simonov no i didn t i forgot the latter replied with no sign of regret and without even apologising to me he went off to order the _hors d œuvres_ so you ve been here a whole hour oh poor fellow zverkov cried ironically for to his notions this was bound to be extremely funny that rascal ferfitchkin followed with his nasty little snigger like a puppy yapping my position struck him too as exquisitely ludicrous and embarrassing it isn t funny at all i cried to ferfitchkin more and more irritated it wasn t my fault but other people s they neglected to let me know it was it was it was simply absurd it s not only absurd but something else as well muttered trudolyubov naively taking my part you are not hard enough upon it it was simply rudeness unintentional of course and how could simonov h m if a trick like that had been played on me observed ferfitchkin i should but you should have ordered something for yourself zverkov interrupted or simply asked for dinner without waiting for us you will allow that i might have done that without your permission i rapped out if i waited it was let us sit down gentlemen cried simonov coming in everything is ready i can answer for the champagne it is capitally frozen you see i did not know your address where was i to look for you he suddenly turned to me but again he seemed to avoid looking at me evidently he had something against me it must have been what happened yesterday all sat down i did the same it was a round table trudolyubov was on my left simonov on my right zverkov was sitting opposite ferfitchkin next to him between him and trudolyubov tell me are you in a government office zverkov went on attending to me seeing that i was embarrassed he seriously thought that he ought to be friendly to me and so to speak cheer me up does he want me to throw a bottle at his head i thought in a fury in my novel surroundings i was unnaturally ready to be irritated in the n office i answered jerkily with my eyes on my plate and ha ave you a go od berth i say what ma a de you leave your original job what ma a de me was that i wanted to leave my original job i drawled more than he hardly able to control myself ferfitchkin went off into a guffaw simonov looked at me ironically trudolyubov left off eating and began looking at me with curiosity zverkov winced but he tried not to notice it and the remuneration what remuneration i mean your sa a lary why are you cross examining me however i told him at once what my salary was i turned horribly red it is not very handsome zverkov observed majestically yes you can t afford to dine at cafés on that ferfitchkin added insolently to my thinking it s very poor trudolyubov observed gravely and how thin you have grown how you have changed added zverkov with a shade of venom in his voice scanning me and my attire with a sort of insolent compassion oh spare his blushes cried ferfitchkin sniggering my dear sir allow me to tell you i am not blushing i broke out at last do you hear i am dining here at this cafe at my own expense not at other people s note that mr ferfitchkin wha at isn t every one here dining at his own expense you would seem to be ferfitchkin flew out at me turning as red as a lobster and looking me in the face with fury tha at i answered feeling i had gone too far and i imagine it would be better to talk of something more intelligent you intend to show off your intelligence i suppose don t disturb yourself that would be quite out of place here why are you clacking away like that my good sir eh have you gone out of your wits in your office enough gentlemen enough zverkov cried authoritatively how stupid it is muttered simonov it really is stupid we have met here a company of friends for a farewell dinner to a comrade and you carry on an altercation said trudolyubov rudely addressing himself to me alone you invited yourself to join us so don t disturb the general harmony enough enough cried zverkov give over gentlemen it s out of place better let me tell you how i nearly got married the day before yesterday and then followed a burlesque narrative of how this gentleman had almost been married two days before there was not a word about the marriage however but the story was adorned with generals colonels and kammer junkers while zverkov almost took the lead among them it was greeted with approving laughter ferfitchkin positively squealed no one paid any attention to me and i sat crushed and humiliated good heavens these are not the people for me i thought and what a fool i have made of myself before them i let ferfitchkin go too far though the brutes imagine they are doing me an honour in letting me sit down with them they don t understand that it s an honour to them and not to me i ve grown thinner my clothes oh damn my trousers zverkov noticed the yellow stain on the knee as soon as he came in but what s the use i must get up at once this very minute take my hat and simply go without a word with contempt and tomorrow i can send a challenge the scoundrels as though i cared about the seven roubles they may think damn it i don t care about the seven roubles i ll go this minute of course i remained i drank sherry and lafitte by the glassful in my discomfiture being unaccustomed to it i was quickly affected my annoyance increased as the wine went to my head i longed all at once to insult them all in a most flagrant manner and then go away to seize the moment and show what i could do so that they would say he s clever though he is absurd and and in fact damn them all i scanned them all insolently with my drowsy eyes but they seemed to have forgotten me altogether they were noisy vociferous cheerful zverkov was talking all the time i began listening zverkov was talking of some exuberant lady whom he had at last led on to declaring her love of course he was lying like a horse and how he had been helped in this affair by an intimate friend of his a prince kolya an officer in the hussars who had three thousand serfs and yet this kolya who has three thousand serfs has not put in an appearance here tonight to see you off i cut in suddenly for one minute every one was silent you are drunk already trudolyubov deigned to notice me at last glancing contemptuously in my direction zverkov without a word examined me as though i were an insect i dropped my eyes simonov made haste to fill up the glasses with champagne trudolyubov raised his glass as did everyone else but me your health and good luck on the journey he cried to zverkov to old times to our future hurrah they all tossed off their glasses and crowded round zverkov to kiss him i did not move my full glass stood untouched before me why aren t you going to drink it roared trudolyubov losing patience and turning menacingly to me i want to make a speech separately on my own account and then i ll drink it mr trudolyubov spiteful brute muttered simonov i drew myself up in my chair and feverishly seized my glass prepared for something extraordinary though i did not know myself precisely what i was going to say _silence _ cried ferfitchkin now for a display of wit zverkov waited very gravely knowing what was coming mr lieutenant zverkov i began let me tell you that i hate phrases phrasemongers and men in corsets that s the first point and there is a second one to follow it there was a general stir the second point is i hate ribaldry and ribald talkers especially ribald talkers the third point i love justice truth and honesty i went on almost mechanically for i was beginning to shiver with horror myself and had no idea how i came to be talking like this i love thought monsieur zverkov i love true comradeship on an equal footing and not h m i love but however why not i will drink your health too mr zverkov seduce the circassian girls shoot the enemies of the fatherland and and to your health monsieur zverkov zverkov got up from his seat bowed to me and said i am very much obliged to you he was frightfully offended and turned pale damn the fellow roared trudolyubov bringing his fist down on the table well he wants a punch in the face for that squealed ferfitchkin we ought to turn him out muttered simonov not a word gentlemen not a movement cried zverkov solemnly checking the general indignation i thank you all but i can show him for myself how much value i attach to his words mr ferfitchkin you will give me satisfaction tomorrow for your words just now i said aloud turning with dignity to ferfitchkin a duel you mean certainly he answered but probably i was so ridiculous as i challenged him and it was so out of keeping with my appearance that everyone including ferfitchkin was prostrate with laughter yes let him alone of course he is quite drunk trudolyubov said with disgust i shall never forgive myself for letting him join us simonov muttered again now is the time to throw a bottle at their heads i thought to myself i picked up the bottle and filled my glass no i d better sit on to the end i went on thinking you would be pleased my friends if i went away nothing will induce me to go i ll go on sitting here and drinking to the end on purpose as a sign that i don t think you of the slightest consequence i will go on sitting and drinking because this is a public house and i paid my entrance money i ll sit here and drink for i look upon you as so many pawns as inanimate pawns i ll sit here and drink and sing if i want to yes sing for i have the right to to sing h m but i did not sing i simply tried not to look at any of them i assumed most unconcerned attitudes and waited with impatience for them to speak _first_ but alas they did not address me and oh how i wished how i wished at that moment to be reconciled to them it struck eight at last nine they moved from the table to the sofa zverkov stretched himself on a lounge and put one foot on a round table wine was brought there he did as a fact order three bottles on his own account i of course was not invited to join them they all sat round him on the sofa they listened to him almost with reverence it was evident that they were fond of him what for what for i wondered from time to time they were moved to drunken enthusiasm and kissed each other they talked of the caucasus of the nature of true passion of snug berths in the service of the income of an hussar called podharzhevsky whom none of them knew personally and rejoiced in the largeness of it of the extraordinary grace and beauty of a princess d whom none of them had ever seen then it came to shakespeare s being immortal i smiled contemptuously and walked up and down the other side of the room opposite the sofa from the table to the stove and back again i tried my very utmost to show them that i could do without them and yet i purposely made a noise with my boots thumping with my heels but it was all in vain they paid no attention i had the patience to walk up and down in front of them from eight o clock till eleven in the same place from the table to the stove and back again i walk up and down to please myself and no one can prevent me the waiter who came into the room stopped from time to time to look at me i was somewhat giddy from turning round so often at moments it seemed to me that i was in delirium during those three hours i was three times soaked with sweat and dry again at times with an intense acute pang i was stabbed to the heart by the thought that ten years twenty years forty years would pass and that even in forty years i would remember with loathing and humiliation those filthiest most ludicrous and most awful moments of my life no one could have gone out of his way to degrade himself more shamelessly and i fully realised it fully and yet i went on pacing up and down from the table to the stove oh if you only knew what thoughts and feelings i am capable of how cultured i am i thought at moments mentally addressing the sofa on which my enemies were sitting but my enemies behaved as though i were not in the room once only once they turned towards me just when zverkov was talking about shakespeare and i suddenly gave a contemptuous laugh i laughed in such an affected and disgusting way that they all at once broke off their conversation and silently and gravely for two minutes watched me walking up and down from the table to the stove _taking no notice of them_ but nothing came of it they said nothing and two minutes later they ceased to notice me again it struck eleven friends cried zverkov getting up from the sofa let us all be off now _there _ of course of course the others assented i turned sharply to zverkov i was so harassed so exhausted that i would have cut my throat to put an end to it i was in a fever my hair soaked with perspiration stuck to my forehead and temples zverkov i beg your pardon i said abruptly and resolutely ferfitchkin yours too and everyone s everyone s i have insulted you all aha a duel is not in your line old man ferfitchkin hissed venomously it sent a sharp pang to my heart no it s not the duel i am afraid of ferfitchkin i am ready to fight you tomorrow after we are reconciled i insist upon it in fact and you cannot refuse i want to show you that i am not afraid of a duel you shall fire first and i shall fire into the air he is comforting himself said simonov he s simply raving said trudolyubov but let us pass why are you barring our way what do you want zverkov answered disdainfully they were all flushed their eyes were bright they had been drinking heavily i ask for your friendship zverkov i insulted you but insulted _you_ insulted _me _ understand sir that you never under any circumstances could possibly insult _me_ and that s enough for you out of the way concluded trudolyubov olympia is mine friends that s agreed cried zverkov we won t dispute your right we won t dispute your right the others answered laughing i stood as though spat upon the party went noisily out of the room trudolyubov struck up some stupid song simonov remained behind for a moment to tip the waiters i suddenly went up to him simonov give me six roubles i said with desperate resolution he looked at me in extreme amazement with vacant eyes he too was drunk you don t mean you are coming with us yes i ve no money he snapped out and with a scornful laugh he went out of the room i clutched at his overcoat it was a nightmare simonov i saw you had money why do you refuse me am i a scoundrel beware of refusing me if you knew if you knew why i am asking my whole future my whole plans depend upon it simonov pulled out the money and almost flung it at me take it if you have no sense of shame he pronounced pitilessly and ran to overtake them i was left for a moment alone disorder the remains of dinner a broken wine glass on the floor spilt wine cigarette ends fumes of drink and delirium in my brain an agonising misery in my heart and finally the waiter who had seen and heard all and was looking inquisitively into my face i am going there i cried either they shall all go down on their knees to beg for my friendship or i will give zverkov a slap in the face v so this is it this is it at last contact with real life i muttered as i ran headlong downstairs this is very different from the pope s leaving rome and going to brazil very different from the ball on lake como you are a scoundrel a thought flashed through my mind if you laugh at this now no matter i cried answering myself now everything is lost there was no trace to be seen of them but that made no difference i knew where they had gone at the steps was standing a solitary night sledge driver in a rough peasant coat powdered over with the still falling wet and as it were warm snow it was hot and steamy the little shaggy piebald horse was also covered with snow and coughing i remember that very well i made a rush for the roughly made sledge but as soon as i raised my foot to get into it the recollection of how simonov had just given me six roubles seemed to double me up and i tumbled into the sledge like a sack no i must do a great deal to make up for all that i cried but i will make up for it or perish on the spot this very night start we set off there was a perfect whirl in my head they won t go down on their knees to beg for my friendship that is a mirage cheap mirage revolting romantic and fantastical that s another ball on lake como and so i am bound to slap zverkov s face it is my duty to and so it is settled i am flying to give him a slap in the face hurry up the driver tugged at the reins as soon as i go in i ll give it him ought i before giving him the slap to say a few words by way of preface no i ll simply go in and give it him they will all be sitting in the drawing room and he with olympia on the sofa that damned olympia she laughed at my looks on one occasion and refused me i ll pull olympia s hair pull zverkov s ears no better one ear and pull him by it round the room maybe they will all begin beating me and will kick me out that s most likely indeed no matter anyway i shall first slap him the initiative will be mine and by the laws of honour that is everything he will be branded and cannot wipe off the slap by any blows by nothing but a duel he will be forced to fight and let them beat me now let them the ungrateful wretches trudolyubov will beat me hardest he is so strong ferfitchkin will be sure to catch hold sideways and tug at my hair but no matter no matter that s what i am going for the blockheads will be forced at last to see the tragedy of it all when they drag me to the door i shall call out to them that in reality they are not worth my little finger get on driver get on i cried to the driver he started and flicked his whip i shouted so savagely we shall fight at daybreak that s a settled thing i ve done with the office ferfitchkin made a joke about it just now but where can i get pistols nonsense i ll get my salary in advance and buy them and powder and bullets that s the second s business and how can it all be done by daybreak and where am i to get a second i have no friends nonsense i cried lashing myself up more and more it s of no consequence the first person i meet in the street is bound to be my second just as he would be bound to pull a drowning man out of water the most eccentric things may happen even if i were to ask the director himself to be my second tomorrow he would be bound to consent if only from a feeling of chivalry and to keep the secret anton antonitch the fact is that at that very minute the disgusting absurdity of my plan and the other side of the question was clearer and more vivid to my imagination than it could be to anyone on earth but get on driver get on you rascal get on ugh sir said the son of toil cold shivers suddenly ran down me wouldn t it be better to go straight home my god my god why did i invite myself to this dinner yesterday but no it s impossible and my walking up and down for three hours from the table to the stove no they they and no one else must pay for my walking up and down they must wipe out this dishonour drive on and what if they give me into custody they won t dare they ll be afraid of the scandal and what if zverkov is so contemptuous that he refuses to fight a duel he is sure to but in that case i ll show them i will turn up at the posting station when he s setting off tomorrow i ll catch him by the leg i ll pull off his coat when he gets into the carriage i ll get my teeth into his hand i ll bite him see what lengths you can drive a desperate man to he may hit me on the head and they may belabour me from behind i will shout to the assembled multitude look at this young puppy who is driving off to captivate the circassian girls after letting me spit in his face of course after that everything will be over the office will have vanished off the face of the earth i shall be arrested i shall be tried i shall be dismissed from the service thrown in prison sent to siberia never mind in fifteen years when they let me out of prison i will trudge off to him a beggar in rags i shall find him in some provincial town he will be married and happy he will have a grown up daughter i shall say to him look monster at my hollow cheeks and my rags i ve lost everything my career my happiness art science _the woman i loved_ and all through you here are pistols i have come to discharge my pistol and and i forgive you then i shall fire into the air and he will hear nothing more of me i was actually on the point of tears though i knew perfectly well at that moment that all this was out of pushkin s _silvio_ and lermontov s _masquerade_ and all at once i felt horribly ashamed so ashamed that i stopped the horse got out of the sledge and stood still in the snow in the middle of the street the driver gazed at me sighing and astonished what was i to do i could not go on there it was evidently stupid and i could not leave things as they were because that would seem as though heavens how could i leave things and after such insults no i cried throwing myself into the sledge again it is ordained it is fate drive on drive on and in my impatience i punched the sledge driver on the back of the neck what are you up to what are you hitting me for the peasant shouted but he whipped up his nag so that it began kicking the wet snow was falling in big flakes i unbuttoned myself regardless of it i forgot everything else for i had finally decided on the slap and felt with horror that it was going to happen _now at once_ and that _no force could stop it_ the deserted street lamps gleamed sullenly in the snowy darkness like torches at a funeral the snow drifted under my great coat under my coat under my cravat and melted there i did not wrap myself up all was lost anyway at last we arrived i jumped out almost unconscious ran up the steps and began knocking and kicking at the door i felt fearfully weak particularly in my legs and knees the door was opened quickly as though they knew i was coming as a fact simonov had warned them that perhaps another gentleman would arrive and this was a place in which one had to give notice and to observe certain precautions it was one of those millinery establishments which were abolished by the police a good time ago by day it really was a shop but at night if one had an introduction one might visit it for other purposes i walked rapidly through the dark shop into the familiar drawing room where there was only one candle burning and stood still in amazement there was no one there where are they i asked somebody but by now of course they had separated before me was standing a person with a stupid smile the madam herself who had seen me before a minute later a door opened and another person came in taking no notice of anything i strode about the room and i believe i talked to myself i felt as though i had been saved from death and was conscious of this joyfully all over i should have given that slap i should certainly certainly have given it but now they were not here and everything had vanished and changed i looked round i could not realise my condition yet i looked mechanically at the girl who had come in and had a glimpse of a fresh young rather pale face with straight dark eyebrows and with grave as it were wondering eyes that attracted me at once i should have hated her if she had been smiling i began looking at her more intently and as it were with effort i had not fully collected my thoughts there was something simple and good natured in her face but something strangely grave i am sure that this stood in her way here and no one of those fools had noticed her she could not however have been called a beauty though she was tall strong looking and well built she was very simply dressed something loathsome stirred within me i went straight up to her i chanced to look into the glass my harassed face struck me as revolting in the extreme pale angry abject with dishevelled hair no matter i am glad of it i thought i am glad that i shall seem repulsive to her i like that vi somewhere behind a screen a clock began wheezing as though oppressed by something as though someone were strangling it after an unnaturally prolonged wheezing there followed a shrill nasty and as it were unexpectedly rapid chime as though someone were suddenly jumping forward it struck two i woke up though i had indeed not been asleep but lying half conscious it was almost completely dark in the narrow cramped low pitched room cumbered up with an enormous wardrobe and piles of cardboard boxes and all sorts of frippery and litter the candle end that had been burning on the table was going out and gave a faint flicker from time to time in a few minutes there would be complete darkness i was not long in coming to myself everything came back to my mind at once without an effort as though it had been in ambush to pounce upon me again and indeed even while i was unconscious a point seemed continually to remain in my memory unforgotten and round it my dreams moved drearily but strange to say everything that had happened to me in that day seemed to me now on waking to be in the far far away past as though i had long long ago lived all that down my head was full of fumes something seemed to be hovering over me rousing me exciting me and making me restless misery and spite seemed surging up in me again and seeking an outlet suddenly i saw beside me two wide open eyes scrutinising me curiously and persistently the look in those eyes was coldly detached sullen as it were utterly remote it weighed upon me a grim idea came into my brain and passed all over my body as a horrible sensation such as one feels when one goes into a damp and mouldy cellar there was something unnatural in those two eyes beginning to look at me only now i recalled too that during those two hours i had not said a single word to this creature and had in fact considered it utterly superfluous in fact the silence had for some reason gratified me now i suddenly realised vividly the hideous idea revolting as a spider of vice which without love grossly and shamelessly begins with that in which true love finds its consummation for a long time we gazed at each other like that but she did not drop her eyes before mine and her expression did not change so that at last i felt uncomfortable what is your name i asked abruptly to put an end to it liza she answered almost in a whisper but somehow far from graciously and she turned her eyes away i was silent what weather the snow it s disgusting i said almost to myself putting my arm under my head despondently and gazing at the ceiling she made no answer this was horrible have you always lived in petersburg i asked a minute later almost angrily turning my head slightly towards her no where do you come from from riga she answered reluctantly are you a german no russian have you been here long where in this house a fortnight she spoke more and more jerkily the candle went out i could no longer distinguish her face have you a father and mother yes no i have where are they there in riga what are they oh nothing nothing why what class are they tradespeople have you always lived with them yes how old are you twenty why did you leave them oh for no reason that answer meant let me alone i feel sick sad we were silent god knows why i did not go away i felt myself more and more sick and dreary the images of the previous day began of themselves apart from my will flitting through my memory in confusion i suddenly recalled something i had seen that morning when full of anxious thoughts i was hurrying to the office i saw them carrying a coffin out yesterday and they nearly dropped it i suddenly said aloud not that i desired to open the conversation but as it were by accident a coffin yes in the haymarket they were bringing it up out of a cellar from a cellar not from a cellar but a basement oh you know down below from a house of ill fame it was filthy all round egg shells litter a stench it was loathsome silence a nasty day to be buried i began simply to avoid being silent nasty in what way the snow the wet i yawned it makes no difference she said suddenly after a brief silence no it s horrid i yawned again the gravediggers must have sworn at getting drenched by the snow and there must have been water in the grave why water in the grave she asked with a sort of curiosity but speaking even more harshly and abruptly than before i suddenly began to feel provoked why there must have been water at the bottom a foot deep you can t dig a dry grave in volkovo cemetery why why why the place is waterlogged it s a regular marsh so they bury them in water i ve seen it myself many times i had never seen it once indeed i had never been in volkovo and had only heard stories of it do you mean to say you don t mind how you die but why should i die she answered as though defending herself why some day you will die and you will die just the same as that dead woman she was a girl like you she died of consumption a wench would have died in hospital she knows all about it already she said wench not girl she was in debt to her madam i retorted more and more provoked by the discussion and went on earning money for her up to the end though she was in consumption some sledge drivers standing by were talking about her to some soldiers and telling them so no doubt they knew her they were laughing they were going to meet in a pot house to drink to her memory a great deal of this was my invention silence followed profound silence she did not stir and is it better to die in a hospital isn t it just the same besides why should i die she added irritably if not now a little later why a little later why indeed now you are young pretty fresh you fetch a high price but after another year of this life you will be very different you will go off in a year anyway in a year you will be worth less i continued malignantly you will go from here to something lower another house a year later to a third lower and lower and in seven years you will come to a basement in the haymarket that will be if you were lucky but it would be much worse if you got some disease consumption say and caught a chill or something or other it s not easy to get over an illness in your way of life if you catch anything you may not get rid of it and so you would die oh well then i shall die she answered quite vindictively and she made a quick movement but one is sorry sorry for whom sorry for life silence have you been engaged to be married eh what s that to you oh i am not cross examining you it s nothing to me why are you so cross of course you may have had your own troubles what is it to me it s simply that i felt sorry sorry for whom sorry for you no need she whispered hardly audibly and again made a faint movement that incensed me at once what i was so gentle with her and she why do you think that you are on the right path i don t think anything that s what s wrong that you don t think realise it while there is still time there still is time you are still young good looking you might love be married be happy not all married women are happy she snapped out in the rude abrupt tone she had used at first not all of course but anyway it is much better than the life here infinitely better besides with love one can live even without happiness even in sorrow life is sweet life is sweet however one lives but here what is there but foulness phew i turned away with disgust i was no longer reasoning coldly i began to feel myself what i was saying and warmed to the subject i was already longing to expound the cherished ideas i had brooded over in my corner something suddenly flared up in me an object had appeared before me never mind my being here i am not an example for you i am perhaps worse than you are i was drunk when i came here though i hastened however to say in self defence besides a man is no example for a woman it s a different thing i may degrade and defile myself but i am not anyone s slave i come and go and that s an end of it i shake it off and i am a different man but you are a slave from the start yes a slave you give up everything your whole freedom if you want to break your chains afterwards you won t be able to you will be more and more fast in the snares it is an accursed bondage i know it i won t speak of anything else maybe you won t understand but tell me no doubt you are in debt to your madam there you see i added though she made no answer but only listened in silence entirely absorbed that s a bondage for you you will never buy your freedom they will see to that it s like selling your soul to the devil and besides perhaps i too am just as unlucky how do you know and wallow in the mud on purpose out of misery you know men take to drink from grief well maybe i am here from grief come tell me what is there good here here you and i came together just now and did not say one word to one another all the time and it was only afterwards you began staring at me like a wild creature and i at you is that loving is that how one human being should meet another it s hideous that s what it is yes she assented sharply and hurriedly i was positively astounded by the promptitude of this yes so the same thought may have been straying through her mind when she was staring at me just before so she too was capable of certain thoughts damn it all this was interesting this was a point of likeness i thought almost rubbing my hands and indeed it s easy to turn a young soul like that it was the exercise of my power that attracted me most she turned her head nearer to me and it seemed to me in the darkness that she propped herself on her arm perhaps she was scrutinising me how i regretted that i could not see her eyes i heard her deep breathing why have you come here i asked her with a note of authority already in my voice oh i don t know but how nice it would be to be living in your father s house it s warm and free you have a home of your own but what if it s worse than this i must take the right tone flashed through my mind i may not get far with sentimentality but it was only a momentary thought i swear she really did interest me besides i was exhausted and moody and cunning so easily goes hand in hand with feeling who denies it i hastened to answer anything may happen i am convinced that someone has wronged you and that you are more sinned against than sinning of course i know nothing of your story but it s not likely a girl like you has come here of her own inclination a girl like me she whispered hardly audibly but i heard it damn it all i was flattering her that was horrid but perhaps it was a good thing she was silent see liza i will tell you about myself if i had had a home from childhood i shouldn t be what i am now i often think that however bad it may be at home anyway they are your father and mother and not enemies strangers once a year at least they ll show their love of you anyway you know you are at home i grew up without a home and perhaps that s why i ve turned so unfeeling i waited again perhaps she doesn t understand i thought and indeed it is absurd it s moralising if i were a father and had a daughter i believe i should love my daughter more than my sons really i began indirectly as though talking of something else to distract her attention i must confess i blushed why so she asked ah so she was listening i don t know liza i knew a father who was a stern austere man but used to go down on his knees to his daughter used to kiss her hands her feet he couldn t make enough of her really when she danced at parties he used to stand for five hours at a stretch gazing at her he was mad over her i understand that she would fall asleep tired at night and he would wake to kiss her in her sleep and make the sign of the cross over her he would go about in a dirty old coat he was stingy to everyone else but would spend his last penny for her giving her expensive presents and it was his greatest delight when she was pleased with what he gave her fathers always love their daughters more than the mothers do some girls live happily at home and i believe i should never let my daughters marry what next she said with a faint smile i should be jealous i really should to think that she should kiss anyone else that she should love a stranger more than her father it s painful to imagine it of course that s all nonsense of course every father would be reasonable at last but i believe before i should let her marry i should worry myself to death i should find fault with all her suitors but i should end by letting her marry whom she herself loved the one whom the daughter loves always seems the worst to the father you know that is always so so many family troubles come from that some are glad to sell their daughters rather than marrying them honourably ah so that was it such a thing liza happens in those accursed families in which there is neither love nor god i retorted warmly and where there is no love there is no sense either there are such families it s true but i am not speaking of them you must have seen wickedness in your own family if you talk like that truly you must have been unlucky h m that sort of thing mostly comes about through poverty and is it any better with the gentry even among the poor honest people who live happily h m yes perhaps another thing liza man is fond of reckoning up his troubles but does not count his joys if he counted them up as he ought he would see that every lot has enough happiness provided for it and what if all goes well with the family if the blessing of god is upon it if the husband is a good one loves you cherishes you never leaves you there is happiness in such a family even sometimes there is happiness in the midst of sorrow and indeed sorrow is everywhere if you marry _you will find out for yourself_ but think of the first years of married life with one you love what happiness what happiness there sometimes is in it and indeed it s the ordinary thing in those early days even quarrels with one s husband end happily some women get up quarrels with their husbands just because they love them indeed i knew a woman like that she seemed to say that because she loved him she would torment him and make him feel it you know that you may torment a man on purpose through love women are particularly given to that thinking to themselves i will love him so i will make so much of him afterwards that it s no sin to torment him a little now and all in the house rejoice in the sight of you and you are happy and gay and peaceful and honourable then there are some women who are jealous if he went off anywhere i knew one such woman she couldn t restrain herself but would jump up at night and run off on the sly to find out where he was whether he was with some other woman that s a pity and the woman knows herself it s wrong and her heart fails her and she suffers but she loves it s all through love and how sweet it is to make up after quarrels to own herself in the wrong or to forgive him and they both are so happy all at once as though they had met anew been married over again as though their love had begun afresh and no one no one should know what passes between husband and wife if they love one another and whatever quarrels there may be between them they ought not to call in their own mother to judge between them and tell tales of one another they are their own judges love is a holy mystery and ought to be hidden from all other eyes whatever happens that makes it holier and better they respect one another more and much is built on respect and if once there has been love if they have been married for love why should love pass away surely one can keep it it is rare that one cannot keep it and if the husband is kind and straightforward why should not love last the first phase of married love will pass it is true but then there will come a love that is better still then there will be the union of souls they will have everything in common there will be no secrets between them and once they have children the most difficult times will seem to them happy so long as there is love and courage even toil will be a joy you may deny yourself bread for your children and even that will be a joy they will love you for it afterwards so you are laying by for your future as the children grow up you feel that you are an example a support for them that even after you die your children will always keep your thoughts and feelings because they have received them from you they will take on your semblance and likeness so you see this is a great duty how can it fail to draw the father and mother nearer people say it s a trial to have children who says that it is heavenly happiness are you fond of little children liza i am awfully fond of them you know a little rosy baby boy at your bosom and what husband s heart is not touched seeing his wife nursing his child a plump little rosy baby sprawling and snuggling chubby little hands and feet clean tiny little nails so tiny that it makes one laugh to look at them eyes that look as if they understand everything and while it sucks it clutches at your bosom with its little hand plays when its father comes up the child tears itself away from the bosom flings itself back looks at its father laughs as though it were fearfully funny and falls to sucking again or it will bite its mother s breast when its little teeth are coming while it looks sideways at her with its little eyes as though to say look i am biting is not all that happiness when they are the three together husband wife and child one can forgive a great deal for the sake of such moments yes liza one must first learn to live oneself before one blames others it s by pictures pictures like that one must get at you i thought to myself though i did speak with real feeling and all at once i flushed crimson what if she were suddenly to burst out laughing what should i do then that idea drove me to fury towards the end of my speech i really was excited and now my vanity was somehow wounded the silence continued i almost nudged her why are you she began and stopped but i understood there was a quiver of something different in her voice not abrupt harsh and unyielding as before but something soft and shamefaced so shamefaced that i suddenly felt ashamed and guilty what i asked with tender curiosity why you what why you speak somehow like a book she said and again there was a note of irony in her voice that remark sent a pang to my heart it was not what i was expecting i did not understand that she was hiding her feelings under irony that this is usually the last refuge of modest and chaste souled people when the privacy of their soul is coarsely and intrusively invaded and that their pride makes them refuse to surrender till the last moment and shrink from giving expression to their feelings before you i ought to have guessed the truth from the timidity with which she had repeatedly approached her sarcasm only bringing herself to utter it at last with an effort but i did not guess and an evil feeling took possession of me wait a bit i thought vii oh hush liza how can you talk about being like a book when it makes even me an outsider feel sick though i don t look at it as an outsider for indeed it touches me to the heart is it possible is it possible that you do not feel sick at being here yourself evidently habit does wonders god knows what habit can do with anyone can you seriously think that you will never grow old that you will always be good looking and that they will keep you here for ever and ever i say nothing of the loathsomeness of the life here though let me tell you this about it about your present life i mean here though you are young now attractive nice with soul and feeling yet you know as soon as i came to myself just now i felt at once sick at being here with you one can only come here when one is drunk but if you were anywhere else living as good people live i should perhaps be more than attracted by you should fall in love with you should be glad of a look from you let alone a word i should hang about your door should go down on my knees to you should look upon you as my betrothed and think it an honour to be allowed to i should not dare to have an impure thought about you but here you see i know that i have only to whistle and you have to come with me whether you like it or not i don t consult your wishes but you mine the lowest labourer hires himself as a workman but he doesn t make a slave of himself altogether besides he knows that he will be free again presently but when are you free only think what you are giving up here what is it you are making a slave of it is your soul together with your body you are selling your soul which you have no right to dispose of you give your love to be outraged by every drunkard love but that s everything you know it s a priceless diamond it s a maiden s treasure love why a man would be ready to give his soul to face death to gain that love but how much is your love worth now you are sold all of you body and soul and there is no need to strive for love when you can have everything without love and you know there is no greater insult to a girl than that do you understand to be sure i have heard that they comfort you poor fools they let you have lovers of your own here but you know that s simply a farce that s simply a sham it s just laughing at you and you are taken in by it why do you suppose he really loves you that lover of yours i don t believe it how can he love you when he knows you may be called away from him any minute he would be a low fellow if he did will he have a grain of respect for you what have you in common with him he laughs at you and robs you that is all his love amounts to you are lucky if he does not beat you very likely he does beat you too ask him if you have got one whether he will marry you he will laugh in your face if he doesn t spit in it or give you a blow though maybe he is not worth a bad halfpenny himself and for what have you ruined your life if you come to think of it for the coffee they give you to drink and the plentiful meals but with what object are they feeding you up an honest girl couldn t swallow the food for she would know what she was being fed for you are in debt here and of course you will always be in debt and you will go on in debt to the end till the visitors here begin to scorn you and that will soon happen don t rely upon your youth all that flies by express train here you know you will be kicked out and not simply kicked out long before that she ll begin nagging at you scolding you abusing you as though you had not sacrificed your health for her had not thrown away your youth and your soul for her benefit but as though you had ruined her beggared her robbed her and don t expect anyone to take your part the others your companions will attack you too win her favour for all are in slavery here and have lost all conscience and pity here long ago they have become utterly vile and nothing on earth is viler more loathsome and more insulting than their abuse and you are laying down everything here unconditionally youth and health and beauty and hope and at twenty two you will look like a woman of five and thirty and you will be lucky if you are not diseased pray to god for that no doubt you are thinking now that you have a gay time and no work to do yet there is no work harder or more dreadful in the world or ever has been one would think that the heart alone would be worn out with tears and you won t dare to say a word not half a word when they drive you away from here you will go away as though you were to blame you will change to another house then to a third then somewhere else till you come down at last to the haymarket there you will be beaten at every turn that is good manners there the visitors don t know how to be friendly without beating you you don t believe that it is so hateful there go and look for yourself some time you can see with your own eyes once one new year s day i saw a woman at a door they had turned her out as a joke to give her a taste of the frost because she had been crying so much and they shut the door behind her at nine o clock in the morning she was already quite drunk dishevelled half naked covered with bruises her face was powdered but she had a black eye blood was trickling from her nose and her teeth some cabman had just given her a drubbing she was sitting on the stone steps a salt fish of some sort was in her hand she was crying wailing something about her luck and beating with the fish on the steps and cabmen and drunken soldiers were crowding in the doorway taunting her you don t believe that you will ever be like that i should be sorry to believe it too but how do you know maybe ten years eight years ago that very woman with the salt fish came here fresh as a cherub innocent pure knowing no evil blushing at every word perhaps she was like you proud ready to take offence not like the others perhaps she looked like a queen and knew what happiness was in store for the man who should love her and whom she should love do you see how it ended and what if at that very minute when she was beating on the filthy steps with that fish drunken and dishevelled what if at that very minute she recalled the pure early days in her father s house when she used to go to school and the neighbour s son watched for her on the way declaring that he would love her as long as he lived that he would devote his life to her and when they vowed to love one another for ever and be married as soon as they were grown up no liza it would be happy for you if you were to die soon of consumption in some corner in some cellar like that woman just now in the hospital do you say you will be lucky if they take you but what if you are still of use to the madam here consumption is a queer disease it is not like fever the patient goes on hoping till the last minute and says he is all right he deludes himself and that just suits your madam don t doubt it that s how it is you have sold your soul and what is more you owe money so you daren t say a word but when you are dying all will abandon you all will turn away from you for then there will be nothing to get from you what s more they will reproach you for cumbering the place for being so long over dying however you beg you won t get a drink of water without abuse whenever are you going off you nasty hussy you won t let us sleep with your moaning you make the gentlemen sick that s true i have heard such things said myself they will thrust you dying into the filthiest corner in the cellar in the damp and darkness what will your thoughts be lying there alone when you die strange hands will lay you out with grumbling and impatience no one will bless you no one will sigh for you they only want to get rid of you as soon as may be they will buy a coffin take you to the grave as they did that poor woman today and celebrate your memory at the tavern in the grave sleet filth wet snow no need to put themselves out for you let her down vanuha it s just like her luck even here she is head foremost the hussy shorten the cord you rascal it s all right as it is all right is it why she s on her side she was a fellow creature after all but never mind throw the earth on her and they won t care to waste much time quarrelling over you they will scatter the wet blue clay as quick as they can and go off to the tavern and there your memory on earth will end other women have children to go to their graves fathers husbands while for you neither tear nor sigh nor remembrance no one in the whole world will ever come to you your name will vanish from the face of the earth as though you had never existed never been born at all nothing but filth and mud however you knock at your coffin lid at night when the dead arise however you cry let me out kind people to live in the light of day my life was no life at all my life has been thrown away like a dish clout it was drunk away in the tavern at the haymarket let me out kind people to live in the world again and i worked myself up to such a pitch that i began to have a lump in my throat myself and and all at once i stopped sat up in dismay and bending over apprehensively began to listen with a beating heart i had reason to be troubled i had felt for some time that i was turning her soul upside down and rending her heart and and the more i was convinced of it the more eagerly i desired to gain my object as quickly and as effectually as possible it was the exercise of my skill that carried me away yet it was not merely sport i knew i was speaking stiffly artificially even bookishly in fact i could not speak except like a book but that did not trouble me i knew i felt that i should be understood and that this very bookishness might be an assistance but now having attained my effect i was suddenly panic stricken never before had i witnessed such despair she was lying on her face thrusting her face into the pillow and clutching it in both hands her heart was being torn her youthful body was shuddering all over as though in convulsions suppressed sobs rent her bosom and suddenly burst out in weeping and wailing then she pressed closer into the pillow she did not want anyone here not a living soul to know of her anguish and her tears she bit the pillow bit her hand till it bled i saw that afterwards or thrusting her fingers into her dishevelled hair seemed rigid with the effort of restraint holding her breath and clenching her teeth i began saying something begging her to calm herself but felt that i did not dare and all at once in a sort of cold shiver almost in terror began fumbling in the dark trying hurriedly to get dressed to go it was dark though i tried my best i could not finish dressing quickly suddenly i felt a box of matches and a candlestick with a whole candle in it as soon as the room was lighted up liza sprang up sat up in bed and with a contorted face with a half insane smile looked at me almost senselessly i sat down beside her and took her hands she came to herself made an impulsive movement towards me would have caught hold of me but did not dare and slowly bowed her head before me liza my dear i was wrong forgive me my dear i began but she squeezed my hand in her fingers so tightly that i felt i was saying the wrong thing and stopped this is my address liza come to me i will come she answered resolutely her head still bowed but now i am going good bye till we meet again i got up she too stood up and suddenly flushed all over gave a shudder snatched up a shawl that was lying on a chair and muffled herself in it to her chin as she did this she gave another sickly smile blushed and looked at me strangely i felt wretched i was in haste to get away to disappear wait a minute she said suddenly in the passage just at the doorway stopping me with her hand on my overcoat she put down the candle in hot haste and ran off evidently she had thought of something or wanted to show me something as she ran away she flushed her eyes shone and there was a smile on her lips what was the meaning of it against my will i waited she came back a minute later with an expression that seemed to ask forgiveness for something in fact it was not the same face not the same look as the evening before sullen mistrustful and obstinate her eyes now were imploring soft and at the same time trustful caressing timid the expression with which children look at people they are very fond of of whom they are asking a favour her eyes were a light hazel they were lovely eyes full of life and capable of expressing love as well as sullen hatred making no explanation as though i as a sort of higher being must understand everything without explanations she held out a piece of paper to me her whole face was positively beaming at that instant with naive almost childish triumph i unfolded it it was a letter to her from a medical student or someone of that sort a very high flown and flowery but extremely respectful love letter i don t recall the words now but i remember well that through the high flown phrases there was apparent a genuine feeling which cannot be feigned when i had finished reading it i met her glowing questioning and childishly impatient eyes fixed upon me she fastened her eyes upon my face and waited impatiently for what i should say in a few words hurriedly but with a sort of joy and pride she explained to me that she had been to a dance somewhere in a private house a family of very nice people _who knew nothing_ absolutely nothing for she had only come here so lately and it had all happened and she hadn t made up her mind to stay and was certainly going away as soon as she had paid her debt and at that party there had been the student who had danced with her all the evening he had talked to her and it turned out that he had known her in old days at riga when he was a child they had played together but a very long time ago and he knew her parents but _about this_ he knew nothing nothing whatever and had no suspicion and the day after the dance three days ago he had sent her that letter through the friend with whom she had gone to the party and well that was all she dropped her shining eyes with a sort of bashfulness as she finished the poor girl was keeping that student s letter as a precious treasure and had run to fetch it her only treasure because she did not want me to go away without knowing that she too was honestly and genuinely loved that she too was addressed respectfully no doubt that letter was destined to lie in her box and lead to nothing but none the less i am certain that she would keep it all her life as a precious treasure as her pride and justification and now at such a minute she had thought of that letter and brought it with naive pride to raise herself in my eyes that i might see that i too might think well of her i said nothing pressed her hand and went out i so longed to get away i walked all the way home in spite of the fact that the melting snow was still falling in heavy flakes i was exhausted shattered in bewilderment but behind the bewilderment the truth was already gleaming the loathsome truth viii it was some time however before i consented to recognise that truth waking up in the morning after some hours of heavy leaden sleep and immediately realising all that had happened on the previous day i was positively amazed at my last night s _sentimentality_ with liza at all those outcries of horror and pity to think of having such an attack of womanish hysteria pah i concluded and what did i thrust my address upon her for what if she comes let her come though it doesn t matter but _obviously_ that was not now the chief and the most important matter i had to make haste and at all costs save my reputation in the eyes of zverkov and simonov as quickly as possible that was the chief business and i was so taken up that morning that i actually forgot all about liza first of all i had at once to repay what i had borrowed the day before from simonov i resolved on a desperate measure to borrow fifteen roubles straight off from anton antonitch as luck would have it he was in the best of humours that morning and gave it to me at once on the first asking i was so delighted at this that as i signed the iou with a swaggering air i told him casually that the night before i had been keeping it up with some friends at the hôtel de paris we were giving a farewell party to a comrade in fact i might say a friend of my childhood and you know a desperate rake fearfully spoilt of course he belongs to a good family and has considerable means a brilliant career he is witty charming a regular lovelace you understand we drank an extra half dozen and and it went off all right all this was uttered very easily unconstrainedly and complacently on reaching home i promptly wrote to simonov to this hour i am lost in admiration when i recall the truly gentlemanly good humoured candid tone of my letter with tact and good breeding and above all entirely without superfluous words i blamed myself for all that had happened i defended myself if i really may be allowed to defend myself by alleging that being utterly unaccustomed to wine i had been intoxicated with the first glass which i said i had drunk before they arrived while i was waiting for them at the hôtel de paris between five and six o clock i begged simonov s pardon especially i asked him to convey my explanations to all the others especially to zverkov whom i seemed to remember as though in a dream i had insulted i added that i would have called upon all of them myself but my head ached and besides i had not the face to i was particularly pleased with a certain lightness almost carelessness strictly within the bounds of politeness however which was apparent in my style and better than any possible arguments gave them at once to understand that i took rather an independent view of all that unpleasantness last night that i was by no means so utterly crushed as you my friends probably imagine but on the contrary looked upon it as a gentleman serenely respecting himself should look upon it on a young hero s past no censure is cast there is actually an aristocratic playfulness about it i thought admiringly as i read over the letter and it s all because i am an intellectual and cultivated man another man in my place would not have known how to extricate himself but here i have got out of it and am as jolly as ever again and all because i am a cultivated and educated man of our day and indeed perhaps everything was due to the wine yesterday h m no it was not the wine i did not drink anything at all between five and six when i was waiting for them i had lied to simonov i had lied shamelessly and indeed i wasn t ashamed now hang it all though the great thing was that i was rid of it i put six roubles in the letter sealed it up and asked apollon to take it to simonov when he learned that there was money in the letter apollon became more respectful and agreed to take it towards evening i went out for a walk my head was still aching and giddy after yesterday but as evening came on and the twilight grew denser my impressions and following them my thoughts grew more and more different and confused something was not dead within me in the depths of my heart and conscience it would not die and it showed itself in acute depression for the most part i jostled my way through the most crowded business streets along myeshtchansky street along sadovy street and in yusupov garden i always liked particularly sauntering along these streets in the dusk just when there were crowds of working people of all sorts going home from their daily work with faces looking cross with anxiety what i liked was just that cheap bustle that bare prose on this occasion the jostling of the streets irritated me more than ever i could not make out what was wrong with me i could not find the clue something seemed rising up continually in my soul painfully and refusing to be appeased i returned home completely upset it was just as though some crime were lying on my conscience the thought that liza was coming worried me continually it seemed queer to me that of all my recollections of yesterday this tormented me as it were especially as it were quite separately everything else i had quite succeeded in forgetting by the evening i dismissed it all and was still perfectly satisfied with my letter to simonov but on this point i was not satisfied at all it was as though i were worried only by liza what if she comes i thought incessantly well it doesn t matter let her come h m it s horrid that she should see for instance how i live yesterday i seemed such a hero to her while now h m it s horrid though that i have let myself go so the room looks like a beggar s and i brought myself to go out to dinner in such a suit and my american leather sofa with the stuffing sticking out and my dressing gown which will not cover me such tatters and she will see all this and she will see apollon that beast is certain to insult her he will fasten upon her in order to be rude to me and i of course shall be panic stricken as usual i shall begin bowing and scraping before her and pulling my dressing gown round me i shall begin smiling telling lies oh the beastliness and it isn t the beastliness of it that matters most there is something more important more loathsome viler yes viler and to put on that dishonest lying mask again when i reached that thought i fired up all at once why dishonest how dishonest i was speaking sincerely last night i remember there was real feeling in me too what i wanted was to excite an honourable feeling in her her crying was a good thing it will have a good effect yet i could not feel at ease all that evening even when i had come back home even after nine o clock when i calculated that liza could not possibly come still she haunted me and what was worse she came back to my mind always in the same position one moment out of all that had happened last night stood vividly before my imagination the moment when i struck a match and saw her pale distorted face with its look of torture and what a pitiful what an unnatural what a distorted smile she had at that moment but i did not know then that fifteen years later i should still in my imagination see liza always with the pitiful distorted inappropriate smile which was on her face at that minute next day i was ready again to look upon it all as nonsense due to over excited nerves and above all as _exaggerated_ i was always conscious of that weak point of mine and sometimes very much afraid of it i exaggerate everything that is where i go wrong i repeated to myself every hour but however liza will very likely come all the same was the refrain with which all my reflections ended i was so uneasy that i sometimes flew into a fury she ll come she is certain to come i cried running about the room if not today she will come tomorrow she ll find me out the damnable romanticism of these pure hearts oh the vileness oh the silliness oh the stupidity of these wretched sentimental souls why how fail to understand how could one fail to understand but at this point i stopped short and in great confusion indeed and how few how few words i thought in passing were needed how little of the idyllic and affectedly bookishly artificially idyllic too had sufficed to turn a whole human life at once according to my will that s virginity to be sure freshness of soil at times a thought occurred to me to go to her to tell her all and beg her not to come to me but this thought stirred such wrath in me that i believed i should have crushed that damned liza if she had chanced to be near me at the time i should have insulted her have spat at her have turned her out have struck her one day passed however another and another she did not come and i began to grow calmer i felt particularly bold and cheerful after nine o clock i even sometimes began dreaming and rather sweetly i for instance became the salvation of liza simply through her coming to me and my talking to her i develop her educate her finally i notice that she loves me loves me passionately i pretend not to understand i don t know however why i pretend just for effect perhaps at last all confusion transfigured trembling and sobbing she flings herself at my feet and says that i am her saviour and that she loves me better than anything in the world i am amazed but liza i say can you imagine that i have not noticed your love i saw it all i divined it but i did not dare to approach you first because i had an influence over you and was afraid that you would force yourself from gratitude to respond to my love would try to rouse in your heart a feeling which was perhaps absent and i did not wish that because it would be tyranny it would be indelicate in short i launch off at that point into european inexplicably lofty subtleties a la george sand but now now you are mine you are my creation you are pure you are good you are my noble wife into my house come bold and free its rightful mistress there to be then we begin living together go abroad and so on and so on in fact in the end it seemed vulgar to me myself and i began putting out my tongue at myself besides they won t let her out the hussy i thought they don t let them go out very readily especially in the evening for some reason i fancied she would come in the evening and at seven o clock precisely though she did say she was not altogether a slave there yet and had certain rights so h m damn it all she will come she is sure to come it was a good thing in fact that apollon distracted my attention at that time by his rudeness he drove me beyond all patience he was the bane of my life the curse laid upon me by providence we had been squabbling continually for years and i hated him my god how i hated him i believe i had never hated anyone in my life as i hated him especially at some moments he was an elderly dignified man who worked part of his time as a tailor but for some unknown reason he despised me beyond all measure and looked down upon me insufferably though indeed he looked down upon everyone simply to glance at that flaxen smoothly brushed head at the tuft of hair he combed up on his forehead and oiled with sunflower oil at that dignified mouth compressed into the shape of the letter v made one feel one was confronting a man who never doubted of himself he was a pedant to the most extreme point the greatest pedant i had met on earth and with that had a vanity only befitting alexander of macedon he was in love with every button on his coat every nail on his fingers absolutely in love with them and he looked it in his behaviour to me he was a perfect tyrant he spoke very little to me and if he chanced to glance at me he gave me a firm majestically self confident and invariably ironical look that drove me sometimes to fury he did his work with the air of doing me the greatest favour though he did scarcely anything for me and did not indeed consider himself bound to do anything there could be no doubt that he looked upon me as the greatest fool on earth and that he did not get rid of me was simply that he could get wages from me every month he consented to do nothing for me for seven roubles a month many sins should be forgiven me for what i suffered from him my hatred reached such a point that sometimes his very step almost threw me into convulsions what i loathed particularly was his lisp his tongue must have been a little too long or something of that sort for he continually lisped and seemed to be very proud of it imagining that it greatly added to his dignity he spoke in a slow measured tone with his hands behind his back and his eyes fixed on the ground he maddened me particularly when he read aloud the psalms to himself behind his partition many a battle i waged over that reading but he was awfully fond of reading aloud in the evenings in a slow even sing song voice as though over the dead it is interesting that that is how he has ended he hires himself out to read the psalms over the dead and at the same time he kills rats and makes blacking but at that time i could not get rid of him it was as though he were chemically combined with my existence besides nothing would have induced him to consent to leave me i could not live in furnished lodgings my lodging was my private solitude my shell my cave in which i concealed myself from all mankind and apollon seemed to me for some reason an integral part of that flat and for seven years i could not turn him away to be two or three days behind with his wages for instance was impossible he would have made such a fuss i should not have known where to hide my head but i was so exasperated with everyone during those days that i made up my mind for some reason and with some object to _punish_ apollon and not to pay him for a fortnight the wages that were owing him i had for a long time for the last two years been intending to do this simply in order to teach him not to give himself airs with me and to show him that if i liked i could withhold his wages i purposed to say nothing to him about it and was purposely silent indeed in order to score off his pride and force him to be the first to speak of his wages then i would take the seven roubles out of a drawer show him i have the money put aside on purpose but that i won t i won t i simply won t pay him his wages i won t just because that is what i wish because i am master and it is for me to decide because he has been disrespectful because he has been rude but if he were to ask respectfully i might be softened and give it to him otherwise he might wait another fortnight another three weeks a whole month but angry as i was yet he got the better of me i could not hold out for four days he began as he always did begin in such cases for there had been such cases already there had been attempts and it may be observed i knew all this beforehand i knew his nasty tactics by heart he would begin by fixing upon me an exceedingly severe stare keeping it up for several minutes at a time particularly on meeting me or seeing me out of the house if i held out and pretended not to notice these stares he would still in silence proceed to further tortures all at once _à propos_ of nothing he would walk softly and smoothly into my room when i was pacing up and down or reading stand at the door one hand behind his back and one foot behind the other and fix upon me a stare more than severe utterly contemptuous if i suddenly asked him what he wanted he would make me no answer but continue staring at me persistently for some seconds then with a peculiar compression of his lips and a most significant air deliberately turn round and deliberately go back to his room two hours later he would come out again and again present himself before me in the same way it had happened that in my fury i did not even ask him what he wanted but simply raised my head sharply and imperiously and began staring back at him so we stared at one another for two minutes at last he turned with deliberation and dignity and went back again for two hours if i were still not brought to reason by all this but persisted in my revolt he would suddenly begin sighing while he looked at me long deep sighs as though measuring by them the depths of my moral degradation and of course it ended at last by his triumphing completely i raged and shouted but still was forced to do what he wanted this time the usual staring manoeuvres had scarcely begun when i lost my temper and flew at him in a fury i was irritated beyond endurance apart from him stay i cried in a frenzy as he was slowly and silently turning with one hand behind his back to go to his room stay come back come back i tell you and i must have bawled so unnaturally that he turned round and even looked at me with some wonder however he persisted in saying nothing and that infuriated me how dare you come and look at me like that without being sent for answer after looking at me calmly for half a minute he began turning round again stay i roared running up to him don t stir there answer now what did you come in to look at if you have any order to give me it s my duty to carry it out he answered after another silent pause with a slow measured lisp raising his eyebrows and calmly twisting his head from one side to another all this with exasperating composure that s not what i am asking you about you torturer i shouted turning crimson with anger i ll tell you why you came here myself you see i don t give you your wages you are so proud you don t want to bow down and ask for it and so you come to punish me with your stupid stares to worry me and you have no sus pic ion how stupid it is stupid stupid stupid stupid he would have turned round again without a word but i seized him listen i shouted to him here s the money do you see here it is i took it out of the table drawer here s the seven roubles complete but you are not going to have it you are not going to have it until you come respectfully with bowed head to beg my pardon do you hear that cannot be he answered with the most unnatural self confidence it shall be so i said i give you my word of honour it shall be and there s nothing for me to beg your pardon for he went on as though he had not noticed my exclamations at all why besides you called me a torturer for which i can summon you at the police station at any time for insulting behaviour go summon me i roared go at once this very minute this very second you are a torturer all the same a torturer but he merely looked at me then turned and regardless of my loud calls to him he walked to his room with an even step and without looking round if it had not been for liza nothing of this would have happened i decided inwardly then after waiting a minute i went myself behind his screen with a dignified and solemn air though my heart was beating slowly and violently apollon i said quietly and emphatically though i was breathless go at once without a minute s delay and fetch the police officer he had meanwhile settled himself at his table put on his spectacles and taken up some sewing but hearing my order he burst into a guffaw at once go this minute go on or else you can t imagine what will happen you are certainly out of your mind he observed without even raising his head lisping as deliberately as ever and threading his needle whoever heard of a man sending for the police against himself and as for being frightened you are upsetting yourself about nothing for nothing will come of it go i shrieked clutching him by the shoulder i felt i should strike him in a minute but i did not notice the door from the passage softly and slowly open at that instant and a figure come in stop short and begin staring at us in perplexity i glanced nearly swooned with shame and rushed back to my room there clutching at my hair with both hands i leaned my head against the wall and stood motionless in that position two minutes later i heard apollon s deliberate footsteps there is some woman asking for you he said looking at me with peculiar severity then he stood aside and let in liza he would not go away but stared at us sarcastically go away go away i commanded in desperation at that moment my clock began whirring and wheezing and struck seven ix into my house come bold and free its rightful mistress there to be i stood before her crushed crestfallen revoltingly confused and i believe i smiled as i did my utmost to wrap myself in the skirts of my ragged wadded dressing gown exactly as i had imagined the scene not long before in a fit of depression after standing over us for a couple of minutes apollon went away but that did not make me more at ease what made it worse was that she too was overwhelmed with confusion more so in fact than i should have expected at the sight of me of course sit down i said mechanically moving a chair up to the table and i sat down on the sofa she obediently sat down at once and gazed at me open eyed evidently expecting something from me at once this naïveté of expectation drove me to fury but i restrained myself she ought to have tried not to notice as though everything had been as usual while instead of that she and i dimly felt that i should make her pay dearly for _all this_ you have found me in a strange position liza i began stammering and knowing that this was the wrong way to begin no no don t imagine anything i cried seeing that she had suddenly flushed i am not ashamed of my poverty on the contrary i look with pride on my poverty i am poor but honourable one can be poor and honourable i muttered however would you like tea no she was beginning wait a minute i leapt up and ran to apollon i had to get out of the room somehow apollon i whispered in feverish haste flinging down before him the seven roubles which had remained all the time in my clenched fist here are your wages you see i give them to you but for that you must come to my rescue bring me tea and a dozen rusks from the restaurant if you won t go you ll make me a miserable man you don t know what this woman is this is everything you may be imagining something but you don t know what that woman is apollon who had already sat down to his work and put on his spectacles again at first glanced askance at the money without speaking or putting down his needle then without paying the slightest attention to me or making any answer he went on busying himself with his needle which he had not yet threaded i waited before him for three minutes with my arms crossed _à la napoléon_ my temples were moist with sweat i was pale i felt it but thank god he must have been moved to pity looking at me having threaded his needle he deliberately got up from his seat deliberately moved back his chair deliberately took off his spectacles deliberately counted the money and finally asking me over his shoulder shall i get a whole portion deliberately walked out of the room as i was going back to liza the thought occurred to me on the way shouldn t i run away just as i was in my dressing gown no matter where and then let happen what would i sat down again she looked at me uneasily for some minutes we were silent i will kill him i shouted suddenly striking the table with my fist so that the ink spurted out of the inkstand what are you saying she cried starting i will kill him kill him i shrieked suddenly striking the table in absolute frenzy and at the same time fully understanding how stupid it was to be in such a frenzy you don t know liza what that torturer is to me he is my torturer he has gone now to fetch some rusks he and suddenly i burst into tears it was an hysterical attack how ashamed i felt in the midst of my sobs but still i could not restrain them she was frightened what is the matter what is wrong she cried fussing about me water give me water over there i muttered in a faint voice though i was inwardly conscious that i could have got on very well without water and without muttering in a faint voice but i was what is called _putting it on_ to save appearances though the attack was a genuine one she gave me water looking at me in bewilderment at that moment apollon brought in the tea it suddenly seemed to me that this commonplace prosaic tea was horribly undignified and paltry after all that had happened and i blushed crimson liza looked at apollon with positive alarm he went out without a glance at either of us liza do you despise me i asked looking at her fixedly trembling with impatience to know what she was thinking she was confused and did not know what to answer drink your tea i said to her angrily i was angry with myself but of course it was she who would have to pay for it a horrible spite against her suddenly surged up in my heart i believe i could have killed her to revenge myself on her i swore inwardly not to say a word to her all the time she is the cause of it all i thought our silence lasted for five minutes the tea stood on the table we did not touch it i had got to the point of purposely refraining from beginning in order to embarrass her further it was awkward for her to begin alone several times she glanced at me with mournful perplexity i was obstinately silent i was of course myself the chief sufferer because i was fully conscious of the disgusting meanness of my spiteful stupidity and yet at the same time i could not restrain myself i want to get away from there altogether she began to break the silence in some way but poor girl that was just what she ought not to have spoken about at such a stupid moment to a man so stupid as i was my heart positively ached with pity for her tactless and unnecessary straightforwardness but something hideous at once stifled all compassion in me it even provoked me to greater venom i did not care what happened another five minutes passed perhaps i am in your way she began timidly hardly audibly and was getting up but as soon as i saw this first impulse of wounded dignity i positively trembled with spite and at once burst out why have you come to me tell me that please i began gasping for breath and regardless of logical connection in my words i longed to have it all out at once at one burst i did not even trouble how to begin why have you come answer answer i cried hardly knowing what i was doing i ll tell you my good girl why you have come you ve come because i talked sentimental stuff to you then so now you are soft as butter and longing for fine sentiments again so you may as well know that i was laughing at you then and i am laughing at you now why are you shuddering yes i was laughing at you i had been insulted just before at dinner by the fellows who came that evening before me i came to you meaning to thrash one of them an officer but i didn t succeed i didn t find him i had to avenge the insult on someone to get back my own again you turned up i vented my spleen on you and laughed at you i had been humiliated so i wanted to humiliate i had been treated like a rag so i wanted to show my power that s what it was and you imagined i had come there on purpose to save you yes you imagined that you imagined that i knew that she would perhaps be muddled and not take it all in exactly but i knew too that she would grasp the gist of it very well indeed and so indeed she did she turned white as a handkerchief tried to say something and her lips worked painfully but she sank on a chair as though she had been felled by an axe and all the time afterwards she listened to me with her lips parted and her eyes wide open shuddering with awful terror the cynicism the cynicism of my words overwhelmed her save you i went on jumping up from my chair and running up and down the room before her save you from what but perhaps i am worse than you myself why didn t you throw it in my teeth when i was giving you that sermon but what did you come here yourself for was it to read us a sermon power power was what i wanted then sport was what i wanted i wanted to wring out your tears your humiliation your hysteria that was what i wanted then of course i couldn t keep it up then because i am a wretched creature i was frightened and the devil knows why gave you my address in my folly afterwards before i got home i was cursing and swearing at you because of that address i hated you already because of the lies i had told you because i only like playing with words only dreaming but do you know what i really want is that you should all go to hell that is what i want i want peace yes i d sell the whole world for a farthing straight off so long as i was left in peace is the world to go to pot or am i to go without my tea i say that the world may go to pot for me so long as i always get my tea did you know that or not well anyway i know that i am a blackguard a scoundrel an egoist a sluggard here i have been shuddering for the last three days at the thought of your coming and do you know what has worried me particularly for these three days that i posed as such a hero to you and now you would see me in a wretched torn dressing gown beggarly loathsome i told you just now that i was not ashamed of my poverty so you may as well know that i am ashamed of it i am more ashamed of it than of anything more afraid of it than of being found out if i were a thief because i am as vain as though i had been skinned and the very air blowing on me hurt surely by now you must realise that i shall never forgive you for having found me in this wretched dressing gown just as i was flying at apollon like a spiteful cur the saviour the former hero was flying like a mangy unkempt sheep dog at his lackey and the lackey was jeering at him and i shall never forgive you for the tears i could not help shedding before you just now like some silly woman put to shame and for what i am confessing to you now i shall never forgive you either yes you must answer for it all because you turned up like this because i am a blackguard because i am the nastiest stupidest absurdest and most envious of all the worms on earth who are not a bit better than i am but the devil knows why are never put to confusion while i shall always be insulted by every louse that is my doom and what is it to me that you don t understand a word of this and what do i care what do i care about you and whether you go to ruin there or not do you understand how i shall hate you now after saying this for having been here and listening why it s not once in a lifetime a man speaks out like this and then it is in hysterics what more do you want why do you still stand confronting me after all this why are you worrying me why don t you go but at this point a strange thing happened i was so accustomed to think and imagine everything from books and to picture everything in the world to myself just as i had made it up in my dreams beforehand that i could not all at once take in this strange circumstance what happened was this liza insulted and crushed by me understood a great deal more than i imagined she understood from all this what a woman understands first of all if she feels genuine love that is that i was myself unhappy the frightened and wounded expression on her face was followed first by a look of sorrowful perplexity when i began calling myself a scoundrel and a blackguard and my tears flowed the tirade was accompanied throughout by tears her whole face worked convulsively she was on the point of getting up and stopping me when i finished she took no notice of my shouting why are you here why don t you go away but realised only that it must have been very bitter to me to say all this besides she was so crushed poor girl she considered herself infinitely beneath me how could she feel anger or resentment she suddenly leapt up from her chair with an irresistible impulse and held out her hands yearning towards me though still timid and not daring to stir at this point there was a revulsion in my heart too then she suddenly rushed to me threw her arms round me and burst into tears i too could not restrain myself and sobbed as i never had before they won t let me i can t be good i managed to articulate then i went to the sofa fell on it face downwards and sobbed on it for a quarter of an hour in genuine hysterics she came close to me put her arms round me and stayed motionless in that position but the trouble was that the hysterics could not go on for ever and i am writing the loathsome truth lying face downwards on the sofa with my face thrust into my nasty leather pillow i began by degrees to be aware of a far away involuntary but irresistible feeling that it would be awkward now for me to raise my head and look liza straight in the face why was i ashamed i don t know but i was ashamed the thought too came into my overwrought brain that our parts now were completely changed that she was now the heroine while i was just a crushed and humiliated creature as she had been before me that night four days before and all this came into my mind during the minutes i was lying on my face on the sofa my god surely i was not envious of her then i don t know to this day i cannot decide and at the time of course i was still less able to understand what i was feeling than now i cannot get on without domineering and tyrannising over someone but there is no explaining anything by reasoning and so it is useless to reason i conquered myself however and raised my head i had to do so sooner or later and i am convinced to this day that it was just because i was ashamed to look at her that another feeling was suddenly kindled and flamed up in my heart a feeling of mastery and possession my eyes gleamed with passion and i gripped her hands tightly how i hated her and how i was drawn to her at that minute the one feeling intensified the other it was almost like an act of vengeance at first there was a look of amazement even of terror on her face but only for one instant she warmly and rapturously embraced me x a quarter of an hour later i was rushing up and down the room in frenzied impatience from minute to minute i went up to the screen and peeped through the crack at liza she was sitting on the ground with her head leaning against the bed and must have been crying but she did not go away and that irritated me this time she understood it all i had insulted her finally but there s no need to describe it she realised that my outburst of passion had been simply revenge a fresh humiliation and that to my earlier almost causeless hatred was added now a _personal hatred_ born of envy though i do not maintain positively that she understood all this distinctly but she certainly did fully understand that i was a despicable man and what was worse incapable of loving her i know i shall be told that this is incredible but it is incredible to be as spiteful and stupid as i was it may be added that it was strange i should not love her or at any rate appreciate her love why is it strange in the first place by then i was incapable of love for i repeat with me loving meant tyrannising and showing my moral superiority i have never in my life been able to imagine any other sort of love and have nowadays come to the point of sometimes thinking that love really consists in the right freely given by the beloved object to tyrannise over her even in my underground dreams i did not imagine love except as a struggle i began it always with hatred and ended it with moral subjugation and afterwards i never knew what to do with the subjugated object and what is there to wonder at in that since i had succeeded in so corrupting myself since i was so out of touch with real life as to have actually thought of reproaching her and putting her to shame for having come to me to hear fine sentiments and did not even guess that she had come not to hear fine sentiments but to love me because to a woman all reformation all salvation from any sort of ruin and all moral renewal is included in love and can only show itself in that form i did not hate her so much however when i was running about the room and peeping through the crack in the screen i was only insufferably oppressed by her being here i wanted her to disappear i wanted peace to be left alone in my underground world real life oppressed me with its novelty so much that i could hardly breathe but several minutes passed and she still remained without stirring as though she were unconscious i had the shamelessness to tap softly at the screen as though to remind her she started sprang up and flew to seek her kerchief her hat her coat as though making her escape from me two minutes later she came from behind the screen and looked with heavy eyes at me i gave a spiteful grin which was forced however to _keep up appearances_ and i turned away from her eyes good bye she said going towards the door i ran up to her seized her hand opened it thrust something in it and closed it again then i turned at once and dashed away in haste to the other corner of the room to avoid seeing anyway i did mean a moment since to tell a lie to write that i did this accidentally not knowing what i was doing through foolishness through losing my head but i don t want to lie and so i will say straight out that i opened her hand and put the money in it from spite it came into my head to do this while i was running up and down the room and she was sitting behind the screen but this i can say for certain though i did that cruel thing purposely it was not an impulse from the heart but came from my evil brain this cruelty was so affected so purposely made up so completely a product of the brain of books that i could not even keep it up a minute first i dashed away to avoid seeing her and then in shame and despair rushed after liza i opened the door in the passage and began listening liza liza i cried on the stairs but in a low voice not boldly there was no answer but i fancied i heard her footsteps lower down on the stairs liza i cried more loudly no answer but at that minute i heard the stiff outer glass door open heavily with a creak and slam violently the sound echoed up the stairs she had gone i went back to my room in hesitation i felt horribly oppressed i stood still at the table beside the chair on which she had sat and looked aimlessly before me a minute passed suddenly i started straight before me on the table i saw in short i saw a crumpled blue five rouble note the one i had thrust into her hand a minute before it was the same note it could be no other there was no other in the flat so she had managed to fling it from her hand on the table at the moment when i had dashed into the further corner well i might have expected that she would do that might i have expected it no i was such an egoist i was so lacking in respect for my fellow creatures that i could not even imagine she would do so i could not endure it a minute later i flew like a madman to dress flinging on what i could at random and ran headlong after her she could not have got two hundred paces away when i ran out into the street it was a still night and the snow was coming down in masses and falling almost perpendicularly covering the pavement and the empty street as though with a pillow there was no one in the street no sound was to be heard the street lamps gave a disconsolate and useless glimmer i ran two hundred paces to the cross roads and stopped short where had she gone and why was i running after her why to fall down before her to sob with remorse to kiss her feet to entreat her forgiveness i longed for that my whole breast was being rent to pieces and never never shall i recall that minute with indifference but what for i thought should i not begin to hate her perhaps even tomorrow just because i had kissed her feet today should i give her happiness had i not recognised that day for the hundredth time what i was worth should i not torture her i stood in the snow gazing into the troubled darkness and pondered this and will it not be better i mused fantastically afterwards at home stifling the living pang of my heart with fantastic dreams will it not be better that she should keep the resentment of the insult for ever resentment why it is purification it is a most stinging and painful consciousness tomorrow i should have defiled her soul and have exhausted her heart while now the feeling of insult will never die in her heart and however loathsome the filth awaiting her the feeling of insult will elevate and purify her by hatred h m perhaps too by forgiveness will all that make things easier for her though and indeed i will ask on my own account here an idle question which is better cheap happiness or exalted sufferings well which is better so i dreamed as i sat at home that evening almost dead with the pain in my soul never had i endured such suffering and remorse yet could there have been the faintest doubt when i ran out from my lodging that i should turn back half way i never met liza again and i have heard nothing of her i will add too that i remained for a long time afterwards pleased with the phrase about the benefit from resentment and hatred in spite of the fact that i almost fell ill from misery even now so many years later all this is somehow a very evil memory i have many evil memories now but hadn t i better end my notes here i believe i made a mistake in beginning to write them anyway i have felt ashamed all the time i ve been writing this story so it s hardly literature so much as a corrective punishment why to tell long stories showing how i have spoiled my life through morally rotting in my corner through lack of fitting environment through divorce from real life and rankling spite in my underground world would certainly not be interesting a novel needs a hero and all the traits for an anti hero are _expressly_ gathered together here and what matters most it all produces an unpleasant impression for we are all divorced from life we are all cripples every one of us more or less we are so divorced from it that we feel at once a sort of loathing for real life and so cannot bear to be reminded of it why we have come almost to looking upon real life as an effort almost as hard work and we are all privately agreed that it is better in books and why do we fuss and fume sometimes why are we perverse and ask for something else we don t know what ourselves it would be the worse for us if our petulant prayers were answered come try give any one of us for instance a little more independence untie our hands widen the spheres of our activity relax the control and we yes i assure you we should be begging to be under control again at once i know that you will very likely be angry with me for that and will begin shouting and stamping speak for yourself you will say and for your miseries in your underground holes and don t dare to say all of us excuse me gentlemen i am not justifying myself with that all of us as for what concerns me in particular i have only in my life carried to an extreme what you have not dared to carry halfway and what s more you have taken your cowardice for good sense and have found comfort in deceiving yourselves so that perhaps after all there is more life in me than in you look into it more carefully why we don t even know what living means now what it is and what it is called leave us alone without books and we shall be lost and in confusion at once we shall not know what to join on to what to cling to what to love and what to hate what to respect and what to despise we are oppressed at being men men with a real individual body and blood we are ashamed of it we think it a disgrace and try to contrive to be some sort of impossible generalised man we are stillborn and for generations past have been begotten not by living fathers and that suits us better and better we are developing a taste for it soon we shall contrive to be born somehow from an idea but enough i don t want to write more from underground the notes of this paradoxalist do not end here however he could not refrain from going on with them but it seems to us that we may stop here end of the project gutenberg ebook notes from the underground updated editions will replace the previous one the old editions will be renamed creating the works from print editions not protected by u s copyright law means that no one owns a united states copyright in these works so the foundation and you can copy and distribute it in the united states without permission and without paying copyright royalties special rules set forth in the general terms of use part of this license apply to copying and distributing project gutenberg tm electronic works to protect the project gutenberg tm concept and trademark project gutenberg is a registered trademark and may not be used if you charge for an ebook except by following the terms of the trademark license including paying royalties for use of the project gutenberg trademark if you do not charge anything for copies of this ebook complying with the trademark license is very easy you may use this ebook for nearly any purpose such as creation of derivative works reports performances and research project gutenberg ebooks may be modified and printed and given away you may do practically anything in the united states with ebooks not protected by u s copyright law redistribution is subject to the trademark license especially commercial redistribution start full license the full project gutenberg license please read this before you distribute or use this work to protect the project gutenberg tm mission of promoting the free distribution of electronic works by using or distributing this work or any other work associated in any way with the phrase project gutenberg you agree to comply with all the terms of the full project gutenberg tm license available with this file or online at www gutenberg org license section general terms of use and redistributing project gutenberg tm electronic works a by reading or using any part of this project gutenberg tm electronic work you indicate that you have read understand agree to and accept all the terms of this license and intellectual property trademark copyright agreement if you do not agree to abide by all the terms of this agreement you must cease using and return or destroy all copies of project gutenberg tm electronic works in your possession if you paid a fee for obtaining a copy of or access to a project gutenberg tm electronic work and you do not agree to be bound by the terms of this agreement you may obtain a refund from the person or entity to whom you paid the fee as set forth in paragraph e b project gutenberg is a registered trademark it may only be used on or associated in any way with an electronic work by people who agree to be bound by the terms of this agreement there are a few things that you can do with most project gutenberg tm electronic works even without complying with the full terms of this agreement see paragraph c below there are a lot of things you can do with project gutenberg tm electronic works if you follow the terms of this agreement and help preserve free future access to project gutenberg tm electronic works see paragraph e below c the project gutenberg literary archive foundation the foundation or pglaf owns a compilation copyright in the collection of project gutenberg tm electronic works nearly all the individual works in the collection are in the public domain in the united states if an individual work is unprotected by copyright law in the united states and you are located in the united states we do not claim a right to prevent you from copying distributing performing displaying or creating derivative works based on the work as long as all references to project gutenberg are removed of course we hope that you will support the project gutenberg tm mission of promoting free access to electronic works by freely sharing project gutenberg tm works in compliance with the terms of this agreement for keeping the project gutenberg tm name associated with the work you can easily comply with the terms of this agreement by keeping this work in the same format with its attached full project gutenberg tm license when you share it without charge with others d the copyright laws of the place where you are located also govern what you can do with this work copyright laws in most countries are in a constant state of change if you are outside the united states check the laws of your country in addition to the terms of this agreement before downloading copying displaying performing distributing or creating derivative works based on this work or any other project gutenberg tm work the foundation makes no representations concerning the copyright status of any work in any country other than the united states e unless you have removed all references to project gutenberg e the following sentence with active links to or other immediate access to the full project gutenberg tm license must appear prominently whenever any copy of a project gutenberg tm work any work on which the phrase project gutenberg appears or with which the phrase project gutenberg is associated is accessed displayed performed viewed copied or distributed this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or re use it under the terms of the project gutenberg license included with this ebook or online at www gutenberg org if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook e if an individual project gutenberg tm electronic work is derived from texts not protected by u s copyright law does not contain a notice indicating that it is posted with permission of the copyright holder the work can be copied and distributed to anyone in the united states without paying any fees or charges if you are redistributing or providing access to a work with the phrase project gutenberg associated with or appearing on the work you must comply either with the requirements of paragraphs e through e or obtain permission for the use of the work and the project gutenberg tm trademark as set forth in paragraphs e or e e if an individual project gutenberg tm electronic work is posted with the permission of the copyright holder your use and distribution must comply with both paragraphs e through e and any additional terms imposed by the copyright holder additional terms will be linked to the project gutenberg tm license for all works posted with the permission of the copyright holder found at the beginning of this work e do not unlink or detach or remove the full project gutenberg tm license terms from this work or any files containing a part of this work or any other work associated with project gutenberg tm e do not copy display perform distribute or redistribute this electronic work or any part of this electronic work without prominently displaying the sentence set forth in paragraph e with active links or immediate access to the full terms of the project gutenberg tm license e you may convert to and distribute this work in any binary compressed marked up nonproprietary or proprietary form including any word processing or hypertext form however if you provide access to or distribute copies of a project gutenberg tm work in a format other than plain vanilla ascii or other format used in the official version posted on the official project gutenberg tm website www gutenberg org you must at no additional cost fee or expense to the user provide a copy a means of exporting a copy or a means of obtaining a copy upon request of the work in its original plain vanilla ascii or other form any alternate format must include the full project gutenberg tm license as specified in paragraph e e do not charge a fee for access to viewing displaying performing copying or distributing any project gutenberg tm works unless you comply with paragraph e or e e you may charge a reasonable fee for copies of or providing access to or distributing project gutenberg tm electronic works provided that you pay a royalty fee of of the gross profits you derive from the use of project gutenberg tm works calculated using the method you already use to calculate your applicable taxes the fee is owed to the owner of the project gutenberg tm trademark but he has agreed to donate royalties under this paragraph to the project gutenberg literary archive foundation royalty payments must be paid within days following each date on which you prepare or are legally required to prepare your periodic tax returns royalty payments should be clearly marked as such and sent to the project gutenberg literary archive foundation at the address specified in section information about donations to the project gutenberg literary archive foundation you provide a full refund of any money paid by a user who notifies you in writing or by e mail within days of receipt that s he does not agree to the terms of the full project gutenberg tm license you must require such a user to return or destroy all copies of the works possessed in a physical medium and discontinue all use of and all access to other copies of project gutenberg tm works you provide in accordance with paragraph f a full refund of any money paid for a work or a replacement copy if a defect in the electronic work is discovered and reported to you within days of receipt of the work you comply with all other terms of this agreement for free distribution of project gutenberg tm works e if you wish to charge a fee or distribute a project gutenberg tm electronic work or group of works on different terms than are set forth in this agreement you must obtain permission in writing from the project gutenberg literary archive foundation the manager of the project gutenberg tm trademark contact the foundation as set forth in section below f f project gutenberg volunteers and employees expend considerable effort to identify do copyright research on transcribe and proofread works not protected by u s copyright law in creating the project gutenberg tm collection despite these efforts project gutenberg tm electronic works and the medium on which they may be stored may contain defects such as but not limited to incomplete inaccurate or corrupt data transcription errors a copyright or other intellectual property infringement a defective or damaged disk or other medium a computer virus or computer codes that damage or cannot be read by your equipment f limited warranty disclaimer of damages except for the right of replacement or refund described in paragraph f the project gutenberg literary archive foundation the owner of the project gutenberg tm trademark and any other party distributing a project gutenberg tm electronic work under this agreement disclaim all liability to you for damages costs and expenses including legal fees you agree that you have no remedies for negligence strict liability breach of warranty or breach of contract except those provided in paragraph f you agree that the foundation the trademark owner and any distributor under this agreement will not be liable to you for actual direct indirect consequential punitive or incidental damages even if you give notice of the possibility of such damage f limited right of replacement or refund if you discover a defect in this electronic work within days of receiving it you can receive a refund of the money if any you paid for it by sending a written explanation to the person you received the work from if you received the work on a physical medium you must return the medium with your written explanation the person or entity that provided you with the defective work may elect to provide a replacement copy in lieu of a refund if you received the work electronically the person or entity providing it to you may choose to give you a second opportunity to receive the work electronically in lieu of a refund if the second copy is also defective you may demand a refund in writing without further opportunities to fix the problem f except for the limited right of replacement or refund set forth in paragraph f this work is provided to you as is with no other warranties of any kind express or implied including but not limited to warranties of merchantability or fitness for any purpose f some states do not allow disclaimers of certain implied warranties or the exclusion or limitation of certain types of damages if any disclaimer or limitation set forth in this agreement violates the law of the state applicable to this agreement the agreement shall be interpreted to make the maximum disclaimer or limitation permitted by the applicable state law the invalidity or unenforceability of any provision of this agreement shall not void the remaining provisions f indemnity you agree to indemnify and hold the foundation the trademark owner any agent or employee of the foundation anyone providing copies of project gutenberg tm electronic works in accordance with this agreement and any volunteers associated with the production promotion and distribution of project gutenberg tm electronic works harmless from all liability costs and expenses including legal fees that arise directly or indirectly from any of the following which you do or cause to occur a distribution of this or any project gutenberg tm work b alteration modification or additions or deletions to any project gutenberg tm work and c any defect you cause section information about the mission of project gutenberg tm project gutenberg tm is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete old middle aged and new computers it exists because of the efforts of hundreds of volunteers and donations from people in all walks of life volunteers and financial support to provide volunteers with the assistance they need are critical to reaching project gutenberg tm s goals and ensuring that the project gutenberg tm collection will remain freely available for generations to come in the project gutenberg literary archive foundation was created to provide a secure and permanent future for project gutenberg tm and future generations to learn more about the project gutenberg literary archive foundation and how your efforts and donations can help see sections and and the foundation information page at www gutenberg org section information about the project gutenberg literary archive foundation the project gutenberg literary archive foundation is a non profit c educational corporation organized under the laws of the state of mississippi and granted tax exempt status by the internal revenue service the foundation s ein or federal tax identification number is contributions to the project gutenberg literary archive foundation are tax deductible to the full extent permitted by u s federal laws and your state s laws the foundation s business office is located at north west salt lake city ut email contact links and up to date contact information can be found at the foundation s website and official page at www gutenberg org contact section information about donations to the project gutenberg literary archive foundation project gutenberg tm depends upon and cannot survive without widespread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine readable form accessible by the widest array of equipment including outdated equipment many small donations to are particularly important to maintaining tax exempt status with the irs the foundation is committed to complying with the laws regulating charities and charitable donations in all states of the united states compliance requirements are not uniform and it takes a considerable effort much paperwork and many fees to meet and keep up with these requirements we do not solicit donations in locations where we have not received written confirmation of compliance to send donations or determine the status of compliance for any particular state visit www gutenberg org donate while we cannot and do not solicit contributions from states where we have not met the solicitation requirements we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate international donations are gratefully accepted but we cannot make any statements concerning tax treatment of donations received from outside the united states u s laws alone swamp our small staff please check the project gutenberg web pages for current donation methods and addresses donations are accepted in a number of other ways including checks online payments and credit card donations to donate please visit www gutenberg org donate section general information about project gutenberg tm electronic works professor michael s hart was the originator of the project gutenberg tm concept of a library of electronic works that could be freely shared with anyone for forty years he produced and distributed project gutenberg tm ebooks with only a loose network of volunteer support project gutenberg tm ebooks are often created from several printed editions all of which are confirmed as not protected by copyright in the u s unless a copyright notice is included thus we do not necessarily keep ebooks in compliance with any particular paper edition most people start at our website which has the main pg search facility www gutenberg org this website includes information about project gutenberg tm including how to make donations to the project gutenberg literary archive foundation how to help produce our new ebooks and how to subscribe to our email newsletter to hear about new ebooks"
  },
  {
    "objectID": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#split-text-into-sentences",
    "href": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#split-text-into-sentences",
    "title": "Text summarizer in Python, Notes from underground",
    "section": "Split text into sentences",
    "text": "Split text into sentences\nWe split (tokenize) the text into sentences using NLTK sent_tokenize() method. We will evaluate the importance of each of sentences, then decide if we should each include in our summary.\n\n# split (tokenize) the sentences\nsentences = nltk.sent_tokenize(text)\nprint(sentences)\n\n['\\ufeffThe Project Gutenberg eBook of Notes from the Underground, by Fyodor Dostoyevsky This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever.', 'You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org.', 'If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.', 'Title: Notes from the Underground Author: Fyodor Dostoyevsky Translator: Constance Garnett Release Date: July, 1996 [eBook #600] [Most recently updated: December 26, 2021] Language: English Produced by: Judith Boss.', 'HTML version by Al Haines *** START OF THE PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** Notes from the Underground by Fyodor Dostoyevsky Contents NOTES FROM THE UNDERGROUND PART I Underground I II III IV V VI VII VIII IX X XI PART II À Propos of the Wet Snow I II III IV V VI VII VIII IX X NOTES FROM THE UNDERGROUND[*] A NOVEL * The author of the diary and the diary itself are, of course, imaginary.', 'Nevertheless it is clear that such persons as the writer of these notes not only may, but positively must, exist in our society, when we consider the circumstances in the midst of which our society is formed.', 'I have tried to expose to the view of the public more distinctly than is commonly done, one of the characters of the recent past.', 'He is one of the representatives of a generation still living.', 'In this fragment, entitled “Underground,” this person introduces himself and his views, and, as it were, tries to explain the causes owing to which he has made his appearance and was bound to make his appearance in our midst.', 'In the second fragment there are added the actual notes of this person concerning certain events in his life.—AUTHOR’S NOTE.', 'PART I Underground I I am a sick man....', 'I am a spiteful man.', 'I am an unattractive man.', 'I believe my liver is diseased.', 'However, I know nothing at all about my disease, and do not know for certain what ails me.', 'I don’t consult a doctor for it, and never have, though I have a respect for medicine and doctors.', 'Besides, I am extremely superstitious, sufficiently so to respect medicine, anyway (I am well-educated enough not to be superstitious, but I am superstitious).', 'No, I refuse to consult a doctor from spite.', 'That you probably will not understand.', 'Well, I understand it, though.', 'Of course, I can’t explain who it is precisely that I am mortifying in this case by my spite: I am perfectly well aware that I cannot “pay out” the doctors by not consulting them; I know better than anyone that by all this I am only injuring myself and no one else.', 'But still, if I don’t consult a doctor it is from spite.', 'My liver is bad, well—let it get worse!', 'I have been going on like that for a long time—twenty years.', 'Now I am forty.', 'I used to be in the government service, but am no longer.', 'I was a spiteful official.', 'I was rude and took pleasure in being so.', 'I did not take bribes, you see, so I was bound to find a recompense in that, at least.', '(A poor jest, but I will not scratch it out.', 'I wrote it thinking it would sound very witty; but now that I have seen myself that I only wanted to show off in a despicable way, I will not scratch it out on purpose!)', 'When petitioners used to come for information to the table at which I sat, I used to grind my teeth at them, and felt intense enjoyment when I succeeded in making anybody unhappy.', 'I almost did succeed.', 'For the most part they were all timid people—of course, they were petitioners.', 'But of the uppish ones there was one officer in particular I could not endure.', 'He simply would not be humble, and clanked his sword in a disgusting way.', 'I carried on a feud with him for eighteen months over that sword.', 'At last I got the better of him.', 'He left off clanking it.', 'That happened in my youth, though.', 'But do you know, gentlemen, what was the chief point about my spite?', 'Why, the whole point, the real sting of it lay in the fact that continually, even in the moment of the acutest spleen, I was inwardly conscious with shame that I was not only not a spiteful but not even an embittered man, that I was simply scaring sparrows at random and amusing myself by it.', 'I might foam at the mouth, but bring me a doll to play with, give me a cup of tea with sugar in it, and maybe I should be appeased.', 'I might even be genuinely touched, though probably I should grind my teeth at myself afterwards and lie awake at night with shame for months after.', 'That was my way.', 'I was lying when I said just now that I was a spiteful official.', 'I was lying from spite.', 'I was simply amusing myself with the petitioners and with the officer, and in reality I never could become spiteful.', 'I was conscious every moment in myself of many, very many elements absolutely opposite to that.', 'I felt them positively swarming in me, these opposite elements.', 'I knew that they had been swarming in me all my life and craving some outlet from me, but I would not let them, would not let them, purposely would not let them come out.', 'They tormented me till I was ashamed: they drove me to convulsions and—sickened me, at last, how they sickened me!', 'Now, are not you fancying, gentlemen, that I am expressing remorse for something now, that I am asking your forgiveness for something?', 'I am sure you are fancying that ...', 'However, I assure you I do not care if you are....', 'It was not only that I could not become spiteful, I did not know how to become anything; neither spiteful nor kind, neither a rascal nor an honest man, neither a hero nor an insect.', 'Now, I am living out my life in my corner, taunting myself with the spiteful and useless consolation that an intelligent man cannot become anything seriously, and it is only the fool who becomes anything.', 'Yes, a man in the nineteenth century must and morally ought to be pre-eminently a characterless creature; a man of character, an active man is pre-eminently a limited creature.', 'That is my conviction of forty years.', 'I am forty years old now, and you know forty years is a whole lifetime; you know it is extreme old age.', 'To live longer than forty years is bad manners, is vulgar, immoral.', 'Who does live beyond forty?', 'Answer that, sincerely and honestly I will tell you who do: fools and worthless fellows.', 'I tell all old men that to their face, all these venerable old men, all these silver-haired and reverend seniors!', 'I tell the whole world that to its face!', 'I have a right to say so, for I shall go on living to sixty myself.', 'To seventy!', 'To eighty!', '... Stay, let me take breath ... You imagine no doubt, gentlemen, that I want to amuse you.', 'You are mistaken in that, too.', 'I am by no means such a mirthful person as you imagine, or as you may imagine; however, irritated by all this babble (and I feel that you are irritated) you think fit to ask me who I am—then my answer is, I am a collegiate assessor.', 'I was in the service that I might have something to eat (and solely for that reason), and when last year a distant relation left me six thousand roubles in his will I immediately retired from the service and settled down in my corner.', 'I used to live in this corner before, but now I have settled down in it.', 'My room is a wretched, horrid one in the outskirts of the town.', 'My servant is an old country-woman, ill-natured from stupidity, and, moreover, there is always a nasty smell about her.', 'I am told that the Petersburg climate is bad for me, and that with my small means it is very expensive to live in Petersburg.', 'I know all that better than all these sage and experienced counsellors and monitors....', 'But I am remaining in Petersburg; I am not going away from Petersburg!', 'I am not going away because ... ech!', 'Why, it is absolutely no matter whether I am going away or not going away.', 'But what can a decent man speak of with most pleasure?', 'Answer: Of himself.', 'Well, so I will talk about myself.', 'II I want now to tell you, gentlemen, whether you care to hear it or not, why I could not even become an insect.', 'I tell you solemnly, that I have many times tried to become an insect.', 'But I was not equal even to that.', 'I swear, gentlemen, that to be too conscious is an illness—a real thorough-going illness.', 'For man’s everyday needs, it would have been quite enough to have the ordinary human consciousness, that is, half or a quarter of the amount which falls to the lot of a cultivated man of our unhappy nineteenth century, especially one who has the fatal ill-luck to inhabit Petersburg, the most theoretical and intentional town on the whole terrestrial globe.', '(There are intentional and unintentional towns.)', 'It would have been quite enough, for instance, to have the consciousness by which all so-called direct persons and men of action live.', 'I bet you think I am writing all this from affectation, to be witty at the expense of men of action; and what is more, that from ill-bred affectation, I am clanking a sword like my officer.', 'But, gentlemen, whoever can pride himself on his diseases and even swagger over them?', 'Though, after all, everyone does do that; people do pride themselves on their diseases, and I do, may be, more than anyone.', 'We will not dispute it; my contention was absurd.', 'But yet I am firmly persuaded that a great deal of consciousness, every sort of consciousness, in fact, is a disease.', 'I stick to that.', 'Let us leave that, too, for a minute.', 'Tell me this: why does it happen that at the very, yes, at the very moments when I am most capable of feeling every refinement of all that is “sublime and beautiful,” as they used to say at one time, it would, as though of design, happen to me not only to feel but to do such ugly things, such that ... Well, in short, actions that all, perhaps, commit; but which, as though purposely, occurred to me at the very time when I was most conscious that they ought not to be committed.', 'The more conscious I was of goodness and of all that was “sublime and beautiful,” the more deeply I sank into my mire and the more ready I was to sink in it altogether.', 'But the chief point was that all this was, as it were, not accidental in me, but as though it were bound to be so.', 'It was as though it were my most normal condition, and not in the least disease or depravity, so that at last all desire in me to struggle against this depravity passed.', 'It ended by my almost believing (perhaps actually believing) that this was perhaps my normal condition.', 'But at first, in the beginning, what agonies I endured in that struggle!', 'I did not believe it was the same with other people, and all my life I hid this fact about myself as a secret.', 'I was ashamed (even now, perhaps, I am ashamed): I got to the point of feeling a sort of secret abnormal, despicable enjoyment in returning home to my corner on some disgusting Petersburg night, acutely conscious that that day I had committed a loathsome action again, that what was done could never be undone, and secretly, inwardly gnawing, gnawing at myself for it, tearing and consuming myself till at last the bitterness turned into a sort of shameful accursed sweetness, and at last—into positive real enjoyment!', 'Yes, into enjoyment, into enjoyment!', 'I insist upon that.', 'I have spoken of this because I keep wanting to know for a fact whether other people feel such enjoyment?', 'I will explain; the enjoyment was just from the too intense consciousness of one’s own degradation; it was from feeling oneself that one had reached the last barrier, that it was horrible, but that it could not be otherwise; that there was no escape for you; that you never could become a different man; that even if time and faith were still left you to change into something different you would most likely not wish to change; or if you did wish to, even then you would do nothing; because perhaps in reality there was nothing for you to change into.', 'And the worst of it was, and the root of it all, that it was all in accord with the normal fundamental laws of over-acute consciousness, and with the inertia that was the direct result of those laws, and that consequently one was not only unable to change but could do absolutely nothing.', 'Thus it would follow, as the result of acute consciousness, that one is not to blame in being a scoundrel; as though that were any consolation to the scoundrel once he has come to realise that he actually is a scoundrel.', 'But enough.... Ech, I have talked a lot of nonsense, but what have I explained?', 'How is enjoyment in this to be explained?', 'But I will explain it.', 'I will get to the bottom of it!', 'That is why I have taken up my pen....', 'I, for instance, have a great deal of _amour propre_.', 'I am as suspicious and prone to take offence as a humpback or a dwarf.', 'But upon my word I sometimes have had moments when if I had happened to be slapped in the face I should, perhaps, have been positively glad of it.', 'I say, in earnest, that I should probably have been able to discover even in that a peculiar sort of enjoyment—the enjoyment, of course, of despair; but in despair there are the most intense enjoyments, especially when one is very acutely conscious of the hopelessness of one’s position.', 'And when one is slapped in the face—why then the consciousness of being rubbed into a pulp would positively overwhelm one.', 'The worst of it is, look at it which way one will, it still turns out that I was always the most to blame in everything.', 'And what is most humiliating of all, to blame for no fault of my own but, so to say, through the laws of nature.', 'In the first place, to blame because I am cleverer than any of the people surrounding me.', '(I have always considered myself cleverer than any of the people surrounding me, and sometimes, would you believe it, have been positively ashamed of it.', 'At any rate, I have all my life, as it were, turned my eyes away and never could look people straight in the face.)', 'To blame, finally, because even if I had had magnanimity, I should only have had more suffering from the sense of its uselessness.', 'I should certainly have never been able to do anything from being magnanimous—neither to forgive, for my assailant would perhaps have slapped me from the laws of nature, and one cannot forgive the laws of nature; nor to forget, for even if it were owing to the laws of nature, it is insulting all the same.', 'Finally, even if I had wanted to be anything but magnanimous, had desired on the contrary to revenge myself on my assailant, I could not have revenged myself on any one for anything because I should certainly never have made up my mind to do anything, even if I had been able to.', 'Why should I not have made up my mind?', 'About that in particular I want to say a few words.', 'III With people who know how to revenge themselves and to stand up for themselves in general, how is it done?', 'Why, when they are possessed, let us suppose, by the feeling of revenge, then for the time there is nothing else but that feeling left in their whole being.', 'Such a gentleman simply dashes straight for his object like an infuriated bull with its horns down, and nothing but a wall will stop him.', '(By the way: facing the wall, such gentlemen—that is, the “direct” persons and men of action—are genuinely nonplussed.', 'For them a wall is not an evasion, as for us people who think and consequently do nothing; it is not an excuse for turning aside, an excuse for which we are always very glad, though we scarcely believe in it ourselves, as a rule.', 'No, they are nonplussed in all sincerity.', 'The wall has for them something tranquillising, morally soothing, final—maybe even something mysterious ... but of the wall later.)', 'Well, such a direct person I regard as the real normal man, as his tender mother nature wished to see him when she graciously brought him into being on the earth.', 'I envy such a man till I am green in the face.', 'He is stupid.', 'I am not disputing that, but perhaps the normal man should be stupid, how do you know?', 'Perhaps it is very beautiful, in fact.', 'And I am the more persuaded of that suspicion, if one can call it so, by the fact that if you take, for instance, the antithesis of the normal man, that is, the man of acute consciousness, who has come, of course, not out of the lap of nature but out of a retort (this is almost mysticism, gentlemen, but I suspect this, too), this retort-made man is sometimes so nonplussed in the presence of his antithesis that with all his exaggerated consciousness he genuinely thinks of himself as a mouse and not a man.', 'It may be an acutely conscious mouse, yet it is a mouse, while the other is a man, and therefore, et caetera, et caetera.', 'And the worst of it is, he himself, his very own self, looks on himself as a mouse; no one asks him to do so; and that is an important point.', 'Now let us look at this mouse in action.', 'Let us suppose, for instance, that it feels insulted, too (and it almost always does feel insulted), and wants to revenge itself, too.', 'There may even be a greater accumulation of spite in it than in _l’homme de la nature et de la vérité_.', 'The base and nasty desire to vent that spite on its assailant rankles perhaps even more nastily in it than in _l’homme de la nature et de la vérité_.', 'For through his innate stupidity the latter looks upon his revenge as justice pure and simple; while in consequence of his acute consciousness the mouse does not believe in the justice of it.', 'To come at last to the deed itself, to the very act of revenge.', 'Apart from the one fundamental nastiness the luckless mouse succeeds in creating around it so many other nastinesses in the form of doubts and questions, adds to the one question so many unsettled questions that there inevitably works up around it a sort of fatal brew, a stinking mess, made up of its doubts, emotions, and of the contempt spat upon it by the direct men of action who stand solemnly about it as judges and arbitrators, laughing at it till their healthy sides ache.', 'Of course the only thing left for it is to dismiss all that with a wave of its paw, and, with a smile of assumed contempt in which it does not even itself believe, creep ignominiously into its mouse-hole.', 'There in its nasty, stinking, underground home our insulted, crushed and ridiculed mouse promptly becomes absorbed in cold, malignant and, above all, everlasting spite.', 'For forty years together it will remember its injury down to the smallest, most ignominious details, and every time will add, of itself, details still more ignominious, spitefully teasing and tormenting itself with its own imagination.', 'It will itself be ashamed of its imaginings, but yet it will recall it all, it will go over and over every detail, it will invent unheard of things against itself, pretending that those things might happen, and will forgive nothing.', 'Maybe it will begin to revenge itself, too, but, as it were, piecemeal, in trivial ways, from behind the stove, incognito, without believing either in its own right to vengeance, or in the success of its revenge, knowing that from all its efforts at revenge it will suffer a hundred times more than he on whom it revenges itself, while he, I daresay, will not even scratch himself.', 'On its deathbed it will recall it all over again, with interest accumulated over all the years and ...', 'But it is just in that cold, abominable half despair, half belief, in that conscious burying oneself alive for grief in the underworld for forty years, in that acutely recognised and yet partly doubtful hopelessness of one’s position, in that hell of unsatisfied desires turned inward, in that fever of oscillations, of resolutions determined for ever and repented of again a minute later—that the savour of that strange enjoyment of which I have spoken lies.', 'It is so subtle, so difficult of analysis, that persons who are a little limited, or even simply persons of strong nerves, will not understand a single atom of it.', '“Possibly,” you will add on your own account with a grin, “people will not understand it either who have never received a slap in the face,” and in that way you will politely hint to me that I, too, perhaps, have had the experience of a slap in the face in my life, and so I speak as one who knows.', 'I bet that you are thinking that.', 'But set your minds at rest, gentlemen, I have not received a slap in the face, though it is absolutely a matter of indifference to me what you may think about it.', 'Possibly, I even regret, myself, that I have given so few slaps in the face during my life.', 'But enough ... not another word on that subject of such extreme interest to you.', 'I will continue calmly concerning persons with strong nerves who do not understand a certain refinement of enjoyment.', 'Though in certain circumstances these gentlemen bellow their loudest like bulls, though this, let us suppose, does them the greatest credit, yet, as I have said already, confronted with the impossible they subside at once.', 'The impossible means the stone wall!', 'What stone wall?', 'Why, of course, the laws of nature, the deductions of natural science, mathematics.', 'As soon as they prove to you, for instance, that you are descended from a monkey, then it is no use scowling, accept it for a fact.', 'When they prove to you that in reality one drop of your own fat must be dearer to you than a hundred thousand of your fellow-creatures, and that this conclusion is the final solution of all so-called virtues and duties and all such prejudices and fancies, then you have just to accept it, there is no help for it, for twice two is a law of mathematics.', 'Just try refuting it.', '“Upon my word, they will shout at you, it is no use protesting: it is a case of twice two makes four!', 'Nature does not ask your permission, she has nothing to do with your wishes, and whether you like her laws or dislike them, you are bound to accept her as she is, and consequently all her conclusions.', 'A wall, you see, is a wall ... and so on, and so on.” Merciful Heavens!', 'but what do I care for the laws of nature and arithmetic, when, for some reason I dislike those laws and the fact that twice two makes four?', 'Of course I cannot break through the wall by battering my head against it if I really have not the strength to knock it down, but I am not going to be reconciled to it simply because it is a stone wall and I have not the strength.', 'As though such a stone wall really were a consolation, and really did contain some word of conciliation, simply because it is as true as twice two makes four.', 'Oh, absurdity of absurdities!', 'How much better it is to understand it all, to recognise it all, all the impossibilities and the stone wall; not to be reconciled to one of those impossibilities and stone walls if it disgusts you to be reconciled to it; by the way of the most inevitable, logical combinations to reach the most revolting conclusions on the everlasting theme, that even for the stone wall you are yourself somehow to blame, though again it is as clear as day you are not to blame in the least, and therefore grinding your teeth in silent impotence to sink into luxurious inertia, brooding on the fact that there is no one even for you to feel vindictive against, that you have not, and perhaps never will have, an object for your spite, that it is a sleight of hand, a bit of juggling, a card-sharper’s trick, that it is simply a mess, no knowing what and no knowing who, but in spite of all these uncertainties and jugglings, still there is an ache in you, and the more you do not know, the worse the ache.', 'IV “Ha, ha, ha!', 'You will be finding enjoyment in toothache next,” you cry, with a laugh.', '“Well, even in toothache there is enjoyment,” I answer.', 'I had toothache for a whole month and I know there is.', 'In that case, of course, people are not spiteful in silence, but moan; but they are not candid moans, they are malignant moans, and the malignancy is the whole point.', 'The enjoyment of the sufferer finds expression in those moans; if he did not feel enjoyment in them he would not moan.', 'It is a good example, gentlemen, and I will develop it.', 'Those moans express in the first place all the aimlessness of your pain, which is so humiliating to your consciousness; the whole legal system of nature on which you spit disdainfully, of course, but from which you suffer all the same while she does not.', 'They express the consciousness that you have no enemy to punish, but that you have pain; the consciousness that in spite of all possible Wagenheims you are in complete slavery to your teeth; that if someone wishes it, your teeth will leave off aching, and if he does not, they will go on aching another three months; and that finally if you are still contumacious and still protest, all that is left you for your own gratification is to thrash yourself or beat your wall with your fist as hard as you can, and absolutely nothing more.', 'Well, these mortal insults, these jeers on the part of someone unknown, end at last in an enjoyment which sometimes reaches the highest degree of voluptuousness.', 'I ask you, gentlemen, listen sometimes to the moans of an educated man of the nineteenth century suffering from toothache, on the second or third day of the attack, when he is beginning to moan, not as he moaned on the first day, that is, not simply because he has toothache, not just as any coarse peasant, but as a man affected by progress and European civilisation, a man who is “divorced from the soil and the national elements,” as they express it now-a-days.', 'His moans become nasty, disgustingly malignant, and go on for whole days and nights.', 'And of course he knows himself that he is doing himself no sort of good with his moans; he knows better than anyone that he is only lacerating and harassing himself and others for nothing; he knows that even the audience before whom he is making his efforts, and his whole family, listen to him with loathing, do not put a ha’porth of faith in him, and inwardly understand that he might moan differently, more simply, without trills and flourishes, and that he is only amusing himself like that from ill-humour, from malignancy.', 'Well, in all these recognitions and disgraces it is that there lies a voluptuous pleasure.', 'As though he would say: “I am worrying you, I am lacerating your hearts, I am keeping everyone in the house awake.', 'Well, stay awake then, you, too, feel every minute that I have toothache.', 'I am not a hero to you now, as I tried to seem before, but simply a nasty person, an impostor.', 'Well, so be it, then!', 'I am very glad that you see through me.', 'It is nasty for you to hear my despicable moans: well, let it be nasty; here I will let you have a nastier flourish in a minute....” You do not understand even now, gentlemen?', 'No, it seems our development and our consciousness must go further to understand all the intricacies of this pleasure.', 'You laugh?', 'Delighted.', 'My jests, gentlemen, are of course in bad taste, jerky, involved, lacking self-confidence.', 'But of course that is because I do not respect myself.', 'Can a man of perception respect himself at all?', 'V Come, can a man who attempts to find enjoyment in the very feeling of his own degradation possibly have a spark of respect for himself?', 'I am not saying this now from any mawkish kind of remorse.', 'And, indeed, I could never endure saying, “Forgive me, Papa, I won’t do it again,” not because I am incapable of saying that—on the contrary, perhaps just because I have been too capable of it, and in what a way, too.', 'As though of design I used to get into trouble in cases when I was not to blame in any way.', 'That was the nastiest part of it.', 'At the same time I was genuinely touched and penitent, I used to shed tears and, of course, deceived myself, though I was not acting in the least and there was a sick feeling in my heart at the time.... For that one could not blame even the laws of nature, though the laws of nature have continually all my life offended me more than anything.', 'It is loathsome to remember it all, but it was loathsome even then.', 'Of course, a minute or so later I would realise wrathfully that it was all a lie, a revolting lie, an affected lie, that is, all this penitence, this emotion, these vows of reform.', 'You will ask why did I worry myself with such antics: answer, because it was very dull to sit with one’s hands folded, and so one began cutting capers.', 'That is really it.', 'Observe yourselves more carefully, gentlemen, then you will understand that it is so.', 'I invented adventures for myself and made up a life, so as at least to live in some way.', 'How many times it has happened to me—well, for instance, to take offence simply on purpose, for nothing; and one knows oneself, of course, that one is offended at nothing; that one is putting it on, but yet one brings oneself at last to the point of being really offended.', 'All my life I have had an impulse to play such pranks, so that in the end I could not control it in myself.', 'Another time, twice, in fact, I tried hard to be in love.', 'I suffered, too, gentlemen, I assure you.', 'In the depth of my heart there was no faith in my suffering, only a faint stir of mockery, but yet I did suffer, and in the real, orthodox way; I was jealous, beside myself ... and it was all from _ennui_, gentlemen, all from _ennui;_ inertia overcame me.', 'You know the direct, legitimate fruit of consciousness is inertia, that is, conscious sitting-with-the-hands-folded.', 'I have referred to this already.', 'I repeat, I repeat with emphasis: all “direct” persons and men of action are active just because they are stupid and limited.', 'How explain that?', 'I will tell you: in consequence of their limitation they take immediate and secondary causes for primary ones, and in that way persuade themselves more quickly and easily than other people do that they have found an infallible foundation for their activity, and their minds are at ease and you know that is the chief thing.', 'To begin to act, you know, you must first have your mind completely at ease and no trace of doubt left in it.', 'Why, how am I, for example, to set my mind at rest?', 'Where are the primary causes on which I am to build?', 'Where are my foundations?', 'Where am I to get them from?', 'I exercise myself in reflection, and consequently with me every primary cause at once draws after itself another still more primary, and so on to infinity.', 'That is just the essence of every sort of consciousness and reflection.', 'It must be a case of the laws of nature again.', 'What is the result of it in the end?', 'Why, just the same.', 'Remember I spoke just now of vengeance.', '(I am sure you did not take it in.)', 'I said that a man revenges himself because he sees justice in it.', 'Therefore he has found a primary cause, that is, justice.', 'And so he is at rest on all sides, and consequently he carries out his revenge calmly and successfully, being persuaded that he is doing a just and honest thing.', 'But I see no justice in it, I find no sort of virtue in it either, and consequently if I attempt to revenge myself, it is only out of spite.', 'Spite, of course, might overcome everything, all my doubts, and so might serve quite successfully in place of a primary cause, precisely because it is not a cause.', 'But what is to be done if I have not even spite (I began with that just now, you know).', 'In consequence again of those accursed laws of consciousness, anger in me is subject to chemical disintegration.', 'You look into it, the object flies off into air, your reasons evaporate, the criminal is not to be found, the wrong becomes not a wrong but a phantom, something like the toothache, for which no one is to blame, and consequently there is only the same outlet left again—that is, to beat the wall as hard as you can.', 'So you give it up with a wave of the hand because you have not found a fundamental cause.', 'And try letting yourself be carried away by your feelings, blindly, without reflection, without a primary cause, repelling consciousness at least for a time; hate or love, if only not to sit with your hands folded.', 'The day after tomorrow, at the latest, you will begin despising yourself for having knowingly deceived yourself.', 'Result: a soap-bubble and inertia.', 'Oh, gentlemen, do you know, perhaps I consider myself an intelligent man, only because all my life I have been able neither to begin nor to finish anything.', 'Granted I am a babbler, a harmless vexatious babbler, like all of us.', 'But what is to be done if the direct and sole vocation of every intelligent man is babble, that is, the intentional pouring of water through a sieve?', 'VI Oh, if I had done nothing simply from laziness!', 'Heavens, how I should have respected myself, then.', 'I should have respected myself because I should at least have been capable of being lazy; there would at least have been one quality, as it were, positive in me, in which I could have believed myself.', 'Question: What is he?', 'Answer: A sluggard; how very pleasant it would have been to hear that of oneself!', 'It would mean that I was positively defined, it would mean that there was something to say about me.', '“Sluggard”—why, it is a calling and vocation, it is a career.', 'Do not jest, it is so.', 'I should then be a member of the best club by right, and should find my occupation in continually respecting myself.', 'I knew a gentleman who prided himself all his life on being a connoisseur of Lafitte.', 'He considered this as his positive virtue, and never doubted himself.', 'He died, not simply with a tranquil, but with a triumphant conscience, and he was quite right, too.', 'Then I should have chosen a career for myself, I should have been a sluggard and a glutton, not a simple one, but, for instance, one with sympathies for everything sublime and beautiful.', 'How do you like that?', 'I have long had visions of it.', 'That “sublime and beautiful” weighs heavily on my mind at forty But that is at forty; then—oh, then it would have been different!', 'I should have found for myself a form of activity in keeping with it, to be precise, drinking to the health of everything “sublime and beautiful.” I should have snatched at every opportunity to drop a tear into my glass and then to drain it to all that is “sublime and beautiful.” I should then have turned everything into the sublime and the beautiful; in the nastiest, unquestionable trash, I should have sought out the sublime and the beautiful.', 'I should have exuded tears like a wet sponge.', 'An artist, for instance, paints a picture worthy of Gay.', 'At once I drink to the health of the artist who painted the picture worthy of Gay, because I love all that is “sublime and beautiful.” An author has written _As you will:_ at once I drink to the health of “anyone you will” because I love all that is “sublime and beautiful.” I should claim respect for doing so.', 'I should persecute anyone who would not show me respect.', 'I should live at ease, I should die with dignity, why, it is charming, perfectly charming!', 'And what a good round belly I should have grown, what a treble chin I should have established, what a ruby nose I should have coloured for myself, so that everyone would have said, looking at me: “Here is an asset!', 'Here is something real and solid!” And, say what you like, it is very agreeable to hear such remarks about oneself in this negative age.', 'VII But these are all golden dreams.', 'Oh, tell me, who was it first announced, who was it first proclaimed, that man only does nasty things because he does not know his own interests; and that if he were enlightened, if his eyes were opened to his real normal interests, man would at once cease to do nasty things, would at once become good and noble because, being enlightened and understanding his real advantage, he would see his own advantage in the good and nothing else, and we all know that not one man can, consciously, act against his own interests, consequently, so to say, through necessity, he would begin doing good?', 'Oh, the babe!', 'Oh, the pure, innocent child!', 'Why, in the first place, when in all these thousands of years has there been a time when man has acted only from his own interest?', 'What is to be done with the millions of facts that bear witness that men, _consciously_, that is fully understanding their real interests, have left them in the background and have rushed headlong on another path, to meet peril and danger, compelled to this course by nobody and by nothing, but, as it were, simply disliking the beaten track, and have obstinately, wilfully, struck out another difficult, absurd way, seeking it almost in the darkness.', 'So, I suppose, this obstinacy and perversity were pleasanter to them than any advantage....', 'Advantage!', 'What is advantage?', 'And will you take it upon yourself to define with perfect accuracy in what the advantage of man consists?', 'And what if it so happens that a man’s advantage, _sometimes_, not only may, but even must, consist in his desiring in certain cases what is harmful to himself and not advantageous.', 'And if so, if there can be such a case, the whole principle falls into dust.', 'What do you think—are there such cases?', 'You laugh; laugh away, gentlemen, but only answer me: have man’s advantages been reckoned up with perfect certainty?', 'Are there not some which not only have not been included but cannot possibly be included under any classification?', 'You see, you gentlemen have, to the best of my knowledge, taken your whole register of human advantages from the averages of statistical figures and politico-economical formulas.', 'Your advantages are prosperity, wealth, freedom, peace—and so on, and so on.', 'So that the man who should, for instance, go openly and knowingly in opposition to all that list would to your thinking, and indeed mine, too, of course, be an obscurantist or an absolute madman: would not he?', 'But, you know, this is what is surprising: why does it so happen that all these statisticians, sages and lovers of humanity, when they reckon up human advantages invariably leave out one?', 'They don’t even take it into their reckoning in the form in which it should be taken, and the whole reckoning depends upon that.', 'It would be no greater matter, they would simply have to take it, this advantage, and add it to the list.', 'But the trouble is, that this strange advantage does not fall under any classification and is not in place in any list.', 'I have a friend for instance ... Ech!', 'gentlemen, but of course he is your friend, too; and indeed there is no one, no one to whom he is not a friend!', 'When he prepares for any undertaking this gentleman immediately explains to you, elegantly and clearly, exactly how he must act in accordance with the laws of reason and truth.', 'What is more, he will talk to you with excitement and passion of the true normal interests of man; with irony he will upbraid the short-sighted fools who do not understand their own interests, nor the true significance of virtue; and, within a quarter of an hour, without any sudden outside provocation, but simply through something inside him which is stronger than all his interests, he will go off on quite a different tack—that is, act in direct opposition to what he has just been saying about himself, in opposition to the laws of reason, in opposition to his own advantage, in fact in opposition to everything ...', 'I warn you that my friend is a compound personality and therefore it is difficult to blame him as an individual.', 'The fact is, gentlemen, it seems there must really exist something that is dearer to almost every man than his greatest advantages, or (not to be illogical) there is a most advantageous advantage (the very one omitted of which we spoke just now) which is more important and more advantageous than all other advantages, for the sake of which a man if necessary is ready to act in opposition to all laws; that is, in opposition to reason, honour, peace, prosperity—in fact, in opposition to all those excellent and useful things if only he can attain that fundamental, most advantageous advantage which is dearer to him than all.', '“Yes, but it’s advantage all the same,” you will retort.', 'But excuse me, I’ll make the point clear, and it is not a case of playing upon words.', 'What matters is, that this advantage is remarkable from the very fact that it breaks down all our classifications, and continually shatters every system constructed by lovers of mankind for the benefit of mankind.', 'In fact, it upsets everything.', 'But before I mention this advantage to you, I want to compromise myself personally, and therefore I boldly declare that all these fine systems, all these theories for explaining to mankind their real normal interests, in order that inevitably striving to pursue these interests they may at once become good and noble—are, in my opinion, so far, mere logical exercises!', 'Yes, logical exercises.', 'Why, to maintain this theory of the regeneration of mankind by means of the pursuit of his own advantage is to my mind almost the same thing ... as to affirm, for instance, following Buckle, that through civilisation mankind becomes softer, and consequently less bloodthirsty and less fitted for warfare.', 'Logically it does seem to follow from his arguments.', 'But man has such a predilection for systems and abstract deductions that he is ready to distort the truth intentionally, he is ready to deny the evidence of his senses only to justify his logic.', 'I take this example because it is the most glaring instance of it.', 'Only look about you: blood is being spilt in streams, and in the merriest way, as though it were champagne.', 'Take the whole of the nineteenth century in which Buckle lived.', 'Take Napoleon—the Great and also the present one.', 'Take North America—the eternal union.', 'Take the farce of Schleswig-Holstein.... And what is it that civilisation softens in us?', 'The only gain of civilisation for mankind is the greater capacity for variety of sensations—and absolutely nothing more.', 'And through the development of this many-sidedness man may come to finding enjoyment in bloodshed.', 'In fact, this has already happened to him.', 'Have you noticed that it is the most civilised gentlemen who have been the subtlest slaughterers, to whom the Attilas and Stenka Razins could not hold a candle, and if they are not so conspicuous as the Attilas and Stenka Razins it is simply because they are so often met with, are so ordinary and have become so familiar to us.', 'In any case civilisation has made mankind if not more bloodthirsty, at least more vilely, more loathsomely bloodthirsty.', 'In old days he saw justice in bloodshed and with his conscience at peace exterminated those he thought proper.', 'Now we do think bloodshed abominable and yet we engage in this abomination, and with more energy than ever.', 'Which is worse?', 'Decide that for yourselves.', 'They say that Cleopatra (excuse an instance from Roman history) was fond of sticking gold pins into her slave-girls’ breasts and derived gratification from their screams and writhings.', 'You will say that that was in the comparatively barbarous times; that these are barbarous times too, because also, comparatively speaking, pins are stuck in even now; that though man has now learned to see more clearly than in barbarous ages, he is still far from having learnt to act as reason and science would dictate.', 'But yet you are fully convinced that he will be sure to learn when he gets rid of certain old bad habits, and when common sense and science have completely re-educated human nature and turned it in a normal direction.', 'You are confident that then man will cease from _intentional_ error and will, so to say, be compelled not to want to set his will against his normal interests.', 'That is not all; then, you say, science itself will teach man (though to my mind it’s a superfluous luxury) that he never has really had any caprice or will of his own, and that he himself is something of the nature of a piano-key or the stop of an organ, and that there are, besides, things called the laws of nature; so that everything he does is not done by his willing it, but is done of itself, by the laws of nature.', 'Consequently we have only to discover these laws of nature, and man will no longer have to answer for his actions and life will become exceedingly easy for him.', 'All human actions will then, of course, be tabulated according to these laws, mathematically, like tables of logarithms up to 108,000, and entered in an index; or, better still, there would be published certain edifying works of the nature of encyclopaedic lexicons, in which everything will be so clearly calculated and explained that there will be no more incidents or adventures in the world.', 'Then—this is all what you say—new economic relations will be established, all ready-made and worked out with mathematical exactitude, so that every possible question will vanish in the twinkling of an eye, simply because every possible answer to it will be provided.', 'Then the “Palace of Crystal” will be built.', 'Then ...', 'In fact, those will be halcyon days.', 'Of course there is no guaranteeing (this is my comment) that it will not be, for instance, frightfully dull then (for what will one have to do when everything will be calculated and tabulated), but on the other hand everything will be extraordinarily rational.', 'Of course boredom may lead you to anything.', 'It is boredom sets one sticking golden pins into people, but all that would not matter.', 'What is bad (this is my comment again) is that I dare say people will be thankful for the gold pins then.', 'Man is stupid, you know, phenomenally stupid; or rather he is not at all stupid, but he is so ungrateful that you could not find another like him in all creation.', 'I, for instance, would not be in the least surprised if all of a sudden, _à propos_ of nothing, in the midst of general prosperity a gentleman with an ignoble, or rather with a reactionary and ironical, countenance were to arise and, putting his arms akimbo, say to us all: “I say, gentleman, hadn’t we better kick over the whole show and scatter rationalism to the winds, simply to send these logarithms to the devil, and to enable us to live once more at our own sweet foolish will!” That again would not matter, but what is annoying is that he would be sure to find followers—such is the nature of man.', 'And all that for the most foolish reason, which, one would think, was hardly worth mentioning: that is, that man everywhere and at all times, whoever he may be, has preferred to act as he chose and not in the least as his reason and advantage dictated.', 'And one may choose what is contrary to one’s own interests, and sometimes one _positively ought_ (that is my idea).', 'One’s own free unfettered choice, one’s own caprice, however wild it may be, one’s own fancy worked up at times to frenzy—is that very “most advantageous advantage” which we have overlooked, which comes under no classification and against which all systems and theories are continually being shattered to atoms.', 'And how do these wiseacres know that man wants a normal, a virtuous choice?', 'What has made them conceive that man must want a rationally advantageous choice?', 'What man wants is simply _independent_ choice, whatever that independence may cost and wherever it may lead.', 'And choice, of course, the devil only knows what choice.', 'VIII “Ha!', 'ha!', 'ha!', 'But you know there is no such thing as choice in reality, say what you like,” you will interpose with a chuckle.', '“Science has succeeded in so far analysing man that we know already that choice and what is called freedom of will is nothing else than—” Stay, gentlemen, I meant to begin with that myself I confess, I was rather frightened.', 'I was just going to say that the devil only knows what choice depends on, and that perhaps that was a very good thing, but I remembered the teaching of science ... and pulled myself up.', 'And here you have begun upon it.', 'Indeed, if there really is some day discovered a formula for all our desires and caprices—that is, an explanation of what they depend upon, by what laws they arise, how they develop, what they are aiming at in one case and in another and so on, that is a real mathematical formula—then, most likely, man will at once cease to feel desire, indeed, he will be certain to.', 'For who would want to choose by rule?', 'Besides, he will at once be transformed from a human being into an organ-stop or something of the sort; for what is a man without desires, without free will and without choice, if not a stop in an organ?', 'What do you think?', 'Let us reckon the chances—can such a thing happen or not?', '“H’m!” you decide.', '“Our choice is usually mistaken from a false view of our advantage.', 'We sometimes choose absolute nonsense because in our foolishness we see in that nonsense the easiest means for attaining a supposed advantage.', 'But when all that is explained and worked out on paper (which is perfectly possible, for it is contemptible and senseless to suppose that some laws of nature man will never understand), then certainly so-called desires will no longer exist.', 'For if a desire should come into conflict with reason we shall then reason and not desire, because it will be impossible retaining our reason to be _senseless_ in our desires, and in that way knowingly act against reason and desire to injure ourselves.', 'And as all choice and reasoning can be really calculated—because there will some day be discovered the laws of our so-called free will—so, joking apart, there may one day be something like a table constructed of them, so that we really shall choose in accordance with it.', 'If, for instance, some day they calculate and prove to me that I made a long nose at someone because I could not help making a long nose at him and that I had to do it in that particular way, what _freedom_ is left me, especially if I am a learned man and have taken my degree somewhere?', 'Then I should be able to calculate my whole life for thirty years beforehand.', 'In short, if this could be arranged there would be nothing left for us to do; anyway, we should have to understand that.', 'And, in fact, we ought unwearyingly to repeat to ourselves that at such and such a time and in such and such circumstances nature does not ask our leave; that we have got to take her as she is and not fashion her to suit our fancy, and if we really aspire to formulas and tables of rules, and well, even ... to the chemical retort, there’s no help for it, we must accept the retort too, or else it will be accepted without our consent....” Yes, but here I come to a stop!', 'Gentlemen, you must excuse me for being over-philosophical; it’s the result of forty years underground!', 'Allow me to indulge my fancy.', 'You see, gentlemen, reason is an excellent thing, there’s no disputing that, but reason is nothing but reason and satisfies only the rational side of man’s nature, while will is a manifestation of the whole life, that is, of the whole human life including reason and all the impulses.', 'And although our life, in this manifestation of it, is often worthless, yet it is life and not simply extracting square roots.', 'Here I, for instance, quite naturally want to live, in order to satisfy all my capacities for life, and not simply my capacity for reasoning, that is, not simply one twentieth of my capacity for life.', 'What does reason know?', 'Reason only knows what it has succeeded in learning (some things, perhaps, it will never learn; this is a poor comfort, but why not say so frankly?)', 'and human nature acts as a whole, with everything that is in it, consciously or unconsciously, and, even if it goes wrong, it lives.', 'I suspect, gentlemen, that you are looking at me with compassion; you tell me again that an enlightened and developed man, such, in short, as the future man will be, cannot consciously desire anything disadvantageous to himself, that that can be proved mathematically.', 'I thoroughly agree, it can—by mathematics.', 'But I repeat for the hundredth time, there is one case, one only, when man may consciously, purposely, desire what is injurious to himself, what is stupid, very stupid—simply in order to have the right to desire for himself even what is very stupid and not to be bound by an obligation to desire only what is sensible.', 'Of course, this very stupid thing, this caprice of ours, may be in reality, gentlemen, more advantageous for us than anything else on earth, especially in certain cases.', 'And in particular it may be more advantageous than any advantage even when it does us obvious harm, and contradicts the soundest conclusions of our reason concerning our advantage—for in any circumstances it preserves for us what is most precious and most important—that is, our personality, our individuality.', 'Some, you see, maintain that this really is the most precious thing for mankind; choice can, of course, if it chooses, be in agreement with reason; and especially if this be not abused but kept within bounds.', 'It is profitable and sometimes even praiseworthy.', 'But very often, and even most often, choice is utterly and stubbornly opposed to reason ... and ... and ... do you know that that, too, is profitable, sometimes even praiseworthy?', 'Gentlemen, let us suppose that man is not stupid.', '(Indeed one cannot refuse to suppose that, if only from the one consideration, that, if man is stupid, then who is wise?)', 'But if he is not stupid, he is monstrously ungrateful!', 'Phenomenally ungrateful.', 'In fact, I believe that the best definition of man is the ungrateful biped.', 'But that is not all, that is not his worst defect; his worst defect is his perpetual moral obliquity, perpetual—from the days of the Flood to the Schleswig-Holstein period.', 'Moral obliquity and consequently lack of good sense; for it has long been accepted that lack of good sense is due to no other cause than moral obliquity.', 'Put it to the test and cast your eyes upon the history of mankind.', 'What will you see?', 'Is it a grand spectacle?', 'Grand, if you like.', 'Take the Colossus of Rhodes, for instance, that’s worth something.', 'With good reason Mr. Anaevsky testifies of it that some say that it is the work of man’s hands, while others maintain that it has been created by nature herself.', 'Is it many-coloured?', 'May be it is many-coloured, too: if one takes the dress uniforms, military and civilian, of all peoples in all ages—that alone is worth something, and if you take the undress uniforms you will never get to the end of it; no historian would be equal to the job.', 'Is it monotonous?', 'May be it’s monotonous too: it’s fighting and fighting; they are fighting now, they fought first and they fought last—you will admit, that it is almost too monotonous.', 'In short, one may say anything about the history of the world—anything that might enter the most disordered imagination.', 'The only thing one can’t say is that it’s rational.', 'The very word sticks in one’s throat.', 'And, indeed, this is the odd thing that is continually happening: there are continually turning up in life moral and rational persons, sages and lovers of humanity who make it their object to live all their lives as morally and rationally as possible, to be, so to speak, a light to their neighbours simply in order to show them that it is possible to live morally and rationally in this world.', 'And yet we all know that those very people sooner or later have been false to themselves, playing some queer trick, often a most unseemly one.', 'Now I ask you: what can be expected of man since he is a being endowed with strange qualities?', 'Shower upon him every earthly blessing, drown him in a sea of happiness, so that nothing but bubbles of bliss can be seen on the surface; give him economic prosperity, such that he should have nothing else to do but sleep, eat cakes and busy himself with the continuation of his species, and even then out of sheer ingratitude, sheer spite, man would play you some nasty trick.', 'He would even risk his cakes and would deliberately desire the most fatal rubbish, the most uneconomical absurdity, simply to introduce into all this positive good sense his fatal fantastic element.', 'It is just his fantastic dreams, his vulgar folly that he will desire to retain, simply in order to prove to himself—as though that were so necessary—that men still are men and not the keys of a piano, which the laws of nature threaten to control so completely that soon one will be able to desire nothing but by the calendar.', 'And that is not all: even if man really were nothing but a piano-key, even if this were proved to him by natural science and mathematics, even then he would not become reasonable, but would purposely do something perverse out of simple ingratitude, simply to gain his point.', 'And if he does not find means he will contrive destruction and chaos, will contrive sufferings of all sorts, only to gain his point!', 'He will launch a curse upon the world, and as only man can curse (it is his privilege, the primary distinction between him and other animals), may be by his curse alone he will attain his object—that is, convince himself that he is a man and not a piano-key!', 'If you say that all this, too, can be calculated and tabulated—chaos and darkness and curses, so that the mere possibility of calculating it all beforehand would stop it all, and reason would reassert itself, then man would purposely go mad in order to be rid of reason and gain his point!', 'I believe in it, I answer for it, for the whole work of man really seems to consist in nothing but proving to himself every minute that he is a man and not a piano-key!', 'It may be at the cost of his skin, it may be by cannibalism!', 'And this being so, can one help being tempted to rejoice that it has not yet come off, and that desire still depends on something we don’t know?', 'You will scream at me (that is, if you condescend to do so) that no one is touching my free will, that all they are concerned with is that my will should of itself, of its own free will, coincide with my own normal interests, with the laws of nature and arithmetic.', 'Good heavens, gentlemen, what sort of free will is left when we come to tabulation and arithmetic, when it will all be a case of twice two make four?', 'Twice two makes four without my will.', 'As if free will meant that!', 'IX Gentlemen, I am joking, and I know myself that my jokes are not brilliant, but you know one can take everything as a joke.', 'I am, perhaps, jesting against the grain.', 'Gentlemen, I am tormented by questions; answer them for me.', 'You, for instance, want to cure men of their old habits and reform their will in accordance with science and good sense.', 'But how do you know, not only that it is possible, but also that it is _desirable_ to reform man in that way?', 'And what leads you to the conclusion that man’s inclinations _need_ reforming?', 'In short, how do you know that such a reformation will be a benefit to man?', 'And to go to the root of the matter, why are you so positively convinced that not to act against his real normal interests guaranteed by the conclusions of reason and arithmetic is certainly always advantageous for man and must always be a law for mankind?', 'So far, you know, this is only your supposition.', 'It may be the law of logic, but not the law of humanity.', 'You think, gentlemen, perhaps that I am mad?', 'Allow me to defend myself.', 'I agree that man is pre-eminently a creative animal, predestined to strive consciously for an object and to engage in engineering—that is, incessantly and eternally to make new roads, _wherever they may lead_.', 'But the reason why he wants sometimes to go off at a tangent may just be that he is _predestined_ to make the road, and perhaps, too, that however stupid the “direct” practical man may be, the thought sometimes will occur to him that the road almost always does lead _somewhere_, and that the destination it leads to is less important than the process of making it, and that the chief thing is to save the well-conducted child from despising engineering, and so giving way to the fatal idleness, which, as we all know, is the mother of all the vices.', 'Man likes to make roads and to create, that is a fact beyond dispute.', 'But why has he such a passionate love for destruction and chaos also?', 'Tell me that!', 'But on that point I want to say a couple of words myself.', 'May it not be that he loves chaos and destruction (there can be no disputing that he does sometimes love it) because he is instinctively afraid of attaining his object and completing the edifice he is constructing?', 'Who knows, perhaps he only loves that edifice from a distance, and is by no means in love with it at close quarters; perhaps he only loves building it and does not want to live in it, but will leave it, when completed, for the use of _les animaux domestiques_—such as the ants, the sheep, and so on.', 'Now the ants have quite a different taste.', 'They have a marvellous edifice of that pattern which endures for ever—the ant-heap.', 'With the ant-heap the respectable race of ants began and with the ant-heap they will probably end, which does the greatest credit to their perseverance and good sense.', 'But man is a frivolous and incongruous creature, and perhaps, like a chess player, loves the process of the game, not the end of it.', 'And who knows (there is no saying with certainty), perhaps the only goal on earth to which mankind is striving lies in this incessant process of attaining, in other words, in life itself, and not in the thing to be attained, which must always be expressed as a formula, as positive as twice two makes four, and such positiveness is not life, gentlemen, but is the beginning of death.', 'Anyway, man has always been afraid of this mathematical certainty, and I am afraid of it now.', 'Granted that man does nothing but seek that mathematical certainty, he traverses oceans, sacrifices his life in the quest, but to succeed, really to find it, dreads, I assure you.', 'He feels that when he has found it there will be nothing for him to look for.', 'When workmen have finished their work they do at least receive their pay, they go to the tavern, then they are taken to the police-station—and there is occupation for a week.', 'But where can man go?', 'Anyway, one can observe a certain awkwardness about him when he has attained such objects.', 'He loves the process of attaining, but does not quite like to have attained, and that, of course, is very absurd.', 'In fact, man is a comical creature; there seems to be a kind of jest in it all.', 'But yet mathematical certainty is after all, something insufferable.', 'Twice two makes four seems to me simply a piece of insolence.', 'Twice two makes four is a pert coxcomb who stands with arms akimbo barring your path and spitting.', 'I admit that twice two makes four is an excellent thing, but if we are to give everything its due, twice two makes five is sometimes a very charming thing too.', 'And why are you so firmly, so triumphantly, convinced that only the normal and the positive—in other words, only what is conducive to welfare—is for the advantage of man?', 'Is not reason in error as regards advantage?', 'Does not man, perhaps, love something besides well-being?', 'Perhaps he is just as fond of suffering?', 'Perhaps suffering is just as great a benefit to him as well-being?', 'Man is sometimes extraordinarily, passionately, in love with suffering, and that is a fact.', 'There is no need to appeal to universal history to prove that; only ask yourself, if you are a man and have lived at all.', 'As far as my personal opinion is concerned, to care only for well-being seems to me positively ill-bred.', 'Whether it’s good or bad, it is sometimes very pleasant, too, to smash things.', 'I hold no brief for suffering nor for well-being either.', 'I am standing for ... my caprice, and for its being guaranteed to me when necessary.', 'Suffering would be out of place in vaudevilles, for instance; I know that.', 'In the “Palace of Crystal” it is unthinkable; suffering means doubt, negation, and what would be the good of a “palace of crystal” if there could be any doubt about it?', 'And yet I think man will never renounce real suffering, that is, destruction and chaos.', 'Why, suffering is the sole origin of consciousness.', 'Though I did lay it down at the beginning that consciousness is the greatest misfortune for man, yet I know man prizes it and would not give it up for any satisfaction.', 'Consciousness, for instance, is infinitely superior to twice two makes four.', 'Once you have mathematical certainty there is nothing left to do or to understand.', 'There will be nothing left but to bottle up your five senses and plunge into contemplation.', 'While if you stick to consciousness, even though the same result is attained, you can at least flog yourself at times, and that will, at any rate, liven you up.', 'Reactionary as it is, corporal punishment is better than nothing.', 'X You believe in a palace of crystal that can never be destroyed—a palace at which one will not be able to put out one’s tongue or make a long nose on the sly.', 'And perhaps that is just why I am afraid of this edifice, that it is of crystal and can never be destroyed and that one cannot put one’s tongue out at it even on the sly.', 'You see, if it were not a palace, but a hen-house, I might creep into it to avoid getting wet, and yet I would not call the hen-house a palace out of gratitude to it for keeping me dry.', 'You laugh and say that in such circumstances a hen-house is as good as a mansion.', 'Yes, I answer, if one had to live simply to keep out of the rain.', 'But what is to be done if I have taken it into my head that that is not the only object in life, and that if one must live one had better live in a mansion?', 'That is my choice, my desire.', 'You will only eradicate it when you have changed my preference.', 'Well, do change it, allure me with something else, give me another ideal.', 'But meanwhile I will not take a hen-house for a mansion.', 'The palace of crystal may be an idle dream, it may be that it is inconsistent with the laws of nature and that I have invented it only through my own stupidity, through the old-fashioned irrational habits of my generation.', 'But what does it matter to me that it is inconsistent?', 'That makes no difference since it exists in my desires, or rather exists as long as my desires exist.', 'Perhaps you are laughing again?', 'Laugh away; I will put up with any mockery rather than pretend that I am satisfied when I am hungry.', 'I know, anyway, that I will not be put off with a compromise, with a recurring zero, simply because it is consistent with the laws of nature and actually exists.', 'I will not accept as the crown of my desires a block of buildings with tenements for the poor on a lease of a thousand years, and perhaps with a sign-board of a dentist hanging out.', 'Destroy my desires, eradicate my ideals, show me something better, and I will follow you.', 'You will say, perhaps, that it is not worth your trouble; but in that case I can give you the same answer.', 'We are discussing things seriously; but if you won’t deign to give me your attention, I will drop your acquaintance.', 'I can retreat into my underground hole.', 'But while I am alive and have desires I would rather my hand were withered off than bring one brick to such a building!', 'Don’t remind me that I have just rejected the palace of crystal for the sole reason that one cannot put out one’s tongue at it.', 'I did not say because I am so fond of putting my tongue out.', 'Perhaps the thing I resented was, that of all your edifices there has not been one at which one could not put out one’s tongue.', 'On the contrary, I would let my tongue be cut off out of gratitude if things could be so arranged that I should lose all desire to put it out.', 'It is not my fault that things cannot be so arranged, and that one must be satisfied with model flats.', 'Then why am I made with such desires?', 'Can I have been constructed simply in order to come to the conclusion that all my construction is a cheat?', 'Can this be my whole purpose?', 'I do not believe it.', 'But do you know what: I am convinced that we underground folk ought to be kept on a curb.', 'Though we may sit forty years underground without speaking, when we do come out into the light of day and break out we talk and talk and talk.... XI The long and the short of it is, gentlemen, that it is better to do nothing!', 'Better conscious inertia!', 'And so hurrah for underground!', 'Though I have said that I envy the normal man to the last drop of my bile, yet I should not care to be in his place such as he is now (though I shall not cease envying him).', 'No, no; anyway the underground life is more advantageous.', 'There, at any rate, one can ... Oh, but even now I am lying!', 'I am lying because I know myself that it is not underground that is better, but something different, quite different, for which I am thirsting, but which I cannot find!', 'Damn underground!', 'I will tell you another thing that would be better, and that is, if I myself believed in anything of what I have just written.', 'I swear to you, gentlemen, there is not one thing, not one word of what I have written that I really believe.', 'That is, I believe it, perhaps, but at the same time I feel and suspect that I am lying like a cobbler.', '“Then why have you written all this?” you will say to me.', '“I ought to put you underground for forty years without anything to do and then come to you in your cellar, to find out what stage you have reached!', 'How can a man be left with nothing to do for forty years?” “Isn’t that shameful, isn’t that humiliating?” you will say, perhaps, wagging your heads contemptuously.', '“You thirst for life and try to settle the problems of life by a logical tangle.', 'And how persistent, how insolent are your sallies, and at the same time what a scare you are in!', 'You talk nonsense and are pleased with it; you say impudent things and are in continual alarm and apologising for them.', 'You declare that you are afraid of nothing and at the same time try to ingratiate yourself in our good opinion.', 'You declare that you are gnashing your teeth and at the same time you try to be witty so as to amuse us.', 'You know that your witticisms are not witty, but you are evidently well satisfied with their literary value.', 'You may, perhaps, have really suffered, but you have no respect for your own suffering.', 'You may have sincerity, but you have no modesty; out of the pettiest vanity you expose your sincerity to publicity and ignominy.', 'You doubtlessly mean to say something, but hide your last word through fear, because you have not the resolution to utter it, and only have a cowardly impudence.', 'You boast of consciousness, but you are not sure of your ground, for though your mind works, yet your heart is darkened and corrupt, and you cannot have a full, genuine consciousness without a pure heart.', 'And how intrusive you are, how you insist and grimace!', 'Lies, lies, lies!” Of course I have myself made up all the things you say.', 'That, too, is from underground.', 'I have been for forty years listening to you through a crack under the floor.', 'I have invented them myself, there was nothing else I could invent.', 'It is no wonder that I have learned it by heart and it has taken a literary form....', 'But can you really be so credulous as to think that I will print all this and give it to you to read too?', 'And another problem: why do I call you “gentlemen,” why do I address you as though you really were my readers?', 'Such confessions as I intend to make are never printed nor given to other people to read.', 'Anyway, I am not strong-minded enough for that, and I don’t see why I should be.', 'But you see a fancy has occurred to me and I want to realise it at all costs.', 'Let me explain.', 'Every man has reminiscences which he would not tell to everyone, but only to his friends.', 'He has other matters in his mind which he would not reveal even to his friends, but only to himself, and that in secret.', 'But there are other things which a man is afraid to tell even to himself, and every decent man has a number of such things stored away in his mind.', 'The more decent he is, the greater the number of such things in his mind.', 'Anyway, I have only lately determined to remember some of my early adventures.', 'Till now I have always avoided them, even with a certain uneasiness.', 'Now, when I am not only recalling them, but have actually decided to write an account of them, I want to try the experiment whether one can, even with oneself, be perfectly open and not take fright at the whole truth.', 'I will observe, in parenthesis, that Heine says that a true autobiography is almost an impossibility, and that man is bound to lie about himself.', 'He considers that Rousseau certainly told lies about himself in his confessions, and even intentionally lied, out of vanity.', 'I am convinced that Heine is right; I quite understand how sometimes one may, out of sheer vanity, attribute regular crimes to oneself, and indeed I can very well conceive that kind of vanity.', 'But Heine judged of people who made their confessions to the public.', 'I write only for myself, and I wish to declare once and for all that if I write as though I were addressing readers, that is simply because it is easier for me to write in that form.', 'It is a form, an empty form—I shall never have readers.', 'I have made this plain already ...', 'I don’t wish to be hampered by any restrictions in the compilation of my notes.', 'I shall not attempt any system or method.', 'I will jot things down as I remember them.', 'But here, perhaps, someone will catch at the word and ask me: if you really don’t reckon on readers, why do you make such compacts with yourself—and on paper too—that is, that you won’t attempt any system or method, that you jot things down as you remember them, and so on, and so on?', 'Why are you explaining?', 'Why do you apologise?', 'Well, there it is, I answer.', 'There is a whole psychology in all this, though.', 'Perhaps it is simply that I am a coward.', 'And perhaps that I purposely imagine an audience before me in order that I may be more dignified while I write.', 'There are perhaps thousands of reasons.', 'Again, what is my object precisely in writing?', 'If it is not for the benefit of the public why should I not simply recall these incidents in my own mind without putting them on paper?', 'Quite so; but yet it is more imposing on paper.', 'There is something more impressive in it; I shall be better able to criticise myself and improve my style.', 'Besides, I shall perhaps obtain actual relief from writing.', 'Today, for instance, I am particularly oppressed by one memory of a distant past.', 'It came back vividly to my mind a few days ago, and has remained haunting me like an annoying tune that one cannot get rid of.', 'And yet I must get rid of it somehow.', 'I have hundreds of such reminiscences; but at times some one stands out from the hundred and oppresses me.', 'For some reason I believe that if I write it down I should get rid of it.', 'Why not try?', 'Besides, I am bored, and I never have anything to do.', 'Writing will be a sort of work.', 'They say work makes man kind-hearted and honest.', 'Well, here is a chance for me, anyway.', 'Snow is falling today, yellow and dingy.', 'It fell yesterday, too, and a few days ago.', 'I fancy it is the wet snow that has reminded me of that incident which I cannot shake off now.', 'And so let it be a story _à propos_ of the falling snow.', 'PART II À Propos of the Wet Snow When from dark error’s subjugation My words of passionate exhortation Had wrenched thy fainting spirit free; And writhing prone in thine affliction Thou didst recall with malediction The vice that had encompassed thee: And when thy slumbering conscience, fretting By recollection’s torturing flame, Thou didst reveal the hideous setting Of thy life’s current ere I came: When suddenly I saw thee sicken, And weeping, hide thine anguished face, Revolted, maddened, horror-stricken, At memories of foul disgrace.', 'NEKRASSOV (_translated by Juliet Soskice_).', 'I At that time I was only twenty-four.', 'My life was even then gloomy, ill-regulated, and as solitary as that of a savage.', 'I made friends with no one and positively avoided talking, and buried myself more and more in my hole.', 'At work in the office I never looked at anyone, and was perfectly well aware that my companions looked upon me, not only as a queer fellow, but even looked upon me—I always fancied this—with a sort of loathing.', 'I sometimes wondered why it was that nobody except me fancied that he was looked upon with aversion?', 'One of the clerks had a most repulsive, pock-marked face, which looked positively villainous.', 'I believe I should not have dared to look at anyone with such an unsightly countenance.', 'Another had such a very dirty old uniform that there was an unpleasant odour in his proximity.', 'Yet not one of these gentlemen showed the slightest self-consciousness—either about their clothes or their countenance or their character in any way.', 'Neither of them ever imagined that they were looked at with repulsion; if they had imagined it they would not have minded—so long as their superiors did not look at them in that way.', 'It is clear to me now that, owing to my unbounded vanity and to the high standard I set for myself, I often looked at myself with furious discontent, which verged on loathing, and so I inwardly attributed the same feeling to everyone.', 'I hated my face, for instance: I thought it disgusting, and even suspected that there was something base in my expression, and so every day when I turned up at the office I tried to behave as independently as possible, and to assume a lofty expression, so that I might not be suspected of being abject.', '“My face may be ugly,” I thought, “but let it be lofty, expressive, and, above all, _extremely_ intelligent.” But I was positively and painfully certain that it was impossible for my countenance ever to express those qualities.', 'And what was worst of all, I thought it actually stupid looking, and I would have been quite satisfied if I could have looked intelligent.', 'In fact, I would even have put up with looking base if, at the same time, my face could have been thought strikingly intelligent.', 'Of course, I hated my fellow clerks one and all, and I despised them all, yet at the same time I was, as it were, afraid of them.', 'In fact, it happened at times that I thought more highly of them than of myself.', 'It somehow happened quite suddenly that I alternated between despising them and thinking them superior to myself.', 'A cultivated and decent man cannot be vain without setting a fearfully high standard for himself, and without despising and almost hating himself at certain moments.', 'But whether I despised them or thought them superior I dropped my eyes almost every time I met anyone.', 'I even made experiments whether I could face so and so’s looking at me, and I was always the first to drop my eyes.', 'This worried me to distraction.', 'I had a sickly dread, too, of being ridiculous, and so had a slavish passion for the conventional in everything external.', 'I loved to fall into the common rut, and had a whole-hearted terror of any kind of eccentricity in myself.', 'But how could I live up to it?', 'I was morbidly sensitive as a man of our age should be.', 'They were all stupid, and as like one another as so many sheep.', 'Perhaps I was the only one in the office who fancied that I was a coward and a slave, and I fancied it just because I was more highly developed.', 'But it was not only that I fancied it, it really was so.', 'I was a coward and a slave.', 'I say this without the slightest embarrassment.', 'Every decent man of our age must be a coward and a slave.', 'That is his normal condition.', 'Of that I am firmly persuaded.', 'He is made and constructed to that very end.', 'And not only at the present time owing to some casual circumstances, but always, at all times, a decent man is bound to be a coward and a slave.', 'It is the law of nature for all decent people all over the earth.', 'If anyone of them happens to be valiant about something, he need not be comforted nor carried away by that; he would show the white feather just the same before something else.', 'That is how it invariably and inevitably ends.', 'Only donkeys and mules are valiant, and they only till they are pushed up to the wall.', 'It is not worth while to pay attention to them for they really are of no consequence.', 'Another circumstance, too, worried me in those days: that there was no one like me and I was unlike anyone else.', '“I am alone and they are _everyone_,” I thought—and pondered.', 'From that it is evident that I was still a youngster.', 'The very opposite sometimes happened.', 'It was loathsome sometimes to go to the office; things reached such a point that I often came home ill.', 'But all at once, _à propos_ of nothing, there would come a phase of scepticism and indifference (everything happened in phases to me), and I would laugh myself at my intolerance and fastidiousness, I would reproach myself with being _romantic_.', 'At one time I was unwilling to speak to anyone, while at other times I would not only talk, but go to the length of contemplating making friends with them.', 'All my fastidiousness would suddenly, for no rhyme or reason, vanish.', 'Who knows, perhaps I never had really had it, and it had simply been affected, and got out of books.', 'I have not decided that question even now.', 'Once I quite made friends with them, visited their homes, played preference, drank vodka, talked of promotions....', 'But here let me make a digression.', 'We Russians, speaking generally, have never had those foolish transcendental “romantics”—German, and still more French—on whom nothing produces any effect; if there were an earthquake, if all France perished at the barricades, they would still be the same, they would not even have the decency to affect a change, but would still go on singing their transcendental songs to the hour of their death, because they are fools.', 'We, in Russia, have no fools; that is well known.', 'That is what distinguishes us from foreign lands.', 'Consequently these transcendental natures are not found amongst us in their pure form.', 'The idea that they are is due to our “realistic” journalists and critics of that day, always on the look out for Kostanzhoglos and Uncle Pyotr Ivanitchs and foolishly accepting them as our ideal; they have slandered our romantics, taking them for the same transcendental sort as in Germany or France.', 'On the contrary, the characteristics of our “romantics” are absolutely and directly opposed to the transcendental European type, and no European standard can be applied to them.', '(Allow me to make use of this word “romantic”—an old-fashioned and much respected word which has done good service and is familiar to all.)', 'The characteristics of our romantic are to understand everything, _to see everything and to see it often incomparably more clearly than our most realistic minds see it;_ to refuse to accept anyone or anything, but at the same time not to despise anything; to give way, to yield, from policy; never to lose sight of a useful practical object (such as rent-free quarters at the government expense, pensions, decorations), to keep their eye on that object through all the enthusiasms and volumes of lyrical poems, and at the same time to preserve “the sublime and the beautiful” inviolate within them to the hour of their death, and to preserve themselves also, incidentally, like some precious jewel wrapped in cotton wool if only for the benefit of “the sublime and the beautiful.” Our “romantic” is a man of great breadth and the greatest rogue of all our rogues, I assure you....', 'I can assure you from experience, indeed.', 'Of course, that is, if he is intelligent.', 'But what am I saying!', 'The romantic is always intelligent, and I only meant to observe that although we have had foolish romantics they don’t count, and they were only so because in the flower of their youth they degenerated into Germans, and to preserve their precious jewel more comfortably, settled somewhere out there—by preference in Weimar or the Black Forest.', 'I, for instance, genuinely despised my official work and did not openly abuse it simply because I was in it myself and got a salary for it.', 'Anyway, take note, I did not openly abuse it.', 'Our romantic would rather go out of his mind—a thing, however, which very rarely happens—than take to open abuse, unless he had some other career in view; and he is never kicked out.', 'At most, they would take him to the lunatic asylum as “the King of Spain” if he should go very mad.', 'But it is only the thin, fair people who go out of their minds in Russia.', 'Innumerable “romantics” attain later in life to considerable rank in the service.', 'Their many-sidedness is remarkable!', 'And what a faculty they have for the most contradictory sensations!', 'I was comforted by this thought even in those days, and I am of the same opinion now.', 'That is why there are so many “broad natures” among us who never lose their ideal even in the depths of degradation; and though they never stir a finger for their ideal, though they are arrant thieves and knaves, yet they tearfully cherish their first ideal and are extraordinarily honest at heart.', 'Yes, it is only among us that the most incorrigible rogue can be absolutely and loftily honest at heart without in the least ceasing to be a rogue.', 'I repeat, our romantics, frequently, become such accomplished rascals (I use the term “rascals” affectionately), suddenly display such a sense of reality and practical knowledge that their bewildered superiors and the public generally can only ejaculate in amazement.', 'Their many-sidedness is really amazing, and goodness knows what it may develop into later on, and what the future has in store for us.', 'It is not a poor material!', 'I do not say this from any foolish or boastful patriotism.', 'But I feel sure that you are again imagining that I am joking.', 'Or perhaps it’s just the contrary and you are convinced that I really think so.', 'Anyway, gentlemen, I shall welcome both views as an honour and a special favour.', 'And do forgive my digression.', 'I did not, of course, maintain friendly relations with my comrades and soon was at loggerheads with them, and in my youth and inexperience I even gave up bowing to them, as though I had cut off all relations.', 'That, however, only happened to me once.', 'As a rule, I was always alone.', 'In the first place I spent most of my time at home, reading.', 'I tried to stifle all that was continually seething within me by means of external impressions.', 'And the only external means I had was reading.', 'Reading, of course, was a great help—exciting me, giving me pleasure and pain.', 'But at times it bored me fearfully.', 'One longed for movement in spite of everything, and I plunged all at once into dark, underground, loathsome vice of the pettiest kind.', 'My wretched passions were acute, smarting, from my continual, sickly irritability I had hysterical impulses, with tears and convulsions.', 'I had no resource except reading, that is, there was nothing in my surroundings which I could respect and which attracted me.', 'I was overwhelmed with depression, too; I had an hysterical craving for incongruity and for contrast, and so I took to vice.', 'I have not said all this to justify myself....', 'But, no!', 'I am lying.', 'I did want to justify myself.', 'I make that little observation for my own benefit, gentlemen.', 'I don’t want to lie.', 'I vowed to myself I would not.', 'And so, furtively, timidly, in solitude, at night, I indulged in filthy vice, with a feeling of shame which never deserted me, even at the most loathsome moments, and which at such moments nearly made me curse.', 'Already even then I had my underground world in my soul.', 'I was fearfully afraid of being seen, of being met, of being recognised.', 'I visited various obscure haunts.', 'One night as I was passing a tavern I saw through a lighted window some gentlemen fighting with billiard cues, and saw one of them thrown out of the window.', 'At other times I should have felt very much disgusted, but I was in such a mood at the time, that I actually envied the gentleman thrown out of the window—and I envied him so much that I even went into the tavern and into the billiard-room.', '“Perhaps,” I thought, “I’ll have a fight, too, and they’ll throw me out of the window.” I was not drunk—but what is one to do—depression will drive a man to such a pitch of hysteria?', 'But nothing happened.', 'It seemed that I was not even equal to being thrown out of the window and I went away without having my fight.', 'An officer put me in my place from the first moment.', 'I was standing by the billiard-table and in my ignorance blocking up the way, and he wanted to pass; he took me by the shoulders and without a word—without a warning or explanation—moved me from where I was standing to another spot and passed by as though he had not noticed me.', 'I could have forgiven blows, but I could not forgive his having moved me without noticing me.', 'Devil knows what I would have given for a real regular quarrel—a more decent, a more _literary_ one, so to speak.', 'I had been treated like a fly.', 'This officer was over six foot, while I was a spindly little fellow.', 'But the quarrel was in my hands.', 'I had only to protest and I certainly would have been thrown out of the window.', 'But I changed my mind and preferred to beat a resentful retreat.', 'I went out of the tavern straight home, confused and troubled, and the next night I went out again with the same lewd intentions, still more furtively, abjectly and miserably than before, as it were, with tears in my eyes—but still I did go out again.', 'Don’t imagine, though, it was cowardice made me slink away from the officer; I never have been a coward at heart, though I have always been a coward in action.', 'Don’t be in a hurry to laugh—I assure you I can explain it all.', 'Oh, if only that officer had been one of the sort who would consent to fight a duel!', 'But no, he was one of those gentlemen (alas, long extinct!)', 'who preferred fighting with cues or, like Gogol’s Lieutenant Pirogov, appealing to the police.', 'They did not fight duels and would have thought a duel with a civilian like me an utterly unseemly procedure in any case—and they looked upon the duel altogether as something impossible, something free-thinking and French.', 'But they were quite ready to bully, especially when they were over six foot.', 'I did not slink away through cowardice, but through an unbounded vanity.', 'I was afraid not of his six foot, not of getting a sound thrashing and being thrown out of the window; I should have had physical courage enough, I assure you; but I had not the moral courage.', 'What I was afraid of was that everyone present, from the insolent marker down to the lowest little stinking, pimply clerk in a greasy collar, would jeer at me and fail to understand when I began to protest and to address them in literary language.', 'For of the point of honour—not of honour, but of the point of honour (_point d’honneur_)—one cannot speak among us except in literary language.', 'You can’t allude to the “point of honour” in ordinary language.', 'I was fully convinced (the sense of reality, in spite of all my romanticism!)', 'that they would all simply split their sides with laughter, and that the officer would not simply beat me, that is, without insulting me, but would certainly prod me in the back with his knee, kick me round the billiard-table, and only then perhaps have pity and drop me out of the window.', 'Of course, this trivial incident could not with me end in that.', 'I often met that officer afterwards in the street and noticed him very carefully.', 'I am not quite sure whether he recognised me, I imagine not; I judge from certain signs.', 'But I—I stared at him with spite and hatred and so it went on ... for several years!', 'My resentment grew even deeper with years.', 'At first I began making stealthy inquiries about this officer.', 'It was difficult for me to do so, for I knew no one.', 'But one day I heard someone shout his surname in the street as I was following him at a distance, as though I were tied to him—and so I learnt his surname.', 'Another time I followed him to his flat, and for ten kopecks learned from the porter where he lived, on which storey, whether he lived alone or with others, and so on—in fact, everything one could learn from a porter.', 'One morning, though I had never tried my hand with the pen, it suddenly occurred to me to write a satire on this officer in the form of a novel which would unmask his villainy.', 'I wrote the novel with relish.', 'I did unmask his villainy, I even exaggerated it; at first I so altered his surname that it could easily be recognised, but on second thoughts I changed it, and sent the story to the _Otetchestvenniya Zapiski_.', 'But at that time such attacks were not the fashion and my story was not printed.', 'That was a great vexation to me.', 'Sometimes I was positively choked with resentment.', 'At last I determined to challenge my enemy to a duel.', 'I composed a splendid, charming letter to him, imploring him to apologise to me, and hinting rather plainly at a duel in case of refusal.', 'The letter was so composed that if the officer had had the least understanding of the sublime and the beautiful he would certainly have flung himself on my neck and have offered me his friendship.', 'And how fine that would have been!', 'How we should have got on together!', '“He could have shielded me with his higher rank, while I could have improved his mind with my culture, and, well ... my ideas, and all sorts of things might have happened.” Only fancy, this was two years after his insult to me, and my challenge would have been a ridiculous anachronism, in spite of all the ingenuity of my letter in disguising and explaining away the anachronism.', 'But, thank God (to this day I thank the Almighty with tears in my eyes) I did not send the letter to him.', 'Cold shivers run down my back when I think of what might have happened if I had sent it.', 'And all at once I revenged myself in the simplest way, by a stroke of genius!', 'A brilliant thought suddenly dawned upon me.', 'Sometimes on holidays I used to stroll along the sunny side of the Nevsky about four o’clock in the afternoon.', 'Though it was hardly a stroll so much as a series of innumerable miseries, humiliations and resentments; but no doubt that was just what I wanted.', 'I used to wriggle along in a most unseemly fashion, like an eel, continually moving aside to make way for generals, for officers of the guards and the hussars, or for ladies.', 'At such minutes there used to be a convulsive twinge at my heart, and I used to feel hot all down my back at the mere thought of the wretchedness of my attire, of the wretchedness and abjectness of my little scurrying figure.', 'This was a regular martyrdom, a continual, intolerable humiliation at the thought, which passed into an incessant and direct sensation, that I was a mere fly in the eyes of all this world, a nasty, disgusting fly—more intelligent, more highly developed, more refined in feeling than any of them, of course—but a fly that was continually making way for everyone, insulted and injured by everyone.', 'Why I inflicted this torture upon myself, why I went to the Nevsky, I don’t know.', 'I felt simply drawn there at every possible opportunity.', 'Already then I began to experience a rush of the enjoyment of which I spoke in the first chapter.', 'After my affair with the officer I felt even more drawn there than before: it was on the Nevsky that I met him most frequently, there I could admire him.', 'He, too, went there chiefly on holidays, He, too, turned out of his path for generals and persons of high rank, and he too, wriggled between them like an eel; but people, like me, or even better dressed than me, he simply walked over; he made straight for them as though there was nothing but empty space before him, and never, under any circumstances, turned aside.', 'I gloated over my resentment watching him and ... always resentfully made way for him.', 'It exasperated me that even in the street I could not be on an even footing with him.', '“Why must you invariably be the first to move aside?” I kept asking myself in hysterical rage, waking up sometimes at three o’clock in the morning.', '“Why is it you and not he?', 'There’s no regulation about it; there’s no written law.', 'Let the making way be equal as it usually is when refined people meet; he moves half-way and you move half-way; you pass with mutual respect.” But that never happened, and I always moved aside, while he did not even notice my making way for him.', 'And lo and behold a bright idea dawned upon me!', '“What,” I thought, “if I meet him and don’t move on one side?', 'What if I don’t move aside on purpose, even if I knock up against him?', 'How would that be?” This audacious idea took such a hold on me that it gave me no peace.', 'I was dreaming of it continually, horribly, and I purposely went more frequently to the Nevsky in order to picture more vividly how I should do it when I did do it.', 'I was delighted.', 'This intention seemed to me more and more practical and possible.', '“Of course I shall not really push him,” I thought, already more good-natured in my joy.', '“I will simply not turn aside, will run up against him, not very violently, but just shouldering each other—just as much as decency permits.', 'I will push against him just as much as he pushes against me.” At last I made up my mind completely.', 'But my preparations took a great deal of time.', 'To begin with, when I carried out my plan I should need to be looking rather more decent, and so I had to think of my get-up.', '“In case of emergency, if, for instance, there were any sort of public scandal (and the public there is of the most _recherché:_ the Countess walks there; Prince D. walks there; all the literary world is there), I must be well dressed; that inspires respect and of itself puts us on an equal footing in the eyes of the society.” With this object I asked for some of my salary in advance, and bought at Tchurkin’s a pair of black gloves and a decent hat.', 'Black gloves seemed to me both more dignified and _bon ton_ than the lemon-coloured ones which I had contemplated at first.', '“The colour is too gaudy, it looks as though one were trying to be conspicuous,” and I did not take the lemon-coloured ones.', 'I had got ready long beforehand a good shirt, with white bone studs; my overcoat was the only thing that held me back.', 'The coat in itself was a very good one, it kept me warm; but it was wadded and it had a raccoon collar which was the height of vulgarity.', 'I had to change the collar at any sacrifice, and to have a beaver one like an officer’s.', 'For this purpose I began visiting the Gostiny Dvor and after several attempts I pitched upon a piece of cheap German beaver.', 'Though these German beavers soon grow shabby and look wretched, yet at first they look exceedingly well, and I only needed it for the occasion.', 'I asked the price; even so, it was too expensive.', 'After thinking it over thoroughly I decided to sell my raccoon collar.', 'The rest of the money—a considerable sum for me, I decided to borrow from Anton Antonitch Syetotchkin, my immediate superior, an unassuming person, though grave and judicious.', 'He never lent money to anyone, but I had, on entering the service, been specially recommended to him by an important personage who had got me my berth.', 'I was horribly worried.', 'To borrow from Anton Antonitch seemed to me monstrous and shameful.', 'I did not sleep for two or three nights.', 'Indeed, I did not sleep well at that time, I was in a fever; I had a vague sinking at my heart or else a sudden throbbing, throbbing, throbbing!', 'Anton Antonitch was surprised at first, then he frowned, then he reflected, and did after all lend me the money, receiving from me a written authorisation to take from my salary a fortnight later the sum that he had lent me.', 'In this way everything was at last ready.', 'The handsome beaver replaced the mean-looking raccoon, and I began by degrees to get to work.', 'It would never have done to act offhand, at random; the plan had to be carried out skilfully, by degrees.', 'But I must confess that after many efforts I began to despair: we simply could not run into each other.', 'I made every preparation, I was quite determined—it seemed as though we should run into one another directly—and before I knew what I was doing I had stepped aside for him again and he had passed without noticing me.', 'I even prayed as I approached him that God would grant me determination.', 'One time I had made up my mind thoroughly, but it ended in my stumbling and falling at his feet because at the very last instant when I was six inches from him my courage failed me.', 'He very calmly stepped over me, while I flew on one side like a ball.', 'That night I was ill again, feverish and delirious.', 'And suddenly it ended most happily.', 'The night before I had made up my mind not to carry out my fatal plan and to abandon it all, and with that object I went to the Nevsky for the last time, just to see how I would abandon it all.', 'Suddenly, three paces from my enemy, I unexpectedly made up my mind—I closed my eyes, and we ran full tilt, shoulder to shoulder, against one another!', 'I did not budge an inch and passed him on a perfectly equal footing!', 'He did not even look round and pretended not to notice it; but he was only pretending, I am convinced of that.', 'I am convinced of that to this day!', 'Of course, I got the worst of it—he was stronger, but that was not the point.', 'The point was that I had attained my object, I had kept up my dignity, I had not yielded a step, and had put myself publicly on an equal social footing with him.', 'I returned home feeling that I was fully avenged for everything.', 'I was delighted.', 'I was triumphant and sang Italian arias.', 'Of course, I will not describe to you what happened to me three days later; if you have read my first chapter you can guess for yourself.', 'The officer was afterwards transferred; I have not seen him now for fourteen years.', 'What is the dear fellow doing now?', 'Whom is he walking over?', 'II But the period of my dissipation would end and I always felt very sick afterwards.', 'It was followed by remorse—I tried to drive it away; I felt too sick.', 'By degrees, however, I grew used to that too.', 'I grew used to everything, or rather I voluntarily resigned myself to enduring it.', 'But I had a means of escape that reconciled everything—that was to find refuge in “the sublime and the beautiful,” in dreams, of course.', 'I was a terrible dreamer, I would dream for three months on end, tucked away in my corner, and you may believe me that at those moments I had no resemblance to the gentleman who, in the perturbation of his chicken heart, put a collar of German beaver on his great-coat.', 'I suddenly became a hero.', 'I would not have admitted my six-foot lieutenant even if he had called on me.', 'I could not even picture him before me then.', 'What were my dreams and how I could satisfy myself with them—it is hard to say now, but at the time I was satisfied with them.', 'Though, indeed, even now, I am to some extent satisfied with them.', 'Dreams were particularly sweet and vivid after a spell of dissipation; they came with remorse and with tears, with curses and transports.', 'There were moments of such positive intoxication, of such happiness, that there was not the faintest trace of irony within me, on my honour.', 'I had faith, hope, love.', 'I believed blindly at such times that by some miracle, by some external circumstance, all this would suddenly open out, expand; that suddenly a vista of suitable activity—beneficent, good, and, above all, _ready made_ (what sort of activity I had no idea, but the great thing was that it should be all ready for me)—would rise up before me—and I should come out into the light of day, almost riding a white horse and crowned with laurel.', 'Anything but the foremost place I could not conceive for myself, and for that very reason I quite contentedly occupied the lowest in reality.', 'Either to be a hero or to grovel in the mud—there was nothing between.', 'That was my ruin, for when I was in the mud I comforted myself with the thought that at other times I was a hero, and the hero was a cloak for the mud: for an ordinary man it was shameful to defile himself, but a hero was too lofty to be utterly defiled, and so he might defile himself.', 'It is worth noting that these attacks of the “sublime and the beautiful” visited me even during the period of dissipation and just at the times when I was touching the bottom.', 'They came in separate spurts, as though reminding me of themselves, but did not banish the dissipation by their appearance.', 'On the contrary, they seemed to add a zest to it by contrast, and were only sufficiently present to serve as an appetising sauce.', 'That sauce was made up of contradictions and sufferings, of agonising inward analysis, and all these pangs and pin-pricks gave a certain piquancy, even a significance to my dissipation—in fact, completely answered the purpose of an appetising sauce.', 'There was a certain depth of meaning in it.', 'And I could hardly have resigned myself to the simple, vulgar, direct debauchery of a clerk and have endured all the filthiness of it.', 'What could have allured me about it then and have drawn me at night into the street?', 'No, I had a lofty way of getting out of it all.', 'And what loving-kindness, oh Lord, what loving-kindness I felt at times in those dreams of mine!', 'in those “flights into the sublime and the beautiful”; though it was fantastic love, though it was never applied to anything human in reality, yet there was so much of this love that one did not feel afterwards even the impulse to apply it in reality; that would have been superfluous.', 'Everything, however, passed satisfactorily by a lazy and fascinating transition into the sphere of art, that is, into the beautiful forms of life, lying ready, largely stolen from the poets and novelists and adapted to all sorts of needs and uses.', 'I, for instance, was triumphant over everyone; everyone, of course, was in dust and ashes, and was forced spontaneously to recognise my superiority, and I forgave them all.', 'I was a poet and a grand gentleman, I fell in love; I came in for countless millions and immediately devoted them to humanity, and at the same time I confessed before all the people my shameful deeds, which, of course, were not merely shameful, but had in them much that was “sublime and beautiful” something in the Manfred style.', 'Everyone would kiss me and weep (what idiots they would be if they did not), while I should go barefoot and hungry preaching new ideas and fighting a victorious Austerlitz against the obscurantists.', 'Then the band would play a march, an amnesty would be declared, the Pope would agree to retire from Rome to Brazil; then there would be a ball for the whole of Italy at the Villa Borghese on the shores of Lake Como, Lake Como being for that purpose transferred to the neighbourhood of Rome; then would come a scene in the bushes, and so on, and so on—as though you did not know all about it?', 'You will say that it is vulgar and contemptible to drag all this into public after all the tears and transports which I have myself confessed.', 'But why is it contemptible?', 'Can you imagine that I am ashamed of it all, and that it was stupider than anything in your life, gentlemen?', 'And I can assure you that some of these fancies were by no means badly composed....', 'It did not all happen on the shores of Lake Como.', 'And yet you are right—it really is vulgar and contemptible.', 'And most contemptible of all it is that now I am attempting to justify myself to you.', 'And even more contemptible than that is my making this remark now.', 'But that’s enough, or there will be no end to it; each step will be more contemptible than the last....', 'I could never stand more than three months of dreaming at a time without feeling an irresistible desire to plunge into society.', 'To plunge into society meant to visit my superior at the office, Anton Antonitch Syetotchkin.', 'He was the only permanent acquaintance I have had in my life, and I wonder at the fact myself now.', 'But I only went to see him when that phase came over me, and when my dreams had reached such a point of bliss that it became essential at once to embrace my fellows and all mankind; and for that purpose I needed, at least, one human being, actually existing.', 'I had to call on Anton Antonitch, however, on Tuesday—his at-home day; so I had always to time my passionate desire to embrace humanity so that it might fall on a Tuesday.', 'This Anton Antonitch lived on the fourth storey in a house in Five Corners, in four low-pitched rooms, one smaller than the other, of a particularly frugal and sallow appearance.', 'He had two daughters and their aunt, who used to pour out the tea.', 'Of the daughters one was thirteen and another fourteen, they both had snub noses, and I was awfully shy of them because they were always whispering and giggling together.', 'The master of the house usually sat in his study on a leather couch in front of the table with some grey-headed gentleman, usually a colleague from our office or some other department.', 'I never saw more than two or three visitors there, always the same.', 'They talked about the excise duty; about business in the senate, about salaries, about promotions, about His Excellency, and the best means of pleasing him, and so on.', 'I had the patience to sit like a fool beside these people for four hours at a stretch, listening to them without knowing what to say to them or venturing to say a word.', 'I became stupefied, several times I felt myself perspiring, I was overcome by a sort of paralysis; but this was pleasant and good for me.', 'On returning home I deferred for a time my desire to embrace all mankind.', 'I had however one other acquaintance of a sort, Simonov, who was an old schoolfellow.', 'I had a number of schoolfellows, indeed, in Petersburg, but I did not associate with them and had even given up nodding to them in the street.', 'I believe I had transferred into the department I was in simply to avoid their company and to cut off all connection with my hateful childhood.', 'Curses on that school and all those terrible years of penal servitude!', 'In short, I parted from my schoolfellows as soon as I got out into the world.', 'There were two or three left to whom I nodded in the street.', 'One of them was Simonov, who had in no way been distinguished at school, was of a quiet and equable disposition; but I discovered in him a certain independence of character and even honesty I don’t even suppose that he was particularly stupid.', 'I had at one time spent some rather soulful moments with him, but these had not lasted long and had somehow been suddenly clouded over.', 'He was evidently uncomfortable at these reminiscences, and was, I fancy, always afraid that I might take up the same tone again.', 'I suspected that he had an aversion for me, but still I went on going to see him, not being quite certain of it.', 'And so on one occasion, unable to endure my solitude and knowing that as it was Thursday Anton Antonitch’s door would be closed, I thought of Simonov.', 'Climbing up to his fourth storey I was thinking that the man disliked me and that it was a mistake to go and see him.', 'But as it always happened that such reflections impelled me, as though purposely, to put myself into a false position, I went in.', 'It was almost a year since I had last seen Simonov.', 'III I found two of my old schoolfellows with him.', 'They seemed to be discussing an important matter.', 'All of them took scarcely any notice of my entrance, which was strange, for I had not met them for years.', 'Evidently they looked upon me as something on the level of a common fly.', 'I had not been treated like that even at school, though they all hated me.', 'I knew, of course, that they must despise me now for my lack of success in the service, and for my having let myself sink so low, going about badly dressed and so on—which seemed to them a sign of my incapacity and insignificance.', 'But I had not expected such contempt.', 'Simonov was positively surprised at my turning up.', 'Even in old days he had always seemed surprised at my coming.', 'All this disconcerted me: I sat down, feeling rather miserable, and began listening to what they were saying.', 'They were engaged in warm and earnest conversation about a farewell dinner which they wanted to arrange for the next day to a comrade of theirs called Zverkov, an officer in the army, who was going away to a distant province.', 'This Zverkov had been all the time at school with me too.', 'I had begun to hate him particularly in the upper forms.', 'In the lower forms he had simply been a pretty, playful boy whom everybody liked.', 'I had hated him, however, even in the lower forms, just because he was a pretty and playful boy.', 'He was always bad at his lessons and got worse and worse as he went on; however, he left with a good certificate, as he had powerful interests.', 'During his last year at school he came in for an estate of two hundred serfs, and as almost all of us were poor he took up a swaggering tone among us.', 'He was vulgar in the extreme, but at the same time he was a good-natured fellow, even in his swaggering.', 'In spite of superficial, fantastic and sham notions of honour and dignity, all but very few of us positively grovelled before Zverkov, and the more so the more he swaggered.', 'And it was not from any interested motive that they grovelled, but simply because he had been favoured by the gifts of nature.', 'Moreover, it was, as it were, an accepted idea among us that Zverkov was a specialist in regard to tact and the social graces.', 'This last fact particularly infuriated me.', 'I hated the abrupt self-confident tone of his voice, his admiration of his own witticisms, which were often frightfully stupid, though he was bold in his language; I hated his handsome, but stupid face (for which I would, however, have gladly exchanged my intelligent one), and the free-and-easy military manners in fashion in the “’forties.” I hated the way in which he used to talk of his future conquests of women (he did not venture to begin his attack upon women until he had the epaulettes of an officer, and was looking forward to them with impatience), and boasted of the duels he would constantly be fighting.', 'I remember how I, invariably so taciturn, suddenly fastened upon Zverkov, when one day talking at a leisure moment with his schoolfellows of his future relations with the fair sex, and growing as sportive as a puppy in the sun, he all at once declared that he would not leave a single village girl on his estate unnoticed, that that was his _droit de seigneur_, and that if the peasants dared to protest he would have them all flogged and double the tax on them, the bearded rascals.', 'Our servile rabble applauded, but I attacked him, not from compassion for the girls and their fathers, but simply because they were applauding such an insect.', 'I got the better of him on that occasion, but though Zverkov was stupid he was lively and impudent, and so laughed it off, and in such a way that my victory was not really complete; the laugh was on his side.', 'He got the better of me on several occasions afterwards, but without malice, jestingly, casually.', 'I remained angrily and contemptuously silent and would not answer him.', 'When we left school he made advances to me; I did not rebuff them, for I was flattered, but we soon parted and quite naturally.', 'Afterwards I heard of his barrack-room success as a lieutenant, and of the fast life he was leading.', 'Then there came other rumours—of his successes in the service.', 'By then he had taken to cutting me in the street, and I suspected that he was afraid of compromising himself by greeting a personage as insignificant as me.', 'I saw him once in the theatre, in the third tier of boxes.', 'By then he was wearing shoulder-straps.', 'He was twisting and twirling about, ingratiating himself with the daughters of an ancient General.', 'In three years he had gone off considerably, though he was still rather handsome and adroit.', 'One could see that by the time he was thirty he would be corpulent.', 'So it was to this Zverkov that my schoolfellows were going to give a dinner on his departure.', 'They had kept up with him for those three years, though privately they did not consider themselves on an equal footing with him, I am convinced of that.', 'Of Simonov’s two visitors, one was Ferfitchkin, a Russianised German—a little fellow with the face of a monkey, a blockhead who was always deriding everyone, a very bitter enemy of mine from our days in the lower forms—a vulgar, impudent, swaggering fellow, who affected a most sensitive feeling of personal honour, though, of course, he was a wretched little coward at heart.', 'He was one of those worshippers of Zverkov who made up to the latter from interested motives, and often borrowed money from him.', 'Simonov’s other visitor, Trudolyubov, was a person in no way remarkable—a tall young fellow, in the army, with a cold face, fairly honest, though he worshipped success of every sort, and was only capable of thinking of promotion.', 'He was some sort of distant relation of Zverkov’s, and this, foolish as it seems, gave him a certain importance among us.', 'He always thought me of no consequence whatever; his behaviour to me, though not quite courteous, was tolerable.', '“Well, with seven roubles each,” said Trudolyubov, “twenty-one roubles between the three of us, we ought to be able to get a good dinner.', 'Zverkov, of course, won’t pay.” “Of course not, since we are inviting him,” Simonov decided.', '“Can you imagine,” Ferfitchkin interrupted hotly and conceitedly, like some insolent flunkey boasting of his master the General’s decorations, “can you imagine that Zverkov will let us pay alone?', 'He will accept from delicacy, but he will order half a dozen bottles of champagne.” “Do we want half a dozen for the four of us?” observed Trudolyubov, taking notice only of the half dozen.', '“So the three of us, with Zverkov for the fourth, twenty-one roubles, at the Hôtel de Paris at five o’clock tomorrow,” Simonov, who had been asked to make the arrangements, concluded finally.', '“How twenty-one roubles?” I asked in some agitation, with a show of being offended; “if you count me it will not be twenty-one, but twenty-eight roubles.” It seemed to me that to invite myself so suddenly and unexpectedly would be positively graceful, and that they would all be conquered at once and would look at me with respect.', '“Do you want to join, too?” Simonov observed, with no appearance of pleasure, seeming to avoid looking at me.', 'He knew me through and through.', 'It infuriated me that he knew me so thoroughly.', '“Why not?', 'I am an old schoolfellow of his, too, I believe, and I must own I feel hurt that you have left me out,” I said, boiling over again.', '“And where were we to find you?” Ferfitchkin put in roughly.', '“You never were on good terms with Zverkov,” Trudolyubov added, frowning.', 'But I had already clutched at the idea and would not give it up.', '“It seems to me that no one has a right to form an opinion upon that,” I retorted in a shaking voice, as though something tremendous had happened.', '“Perhaps that is just my reason for wishing it now, that I have not always been on good terms with him.” “Oh, there’s no making you out ... with these refinements,” Trudolyubov jeered.', '“We’ll put your name down,” Simonov decided, addressing me.', '“Tomorrow at five-o’clock at the Hôtel de Paris.” “What about the money?” Ferfitchkin began in an undertone, indicating me to Simonov, but he broke off, for even Simonov was embarrassed.', '“That will do,” said Trudolyubov, getting up.', '“If he wants to come so much, let him.” “But it’s a private thing, between us friends,” Ferfitchkin said crossly, as he, too, picked up his hat.', '“It’s not an official gathering.” “We do not want at all, perhaps ...” They went away.', 'Ferfitchkin did not greet me in any way as he went out, Trudolyubov barely nodded.', 'Simonov, with whom I was left _tête-à-tête_, was in a state of vexation and perplexity, and looked at me queerly.', 'He did not sit down and did not ask me to.', '“H’m ... yes ... tomorrow, then.', 'Will you pay your subscription now?', 'I just ask so as to know,” he muttered in embarrassment.', 'I flushed crimson, as I did so I remembered that I had owed Simonov fifteen roubles for ages—which I had, indeed, never forgotten, though I had not paid it.', '“You will understand, Simonov, that I could have no idea when I came here....', 'I am very much vexed that I have forgotten....” “All right, all right, that doesn’t matter.', 'You can pay tomorrow after the dinner.', 'I simply wanted to know....', 'Please don’t...” He broke off and began pacing the room still more vexed.', 'As he walked he began to stamp with his heels.', '“Am I keeping you?” I asked, after two minutes of silence.', '“Oh!” he said, starting, “that is—to be truthful—yes.', 'I have to go and see someone ... not far from here,” he added in an apologetic voice, somewhat abashed.', '“My goodness, why didn’t you say so?” I cried, seizing my cap, with an astonishingly free-and-easy air, which was the last thing I should have expected of myself.', '“It’s close by ... not two paces away,” Simonov repeated, accompanying me to the front door with a fussy air which did not suit him at all.', '“So five o’clock, punctually, tomorrow,” he called down the stairs after me.', 'He was very glad to get rid of me.', 'I was in a fury.', '“What possessed me, what possessed me to force myself upon them?” I wondered, grinding my teeth as I strode along the street, “for a scoundrel, a pig like that Zverkov!', 'Of course I had better not go; of course, I must just snap my fingers at them.', 'I am not bound in any way.', 'I’ll send Simonov a note by tomorrow’s post....” But what made me furious was that I knew for certain that I should go, that I should make a point of going; and the more tactless, the more unseemly my going would be, the more certainly I would go.', 'And there was a positive obstacle to my going: I had no money.', 'All I had was nine roubles, I had to give seven of that to my servant, Apollon, for his monthly wages.', 'That was all I paid him—he had to keep himself.', 'Not to pay him was impossible, considering his character.', 'But I will talk about that fellow, about that plague of mine, another time.', 'However, I knew I should go and should not pay him his wages.', 'That night I had the most hideous dreams.', 'No wonder; all the evening I had been oppressed by memories of my miserable days at school, and I could not shake them off.', 'I was sent to the school by distant relations, upon whom I was dependent and of whom I have heard nothing since—they sent me there a forlorn, silent boy, already crushed by their reproaches, already troubled by doubt, and looking with savage distrust at everyone.', 'My schoolfellows met me with spiteful and merciless jibes because I was not like any of them.', 'But I could not endure their taunts; I could not give in to them with the ignoble readiness with which they gave in to one another.', 'I hated them from the first, and shut myself away from everyone in timid, wounded and disproportionate pride.', 'Their coarseness revolted me.', 'They laughed cynically at my face, at my clumsy figure; and yet what stupid faces they had themselves.', 'In our school the boys’ faces seemed in a special way to degenerate and grow stupider.', 'How many fine-looking boys came to us!', 'In a few years they became repulsive.', 'Even at sixteen I wondered at them morosely; even then I was struck by the pettiness of their thoughts, the stupidity of their pursuits, their games, their conversations.', 'They had no understanding of such essential things, they took no interest in such striking, impressive subjects, that I could not help considering them inferior to myself.', 'It was not wounded vanity that drove me to it, and for God’s sake do not thrust upon me your hackneyed remarks, repeated to nausea, that “I was only a dreamer,” while they even then had an understanding of life.', 'They understood nothing, they had no idea of real life, and I swear that that was what made me most indignant with them.', 'On the contrary, the most obvious, striking reality they accepted with fantastic stupidity and even at that time were accustomed to respect success.', 'Everything that was just, but oppressed and looked down upon, they laughed at heartlessly and shamefully.', 'They took rank for intelligence; even at sixteen they were already talking about a snug berth.', 'Of course, a great deal of it was due to their stupidity, to the bad examples with which they had always been surrounded in their childhood and boyhood.', 'They were monstrously depraved.', 'Of course a great deal of that, too, was superficial and an assumption of cynicism; of course there were glimpses of youth and freshness even in their depravity; but even that freshness was not attractive, and showed itself in a certain rakishness.', 'I hated them horribly, though perhaps I was worse than any of them.', 'They repaid me in the same way, and did not conceal their aversion for me.', 'But by then I did not desire their affection: on the contrary, I continually longed for their humiliation.', 'To escape from their derision I purposely began to make all the progress I could with my studies and forced my way to the very top.', 'This impressed them.', 'Moreover, they all began by degrees to grasp that I had already read books none of them could read, and understood things (not forming part of our school curriculum) of which they had not even heard.', 'They took a savage and sarcastic view of it, but were morally impressed, especially as the teachers began to notice me on those grounds.', 'The mockery ceased, but the hostility remained, and cold and strained relations became permanent between us.', 'In the end I could not put up with it: with years a craving for society, for friends, developed in me.', 'I attempted to get on friendly terms with some of my schoolfellows; but somehow or other my intimacy with them was always strained and soon ended of itself.', 'Once, indeed, I did have a friend.', 'But I was already a tyrant at heart; I wanted to exercise unbounded sway over him; I tried to instil into him a contempt for his surroundings; I required of him a disdainful and complete break with those surroundings.', 'I frightened him with my passionate affection; I reduced him to tears, to hysterics.', 'He was a simple and devoted soul; but when he devoted himself to me entirely I began to hate him immediately and repulsed him—as though all I needed him for was to win a victory over him, to subjugate him and nothing else.', 'But I could not subjugate all of them; my friend was not at all like them either, he was, in fact, a rare exception.', 'The first thing I did on leaving school was to give up the special job for which I had been destined so as to break all ties, to curse my past and shake the dust from off my feet.... And goodness knows why, after all that, I should go trudging off to Simonov’s!', 'Early next morning I roused myself and jumped out of bed with excitement, as though it were all about to happen at once.', 'But I believed that some radical change in my life was coming, and would inevitably come that day.', 'Owing to its rarity, perhaps, any external event, however trivial, always made me feel as though some radical change in my life were at hand.', 'I went to the office, however, as usual, but sneaked away home two hours earlier to get ready.', 'The great thing, I thought, is not to be the first to arrive, or they will think I am overjoyed at coming.', 'But there were thousands of such great points to consider, and they all agitated and overwhelmed me.', 'I polished my boots a second time with my own hands; nothing in the world would have induced Apollon to clean them twice a day, as he considered that it was more than his duties required of him.', 'I stole the brushes to clean them from the passage, being careful he should not detect it, for fear of his contempt.', 'Then I minutely examined my clothes and thought that everything looked old, worn and threadbare.', 'I had let myself get too slovenly.', 'My uniform, perhaps, was tidy, but I could not go out to dinner in my uniform.', 'The worst of it was that on the knee of my trousers was a big yellow stain.', 'I had a foreboding that that stain would deprive me of nine-tenths of my personal dignity.', 'I knew, too, that it was very poor to think so.', '“But this is no time for thinking: now I am in for the real thing,” I thought, and my heart sank.', 'I knew, too, perfectly well even then, that I was monstrously exaggerating the facts.', 'But how could I help it?', 'I could not control myself and was already shaking with fever.', 'With despair I pictured to myself how coldly and disdainfully that “scoundrel” Zverkov would meet me; with what dull-witted, invincible contempt the blockhead Trudolyubov would look at me; with what impudent rudeness the insect Ferfitchkin would snigger at me in order to curry favour with Zverkov; how completely Simonov would take it all in, and how he would despise me for the abjectness of my vanity and lack of spirit—and, worst of all, how paltry, _unliterary_, commonplace it would all be.', 'Of course, the best thing would be not to go at all.', 'But that was most impossible of all: if I feel impelled to do anything, I seem to be pitchforked into it.', 'I should have jeered at myself ever afterwards: “So you funked it, you funked it, you funked the _real thing!_” On the contrary, I passionately longed to show all that “rabble” that I was by no means such a spiritless creature as I seemed to myself.', 'What is more, even in the acutest paroxysm of this cowardly fever, I dreamed of getting the upper hand, of dominating them, carrying them away, making them like me—if only for my “elevation of thought and unmistakable wit.” They would abandon Zverkov, he would sit on one side, silent and ashamed, while I should crush him.', 'Then, perhaps, we would be reconciled and drink to our everlasting friendship; but what was most bitter and humiliating for me was that I knew even then, knew fully and for certain, that I needed nothing of all this really, that I did not really want to crush, to subdue, to attract them, and that I did not care a straw really for the result, even if I did achieve it.', 'Oh, how I prayed for the day to pass quickly!', 'In unutterable anguish I went to the window, opened the movable pane and looked out into the troubled darkness of the thickly falling wet snow.', 'At last my wretched little clock hissed out five.', 'I seized my hat and, trying not to look at Apollon, who had been all day expecting his month’s wages, but in his foolishness was unwilling to be the first to speak about it, I slipped between him and the door and, jumping into a high-class sledge, on which I spent my last half rouble, I drove up in grand style to the Hôtel de Paris.', 'IV I had been certain the day before that I should be the first to arrive.', 'But it was not a question of being the first to arrive.', 'Not only were they not there, but I had difficulty in finding our room.', 'The table was not laid even.', 'What did it mean?', 'After a good many questions I elicited from the waiters that the dinner had been ordered not for five, but for six o’clock.', 'This was confirmed at the buffet too.', 'I felt really ashamed to go on questioning them.', 'It was only twenty-five minutes past five.', 'If they changed the dinner hour they ought at least to have let me know—that is what the post is for, and not to have put me in an absurd position in my own eyes and ... and even before the waiters.', 'I sat down; the servant began laying the table; I felt even more humiliated when he was present.', 'Towards six o’clock they brought in candles, though there were lamps burning in the room.', 'It had not occurred to the waiter, however, to bring them in at once when I arrived.', 'In the next room two gloomy, angry-looking persons were eating their dinners in silence at two different tables.', 'There was a great deal of noise, even shouting, in a room further away; one could hear the laughter of a crowd of people, and nasty little shrieks in French: there were ladies at the dinner.', 'It was sickening, in fact.', 'I rarely passed more unpleasant moments, so much so that when they did arrive all together punctually at six I was overjoyed to see them, as though they were my deliverers, and even forgot that it was incumbent upon me to show resentment.', 'Zverkov walked in at the head of them; evidently he was the leading spirit.', 'He and all of them were laughing; but, seeing me, Zverkov drew himself up a little, walked up to me deliberately with a slight, rather jaunty bend from the waist.', 'He shook hands with me in a friendly, but not over-friendly, fashion, with a sort of circumspect courtesy like that of a General, as though in giving me his hand he were warding off something.', 'I had imagined, on the contrary, that on coming in he would at once break into his habitual thin, shrill laugh and fall to making his insipid jokes and witticisms.', 'I had been preparing for them ever since the previous day, but I had not expected such condescension, such high-official courtesy.', 'So, then, he felt himself ineffably superior to me in every respect!', 'If he only meant to insult me by that high-official tone, it would not matter, I thought—I could pay him back for it one way or another.', 'But what if, in reality, without the least desire to be offensive, that sheepshead had a notion in earnest that he was superior to me and could only look at me in a patronising way?', 'The very supposition made me gasp.', '“I was surprised to hear of your desire to join us,” he began, lisping and drawling, which was something new.', '“You and I seem to have seen nothing of one another.', 'You fight shy of us.', 'You shouldn’t.', 'We are not such terrible people as you think.', 'Well, anyway, I am glad to renew our acquaintance.” And he turned carelessly to put down his hat on the window.', '“Have you been waiting long?” Trudolyubov inquired.', '“I arrived at five o’clock as you told me yesterday,” I answered aloud, with an irritability that threatened an explosion.', '“Didn’t you let him know that we had changed the hour?” said Trudolyubov to Simonov.', '“No, I didn’t.', 'I forgot,” the latter replied, with no sign of regret, and without even apologising to me he went off to order the _hors d’œuvres_.', '“So you’ve been here a whole hour?', 'Oh, poor fellow!” Zverkov cried ironically, for to his notions this was bound to be extremely funny.', 'That rascal Ferfitchkin followed with his nasty little snigger like a puppy yapping.', 'My position struck him, too, as exquisitely ludicrous and embarrassing.', '“It isn’t funny at all!” I cried to Ferfitchkin, more and more irritated.', '“It wasn’t my fault, but other people’s.', 'They neglected to let me know.', 'It was ... it was ... it was simply absurd.” “It’s not only absurd, but something else as well,” muttered Trudolyubov, naively taking my part.', '“You are not hard enough upon it.', 'It was simply rudeness—unintentional, of course.', 'And how could Simonov ... h’m!” “If a trick like that had been played on me,” observed Ferfitchkin, “I should ...” “But you should have ordered something for yourself,” Zverkov interrupted, “or simply asked for dinner without waiting for us.” “You will allow that I might have done that without your permission,” I rapped out.', '“If I waited, it was ...” “Let us sit down, gentlemen,” cried Simonov, coming in.', '“Everything is ready; I can answer for the champagne; it is capitally frozen.... You see, I did not know your address, where was I to look for you?” he suddenly turned to me, but again he seemed to avoid looking at me.', 'Evidently he had something against me.', 'It must have been what happened yesterday.', 'All sat down; I did the same.', 'It was a round table.', 'Trudolyubov was on my left, Simonov on my right, Zverkov was sitting opposite, Ferfitchkin next to him, between him and Trudolyubov.', '“Tell me, are you ... in a government office?” Zverkov went on attending to me.', 'Seeing that I was embarrassed he seriously thought that he ought to be friendly to me, and, so to speak, cheer me up.', '“Does he want me to throw a bottle at his head?” I thought, in a fury.', 'In my novel surroundings I was unnaturally ready to be irritated.', '“In the N—— office,” I answered jerkily, with my eyes on my plate.', '“And ha-ave you a go-od berth?', 'I say, what ma-a-de you leave your original job?” “What ma-a-de me was that I wanted to leave my original job,” I drawled more than he, hardly able to control myself.', 'Ferfitchkin went off into a guffaw.', 'Simonov looked at me ironically.', 'Trudolyubov left off eating and began looking at me with curiosity.', 'Zverkov winced, but he tried not to notice it.', '“And the remuneration?” “What remuneration?” “I mean, your sa-a-lary?” “Why are you cross-examining me?” However, I told him at once what my salary was.', 'I turned horribly red.', '“It is not very handsome,” Zverkov observed majestically.', '“Yes, you can’t afford to dine at cafés on that,” Ferfitchkin added insolently.', '“To my thinking it’s very poor,” Trudolyubov observed gravely.', '“And how thin you have grown!', 'How you have changed!” added Zverkov, with a shade of venom in his voice, scanning me and my attire with a sort of insolent compassion.', '“Oh, spare his blushes,” cried Ferfitchkin, sniggering.', '“My dear sir, allow me to tell you I am not blushing,” I broke out at last; “do you hear?', 'I am dining here, at this cafe, at my own expense, not at other people’s—note that, Mr. Ferfitchkin.” “Wha-at?', 'Isn’t every one here dining at his own expense?', 'You would seem to be ...” Ferfitchkin flew out at me, turning as red as a lobster, and looking me in the face with fury.', '“Tha-at,” I answered, feeling I had gone too far, “and I imagine it would be better to talk of something more intelligent.” “You intend to show off your intelligence, I suppose?” “Don’t disturb yourself, that would be quite out of place here.” “Why are you clacking away like that, my good sir, eh?', 'Have you gone out of your wits in your office?” “Enough, gentlemen, enough!” Zverkov cried, authoritatively.', '“How stupid it is!” muttered Simonov.', '“It really is stupid.', 'We have met here, a company of friends, for a farewell dinner to a comrade and you carry on an altercation,” said Trudolyubov, rudely addressing himself to me alone.', '“You invited yourself to join us, so don’t disturb the general harmony.” “Enough, enough!” cried Zverkov.', '“Give over, gentlemen, it’s out of place.', 'Better let me tell you how I nearly got married the day before yesterday....” And then followed a burlesque narrative of how this gentleman had almost been married two days before.', 'There was not a word about the marriage, however, but the story was adorned with generals, colonels and kammer-junkers, while Zverkov almost took the lead among them.', 'It was greeted with approving laughter; Ferfitchkin positively squealed.', 'No one paid any attention to me, and I sat crushed and humiliated.', '“Good Heavens, these are not the people for me!” I thought.', '“And what a fool I have made of myself before them!', 'I let Ferfitchkin go too far, though.', 'The brutes imagine they are doing me an honour in letting me sit down with them.', 'They don’t understand that it’s an honour to them and not to me!', 'I’ve grown thinner!', 'My clothes!', 'Oh, damn my trousers!', 'Zverkov noticed the yellow stain on the knee as soon as he came in....', 'But what’s the use!', 'I must get up at once, this very minute, take my hat and simply go without a word ... with contempt!', 'And tomorrow I can send a challenge.', 'The scoundrels!', 'As though I cared about the seven roubles.', 'They may think....', 'Damn it!', 'I don’t care about the seven roubles.', 'I’ll go this minute!” Of course I remained.', 'I drank sherry and Lafitte by the glassful in my discomfiture.', 'Being unaccustomed to it, I was quickly affected.', 'My annoyance increased as the wine went to my head.', 'I longed all at once to insult them all in a most flagrant manner and then go away.', 'To seize the moment and show what I could do, so that they would say, “He’s clever, though he is absurd,” and ... and ... in fact, damn them all!', 'I scanned them all insolently with my drowsy eyes.', 'But they seemed to have forgotten me altogether.', 'They were noisy, vociferous, cheerful.', 'Zverkov was talking all the time.', 'I began listening.', 'Zverkov was talking of some exuberant lady whom he had at last led on to declaring her love (of course, he was lying like a horse), and how he had been helped in this affair by an intimate friend of his, a Prince Kolya, an officer in the hussars, who had three thousand serfs.', '“And yet this Kolya, who has three thousand serfs, has not put in an appearance here tonight to see you off,” I cut in suddenly.', 'For one minute every one was silent.', '“You are drunk already.” Trudolyubov deigned to notice me at last, glancing contemptuously in my direction.', 'Zverkov, without a word, examined me as though I were an insect.', 'I dropped my eyes.', 'Simonov made haste to fill up the glasses with champagne.', 'Trudolyubov raised his glass, as did everyone else but me.', '“Your health and good luck on the journey!” he cried to Zverkov.', '“To old times, to our future, hurrah!” They all tossed off their glasses, and crowded round Zverkov to kiss him.', 'I did not move; my full glass stood untouched before me.', '“Why, aren’t you going to drink it?” roared Trudolyubov, losing patience and turning menacingly to me.', '“I want to make a speech separately, on my own account ... and then I’ll drink it, Mr. Trudolyubov.” “Spiteful brute!” muttered Simonov.', 'I drew myself up in my chair and feverishly seized my glass, prepared for something extraordinary, though I did not know myself precisely what I was going to say.', '“_Silence!_” cried Ferfitchkin.', '“Now for a display of wit!” Zverkov waited very gravely, knowing what was coming.', '“Mr.', 'Lieutenant Zverkov,” I began, “let me tell you that I hate phrases, phrasemongers and men in corsets ... that’s the first point, and there is a second one to follow it.” There was a general stir.', '“The second point is: I hate ribaldry and ribald talkers.', 'Especially ribald talkers!', 'The third point: I love justice, truth and honesty.” I went on almost mechanically, for I was beginning to shiver with horror myself and had no idea how I came to be talking like this.', '“I love thought, Monsieur Zverkov; I love true comradeship, on an equal footing and not ... H’m ...', 'I love ...', 'But, however, why not?', 'I will drink your health, too, Mr. Zverkov.', 'Seduce the Circassian girls, shoot the enemies of the fatherland and ... and ... to your health, Monsieur Zverkov!” Zverkov got up from his seat, bowed to me and said: “I am very much obliged to you.” He was frightfully offended and turned pale.', '“Damn the fellow!” roared Trudolyubov, bringing his fist down on the table.', '“Well, he wants a punch in the face for that,” squealed Ferfitchkin.', '“We ought to turn him out,” muttered Simonov.', '“Not a word, gentlemen, not a movement!” cried Zverkov solemnly, checking the general indignation.', '“I thank you all, but I can show him for myself how much value I attach to his words.” “Mr.', 'Ferfitchkin, you will give me satisfaction tomorrow for your words just now!” I said aloud, turning with dignity to Ferfitchkin.', '“A duel, you mean?', 'Certainly,” he answered.', 'But probably I was so ridiculous as I challenged him and it was so out of keeping with my appearance that everyone including Ferfitchkin was prostrate with laughter.', '“Yes, let him alone, of course!', 'He is quite drunk,” Trudolyubov said with disgust.', '“I shall never forgive myself for letting him join us,” Simonov muttered again.', '“Now is the time to throw a bottle at their heads,” I thought to myself.', 'I picked up the bottle ... and filled my glass.... “No, I’d better sit on to the end,” I went on thinking; “you would be pleased, my friends, if I went away.', 'Nothing will induce me to go.', 'I’ll go on sitting here and drinking to the end, on purpose, as a sign that I don’t think you of the slightest consequence.', 'I will go on sitting and drinking, because this is a public-house and I paid my entrance money.', 'I’ll sit here and drink, for I look upon you as so many pawns, as inanimate pawns.', 'I’ll sit here and drink ... and sing if I want to, yes, sing, for I have the right to ... to sing ... H’m!” But I did not sing.', 'I simply tried not to look at any of them.', 'I assumed most unconcerned attitudes and waited with impatience for them to speak _first_.', 'But alas, they did not address me!', 'And oh, how I wished, how I wished at that moment to be reconciled to them!', 'It struck eight, at last nine.', 'They moved from the table to the sofa.', 'Zverkov stretched himself on a lounge and put one foot on a round table.', 'Wine was brought there.', 'He did, as a fact, order three bottles on his own account.', 'I, of course, was not invited to join them.', 'They all sat round him on the sofa.', 'They listened to him, almost with reverence.', 'It was evident that they were fond of him.', '“What for?', 'What for?” I wondered.', 'From time to time they were moved to drunken enthusiasm and kissed each other.', 'They talked of the Caucasus, of the nature of true passion, of snug berths in the service, of the income of an hussar called Podharzhevsky, whom none of them knew personally, and rejoiced in the largeness of it, of the extraordinary grace and beauty of a Princess D., whom none of them had ever seen; then it came to Shakespeare’s being immortal.', 'I smiled contemptuously and walked up and down the other side of the room, opposite the sofa, from the table to the stove and back again.', 'I tried my very utmost to show them that I could do without them, and yet I purposely made a noise with my boots, thumping with my heels.', 'But it was all in vain.', 'They paid no attention.', 'I had the patience to walk up and down in front of them from eight o’clock till eleven, in the same place, from the table to the stove and back again.', '“I walk up and down to please myself and no one can prevent me.” The waiter who came into the room stopped, from time to time, to look at me.', 'I was somewhat giddy from turning round so often; at moments it seemed to me that I was in delirium.', 'During those three hours I was three times soaked with sweat and dry again.', 'At times, with an intense, acute pang I was stabbed to the heart by the thought that ten years, twenty years, forty years would pass, and that even in forty years I would remember with loathing and humiliation those filthiest, most ludicrous, and most awful moments of my life.', 'No one could have gone out of his way to degrade himself more shamelessly, and I fully realised it, fully, and yet I went on pacing up and down from the table to the stove.', '“Oh, if you only knew what thoughts and feelings I am capable of, how cultured I am!” I thought at moments, mentally addressing the sofa on which my enemies were sitting.', 'But my enemies behaved as though I were not in the room.', 'Once—only once—they turned towards me, just when Zverkov was talking about Shakespeare, and I suddenly gave a contemptuous laugh.', 'I laughed in such an affected and disgusting way that they all at once broke off their conversation, and silently and gravely for two minutes watched me walking up and down from the table to the stove, _taking no notice of them_.', 'But nothing came of it: they said nothing, and two minutes later they ceased to notice me again.', 'It struck eleven.', '“Friends,” cried Zverkov getting up from the sofa, “let us all be off now, _there!_” “Of course, of course,” the others assented.', 'I turned sharply to Zverkov.', 'I was so harassed, so exhausted, that I would have cut my throat to put an end to it.', 'I was in a fever; my hair, soaked with perspiration, stuck to my forehead and temples.', '“Zverkov, I beg your pardon,” I said abruptly and resolutely.', '“Ferfitchkin, yours too, and everyone’s, everyone’s: I have insulted you all!” “Aha!', 'A duel is not in your line, old man,” Ferfitchkin hissed venomously.', 'It sent a sharp pang to my heart.', '“No, it’s not the duel I am afraid of, Ferfitchkin!', 'I am ready to fight you tomorrow, after we are reconciled.', 'I insist upon it, in fact, and you cannot refuse.', 'I want to show you that I am not afraid of a duel.', 'You shall fire first and I shall fire into the air.” “He is comforting himself,” said Simonov.', '“He’s simply raving,” said Trudolyubov.', '“But let us pass.', 'Why are you barring our way?', 'What do you want?” Zverkov answered disdainfully.', 'They were all flushed, their eyes were bright: they had been drinking heavily.', '“I ask for your friendship, Zverkov; I insulted you, but ...” “Insulted?', '_You_ insulted _me?_ Understand, sir, that you never, under any circumstances, could possibly insult _me_.” “And that’s enough for you.', 'Out of the way!” concluded Trudolyubov.', '“Olympia is mine, friends, that’s agreed!” cried Zverkov.', '“We won’t dispute your right, we won’t dispute your right,” the others answered, laughing.', 'I stood as though spat upon.', 'The party went noisily out of the room.', 'Trudolyubov struck up some stupid song.', 'Simonov remained behind for a moment to tip the waiters.', 'I suddenly went up to him.', '“Simonov!', 'give me six roubles!” I said, with desperate resolution.', 'He looked at me in extreme amazement, with vacant eyes.', 'He, too, was drunk.', '“You don’t mean you are coming with us?” “Yes.” “I’ve no money,” he snapped out, and with a scornful laugh he went out of the room.', 'I clutched at his overcoat.', 'It was a nightmare.', '“Simonov, I saw you had money.', 'Why do you refuse me?', 'Am I a scoundrel?', 'Beware of refusing me: if you knew, if you knew why I am asking!', 'My whole future, my whole plans depend upon it!” Simonov pulled out the money and almost flung it at me.', '“Take it, if you have no sense of shame!” he pronounced pitilessly, and ran to overtake them.', 'I was left for a moment alone.', 'Disorder, the remains of dinner, a broken wine-glass on the floor, spilt wine, cigarette ends, fumes of drink and delirium in my brain, an agonising misery in my heart and finally the waiter, who had seen and heard all and was looking inquisitively into my face.', '“I am going there!” I cried.', '“Either they shall all go down on their knees to beg for my friendship, or I will give Zverkov a slap in the face!” V “So this is it, this is it at last—contact with real life,” I muttered as I ran headlong downstairs.', '“This is very different from the Pope’s leaving Rome and going to Brazil, very different from the ball on Lake Como!” “You are a scoundrel,” a thought flashed through my mind, “if you laugh at this now.” “No matter!” I cried, answering myself.', '“Now everything is lost!” There was no trace to be seen of them, but that made no difference—I knew where they had gone.', 'At the steps was standing a solitary night sledge-driver in a rough peasant coat, powdered over with the still falling, wet, and as it were warm, snow.', 'It was hot and steamy.', 'The little shaggy piebald horse was also covered with snow and coughing, I remember that very well.', 'I made a rush for the roughly made sledge; but as soon as I raised my foot to get into it, the recollection of how Simonov had just given me six roubles seemed to double me up and I tumbled into the sledge like a sack.', '“No, I must do a great deal to make up for all that,” I cried.', '“But I will make up for it or perish on the spot this very night.', 'Start!” We set off.', 'There was a perfect whirl in my head.', '“They won’t go down on their knees to beg for my friendship.', 'That is a mirage, cheap mirage, revolting, romantic and fantastical—that’s another ball on Lake Como.', 'And so I am bound to slap Zverkov’s face!', 'It is my duty to.', 'And so it is settled; I am flying to give him a slap in the face.', 'Hurry up!” The driver tugged at the reins.', '“As soon as I go in I’ll give it him.', 'Ought I before giving him the slap to say a few words by way of preface?', 'No.', 'I’ll simply go in and give it him.', 'They will all be sitting in the drawing-room, and he with Olympia on the sofa.', 'That damned Olympia!', 'She laughed at my looks on one occasion and refused me.', 'I’ll pull Olympia’s hair, pull Zverkov’s ears!', 'No, better one ear, and pull him by it round the room.', 'Maybe they will all begin beating me and will kick me out.', 'That’s most likely, indeed.', 'No matter!', 'Anyway, I shall first slap him; the initiative will be mine; and by the laws of honour that is everything: he will be branded and cannot wipe off the slap by any blows, by nothing but a duel.', 'He will be forced to fight.', 'And let them beat me now.', 'Let them, the ungrateful wretches!', 'Trudolyubov will beat me hardest, he is so strong; Ferfitchkin will be sure to catch hold sideways and tug at my hair.', 'But no matter, no matter!', 'That’s what I am going for.', 'The blockheads will be forced at last to see the tragedy of it all!', 'When they drag me to the door I shall call out to them that in reality they are not worth my little finger.', 'Get on, driver, get on!” I cried to the driver.', 'He started and flicked his whip, I shouted so savagely.', '“We shall fight at daybreak, that’s a settled thing.', 'I’ve done with the office.', 'Ferfitchkin made a joke about it just now.', 'But where can I get pistols?', 'Nonsense!', 'I’ll get my salary in advance and buy them.', 'And powder, and bullets?', 'That’s the second’s business.', 'And how can it all be done by daybreak?', 'and where am I to get a second?', 'I have no friends.', 'Nonsense!” I cried, lashing myself up more and more.', '“It’s of no consequence!', 'The first person I meet in the street is bound to be my second, just as he would be bound to pull a drowning man out of water.', 'The most eccentric things may happen.', 'Even if I were to ask the director himself to be my second tomorrow, he would be bound to consent, if only from a feeling of chivalry, and to keep the secret!', 'Anton Antonitch....” The fact is, that at that very minute the disgusting absurdity of my plan and the other side of the question was clearer and more vivid to my imagination than it could be to anyone on earth.', 'But .... “Get on, driver, get on, you rascal, get on!” “Ugh, sir!” said the son of toil.', 'Cold shivers suddenly ran down me.', 'Wouldn’t it be better ... to go straight home?', 'My God, my God!', 'Why did I invite myself to this dinner yesterday?', 'But no, it’s impossible.', 'And my walking up and down for three hours from the table to the stove?', 'No, they, they and no one else must pay for my walking up and down!', 'They must wipe out this dishonour!', 'Drive on!', 'And what if they give me into custody?', 'They won’t dare!', 'They’ll be afraid of the scandal.', 'And what if Zverkov is so contemptuous that he refuses to fight a duel?', 'He is sure to; but in that case I’ll show them ...', 'I will turn up at the posting station when he’s setting off tomorrow, I’ll catch him by the leg, I’ll pull off his coat when he gets into the carriage.', 'I’ll get my teeth into his hand, I’ll bite him.', '“See what lengths you can drive a desperate man to!” He may hit me on the head and they may belabour me from behind.', 'I will shout to the assembled multitude: “Look at this young puppy who is driving off to captivate the Circassian girls after letting me spit in his face!” Of course, after that everything will be over!', 'The office will have vanished off the face of the earth.', 'I shall be arrested, I shall be tried, I shall be dismissed from the service, thrown in prison, sent to Siberia.', 'Never mind!', 'In fifteen years when they let me out of prison I will trudge off to him, a beggar, in rags.', 'I shall find him in some provincial town.', 'He will be married and happy.', 'He will have a grown-up daughter....', 'I shall say to him: “Look, monster, at my hollow cheeks and my rags!', 'I’ve lost everything—my career, my happiness, art, science, _the woman I loved_, and all through you.', 'Here are pistols.', 'I have come to discharge my pistol and ... and I ... forgive you.', 'Then I shall fire into the air and he will hear nothing more of me....” I was actually on the point of tears, though I knew perfectly well at that moment that all this was out of Pushkin’s _Silvio_ and Lermontov’s _Masquerade_.', 'And all at once I felt horribly ashamed, so ashamed that I stopped the horse, got out of the sledge, and stood still in the snow in the middle of the street.', 'The driver gazed at me, sighing and astonished.', 'What was I to do?', 'I could not go on there—it was evidently stupid, and I could not leave things as they were, because that would seem as though ... Heavens, how could I leave things!', 'And after such insults!', '“No!” I cried, throwing myself into the sledge again.', '“It is ordained!', 'It is fate!', 'Drive on, drive on!” And in my impatience I punched the sledge-driver on the back of the neck.', '“What are you up to?', 'What are you hitting me for?” the peasant shouted, but he whipped up his nag so that it began kicking.', 'The wet snow was falling in big flakes; I unbuttoned myself, regardless of it.', 'I forgot everything else, for I had finally decided on the slap, and felt with horror that it was going to happen _now, at once_, and that _no force could stop it_.', 'The deserted street lamps gleamed sullenly in the snowy darkness like torches at a funeral.', 'The snow drifted under my great-coat, under my coat, under my cravat, and melted there.', 'I did not wrap myself up—all was lost, anyway.', 'At last we arrived.', 'I jumped out, almost unconscious, ran up the steps and began knocking and kicking at the door.', 'I felt fearfully weak, particularly in my legs and knees.', 'The door was opened quickly as though they knew I was coming.', 'As a fact, Simonov had warned them that perhaps another gentleman would arrive, and this was a place in which one had to give notice and to observe certain precautions.', 'It was one of those “millinery establishments” which were abolished by the police a good time ago.', 'By day it really was a shop; but at night, if one had an introduction, one might visit it for other purposes.', 'I walked rapidly through the dark shop into the familiar drawing-room, where there was only one candle burning, and stood still in amazement: there was no one there.', '“Where are they?” I asked somebody.', 'But by now, of course, they had separated.', 'Before me was standing a person with a stupid smile, the “madam” herself, who had seen me before.', 'A minute later a door opened and another person came in.', 'Taking no notice of anything I strode about the room, and, I believe, I talked to myself.', 'I felt as though I had been saved from death and was conscious of this, joyfully, all over: I should have given that slap, I should certainly, certainly have given it!', 'But now they were not here and ... everything had vanished and changed!', 'I looked round.', 'I could not realise my condition yet.', 'I looked mechanically at the girl who had come in: and had a glimpse of a fresh, young, rather pale face, with straight, dark eyebrows, and with grave, as it were wondering, eyes that attracted me at once; I should have hated her if she had been smiling.', 'I began looking at her more intently and, as it were, with effort.', 'I had not fully collected my thoughts.', 'There was something simple and good-natured in her face, but something strangely grave.', 'I am sure that this stood in her way here, and no one of those fools had noticed her.', 'She could not, however, have been called a beauty, though she was tall, strong-looking, and well built.', 'She was very simply dressed.', 'Something loathsome stirred within me.', 'I went straight up to her.', 'I chanced to look into the glass.', 'My harassed face struck me as revolting in the extreme, pale, angry, abject, with dishevelled hair.', '“No matter, I am glad of it,” I thought; “I am glad that I shall seem repulsive to her; I like that.” VI ... Somewhere behind a screen a clock began wheezing, as though oppressed by something, as though someone were strangling it.', 'After an unnaturally prolonged wheezing there followed a shrill, nasty, and as it were unexpectedly rapid, chime—as though someone were suddenly jumping forward.', 'It struck two.', 'I woke up, though I had indeed not been asleep but lying half-conscious.', 'It was almost completely dark in the narrow, cramped, low-pitched room, cumbered up with an enormous wardrobe and piles of cardboard boxes and all sorts of frippery and litter.', 'The candle end that had been burning on the table was going out and gave a faint flicker from time to time.', 'In a few minutes there would be complete darkness.', 'I was not long in coming to myself; everything came back to my mind at once, without an effort, as though it had been in ambush to pounce upon me again.', 'And, indeed, even while I was unconscious a point seemed continually to remain in my memory unforgotten, and round it my dreams moved drearily.', 'But strange to say, everything that had happened to me in that day seemed to me now, on waking, to be in the far, far away past, as though I had long, long ago lived all that down.', 'My head was full of fumes.', 'Something seemed to be hovering over me, rousing me, exciting me, and making me restless.', 'Misery and spite seemed surging up in me again and seeking an outlet.', 'Suddenly I saw beside me two wide open eyes scrutinising me curiously and persistently.', 'The look in those eyes was coldly detached, sullen, as it were utterly remote; it weighed upon me.', 'A grim idea came into my brain and passed all over my body, as a horrible sensation, such as one feels when one goes into a damp and mouldy cellar.', 'There was something unnatural in those two eyes, beginning to look at me only now.', 'I recalled, too, that during those two hours I had not said a single word to this creature, and had, in fact, considered it utterly superfluous; in fact, the silence had for some reason gratified me.', 'Now I suddenly realised vividly the hideous idea—revolting as a spider—of vice, which, without love, grossly and shamelessly begins with that in which true love finds its consummation.', 'For a long time we gazed at each other like that, but she did not drop her eyes before mine and her expression did not change, so that at last I felt uncomfortable.', '“What is your name?” I asked abruptly, to put an end to it.', '“Liza,” she answered almost in a whisper, but somehow far from graciously, and she turned her eyes away.', 'I was silent.', '“What weather!', 'The snow ... it’s disgusting!” I said, almost to myself, putting my arm under my head despondently, and gazing at the ceiling.', 'She made no answer.', 'This was horrible.', '“Have you always lived in Petersburg?” I asked a minute later, almost angrily, turning my head slightly towards her.', '“No.” “Where do you come from?” “From Riga,” she answered reluctantly.', '“Are you a German?” “No, Russian.” “Have you been here long?” “Where?” “In this house?” “A fortnight.” She spoke more and more jerkily.', 'The candle went out; I could no longer distinguish her face.', '“Have you a father and mother?” “Yes ... no ...', 'I have.” “Where are they?” “There ... in Riga.” “What are they?” “Oh, nothing.” “Nothing?', 'Why, what class are they?” “Tradespeople.” “Have you always lived with them?” “Yes.” “How old are you?” “Twenty.” “Why did you leave them?” “Oh, for no reason.” That answer meant “Let me alone; I feel sick, sad.” We were silent.', 'God knows why I did not go away.', 'I felt myself more and more sick and dreary.', 'The images of the previous day began of themselves, apart from my will, flitting through my memory in confusion.', 'I suddenly recalled something I had seen that morning when, full of anxious thoughts, I was hurrying to the office.', '“I saw them carrying a coffin out yesterday and they nearly dropped it,” I suddenly said aloud, not that I desired to open the conversation, but as it were by accident.', '“A coffin?” “Yes, in the Haymarket; they were bringing it up out of a cellar.” “From a cellar?” “Not from a cellar, but a basement.', 'Oh, you know ... down below ... from a house of ill-fame.', 'It was filthy all round ... Egg-shells, litter ... a stench.', 'It was loathsome.” Silence.', '“A nasty day to be buried,” I began, simply to avoid being silent.', '“Nasty, in what way?” “The snow, the wet.” (I yawned.)', '“It makes no difference,” she said suddenly, after a brief silence.', '“No, it’s horrid.” (I yawned again).', '“The gravediggers must have sworn at getting drenched by the snow.', 'And there must have been water in the grave.” “Why water in the grave?” she asked, with a sort of curiosity, but speaking even more harshly and abruptly than before.', 'I suddenly began to feel provoked.', '“Why, there must have been water at the bottom a foot deep.', 'You can’t dig a dry grave in Volkovo Cemetery.” “Why?” “Why?', 'Why, the place is waterlogged.', 'It’s a regular marsh.', 'So they bury them in water.', 'I’ve seen it myself ... many times.” (I had never seen it once, indeed I had never been in Volkovo, and had only heard stories of it.)', '“Do you mean to say, you don’t mind how you die?” “But why should I die?” she answered, as though defending herself.', '“Why, some day you will die, and you will die just the same as that dead woman.', 'She was ... a girl like you.', 'She died of consumption.” “A wench would have died in hospital ...” (She knows all about it already: she said “wench,” not “girl.”) “She was in debt to her madam,” I retorted, more and more provoked by the discussion; “and went on earning money for her up to the end, though she was in consumption.', 'Some sledge-drivers standing by were talking about her to some soldiers and telling them so.', 'No doubt they knew her.', 'They were laughing.', 'They were going to meet in a pot-house to drink to her memory.” A great deal of this was my invention.', 'Silence followed, profound silence.', 'She did not stir.', '“And is it better to die in a hospital?” “Isn’t it just the same?', 'Besides, why should I die?” she added irritably.', '“If not now, a little later.” “Why a little later?” “Why, indeed?', 'Now you are young, pretty, fresh, you fetch a high price.', 'But after another year of this life you will be very different—you will go off.” “In a year?” “Anyway, in a year you will be worth less,” I continued malignantly.', '“You will go from here to something lower, another house; a year later—to a third, lower and lower, and in seven years you will come to a basement in the Haymarket.', 'That will be if you were lucky.', 'But it would be much worse if you got some disease, consumption, say ... and caught a chill, or something or other.', 'It’s not easy to get over an illness in your way of life.', 'If you catch anything you may not get rid of it.', 'And so you would die.” “Oh, well, then I shall die,” she answered, quite vindictively, and she made a quick movement.', '“But one is sorry.” “Sorry for whom?” “Sorry for life.” Silence.', '“Have you been engaged to be married?', 'Eh?” “What’s that to you?” “Oh, I am not cross-examining you.', 'It’s nothing to me.', 'Why are you so cross?', 'Of course you may have had your own troubles.', 'What is it to me?', 'It’s simply that I felt sorry.” “Sorry for whom?” “Sorry for you.” “No need,” she whispered hardly audibly, and again made a faint movement.', 'That incensed me at once.', 'What!', 'I was so gentle with her, and she.... “Why, do you think that you are on the right path?” “I don’t think anything.” “That’s what’s wrong, that you don’t think.', 'Realise it while there is still time.', 'There still is time.', 'You are still young, good-looking; you might love, be married, be happy....” “Not all married women are happy,” she snapped out in the rude abrupt tone she had used at first.', '“Not all, of course, but anyway it is much better than the life here.', 'Infinitely better.', 'Besides, with love one can live even without happiness.', 'Even in sorrow life is sweet; life is sweet, however one lives.', 'But here what is there but ... foulness?', 'Phew!” I turned away with disgust; I was no longer reasoning coldly.', 'I began to feel myself what I was saying and warmed to the subject.', 'I was already longing to expound the cherished ideas I had brooded over in my corner.', 'Something suddenly flared up in me.', 'An object had appeared before me.', '“Never mind my being here, I am not an example for you.', 'I am, perhaps, worse than you are.', 'I was drunk when I came here, though,” I hastened, however, to say in self-defence.', '“Besides, a man is no example for a woman.', 'It’s a different thing.', 'I may degrade and defile myself, but I am not anyone’s slave.', 'I come and go, and that’s an end of it.', 'I shake it off, and I am a different man.', 'But you are a slave from the start.', 'Yes, a slave!', 'You give up everything, your whole freedom.', 'If you want to break your chains afterwards, you won’t be able to; you will be more and more fast in the snares.', 'It is an accursed bondage.', 'I know it.', 'I won’t speak of anything else, maybe you won’t understand, but tell me: no doubt you are in debt to your madam?', 'There, you see,” I added, though she made no answer, but only listened in silence, entirely absorbed, “that’s a bondage for you!', 'You will never buy your freedom.', 'They will see to that.', 'It’s like selling your soul to the devil.... And besides ... perhaps, I too, am just as unlucky—how do you know—and wallow in the mud on purpose, out of misery?', 'You know, men take to drink from grief; well, maybe I am here from grief.', 'Come, tell me, what is there good here?', 'Here you and I ... came together ... just now and did not say one word to one another all the time, and it was only afterwards you began staring at me like a wild creature, and I at you.', 'Is that loving?', 'Is that how one human being should meet another?', 'It’s hideous, that’s what it is!” “Yes!” she assented sharply and hurriedly.', 'I was positively astounded by the promptitude of this “Yes.” So the same thought may have been straying through her mind when she was staring at me just before.', 'So she, too, was capable of certain thoughts?', '“Damn it all, this was interesting, this was a point of likeness!” I thought, almost rubbing my hands.', 'And indeed it’s easy to turn a young soul like that!', 'It was the exercise of my power that attracted me most.', 'She turned her head nearer to me, and it seemed to me in the darkness that she propped herself on her arm.', 'Perhaps she was scrutinising me.', 'How I regretted that I could not see her eyes.', 'I heard her deep breathing.', '“Why have you come here?” I asked her, with a note of authority already in my voice.', '“Oh, I don’t know.” “But how nice it would be to be living in your father’s house!', 'It’s warm and free; you have a home of your own.” “But what if it’s worse than this?” “I must take the right tone,” flashed through my mind.', '“I may not get far with sentimentality.” But it was only a momentary thought.', 'I swear she really did interest me.', 'Besides, I was exhausted and moody.', 'And cunning so easily goes hand-in-hand with feeling.', '“Who denies it!” I hastened to answer.', '“Anything may happen.', 'I am convinced that someone has wronged you, and that you are more sinned against than sinning.', 'Of course, I know nothing of your story, but it’s not likely a girl like you has come here of her own inclination....” “A girl like me?” she whispered, hardly audibly; but I heard it.', 'Damn it all, I was flattering her.', 'That was horrid.', 'But perhaps it was a good thing.... She was silent.', '“See, Liza, I will tell you about myself.', 'If I had had a home from childhood, I shouldn’t be what I am now.', 'I often think that.', 'However bad it may be at home, anyway they are your father and mother, and not enemies, strangers.', 'Once a year at least, they’ll show their love of you.', 'Anyway, you know you are at home.', 'I grew up without a home; and perhaps that’s why I’ve turned so ... unfeeling.” I waited again.', '“Perhaps she doesn’t understand,” I thought, “and, indeed, it is absurd—it’s moralising.” “If I were a father and had a daughter, I believe I should love my daughter more than my sons, really,” I began indirectly, as though talking of something else, to distract her attention.', 'I must confess I blushed.', '“Why so?” she asked.', 'Ah!', 'so she was listening!', '“I don’t know, Liza.', 'I knew a father who was a stern, austere man, but used to go down on his knees to his daughter, used to kiss her hands, her feet, he couldn’t make enough of her, really.', 'When she danced at parties he used to stand for five hours at a stretch, gazing at her.', 'He was mad over her: I understand that!', 'She would fall asleep tired at night, and he would wake to kiss her in her sleep and make the sign of the cross over her.', 'He would go about in a dirty old coat, he was stingy to everyone else, but would spend his last penny for her, giving her expensive presents, and it was his greatest delight when she was pleased with what he gave her.', 'Fathers always love their daughters more than the mothers do.', 'Some girls live happily at home!', 'And I believe I should never let my daughters marry.” “What next?” she said, with a faint smile.', '“I should be jealous, I really should.', 'To think that she should kiss anyone else!', 'That she should love a stranger more than her father!', 'It’s painful to imagine it.', 'Of course, that’s all nonsense, of course every father would be reasonable at last.', 'But I believe before I should let her marry, I should worry myself to death; I should find fault with all her suitors.', 'But I should end by letting her marry whom she herself loved.', 'The one whom the daughter loves always seems the worst to the father, you know.', 'That is always so.', 'So many family troubles come from that.” “Some are glad to sell their daughters, rather than marrying them honourably.” Ah, so that was it!', '“Such a thing, Liza, happens in those accursed families in which there is neither love nor God,” I retorted warmly, “and where there is no love, there is no sense either.', 'There are such families, it’s true, but I am not speaking of them.', 'You must have seen wickedness in your own family, if you talk like that.', 'Truly, you must have been unlucky.', 'H’m!', '... that sort of thing mostly comes about through poverty.” “And is it any better with the gentry?', 'Even among the poor, honest people who live happily?” “H’m ... yes.', 'Perhaps.', 'Another thing, Liza, man is fond of reckoning up his troubles, but does not count his joys.', 'If he counted them up as he ought, he would see that every lot has enough happiness provided for it.', 'And what if all goes well with the family, if the blessing of God is upon it, if the husband is a good one, loves you, cherishes you, never leaves you!', 'There is happiness in such a family!', 'Even sometimes there is happiness in the midst of sorrow; and indeed sorrow is everywhere.', 'If you marry _you will find out for yourself_.', 'But think of the first years of married life with one you love: what happiness, what happiness there sometimes is in it!', 'And indeed it’s the ordinary thing.', 'In those early days even quarrels with one’s husband end happily.', 'Some women get up quarrels with their husbands just because they love them.', 'Indeed, I knew a woman like that: she seemed to say that because she loved him, she would torment him and make him feel it.', 'You know that you may torment a man on purpose through love.', 'Women are particularly given to that, thinking to themselves ‘I will love him so, I will make so much of him afterwards, that it’s no sin to torment him a little now.’ And all in the house rejoice in the sight of you, and you are happy and gay and peaceful and honourable.... Then there are some women who are jealous.', 'If he went off anywhere—I knew one such woman, she couldn’t restrain herself, but would jump up at night and run off on the sly to find out where he was, whether he was with some other woman.', 'That’s a pity.', 'And the woman knows herself it’s wrong, and her heart fails her and she suffers, but she loves—it’s all through love.', 'And how sweet it is to make up after quarrels, to own herself in the wrong or to forgive him!', 'And they both are so happy all at once—as though they had met anew, been married over again; as though their love had begun afresh.', 'And no one, no one should know what passes between husband and wife if they love one another.', 'And whatever quarrels there may be between them they ought not to call in their own mother to judge between them and tell tales of one another.', 'They are their own judges.', 'Love is a holy mystery and ought to be hidden from all other eyes, whatever happens.', 'That makes it holier and better.', 'They respect one another more, and much is built on respect.', 'And if once there has been love, if they have been married for love, why should love pass away?', 'Surely one can keep it!', 'It is rare that one cannot keep it.', 'And if the husband is kind and straightforward, why should not love last?', 'The first phase of married love will pass, it is true, but then there will come a love that is better still.', 'Then there will be the union of souls, they will have everything in common, there will be no secrets between them.', 'And once they have children, the most difficult times will seem to them happy, so long as there is love and courage.', 'Even toil will be a joy, you may deny yourself bread for your children and even that will be a joy, They will love you for it afterwards; so you are laying by for your future.', 'As the children grow up you feel that you are an example, a support for them; that even after you die your children will always keep your thoughts and feelings, because they have received them from you, they will take on your semblance and likeness.', 'So you see this is a great duty.', 'How can it fail to draw the father and mother nearer?', 'People say it’s a trial to have children.', 'Who says that?', 'It is heavenly happiness!', 'Are you fond of little children, Liza?', 'I am awfully fond of them.', 'You know—a little rosy baby boy at your bosom, and what husband’s heart is not touched, seeing his wife nursing his child!', 'A plump little rosy baby, sprawling and snuggling, chubby little hands and feet, clean tiny little nails, so tiny that it makes one laugh to look at them; eyes that look as if they understand everything.', 'And while it sucks it clutches at your bosom with its little hand, plays.', 'When its father comes up, the child tears itself away from the bosom, flings itself back, looks at its father, laughs, as though it were fearfully funny, and falls to sucking again.', 'Or it will bite its mother’s breast when its little teeth are coming, while it looks sideways at her with its little eyes as though to say, ‘Look, I am biting!’ Is not all that happiness when they are the three together, husband, wife and child?', 'One can forgive a great deal for the sake of such moments.', 'Yes, Liza, one must first learn to live oneself before one blames others!” “It’s by pictures, pictures like that one must get at you,” I thought to myself, though I did speak with real feeling, and all at once I flushed crimson.', '“What if she were suddenly to burst out laughing, what should I do then?” That idea drove me to fury.', 'Towards the end of my speech I really was excited, and now my vanity was somehow wounded.', 'The silence continued.', 'I almost nudged her.', '“Why are you—” she began and stopped.', 'But I understood: there was a quiver of something different in her voice, not abrupt, harsh and unyielding as before, but something soft and shamefaced, so shamefaced that I suddenly felt ashamed and guilty.', '“What?” I asked, with tender curiosity.', '“Why, you...” “What?” “Why, you ... speak somehow like a book,” she said, and again there was a note of irony in her voice.', 'That remark sent a pang to my heart.', 'It was not what I was expecting.', 'I did not understand that she was hiding her feelings under irony, that this is usually the last refuge of modest and chaste-souled people when the privacy of their soul is coarsely and intrusively invaded, and that their pride makes them refuse to surrender till the last moment and shrink from giving expression to their feelings before you.', 'I ought to have guessed the truth from the timidity with which she had repeatedly approached her sarcasm, only bringing herself to utter it at last with an effort.', 'But I did not guess, and an evil feeling took possession of me.', '“Wait a bit!” I thought.', 'VII “Oh, hush, Liza!', 'How can you talk about being like a book, when it makes even me, an outsider, feel sick?', 'Though I don’t look at it as an outsider, for, indeed, it touches me to the heart.... Is it possible, is it possible that you do not feel sick at being here yourself?', 'Evidently habit does wonders!', 'God knows what habit can do with anyone.', 'Can you seriously think that you will never grow old, that you will always be good-looking, and that they will keep you here for ever and ever?', 'I say nothing of the loathsomeness of the life here....', 'Though let me tell you this about it—about your present life, I mean; here though you are young now, attractive, nice, with soul and feeling, yet you know as soon as I came to myself just now I felt at once sick at being here with you!', 'One can only come here when one is drunk.', 'But if you were anywhere else, living as good people live, I should perhaps be more than attracted by you, should fall in love with you, should be glad of a look from you, let alone a word; I should hang about your door, should go down on my knees to you, should look upon you as my betrothed and think it an honour to be allowed to.', 'I should not dare to have an impure thought about you.', 'But here, you see, I know that I have only to whistle and you have to come with me whether you like it or not.', 'I don’t consult your wishes, but you mine.', 'The lowest labourer hires himself as a workman, but he doesn’t make a slave of himself altogether; besides, he knows that he will be free again presently.', 'But when are you free?', 'Only think what you are giving up here?', 'What is it you are making a slave of?', 'It is your soul, together with your body; you are selling your soul which you have no right to dispose of!', 'You give your love to be outraged by every drunkard!', 'Love!', 'But that’s everything, you know, it’s a priceless diamond, it’s a maiden’s treasure, love—why, a man would be ready to give his soul, to face death to gain that love.', 'But how much is your love worth now?', 'You are sold, all of you, body and soul, and there is no need to strive for love when you can have everything without love.', 'And you know there is no greater insult to a girl than that, do you understand?', 'To be sure, I have heard that they comfort you, poor fools, they let you have lovers of your own here.', 'But you know that’s simply a farce, that’s simply a sham, it’s just laughing at you, and you are taken in by it!', 'Why, do you suppose he really loves you, that lover of yours?', 'I don’t believe it.', 'How can he love you when he knows you may be called away from him any minute?', 'He would be a low fellow if he did!', 'Will he have a grain of respect for you?', 'What have you in common with him?', 'He laughs at you and robs you—that is all his love amounts to!', 'You are lucky if he does not beat you.', 'Very likely he does beat you, too.', 'Ask him, if you have got one, whether he will marry you.', 'He will laugh in your face, if he doesn’t spit in it or give you a blow—though maybe he is not worth a bad halfpenny himself.', 'And for what have you ruined your life, if you come to think of it?', 'For the coffee they give you to drink and the plentiful meals?', 'But with what object are they feeding you up?', 'An honest girl couldn’t swallow the food, for she would know what she was being fed for.', 'You are in debt here, and, of course, you will always be in debt, and you will go on in debt to the end, till the visitors here begin to scorn you.', 'And that will soon happen, don’t rely upon your youth—all that flies by express train here, you know.', 'You will be kicked out.', 'And not simply kicked out; long before that she’ll begin nagging at you, scolding you, abusing you, as though you had not sacrificed your health for her, had not thrown away your youth and your soul for her benefit, but as though you had ruined her, beggared her, robbed her.', 'And don’t expect anyone to take your part: the others, your companions, will attack you, too, win her favour, for all are in slavery here, and have lost all conscience and pity here long ago.', 'They have become utterly vile, and nothing on earth is viler, more loathsome, and more insulting than their abuse.', 'And you are laying down everything here, unconditionally, youth and health and beauty and hope, and at twenty-two you will look like a woman of five-and-thirty, and you will be lucky if you are not diseased, pray to God for that!', 'No doubt you are thinking now that you have a gay time and no work to do!', 'Yet there is no work harder or more dreadful in the world or ever has been.', 'One would think that the heart alone would be worn out with tears.', 'And you won’t dare to say a word, not half a word when they drive you away from here; you will go away as though you were to blame.', 'You will change to another house, then to a third, then somewhere else, till you come down at last to the Haymarket.', 'There you will be beaten at every turn; that is good manners there, the visitors don’t know how to be friendly without beating you.', 'You don’t believe that it is so hateful there?', 'Go and look for yourself some time, you can see with your own eyes.', 'Once, one New Year’s Day, I saw a woman at a door.', 'They had turned her out as a joke, to give her a taste of the frost because she had been crying so much, and they shut the door behind her.', 'At nine o’clock in the morning she was already quite drunk, dishevelled, half-naked, covered with bruises, her face was powdered, but she had a black-eye, blood was trickling from her nose and her teeth; some cabman had just given her a drubbing.', 'She was sitting on the stone steps, a salt fish of some sort was in her hand; she was crying, wailing something about her luck and beating with the fish on the steps, and cabmen and drunken soldiers were crowding in the doorway taunting her.', 'You don’t believe that you will ever be like that?', 'I should be sorry to believe it, too, but how do you know; maybe ten years, eight years ago that very woman with the salt fish came here fresh as a cherub, innocent, pure, knowing no evil, blushing at every word.', 'Perhaps she was like you, proud, ready to take offence, not like the others; perhaps she looked like a queen, and knew what happiness was in store for the man who should love her and whom she should love.', 'Do you see how it ended?', 'And what if at that very minute when she was beating on the filthy steps with that fish, drunken and dishevelled—what if at that very minute she recalled the pure early days in her father’s house, when she used to go to school and the neighbour’s son watched for her on the way, declaring that he would love her as long as he lived, that he would devote his life to her, and when they vowed to love one another for ever and be married as soon as they were grown up!', 'No, Liza, it would be happy for you if you were to die soon of consumption in some corner, in some cellar like that woman just now.', 'In the hospital, do you say?', 'You will be lucky if they take you, but what if you are still of use to the madam here?', 'Consumption is a queer disease, it is not like fever.', 'The patient goes on hoping till the last minute and says he is all right.', 'He deludes himself And that just suits your madam.', 'Don’t doubt it, that’s how it is; you have sold your soul, and what is more you owe money, so you daren’t say a word.', 'But when you are dying, all will abandon you, all will turn away from you, for then there will be nothing to get from you.', 'What’s more, they will reproach you for cumbering the place, for being so long over dying.', 'However you beg you won’t get a drink of water without abuse: ‘Whenever are you going off, you nasty hussy, you won’t let us sleep with your moaning, you make the gentlemen sick.’ That’s true, I have heard such things said myself.', 'They will thrust you dying into the filthiest corner in the cellar—in the damp and darkness; what will your thoughts be, lying there alone?', 'When you die, strange hands will lay you out, with grumbling and impatience; no one will bless you, no one will sigh for you, they only want to get rid of you as soon as may be; they will buy a coffin, take you to the grave as they did that poor woman today, and celebrate your memory at the tavern.', 'In the grave, sleet, filth, wet snow—no need to put themselves out for you—‘Let her down, Vanuha; it’s just like her luck—even here, she is head-foremost, the hussy.', 'Shorten the cord, you rascal.’ ‘It’s all right as it is.’ ‘All right, is it?', 'Why, she’s on her side!', 'She was a fellow-creature, after all!', 'But, never mind, throw the earth on her.’ And they won’t care to waste much time quarrelling over you.', 'They will scatter the wet blue clay as quick as they can and go off to the tavern ... and there your memory on earth will end; other women have children to go to their graves, fathers, husbands.', 'While for you neither tear, nor sigh, nor remembrance; no one in the whole world will ever come to you, your name will vanish from the face of the earth—as though you had never existed, never been born at all!', 'Nothing but filth and mud, however you knock at your coffin lid at night, when the dead arise, however you cry: ‘Let me out, kind people, to live in the light of day!', 'My life was no life at all; my life has been thrown away like a dish-clout; it was drunk away in the tavern at the Haymarket; let me out, kind people, to live in the world again.’” And I worked myself up to such a pitch that I began to have a lump in my throat myself, and ... and all at once I stopped, sat up in dismay and, bending over apprehensively, began to listen with a beating heart.', 'I had reason to be troubled.', 'I had felt for some time that I was turning her soul upside down and rending her heart, and—and the more I was convinced of it, the more eagerly I desired to gain my object as quickly and as effectually as possible.', 'It was the exercise of my skill that carried me away; yet it was not merely sport....', 'I knew I was speaking stiffly, artificially, even bookishly, in fact, I could not speak except “like a book.” But that did not trouble me: I knew, I felt that I should be understood and that this very bookishness might be an assistance.', 'But now, having attained my effect, I was suddenly panic-stricken.', 'Never before had I witnessed such despair!', 'She was lying on her face, thrusting her face into the pillow and clutching it in both hands.', 'Her heart was being torn.', 'Her youthful body was shuddering all over as though in convulsions.', 'Suppressed sobs rent her bosom and suddenly burst out in weeping and wailing, then she pressed closer into the pillow: she did not want anyone here, not a living soul, to know of her anguish and her tears.', 'She bit the pillow, bit her hand till it bled (I saw that afterwards), or, thrusting her fingers into her dishevelled hair, seemed rigid with the effort of restraint, holding her breath and clenching her teeth.', 'I began saying something, begging her to calm herself, but felt that I did not dare; and all at once, in a sort of cold shiver, almost in terror, began fumbling in the dark, trying hurriedly to get dressed to go.', 'It was dark; though I tried my best I could not finish dressing quickly.', 'Suddenly I felt a box of matches and a candlestick with a whole candle in it.', 'As soon as the room was lighted up, Liza sprang up, sat up in bed, and with a contorted face, with a half insane smile, looked at me almost senselessly.', 'I sat down beside her and took her hands; she came to herself, made an impulsive movement towards me, would have caught hold of me, but did not dare, and slowly bowed her head before me.', '“Liza, my dear, I was wrong ... forgive me, my dear,” I began, but she squeezed my hand in her fingers so tightly that I felt I was saying the wrong thing and stopped.', '“This is my address, Liza, come to me.” “I will come,” she answered resolutely, her head still bowed.', '“But now I am going, good-bye ... till we meet again.” I got up; she, too, stood up and suddenly flushed all over, gave a shudder, snatched up a shawl that was lying on a chair and muffled herself in it to her chin.', 'As she did this she gave another sickly smile, blushed and looked at me strangely.', 'I felt wretched; I was in haste to get away—to disappear.', '“Wait a minute,” she said suddenly, in the passage just at the doorway, stopping me with her hand on my overcoat.', 'She put down the candle in hot haste and ran off; evidently she had thought of something or wanted to show me something.', 'As she ran away she flushed, her eyes shone, and there was a smile on her lips—what was the meaning of it?', 'Against my will I waited: she came back a minute later with an expression that seemed to ask forgiveness for something.', 'In fact, it was not the same face, not the same look as the evening before: sullen, mistrustful and obstinate.', 'Her eyes now were imploring, soft, and at the same time trustful, caressing, timid.', 'The expression with which children look at people they are very fond of, of whom they are asking a favour.', 'Her eyes were a light hazel, they were lovely eyes, full of life, and capable of expressing love as well as sullen hatred.', 'Making no explanation, as though I, as a sort of higher being, must understand everything without explanations, she held out a piece of paper to me.', 'Her whole face was positively beaming at that instant with naive, almost childish, triumph.', 'I unfolded it.', 'It was a letter to her from a medical student or someone of that sort—a very high-flown and flowery, but extremely respectful, love-letter.', 'I don’t recall the words now, but I remember well that through the high-flown phrases there was apparent a genuine feeling, which cannot be feigned.', 'When I had finished reading it I met her glowing, questioning, and childishly impatient eyes fixed upon me.', 'She fastened her eyes upon my face and waited impatiently for what I should say.', 'In a few words, hurriedly, but with a sort of joy and pride, she explained to me that she had been to a dance somewhere in a private house, a family of “very nice people, _who knew nothing_, absolutely nothing, for she had only come here so lately and it had all happened ... and she hadn’t made up her mind to stay and was certainly going away as soon as she had paid her debt...” and at that party there had been the student who had danced with her all the evening.', 'He had talked to her, and it turned out that he had known her in old days at Riga when he was a child, they had played together, but a very long time ago—and he knew her parents, but _about this_ he knew nothing, nothing whatever, and had no suspicion!', 'And the day after the dance (three days ago) he had sent her that letter through the friend with whom she had gone to the party ... and ... well, that was all.', 'She dropped her shining eyes with a sort of bashfulness as she finished.', 'The poor girl was keeping that student’s letter as a precious treasure, and had run to fetch it, her only treasure, because she did not want me to go away without knowing that she, too, was honestly and genuinely loved; that she, too, was addressed respectfully.', 'No doubt that letter was destined to lie in her box and lead to nothing.', 'But none the less, I am certain that she would keep it all her life as a precious treasure, as her pride and justification, and now at such a minute she had thought of that letter and brought it with naive pride to raise herself in my eyes that I might see, that I, too, might think well of her.', 'I said nothing, pressed her hand and went out.', 'I so longed to get away ...', 'I walked all the way home, in spite of the fact that the melting snow was still falling in heavy flakes.', 'I was exhausted, shattered, in bewilderment.', 'But behind the bewilderment the truth was already gleaming.', 'The loathsome truth.', 'VIII It was some time, however, before I consented to recognise that truth.', 'Waking up in the morning after some hours of heavy, leaden sleep, and immediately realising all that had happened on the previous day, I was positively amazed at my last night’s _sentimentality_ with Liza, at all those “outcries of horror and pity.” “To think of having such an attack of womanish hysteria, pah!” I concluded.', 'And what did I thrust my address upon her for?', 'What if she comes?', 'Let her come, though; it doesn’t matter....', 'But _obviously_, that was not now the chief and the most important matter: I had to make haste and at all costs save my reputation in the eyes of Zverkov and Simonov as quickly as possible; that was the chief business.', 'And I was so taken up that morning that I actually forgot all about Liza.', 'First of all I had at once to repay what I had borrowed the day before from Simonov.', 'I resolved on a desperate measure: to borrow fifteen roubles straight off from Anton Antonitch.', 'As luck would have it he was in the best of humours that morning, and gave it to me at once, on the first asking.', 'I was so delighted at this that, as I signed the IOU with a swaggering air, I told him casually that the night before “I had been keeping it up with some friends at the Hôtel de Paris; we were giving a farewell party to a comrade, in fact, I might say a friend of my childhood, and you know—a desperate rake, fearfully spoilt—of course, he belongs to a good family, and has considerable means, a brilliant career; he is witty, charming, a regular Lovelace, you understand; we drank an extra ‘half-dozen’ and ...” And it went off all right; all this was uttered very easily, unconstrainedly and complacently.', 'On reaching home I promptly wrote to Simonov.', 'To this hour I am lost in admiration when I recall the truly gentlemanly, good-humoured, candid tone of my letter.', 'With tact and good-breeding, and, above all, entirely without superfluous words, I blamed myself for all that had happened.', 'I defended myself, “if I really may be allowed to defend myself,” by alleging that being utterly unaccustomed to wine, I had been intoxicated with the first glass, which I said, I had drunk before they arrived, while I was waiting for them at the Hôtel de Paris between five and six o’clock.', 'I begged Simonov’s pardon especially; I asked him to convey my explanations to all the others, especially to Zverkov, whom “I seemed to remember as though in a dream” I had insulted.', 'I added that I would have called upon all of them myself, but my head ached, and besides I had not the face to.', 'I was particularly pleased with a certain lightness, almost carelessness (strictly within the bounds of politeness, however), which was apparent in my style, and better than any possible arguments, gave them at once to understand that I took rather an independent view of “all that unpleasantness last night”; that I was by no means so utterly crushed as you, my friends, probably imagine; but on the contrary, looked upon it as a gentleman serenely respecting himself should look upon it.', '“On a young hero’s past no censure is cast!” “There is actually an aristocratic playfulness about it!” I thought admiringly, as I read over the letter.', '“And it’s all because I am an intellectual and cultivated man!', 'Another man in my place would not have known how to extricate himself, but here I have got out of it and am as jolly as ever again, and all because I am ‘a cultivated and educated man of our day.’ And, indeed, perhaps, everything was due to the wine yesterday.', 'H’m!” ... No, it was not the wine.', 'I did not drink anything at all between five and six when I was waiting for them.', 'I had lied to Simonov; I had lied shamelessly; and indeed I wasn’t ashamed now....', 'Hang it all though, the great thing was that I was rid of it.', 'I put six roubles in the letter, sealed it up, and asked Apollon to take it to Simonov.', 'When he learned that there was money in the letter, Apollon became more respectful and agreed to take it.', 'Towards evening I went out for a walk.', 'My head was still aching and giddy after yesterday.', 'But as evening came on and the twilight grew denser, my impressions and, following them, my thoughts, grew more and more different and confused.', 'Something was not dead within me, in the depths of my heart and conscience it would not die, and it showed itself in acute depression.', 'For the most part I jostled my way through the most crowded business streets, along Myeshtchansky Street, along Sadovy Street and in Yusupov Garden.', 'I always liked particularly sauntering along these streets in the dusk, just when there were crowds of working people of all sorts going home from their daily work, with faces looking cross with anxiety.', 'What I liked was just that cheap bustle, that bare prose.', 'On this occasion the jostling of the streets irritated me more than ever, I could not make out what was wrong with me, I could not find the clue, something seemed rising up continually in my soul, painfully, and refusing to be appeased.', 'I returned home completely upset, it was just as though some crime were lying on my conscience.', 'The thought that Liza was coming worried me continually.', 'It seemed queer to me that of all my recollections of yesterday this tormented me, as it were, especially, as it were, quite separately.', 'Everything else I had quite succeeded in forgetting by the evening; I dismissed it all and was still perfectly satisfied with my letter to Simonov.', 'But on this point I was not satisfied at all.', 'It was as though I were worried only by Liza.', '“What if she comes,” I thought incessantly, “well, it doesn’t matter, let her come!', 'H’m!', 'it’s horrid that she should see, for instance, how I live.', 'Yesterday I seemed such a hero to her, while now, h’m!', 'It’s horrid, though, that I have let myself go so, the room looks like a beggar’s.', 'And I brought myself to go out to dinner in such a suit!', 'And my American leather sofa with the stuffing sticking out.', 'And my dressing-gown, which will not cover me, such tatters, and she will see all this and she will see Apollon.', 'That beast is certain to insult her.', 'He will fasten upon her in order to be rude to me.', 'And I, of course, shall be panic-stricken as usual, I shall begin bowing and scraping before her and pulling my dressing-gown round me, I shall begin smiling, telling lies.', 'Oh, the beastliness!', 'And it isn’t the beastliness of it that matters most!', 'There is something more important, more loathsome, viler!', 'Yes, viler!', 'And to put on that dishonest lying mask again!', '...” When I reached that thought I fired up all at once.', '“Why dishonest?', 'How dishonest?', 'I was speaking sincerely last night.', 'I remember there was real feeling in me, too.', 'What I wanted was to excite an honourable feeling in her....', 'Her crying was a good thing, it will have a good effect.” Yet I could not feel at ease.', 'All that evening, even when I had come back home, even after nine o’clock, when I calculated that Liza could not possibly come, still she haunted me, and what was worse, she came back to my mind always in the same position.', 'One moment out of all that had happened last night stood vividly before my imagination; the moment when I struck a match and saw her pale, distorted face, with its look of torture.', 'And what a pitiful, what an unnatural, what a distorted smile she had at that moment!', 'But I did not know then, that fifteen years later I should still in my imagination see Liza, always with the pitiful, distorted, inappropriate smile which was on her face at that minute.', 'Next day I was ready again to look upon it all as nonsense, due to over-excited nerves, and, above all, as _exaggerated_.', 'I was always conscious of that weak point of mine, and sometimes very much afraid of it.', '“I exaggerate everything, that is where I go wrong,” I repeated to myself every hour.', 'But, however, “Liza will very likely come all the same,” was the refrain with which all my reflections ended.', 'I was so uneasy that I sometimes flew into a fury: “She’ll come, she is certain to come!” I cried, running about the room, “if not today, she will come tomorrow; she’ll find me out!', 'The damnable romanticism of these pure hearts!', 'Oh, the vileness—oh, the silliness—oh, the stupidity of these ‘wretched sentimental souls!’ Why, how fail to understand?', 'How could one fail to understand?', '...” But at this point I stopped short, and in great confusion, indeed.', 'And how few, how few words, I thought, in passing, were needed; how little of the idyllic (and affectedly, bookishly, artificially idyllic too) had sufficed to turn a whole human life at once according to my will.', 'That’s virginity, to be sure!', 'Freshness of soil!', 'At times a thought occurred to me, to go to her, “to tell her all,” and beg her not to come to me.', 'But this thought stirred such wrath in me that I believed I should have crushed that “damned” Liza if she had chanced to be near me at the time.', 'I should have insulted her, have spat at her, have turned her out, have struck her!', 'One day passed, however, another and another; she did not come and I began to grow calmer.', 'I felt particularly bold and cheerful after nine o’clock, I even sometimes began dreaming, and rather sweetly: I, for instance, became the salvation of Liza, simply through her coming to me and my talking to her....', 'I develop her, educate her.', 'Finally, I notice that she loves me, loves me passionately.', 'I pretend not to understand (I don’t know, however, why I pretend, just for effect, perhaps).', 'At last all confusion, transfigured, trembling and sobbing, she flings herself at my feet and says that I am her saviour, and that she loves me better than anything in the world.', 'I am amazed, but.... “Liza,” I say, “can you imagine that I have not noticed your love?', 'I saw it all, I divined it, but I did not dare to approach you first, because I had an influence over you and was afraid that you would force yourself, from gratitude, to respond to my love, would try to rouse in your heart a feeling which was perhaps absent, and I did not wish that ... because it would be tyranny ... it would be indelicate (in short, I launch off at that point into European, inexplicably lofty subtleties a la George Sand), but now, now you are mine, you are my creation, you are pure, you are good, you are my noble wife.', '‘Into my house come bold and free, Its rightful mistress there to be’.” Then we begin living together, go abroad and so on, and so on.', 'In fact, in the end it seemed vulgar to me myself, and I began putting out my tongue at myself.', 'Besides, they won’t let her out, “the hussy!” I thought.', 'They don’t let them go out very readily, especially in the evening (for some reason I fancied she would come in the evening, and at seven o’clock precisely).', 'Though she did say she was not altogether a slave there yet, and had certain rights; so, h’m!', 'Damn it all, she will come, she is sure to come!', 'It was a good thing, in fact, that Apollon distracted my attention at that time by his rudeness.', 'He drove me beyond all patience!', 'He was the bane of my life, the curse laid upon me by Providence.', 'We had been squabbling continually for years, and I hated him.', 'My God, how I hated him!', 'I believe I had never hated anyone in my life as I hated him, especially at some moments.', 'He was an elderly, dignified man, who worked part of his time as a tailor.', 'But for some unknown reason he despised me beyond all measure, and looked down upon me insufferably.', 'Though, indeed, he looked down upon everyone.', 'Simply to glance at that flaxen, smoothly brushed head, at the tuft of hair he combed up on his forehead and oiled with sunflower oil, at that dignified mouth, compressed into the shape of the letter V, made one feel one was confronting a man who never doubted of himself.', 'He was a pedant, to the most extreme point, the greatest pedant I had met on earth, and with that had a vanity only befitting Alexander of Macedon.', 'He was in love with every button on his coat, every nail on his fingers—absolutely in love with them, and he looked it!', 'In his behaviour to me he was a perfect tyrant, he spoke very little to me, and if he chanced to glance at me he gave me a firm, majestically self-confident and invariably ironical look that drove me sometimes to fury.', 'He did his work with the air of doing me the greatest favour, though he did scarcely anything for me, and did not, indeed, consider himself bound to do anything.', 'There could be no doubt that he looked upon me as the greatest fool on earth, and that “he did not get rid of me” was simply that he could get wages from me every month.', 'He consented to do nothing for me for seven roubles a month.', 'Many sins should be forgiven me for what I suffered from him.', 'My hatred reached such a point that sometimes his very step almost threw me into convulsions.', 'What I loathed particularly was his lisp.', 'His tongue must have been a little too long or something of that sort, for he continually lisped, and seemed to be very proud of it, imagining that it greatly added to his dignity.', 'He spoke in a slow, measured tone, with his hands behind his back and his eyes fixed on the ground.', 'He maddened me particularly when he read aloud the psalms to himself behind his partition.', 'Many a battle I waged over that reading!', 'But he was awfully fond of reading aloud in the evenings, in a slow, even, sing-song voice, as though over the dead.', 'It is interesting that that is how he has ended: he hires himself out to read the psalms over the dead, and at the same time he kills rats and makes blacking.', 'But at that time I could not get rid of him, it was as though he were chemically combined with my existence.', 'Besides, nothing would have induced him to consent to leave me.', 'I could not live in furnished lodgings: my lodging was my private solitude, my shell, my cave, in which I concealed myself from all mankind, and Apollon seemed to me, for some reason, an integral part of that flat, and for seven years I could not turn him away.', 'To be two or three days behind with his wages, for instance, was impossible.', 'He would have made such a fuss, I should not have known where to hide my head.', 'But I was so exasperated with everyone during those days, that I made up my mind for some reason and with some object to _punish_ Apollon and not to pay him for a fortnight the wages that were owing him.', 'I had for a long time—for the last two years—been intending to do this, simply in order to teach him not to give himself airs with me, and to show him that if I liked I could withhold his wages.', 'I purposed to say nothing to him about it, and was purposely silent indeed, in order to score off his pride and force him to be the first to speak of his wages.', 'Then I would take the seven roubles out of a drawer, show him I have the money put aside on purpose, but that I won’t, I won’t, I simply won’t pay him his wages, I won’t just because that is “what I wish,” because “I am master, and it is for me to decide,” because he has been disrespectful, because he has been rude; but if he were to ask respectfully I might be softened and give it to him, otherwise he might wait another fortnight, another three weeks, a whole month....', 'But angry as I was, yet he got the better of me.', 'I could not hold out for four days.', 'He began as he always did begin in such cases, for there had been such cases already, there had been attempts (and it may be observed I knew all this beforehand, I knew his nasty tactics by heart).', 'He would begin by fixing upon me an exceedingly severe stare, keeping it up for several minutes at a time, particularly on meeting me or seeing me out of the house.', 'If I held out and pretended not to notice these stares, he would, still in silence, proceed to further tortures.', 'All at once, _à propos_ of nothing, he would walk softly and smoothly into my room, when I was pacing up and down or reading, stand at the door, one hand behind his back and one foot behind the other, and fix upon me a stare more than severe, utterly contemptuous.', 'If I suddenly asked him what he wanted, he would make me no answer, but continue staring at me persistently for some seconds, then, with a peculiar compression of his lips and a most significant air, deliberately turn round and deliberately go back to his room.', 'Two hours later he would come out again and again present himself before me in the same way.', 'It had happened that in my fury I did not even ask him what he wanted, but simply raised my head sharply and imperiously and began staring back at him.', 'So we stared at one another for two minutes; at last he turned with deliberation and dignity and went back again for two hours.', 'If I were still not brought to reason by all this, but persisted in my revolt, he would suddenly begin sighing while he looked at me, long, deep sighs as though measuring by them the depths of my moral degradation, and, of course, it ended at last by his triumphing completely: I raged and shouted, but still was forced to do what he wanted.', 'This time the usual staring manoeuvres had scarcely begun when I lost my temper and flew at him in a fury.', 'I was irritated beyond endurance apart from him.', '“Stay,” I cried, in a frenzy, as he was slowly and silently turning, with one hand behind his back, to go to his room.', '“Stay!', 'Come back, come back, I tell you!” and I must have bawled so unnaturally, that he turned round and even looked at me with some wonder.', 'However, he persisted in saying nothing, and that infuriated me.', '“How dare you come and look at me like that without being sent for?', 'Answer!” After looking at me calmly for half a minute, he began turning round again.', '“Stay!” I roared, running up to him, “don’t stir!', 'There.', 'Answer, now: what did you come in to look at?” “If you have any order to give me it’s my duty to carry it out,” he answered, after another silent pause, with a slow, measured lisp, raising his eyebrows and calmly twisting his head from one side to another, all this with exasperating composure.', '“That’s not what I am asking you about, you torturer!” I shouted, turning crimson with anger.', '“I’ll tell you why you came here myself: you see, I don’t give you your wages, you are so proud you don’t want to bow down and ask for it, and so you come to punish me with your stupid stares, to worry me and you have no sus-pic-ion how stupid it is—stupid, stupid, stupid, stupid!', '...” He would have turned round again without a word, but I seized him.', '“Listen,” I shouted to him.', '“Here’s the money, do you see, here it is,” (I took it out of the table drawer); “here’s the seven roubles complete, but you are not going to have it, you ... are ... not ... going ... to ... have it until you come respectfully with bowed head to beg my pardon.', 'Do you hear?” “That cannot be,” he answered, with the most unnatural self-confidence.', '“It shall be so,” I said, “I give you my word of honour, it shall be!” “And there’s nothing for me to beg your pardon for,” he went on, as though he had not noticed my exclamations at all.', '“Why, besides, you called me a ‘torturer,’ for which I can summon you at the police-station at any time for insulting behaviour.” “Go, summon me,” I roared, “go at once, this very minute, this very second!', 'You are a torturer all the same!', 'a torturer!” But he merely looked at me, then turned, and regardless of my loud calls to him, he walked to his room with an even step and without looking round.', '“If it had not been for Liza nothing of this would have happened,” I decided inwardly.', 'Then, after waiting a minute, I went myself behind his screen with a dignified and solemn air, though my heart was beating slowly and violently.', '“Apollon,” I said quietly and emphatically, though I was breathless, “go at once without a minute’s delay and fetch the police-officer.” He had meanwhile settled himself at his table, put on his spectacles and taken up some sewing.', 'But, hearing my order, he burst into a guffaw.', '“At once, go this minute!', 'Go on, or else you can’t imagine what will happen.” “You are certainly out of your mind,” he observed, without even raising his head, lisping as deliberately as ever and threading his needle.', '“Whoever heard of a man sending for the police against himself?', 'And as for being frightened—you are upsetting yourself about nothing, for nothing will come of it.” “Go!” I shrieked, clutching him by the shoulder.', 'I felt I should strike him in a minute.', 'But I did not notice the door from the passage softly and slowly open at that instant and a figure come in, stop short, and begin staring at us in perplexity I glanced, nearly swooned with shame, and rushed back to my room.', 'There, clutching at my hair with both hands, I leaned my head against the wall and stood motionless in that position.', 'Two minutes later I heard Apollon’s deliberate footsteps.', '“There is some woman asking for you,” he said, looking at me with peculiar severity.', 'Then he stood aside and let in Liza.', 'He would not go away, but stared at us sarcastically.', '“Go away, go away,” I commanded in desperation.', 'At that moment my clock began whirring and wheezing and struck seven.', 'IX “Into my house come bold and free, Its rightful mistress there to be.” I stood before her crushed, crestfallen, revoltingly confused, and I believe I smiled as I did my utmost to wrap myself in the skirts of my ragged wadded dressing-gown—exactly as I had imagined the scene not long before in a fit of depression.', 'After standing over us for a couple of minutes Apollon went away, but that did not make me more at ease.', 'What made it worse was that she, too, was overwhelmed with confusion, more so, in fact, than I should have expected.', 'At the sight of me, of course.', '“Sit down,” I said mechanically, moving a chair up to the table, and I sat down on the sofa.', 'She obediently sat down at once and gazed at me open-eyed, evidently expecting something from me at once.', 'This naïveté of expectation drove me to fury, but I restrained myself.', 'She ought to have tried not to notice, as though everything had been as usual, while instead of that, she ... and I dimly felt that I should make her pay dearly for _all this_.', '“You have found me in a strange position, Liza,” I began, stammering and knowing that this was the wrong way to begin.', '“No, no, don’t imagine anything,” I cried, seeing that she had suddenly flushed.', '“I am not ashamed of my poverty.... On the contrary, I look with pride on my poverty.', 'I am poor but honourable.... One can be poor and honourable,” I muttered.', '“However ... would you like tea?....” “No,” she was beginning.', '“Wait a minute.” I leapt up and ran to Apollon.', 'I had to get out of the room somehow.', '“Apollon,” I whispered in feverish haste, flinging down before him the seven roubles which had remained all the time in my clenched fist, “here are your wages, you see I give them to you; but for that you must come to my rescue: bring me tea and a dozen rusks from the restaurant.', 'If you won’t go, you’ll make me a miserable man!', 'You don’t know what this woman is....', 'This is—everything!', 'You may be imagining something....', 'But you don’t know what that woman is!', '...” Apollon, who had already sat down to his work and put on his spectacles again, at first glanced askance at the money without speaking or putting down his needle; then, without paying the slightest attention to me or making any answer, he went on busying himself with his needle, which he had not yet threaded.', 'I waited before him for three minutes with my arms crossed _à la Napoléon_.', 'My temples were moist with sweat.', 'I was pale, I felt it.', 'But, thank God, he must have been moved to pity, looking at me.', 'Having threaded his needle he deliberately got up from his seat, deliberately moved back his chair, deliberately took off his spectacles, deliberately counted the money, and finally asking me over his shoulder: “Shall I get a whole portion?” deliberately walked out of the room.', 'As I was going back to Liza, the thought occurred to me on the way: shouldn’t I run away just as I was in my dressing-gown, no matter where, and then let happen what would?', 'I sat down again.', 'She looked at me uneasily.', 'For some minutes we were silent.', '“I will kill him,” I shouted suddenly, striking the table with my fist so that the ink spurted out of the inkstand.', '“What are you saying!” she cried, starting.', '“I will kill him!', 'kill him!” I shrieked, suddenly striking the table in absolute frenzy, and at the same time fully understanding how stupid it was to be in such a frenzy.', '“You don’t know, Liza, what that torturer is to me.', 'He is my torturer....', 'He has gone now to fetch some rusks; he ...” And suddenly I burst into tears.', 'It was an hysterical attack.', 'How ashamed I felt in the midst of my sobs; but still I could not restrain them.', 'She was frightened.', '“What is the matter?', 'What is wrong?” she cried, fussing about me.', '“Water, give me water, over there!” I muttered in a faint voice, though I was inwardly conscious that I could have got on very well without water and without muttering in a faint voice.', 'But I was, what is called, _putting it on_, to save appearances, though the attack was a genuine one.', 'She gave me water, looking at me in bewilderment.', 'At that moment Apollon brought in the tea.', 'It suddenly seemed to me that this commonplace, prosaic tea was horribly undignified and paltry after all that had happened, and I blushed crimson.', 'Liza looked at Apollon with positive alarm.', 'He went out without a glance at either of us.', '“Liza, do you despise me?” I asked, looking at her fixedly, trembling with impatience to know what she was thinking.', 'She was confused, and did not know what to answer.', '“Drink your tea,” I said to her angrily.', 'I was angry with myself, but, of course, it was she who would have to pay for it.', 'A horrible spite against her suddenly surged up in my heart; I believe I could have killed her.', 'To revenge myself on her I swore inwardly not to say a word to her all the time.', '“She is the cause of it all,” I thought.', 'Our silence lasted for five minutes.', 'The tea stood on the table; we did not touch it.', 'I had got to the point of purposely refraining from beginning in order to embarrass her further; it was awkward for her to begin alone.', 'Several times she glanced at me with mournful perplexity.', 'I was obstinately silent.', 'I was, of course, myself the chief sufferer, because I was fully conscious of the disgusting meanness of my spiteful stupidity, and yet at the same time I could not restrain myself.', '“I want to... get away ... from there altogether,” she began, to break the silence in some way, but, poor girl, that was just what she ought not to have spoken about at such a stupid moment to a man so stupid as I was.', 'My heart positively ached with pity for her tactless and unnecessary straightforwardness.', 'But something hideous at once stifled all compassion in me; it even provoked me to greater venom.', 'I did not care what happened.', 'Another five minutes passed.', '“Perhaps I am in your way,” she began timidly, hardly audibly, and was getting up.', 'But as soon as I saw this first impulse of wounded dignity I positively trembled with spite, and at once burst out.', '“Why have you come to me, tell me that, please?” I began, gasping for breath and regardless of logical connection in my words.', 'I longed to have it all out at once, at one burst; I did not even trouble how to begin.', '“Why have you come?', 'Answer, answer,” I cried, hardly knowing what I was doing.', '“I’ll tell you, my good girl, why you have come.', 'You’ve come because I talked sentimental stuff to you then.', 'So now you are soft as butter and longing for fine sentiments again.', 'So you may as well know that I was laughing at you then.', 'And I am laughing at you now.', 'Why are you shuddering?', 'Yes, I was laughing at you!', 'I had been insulted just before, at dinner, by the fellows who came that evening before me.', 'I came to you, meaning to thrash one of them, an officer; but I didn’t succeed, I didn’t find him; I had to avenge the insult on someone to get back my own again; you turned up, I vented my spleen on you and laughed at you.', 'I had been humiliated, so I wanted to humiliate; I had been treated like a rag, so I wanted to show my power.... That’s what it was, and you imagined I had come there on purpose to save you.', 'Yes?', 'You imagined that?', 'You imagined that?” I knew that she would perhaps be muddled and not take it all in exactly, but I knew, too, that she would grasp the gist of it, very well indeed.', 'And so, indeed, she did.', 'She turned white as a handkerchief, tried to say something, and her lips worked painfully; but she sank on a chair as though she had been felled by an axe.', 'And all the time afterwards she listened to me with her lips parted and her eyes wide open, shuddering with awful terror.', 'The cynicism, the cynicism of my words overwhelmed her.... “Save you!” I went on, jumping up from my chair and running up and down the room before her.', '“Save you from what?', 'But perhaps I am worse than you myself.', 'Why didn’t you throw it in my teeth when I was giving you that sermon: ‘But what did you come here yourself for?', 'was it to read us a sermon?’ Power, power was what I wanted then, sport was what I wanted, I wanted to wring out your tears, your humiliation, your hysteria—that was what I wanted then!', 'Of course, I couldn’t keep it up then, because I am a wretched creature, I was frightened, and, the devil knows why, gave you my address in my folly.', 'Afterwards, before I got home, I was cursing and swearing at you because of that address, I hated you already because of the lies I had told you.', 'Because I only like playing with words, only dreaming, but, do you know, what I really want is that you should all go to hell.', 'That is what I want.', 'I want peace; yes, I’d sell the whole world for a farthing, straight off, so long as I was left in peace.', 'Is the world to go to pot, or am I to go without my tea?', 'I say that the world may go to pot for me so long as I always get my tea.', 'Did you know that, or not?', 'Well, anyway, I know that I am a blackguard, a scoundrel, an egoist, a sluggard.', 'Here I have been shuddering for the last three days at the thought of your coming.', 'And do you know what has worried me particularly for these three days?', 'That I posed as such a hero to you, and now you would see me in a wretched torn dressing-gown, beggarly, loathsome.', 'I told you just now that I was not ashamed of my poverty; so you may as well know that I am ashamed of it; I am more ashamed of it than of anything, more afraid of it than of being found out if I were a thief, because I am as vain as though I had been skinned and the very air blowing on me hurt.', 'Surely by now you must realise that I shall never forgive you for having found me in this wretched dressing-gown, just as I was flying at Apollon like a spiteful cur.', 'The saviour, the former hero, was flying like a mangy, unkempt sheep-dog at his lackey, and the lackey was jeering at him!', 'And I shall never forgive you for the tears I could not help shedding before you just now, like some silly woman put to shame!', 'And for what I am confessing to you now, I shall never forgive you either!', 'Yes—you must answer for it all because you turned up like this, because I am a blackguard, because I am the nastiest, stupidest, absurdest and most envious of all the worms on earth, who are not a bit better than I am, but, the devil knows why, are never put to confusion; while I shall always be insulted by every louse, that is my doom!', 'And what is it to me that you don’t understand a word of this!', 'And what do I care, what do I care about you, and whether you go to ruin there or not?', 'Do you understand?', 'How I shall hate you now after saying this, for having been here and listening.', 'Why, it’s not once in a lifetime a man speaks out like this, and then it is in hysterics!', '... What more do you want?', 'Why do you still stand confronting me, after all this?', 'Why are you worrying me?', 'Why don’t you go?” But at this point a strange thing happened.', 'I was so accustomed to think and imagine everything from books, and to picture everything in the world to myself just as I had made it up in my dreams beforehand, that I could not all at once take in this strange circumstance.', 'What happened was this: Liza, insulted and crushed by me, understood a great deal more than I imagined.', 'She understood from all this what a woman understands first of all, if she feels genuine love, that is, that I was myself unhappy.', 'The frightened and wounded expression on her face was followed first by a look of sorrowful perplexity.', 'When I began calling myself a scoundrel and a blackguard and my tears flowed (the tirade was accompanied throughout by tears) her whole face worked convulsively.', 'She was on the point of getting up and stopping me; when I finished she took no notice of my shouting: “Why are you here, why don’t you go away?” but realised only that it must have been very bitter to me to say all this.', 'Besides, she was so crushed, poor girl; she considered herself infinitely beneath me; how could she feel anger or resentment?', 'She suddenly leapt up from her chair with an irresistible impulse and held out her hands, yearning towards me, though still timid and not daring to stir.... At this point there was a revulsion in my heart too.', 'Then she suddenly rushed to me, threw her arms round me and burst into tears.', 'I, too, could not restrain myself, and sobbed as I never had before.', '“They won’t let me ...', 'I can’t be good!” I managed to articulate; then I went to the sofa, fell on it face downwards, and sobbed on it for a quarter of an hour in genuine hysterics.', 'She came close to me, put her arms round me and stayed motionless in that position.', 'But the trouble was that the hysterics could not go on for ever, and (I am writing the loathsome truth) lying face downwards on the sofa with my face thrust into my nasty leather pillow, I began by degrees to be aware of a far-away, involuntary but irresistible feeling that it would be awkward now for me to raise my head and look Liza straight in the face.', 'Why was I ashamed?', 'I don’t know, but I was ashamed.', 'The thought, too, came into my overwrought brain that our parts now were completely changed, that she was now the heroine, while I was just a crushed and humiliated creature as she had been before me that night—four days before.... And all this came into my mind during the minutes I was lying on my face on the sofa.', 'My God!', 'surely I was not envious of her then.', 'I don’t know, to this day I cannot decide, and at the time, of course, I was still less able to understand what I was feeling than now.', 'I cannot get on without domineering and tyrannising over someone, but ... there is no explaining anything by reasoning and so it is useless to reason.', 'I conquered myself, however, and raised my head; I had to do so sooner or later ... and I am convinced to this day that it was just because I was ashamed to look at her that another feeling was suddenly kindled and flamed up in my heart ... a feeling of mastery and possession.', 'My eyes gleamed with passion, and I gripped her hands tightly.', 'How I hated her and how I was drawn to her at that minute!', 'The one feeling intensified the other.', 'It was almost like an act of vengeance.', 'At first there was a look of amazement, even of terror on her face, but only for one instant.', 'She warmly and rapturously embraced me.', 'X A quarter of an hour later I was rushing up and down the room in frenzied impatience, from minute to minute I went up to the screen and peeped through the crack at Liza.', 'She was sitting on the ground with her head leaning against the bed, and must have been crying.', 'But she did not go away, and that irritated me.', 'This time she understood it all.', 'I had insulted her finally, but ... there’s no need to describe it.', 'She realised that my outburst of passion had been simply revenge, a fresh humiliation, and that to my earlier, almost causeless hatred was added now a _personal hatred_, born of envy....', 'Though I do not maintain positively that she understood all this distinctly; but she certainly did fully understand that I was a despicable man, and what was worse, incapable of loving her.', 'I know I shall be told that this is incredible—but it is incredible to be as spiteful and stupid as I was; it may be added that it was strange I should not love her, or at any rate, appreciate her love.', 'Why is it strange?', 'In the first place, by then I was incapable of love, for I repeat, with me loving meant tyrannising and showing my moral superiority.', 'I have never in my life been able to imagine any other sort of love, and have nowadays come to the point of sometimes thinking that love really consists in the right—freely given by the beloved object—to tyrannise over her.', 'Even in my underground dreams I did not imagine love except as a struggle.', 'I began it always with hatred and ended it with moral subjugation, and afterwards I never knew what to do with the subjugated object.', 'And what is there to wonder at in that, since I had succeeded in so corrupting myself, since I was so out of touch with “real life,” as to have actually thought of reproaching her, and putting her to shame for having come to me to hear “fine sentiments”; and did not even guess that she had come not to hear fine sentiments, but to love me, because to a woman all reformation, all salvation from any sort of ruin, and all moral renewal is included in love and can only show itself in that form.', 'I did not hate her so much, however, when I was running about the room and peeping through the crack in the screen.', 'I was only insufferably oppressed by her being here.', 'I wanted her to disappear.', 'I wanted “peace,” to be left alone in my underground world.', 'Real life oppressed me with its novelty so much that I could hardly breathe.', 'But several minutes passed and she still remained, without stirring, as though she were unconscious.', 'I had the shamelessness to tap softly at the screen as though to remind her.... She started, sprang up, and flew to seek her kerchief, her hat, her coat, as though making her escape from me.... Two minutes later she came from behind the screen and looked with heavy eyes at me.', 'I gave a spiteful grin, which was forced, however, to _keep up appearances_, and I turned away from her eyes.', '“Good-bye,” she said, going towards the door.', 'I ran up to her, seized her hand, opened it, thrust something in it and closed it again.', 'Then I turned at once and dashed away in haste to the other corner of the room to avoid seeing, anyway....', 'I did mean a moment since to tell a lie—to write that I did this accidentally, not knowing what I was doing through foolishness, through losing my head.', 'But I don’t want to lie, and so I will say straight out that I opened her hand and put the money in it ... from spite.', 'It came into my head to do this while I was running up and down the room and she was sitting behind the screen.', 'But this I can say for certain: though I did that cruel thing purposely, it was not an impulse from the heart, but came from my evil brain.', 'This cruelty was so affected, so purposely made up, so completely a product of the brain, of books, that I could not even keep it up a minute—first I dashed away to avoid seeing her, and then in shame and despair rushed after Liza.', 'I opened the door in the passage and began listening.', '“Liza!', 'Liza!” I cried on the stairs, but in a low voice, not boldly.', 'There was no answer, but I fancied I heard her footsteps, lower down on the stairs.', '“Liza!” I cried, more loudly.', 'No answer.', 'But at that minute I heard the stiff outer glass door open heavily with a creak and slam violently; the sound echoed up the stairs.', 'She had gone.', 'I went back to my room in hesitation.', 'I felt horribly oppressed.', 'I stood still at the table, beside the chair on which she had sat and looked aimlessly before me.', 'A minute passed, suddenly I started; straight before me on the table I saw....', 'In short, I saw a crumpled blue five-rouble note, the one I had thrust into her hand a minute before.', 'It was the same note; it could be no other, there was no other in the flat.', 'So she had managed to fling it from her hand on the table at the moment when I had dashed into the further corner.', 'Well!', 'I might have expected that she would do that.', 'Might I have expected it?', 'No, I was such an egoist, I was so lacking in respect for my fellow-creatures that I could not even imagine she would do so.', 'I could not endure it.', 'A minute later I flew like a madman to dress, flinging on what I could at random and ran headlong after her.', 'She could not have got two hundred paces away when I ran out into the street.', 'It was a still night and the snow was coming down in masses and falling almost perpendicularly, covering the pavement and the empty street as though with a pillow.', 'There was no one in the street, no sound was to be heard.', 'The street lamps gave a disconsolate and useless glimmer.', 'I ran two hundred paces to the cross-roads and stopped short.', 'Where had she gone?', 'And why was I running after her?', 'Why?', 'To fall down before her, to sob with remorse, to kiss her feet, to entreat her forgiveness!', 'I longed for that, my whole breast was being rent to pieces, and never, never shall I recall that minute with indifference.', 'But—what for?', 'I thought.', 'Should I not begin to hate her, perhaps, even tomorrow, just because I had kissed her feet today?', 'Should I give her happiness?', 'Had I not recognised that day, for the hundredth time, what I was worth?', 'Should I not torture her?', 'I stood in the snow, gazing into the troubled darkness and pondered this.', '“And will it not be better?” I mused fantastically, afterwards at home, stifling the living pang of my heart with fantastic dreams.', '“Will it not be better that she should keep the resentment of the insult for ever?', 'Resentment—why, it is purification; it is a most stinging and painful consciousness!', 'Tomorrow I should have defiled her soul and have exhausted her heart, while now the feeling of insult will never die in her heart, and however loathsome the filth awaiting her—the feeling of insult will elevate and purify her ... by hatred ... h’m!', '... perhaps, too, by forgiveness.... Will all that make things easier for her though?', '...” And, indeed, I will ask on my own account here, an idle question: which is better—cheap happiness or exalted sufferings?', 'Well, which is better?', 'So I dreamed as I sat at home that evening, almost dead with the pain in my soul.', 'Never had I endured such suffering and remorse, yet could there have been the faintest doubt when I ran out from my lodging that I should turn back half-way?', 'I never met Liza again and I have heard nothing of her.', 'I will add, too, that I remained for a long time afterwards pleased with the phrase about the benefit from resentment and hatred in spite of the fact that I almost fell ill from misery.', 'Even now, so many years later, all this is somehow a very evil memory.', 'I have many evil memories now, but ... hadn’t I better end my “Notes” here?', 'I believe I made a mistake in beginning to write them, anyway I have felt ashamed all the time I’ve been writing this story; so it’s hardly literature so much as a corrective punishment.', 'Why, to tell long stories, showing how I have spoiled my life through morally rotting in my corner, through lack of fitting environment, through divorce from real life, and rankling spite in my underground world, would certainly not be interesting; a novel needs a hero, and all the traits for an anti-hero are _expressly_ gathered together here, and what matters most, it all produces an unpleasant impression, for we are all divorced from life, we are all cripples, every one of us, more or less.', 'We are so divorced from it that we feel at once a sort of loathing for real life, and so cannot bear to be reminded of it.', 'Why, we have come almost to looking upon real life as an effort, almost as hard work, and we are all privately agreed that it is better in books.', 'And why do we fuss and fume sometimes?', 'Why are we perverse and ask for something else?', 'We don’t know what ourselves.', 'It would be the worse for us if our petulant prayers were answered.', 'Come, try, give any one of us, for instance, a little more independence, untie our hands, widen the spheres of our activity, relax the control and we ... yes, I assure you ... we should be begging to be under control again at once.', 'I know that you will very likely be angry with me for that, and will begin shouting and stamping.', 'Speak for yourself, you will say, and for your miseries in your underground holes, and don’t dare to say all of us—excuse me, gentlemen, I am not justifying myself with that “all of us.” As for what concerns me in particular I have only in my life carried to an extreme what you have not dared to carry halfway, and what’s more, you have taken your cowardice for good sense, and have found comfort in deceiving yourselves.', 'So that perhaps, after all, there is more life in me than in you.', 'Look into it more carefully!', 'Why, we don’t even know what living means now, what it is, and what it is called?', 'Leave us alone without books and we shall be lost and in confusion at once.', 'We shall not know what to join on to, what to cling to, what to love and what to hate, what to respect and what to despise.', 'We are oppressed at being men—men with a real individual body and blood, we are ashamed of it, we think it a disgrace and try to contrive to be some sort of impossible generalised man.', 'We are stillborn, and for generations past have been begotten, not by living fathers, and that suits us better and better.', 'We are developing a taste for it.', 'Soon we shall contrive to be born somehow from an idea.', 'But enough; I don’t want to write more from “Underground.” [The notes of this paradoxalist do not end here, however.', 'He could not refrain from going on with them, but it seems to us that we may stop here.]', '*** END OF THE PROJECT GUTENBERG EBOOK NOTES FROM THE UNDERGROUND *** Updated editions will replace the previous one--the old editions will be renamed.', 'Creating the works from print editions not protected by U.S. copyright law means that no one owns a United States copyright in these works, so the Foundation (and you!)', 'can copy and distribute it in the United States without permission and without paying copyright royalties.', 'Special rules, set forth in the General Terms of Use part of this license, apply to copying and distributing Project Gutenberg-tm electronic works to protect the PROJECT GUTENBERG-tm concept and trademark.', 'Project Gutenberg is a registered trademark, and may not be used if you charge for an eBook, except by following the terms of the trademark license, including paying royalties for use of the Project Gutenberg trademark.', 'If you do not charge anything for copies of this eBook, complying with the trademark license is very easy.', 'You may use this eBook for nearly any purpose such as creation of derivative works, reports, performances and research.', 'Project Gutenberg eBooks may be modified and printed and given away--you may do practically ANYTHING in the United States with eBooks not protected by U.S. copyright law.', 'Redistribution is subject to the trademark license, especially commercial redistribution.', 'START: FULL LICENSE THE FULL PROJECT GUTENBERG LICENSE PLEASE READ THIS BEFORE YOU DISTRIBUTE OR USE THIS WORK To protect the Project Gutenberg-tm mission of promoting the free distribution of electronic works, by using or distributing this work (or any other work associated in any way with the phrase \"Project Gutenberg\"), you agree to comply with all the terms of the Full Project Gutenberg-tm License available with this file or online at www.gutenberg.org/license.', 'Section 1.', 'General Terms of Use and Redistributing Project Gutenberg-tm electronic works 1.A.', 'By reading or using any part of this Project Gutenberg-tm electronic work, you indicate that you have read, understand, agree to and accept all the terms of this license and intellectual property (trademark/copyright) agreement.', 'If you do not agree to abide by all the terms of this agreement, you must cease using and return or destroy all copies of Project Gutenberg-tm electronic works in your possession.', 'If you paid a fee for obtaining a copy of or access to a Project Gutenberg-tm electronic work and you do not agree to be bound by the terms of this agreement, you may obtain a refund from the person or entity to whom you paid the fee as set forth in paragraph 1.E.8.', '1.B.', '\"Project Gutenberg\" is a registered trademark.', 'It may only be used on or associated in any way with an electronic work by people who agree to be bound by the terms of this agreement.', 'There are a few things that you can do with most Project Gutenberg-tm electronic works even without complying with the full terms of this agreement.', 'See paragraph 1.C below.', 'There are a lot of things you can do with Project Gutenberg-tm electronic works if you follow the terms of this agreement and help preserve free future access to Project Gutenberg-tm electronic works.', 'See paragraph 1.E below.', '1.C.', 'The Project Gutenberg Literary Archive Foundation (\"the Foundation\" or PGLAF), owns a compilation copyright in the collection of Project Gutenberg-tm electronic works.', 'Nearly all the individual works in the collection are in the public domain in the United States.', 'If an individual work is unprotected by copyright law in the United States and you are located in the United States, we do not claim a right to prevent you from copying, distributing, performing, displaying or creating derivative works based on the work as long as all references to Project Gutenberg are removed.', 'Of course, we hope that you will support the Project Gutenberg-tm mission of promoting free access to electronic works by freely sharing Project Gutenberg-tm works in compliance with the terms of this agreement for keeping the Project Gutenberg-tm name associated with the work.', 'You can easily comply with the terms of this agreement by keeping this work in the same format with its attached full Project Gutenberg-tm License when you share it without charge with others.', '1.D.', 'The copyright laws of the place where you are located also govern what you can do with this work.', 'Copyright laws in most countries are in a constant state of change.', 'If you are outside the United States, check the laws of your country in addition to the terms of this agreement before downloading, copying, displaying, performing, distributing or creating derivative works based on this work or any other Project Gutenberg-tm work.', 'The Foundation makes no representations concerning the copyright status of any work in any country other than the United States.', '1.E.', 'Unless you have removed all references to Project Gutenberg: 1.E.1.', 'The following sentence, with active links to, or other immediate access to, the full Project Gutenberg-tm License must appear prominently whenever any copy of a Project Gutenberg-tm work (any work on which the phrase \"Project Gutenberg\" appears, or with which the phrase \"Project Gutenberg\" is associated) is accessed, displayed, performed, viewed, copied or distributed: This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever.', 'You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org.', 'If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook.', '1.E.2.', 'If an individual Project Gutenberg-tm electronic work is derived from texts not protected by U.S. copyright law (does not contain a notice indicating that it is posted with permission of the copyright holder), the work can be copied and distributed to anyone in the United States without paying any fees or charges.', 'If you are redistributing or providing access to a work with the phrase \"Project Gutenberg\" associated with or appearing on the work, you must comply either with the requirements of paragraphs 1.E.1 through 1.E.7 or obtain permission for the use of the work and the Project Gutenberg-tm trademark as set forth in paragraphs 1.E.8 or 1.E.9.', '1.E.3.', 'If an individual Project Gutenberg-tm electronic work is posted with the permission of the copyright holder, your use and distribution must comply with both paragraphs 1.E.1 through 1.E.7 and any additional terms imposed by the copyright holder.', 'Additional terms will be linked to the Project Gutenberg-tm License for all works posted with the permission of the copyright holder found at the beginning of this work.', '1.E.4.', 'Do not unlink or detach or remove the full Project Gutenberg-tm License terms from this work, or any files containing a part of this work or any other work associated with Project Gutenberg-tm.', '1.E.5.', 'Do not copy, display, perform, distribute or redistribute this electronic work, or any part of this electronic work, without prominently displaying the sentence set forth in paragraph 1.E.1 with active links or immediate access to the full terms of the Project Gutenberg-tm License.', '1.E.6.', 'You may convert to and distribute this work in any binary, compressed, marked up, nonproprietary or proprietary form, including any word processing or hypertext form.', 'However, if you provide access to or distribute copies of a Project Gutenberg-tm work in a format other than \"Plain Vanilla ASCII\" or other format used in the official version posted on the official Project Gutenberg-tm website (www.gutenberg.org), you must, at no additional cost, fee or expense to the user, provide a copy, a means of exporting a copy, or a means of obtaining a copy upon request, of the work in its original \"Plain Vanilla ASCII\" or other form.', 'Any alternate format must include the full Project Gutenberg-tm License as specified in paragraph 1.E.1.', '1.E.7.', 'Do not charge a fee for access to, viewing, displaying, performing, copying or distributing any Project Gutenberg-tm works unless you comply with paragraph 1.E.8 or 1.E.9.', '1.E.8.', 'You may charge a reasonable fee for copies of or providing access to or distributing Project Gutenberg-tm electronic works provided that: * You pay a royalty fee of 20% of the gross profits you derive from the use of Project Gutenberg-tm works calculated using the method you already use to calculate your applicable taxes.', 'The fee is owed to the owner of the Project Gutenberg-tm trademark, but he has agreed to donate royalties under this paragraph to the Project Gutenberg Literary Archive Foundation.', 'Royalty payments must be paid within 60 days following each date on which you prepare (or are legally required to prepare) your periodic tax returns.', 'Royalty payments should be clearly marked as such and sent to the Project Gutenberg Literary Archive Foundation at the address specified in Section 4, \"Information about donations to the Project Gutenberg Literary Archive Foundation.\"', '* You provide a full refund of any money paid by a user who notifies you in writing (or by e-mail) within 30 days of receipt that s/he does not agree to the terms of the full Project Gutenberg-tm License.', 'You must require such a user to return or destroy all copies of the works possessed in a physical medium and discontinue all use of and all access to other copies of Project Gutenberg-tm works.', '* You provide, in accordance with paragraph 1.F.3, a full refund of any money paid for a work or a replacement copy, if a defect in the electronic work is discovered and reported to you within 90 days of receipt of the work.', '* You comply with all other terms of this agreement for free distribution of Project Gutenberg-tm works.', '1.E.9.', 'If you wish to charge a fee or distribute a Project Gutenberg-tm electronic work or group of works on different terms than are set forth in this agreement, you must obtain permission in writing from the Project Gutenberg Literary Archive Foundation, the manager of the Project Gutenberg-tm trademark.', 'Contact the Foundation as set forth in Section 3 below.', '1.F.', '1.F.1.', 'Project Gutenberg volunteers and employees expend considerable effort to identify, do copyright research on, transcribe and proofread works not protected by U.S. copyright law in creating the Project Gutenberg-tm collection.', 'Despite these efforts, Project Gutenberg-tm electronic works, and the medium on which they may be stored, may contain \"Defects,\" such as, but not limited to, incomplete, inaccurate or corrupt data, transcription errors, a copyright or other intellectual property infringement, a defective or damaged disk or other medium, a computer virus, or computer codes that damage or cannot be read by your equipment.', '1.F.2.', 'LIMITED WARRANTY, DISCLAIMER OF DAMAGES - Except for the \"Right of Replacement or Refund\" described in paragraph 1.F.3, the Project Gutenberg Literary Archive Foundation, the owner of the Project Gutenberg-tm trademark, and any other party distributing a Project Gutenberg-tm electronic work under this agreement, disclaim all liability to you for damages, costs and expenses, including legal fees.', 'YOU AGREE THAT YOU HAVE NO REMEDIES FOR NEGLIGENCE, STRICT LIABILITY, BREACH OF WARRANTY OR BREACH OF CONTRACT EXCEPT THOSE PROVIDED IN PARAGRAPH 1.F.3.', 'YOU AGREE THAT THE FOUNDATION, THE TRADEMARK OWNER, AND ANY DISTRIBUTOR UNDER THIS AGREEMENT WILL NOT BE LIABLE TO YOU FOR ACTUAL, DIRECT, INDIRECT, CONSEQUENTIAL, PUNITIVE OR INCIDENTAL DAMAGES EVEN IF YOU GIVE NOTICE OF THE POSSIBILITY OF SUCH DAMAGE.', '1.F.3.', 'LIMITED RIGHT OF REPLACEMENT OR REFUND - If you discover a defect in this electronic work within 90 days of receiving it, you can receive a refund of the money (if any) you paid for it by sending a written explanation to the person you received the work from.', 'If you received the work on a physical medium, you must return the medium with your written explanation.', 'The person or entity that provided you with the defective work may elect to provide a replacement copy in lieu of a refund.', 'If you received the work electronically, the person or entity providing it to you may choose to give you a second opportunity to receive the work electronically in lieu of a refund.', 'If the second copy is also defective, you may demand a refund in writing without further opportunities to fix the problem.', '1.F.4.', \"Except for the limited right of replacement or refund set forth in paragraph 1.F.3, this work is provided to you 'AS-IS', WITH NO OTHER WARRANTIES OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO WARRANTIES OF MERCHANTABILITY OR FITNESS FOR ANY PURPOSE.\", '1.F.5.', 'Some states do not allow disclaimers of certain implied warranties or the exclusion or limitation of certain types of damages.', 'If any disclaimer or limitation set forth in this agreement violates the law of the state applicable to this agreement, the agreement shall be interpreted to make the maximum disclaimer or limitation permitted by the applicable state law.', 'The invalidity or unenforceability of any provision of this agreement shall not void the remaining provisions.', '1.F.6.', 'INDEMNITY - You agree to indemnify and hold the Foundation, the trademark owner, any agent or employee of the Foundation, anyone providing copies of Project Gutenberg-tm electronic works in accordance with this agreement, and any volunteers associated with the production, promotion and distribution of Project Gutenberg-tm electronic works, harmless from all liability, costs and expenses, including legal fees, that arise directly or indirectly from any of the following which you do or cause to occur: (a) distribution of this or any Project Gutenberg-tm work, (b) alteration, modification, or additions or deletions to any Project Gutenberg-tm work, and (c) any Defect you cause.', 'Section 2.', 'Information about the Mission of Project Gutenberg-tm Project Gutenberg-tm is synonymous with the free distribution of electronic works in formats readable by the widest variety of computers including obsolete, old, middle-aged and new computers.', 'It exists because of the efforts of hundreds of volunteers and donations from people in all walks of life.', \"Volunteers and financial support to provide volunteers with the assistance they need are critical to reaching Project Gutenberg-tm's goals and ensuring that the Project Gutenberg-tm collection will remain freely available for generations to come.\", 'In 2001, the Project Gutenberg Literary Archive Foundation was created to provide a secure and permanent future for Project Gutenberg-tm and future generations.', 'To learn more about the Project Gutenberg Literary Archive Foundation and how your efforts and donations can help, see Sections 3 and 4 and the Foundation information page at www.gutenberg.org Section 3.', 'Information about the Project Gutenberg Literary Archive Foundation The Project Gutenberg Literary Archive Foundation is a non-profit 501(c)(3) educational corporation organized under the laws of the state of Mississippi and granted tax exempt status by the Internal Revenue Service.', \"The Foundation's EIN or federal tax identification number is 64-6221541.\", \"Contributions to the Project Gutenberg Literary Archive Foundation are tax deductible to the full extent permitted by U.S. federal laws and your state's laws.\", \"The Foundation's business office is located at 809 North 1500 West, Salt Lake City, UT 84116, (801) 596-1887.\", \"Email contact links and up to date contact information can be found at the Foundation's website and official page at www.gutenberg.org/contact Section 4.\", 'Information about Donations to the Project Gutenberg Literary Archive Foundation Project Gutenberg-tm depends upon and cannot survive without widespread public support and donations to carry out its mission of increasing the number of public domain and licensed works that can be freely distributed in machine-readable form accessible by the widest array of equipment including outdated equipment.', 'Many small donations ($1 to $5,000) are particularly important to maintaining tax exempt status with the IRS.', 'The Foundation is committed to complying with the laws regulating charities and charitable donations in all 50 states of the United States.', 'Compliance requirements are not uniform and it takes a considerable effort, much paperwork and many fees to meet and keep up with these requirements.', 'We do not solicit donations in locations where we have not received written confirmation of compliance.', 'To SEND DONATIONS or determine the status of compliance for any particular state visit www.gutenberg.org/donate While we cannot and do not solicit contributions from states where we have not met the solicitation requirements, we know of no prohibition against accepting unsolicited donations from donors in such states who approach us with offers to donate.', 'International donations are gratefully accepted, but we cannot make any statements concerning tax treatment of donations received from outside the United States.', 'U.S. laws alone swamp our small staff.', 'Please check the Project Gutenberg web pages for current donation methods and addresses.', 'Donations are accepted in a number of other ways including checks, online payments and credit card donations.', 'To donate, please visit: www.gutenberg.org/donate Section 5.', 'General Information About Project Gutenberg-tm electronic works Professor Michael S. Hart was the originator of the Project Gutenberg-tm concept of a library of electronic works that could be freely shared with anyone.', 'For forty years, he produced and distributed Project Gutenberg-tm eBooks with only a loose network of volunteer support.', 'Project Gutenberg-tm eBooks are often created from several printed editions, all of which are confirmed as not protected by copyright in the U.S. unless a copyright notice is included.', 'Thus, we do not necessarily keep eBooks in compliance with any particular paper edition.', 'Most people start at our website which has the main PG search facility: www.gutenberg.org This website includes information about Project Gutenberg-tm, including how to make donations to the Project Gutenberg Literary Archive Foundation, how to help produce our new eBooks, and how to subscribe to our email newsletter to hear about new eBooks.']"
  },
  {
    "objectID": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#remove-stop-words",
    "href": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#remove-stop-words",
    "title": "Text summarizer in Python, Notes from underground",
    "section": "Remove stop words",
    "text": "Remove stop words\nStop words are English words which do not add much meaning to a sentence. They can be safely ignored without sacrificing the meaning of the sentence. We already downloaded a file with English stop words in the first section of the notebook.\nHere, we will get the list of stop words and store them in stop_word variable.\n\n# get stop words list\nstop_words = nltk.corpus.stopwords.words('english')\nprint(stop_words)\n\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
  },
  {
    "objectID": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#build-word-histogram",
    "href": "posts/2020-11-19-TextSummarizer-Dostoyevesky.html#build-word-histogram",
    "title": "Text summarizer in Python, Notes from underground",
    "section": "Build word histogram",
    "text": "Build word histogram\nLet’s evaluate the importance of each word based on how many times it appears in the entire text.\nWe will do so by 1) splitting the words in clean_text, 2) removing the stop words, and then 3) checking the frequency of each word as it appears in the text.\n\n# create an empty dictionary to house the word count\nword_count = {}\n\n# loop through tokenized words, remove stop words and save word count to dictionary\nfor word in nltk.word_tokenize(clean_text):\n    # remove stop words\n    if word not in stop_words:\n        # save word count to dictionary\n        if word not in word_count.keys():\n            word_count[word] = 1\n        else:\n            word_count[word] += 1\n\nLet’s plot the word histogram and see the results.\n\n# plt.figure(figsize=(16,10))\n# plt.xticks(rotation = 90)\n# plt.bar(word_count.keys(), word_count.values())\n# plt.show()\n\n\ndef plot_top_words(word_count_dict, show_top_n=20):\n    word_count_table = pd.DataFrame.from_dict(word_count_dict, orient = 'index').rename(columns={0: 'score'})\n    word_count_table.sort_values(by='score').tail(show_top_n).plot(kind='barh', figsize=(10,10))\n    plt.show()\n\n\nplot_top_words(word_count, 20)\n\n\n\n\n\nsentence_score = {}\n\n# loop through tokenized sentence, only take sentences that have less than 30 words, then add word score to form sentence score\nfor sentence in sentences:\n    # check if word in sentence is in word_count dictionary\n    for word in nltk.word_tokenize(sentence.lower()):\n        if word in word_count.keys():\n            # only take sentence that has less than 30 words\n            if len(sentence.split(' ')) &lt; 30:\n                # add word score to sentence score\n                if sentence not in sentence_score.keys():\n                    sentence_score[sentence] = word_count[word]\n                else:\n                    sentence_score[sentence] += word_count[word]\n\n\ndf_sentence_score = pd.DataFrame.from_dict(sentence_score, orient = 'index').rename(columns={0: 'score'})\ndf_sentence_score.sort_values(by='score', ascending = False)\n\n\n\n\n\n\n\n\nscore\n\n\n\n\nPerhaps the thing I resented was, that of all your edifices there has not been one at which one could not put out one’s tongue.\n914\n\n\nYou may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org.\n894\n\n\nAnd no one, no one should know what passes between husband and wife if they love one another.\n877\n\n\nAnd one may choose what is contrary to one’s own interests, and sometimes one _positively ought_ (that is my idea).\n809\n\n\nIn fact, I would even have put up with looking base if, at the same time, my face could have been thought strikingly intelligent.\n767\n\n\n...\n...\n\n\nWhere are my foundations?\n1\n\n\nThat incensed me at once.\n1\n\n\nBut here what is there but ... foulness?\n1\n\n\nIt was a nightmare.\n1\n\n\nIt is fate!\n1\n\n\n\n\n1857 rows × 1 columns\n\n\n\n\nbest_sentences = heapq.nlargest(3, sentence_score, key=sentence_score.get)\n\n\nprint('SUMMARY')\nprint('------------------------')\n\nfor sentence in sentences:\n    if sentence in best_sentences:\n        print (sentence)\n\nSUMMARY\n------------------------\nYou may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org.\nPerhaps the thing I resented was, that of all your edifices there has not been one at which one could not put out one’s tongue.\nAnd no one, no one should know what passes between husband and wife if they love one another.\nYou may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org."
  },
  {
    "objectID": "posts/2020-10-28-PCA-with-sklearn.html",
    "href": "posts/2020-10-28-PCA-with-sklearn.html",
    "title": "Principal component analysis with sklearn",
    "section": "",
    "text": "import seaborn as sns\nfrom sklearn import datasets\nimport pandas as pd\n\n\niris = datasets.load_iris()\n\n\ntarget_names = iris.target_names\n\n\nX = pd.DataFrame(iris.data)\n\n\nX.columns = ['Sepal Length', 'Sepal Width', 'Petal Length', 'Petal Width']\n\n\nfor name in X.columns: \n    X[name] = (X[name]-X[name].mean())/X[name].std()\n\n\nsns.heatmap(X.corr(), vmin=-1, vmax=1, annot=True, fmt='2.2f')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f026c877780&gt;\n\n\n\n\n\n\nimport numpy as np\n\n\n# calculate the Covariance matrix\nQ = X.cov().to_numpy()# find the eigenvalue and eigenvector of the Covariance matrix\nD, V = np.linalg.eigh(Q) # print the results\nnp.set_printoptions(precision=2)\nprint('principal components:')\n\nprincipal components:\n\n\n\nfor i in range(1, len(D)):\n    print('Feature %d : %2.3f'%(i, D[i]))\n\nFeature 1 : 0.147\nFeature 2 : 0.914\nFeature 3 : 2.918\n\n\n\n# perform the linear transformation\nX_new = X.dot(V)# define the columns names to the X_new\nX_new.columns = [\"Feature %d\"%i for i in range(1,5)]# The correlation between different features disappear!\nsns.heatmap(X_new.corr(), vmin=-1, vmax=1, annot=True, fmt='2.2f')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f023c032940&gt;\n\n\n\n\n\n\n# truncation: dimensional reduction\nX_reduced = X.copy()\n\n\nV_trun = V[:,2:4]\n\n\nX_reduced = X_reduced.dot(V_trun)\n\n\nX_reduced['Species'] = iris.target\n\n\nX_reduced.columns = ['Feature 3', 'Feature 4', 'Species']\n\n\nfor i, t in enumerate(target_names):\n    X_reduced['Species'].replace(i, t, inplace=True)\n\n\nsns.scatterplot(data=X_reduced, x='Feature 3', y='Feature 4', hue='Species')\n\n&lt;matplotlib.axes._subplots.AxesSubplot at 0x7f023bf7f400&gt;\n\n\n\n\n\n\nfrom sklearn.decomposition import PCA\n\n\nX3 = X.to_numpy()\npca = PCA(n_components=3, random_state=0)\nX3_reduced = pca.fit(X3).transform(X3)\nX3_reduced = pd.DataFrame(X3_reduced)\nX3_reduced['species'] = iris.target# plot the results in 3D scatter plot\n\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n\n\nfig = plt.figure(figsize=(6,6))\nax = Axes3D(fig)\n\nfor i in range(3):\n    idx = X3_reduced['species']==i\n    ax.scatter(X3_reduced[0][idx], X3_reduced[1][idx], X3_reduced[2][idx], label=target_names[i])\nplt.legend()\nax.view_init(20,75)\nplt.xlabel('Feature 2')\nplt.ylabel('Feature 3')\nax.set_zlabel('Feature 4')\n\nText(0.5, 0, 'Feature 4')"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Kearney Data Science",
    "section": "",
    "text": "---\ntitle: \"Hello World\"\nauthor: \"DRK\"\ndate: \"2022-01-01\"\n---\n\n\n#quarto publish gh-pages"
  },
  {
    "objectID": "posts/2020-12-30-TextSummarizer-tieng-viet.html",
    "href": "posts/2020-12-30-TextSummarizer-tieng-viet.html",
    "title": "Text summarizer in Python, Tieng Viet",
    "section": "",
    "text": "# Natural Language Tool Kit (NLTK)\nimport nltk\nnltk.download('stopwords')\n\n# Regular Expression for text preprocessing\nimport re\n\n# Heap (priority) queue algorithm to get the top sentences\nimport heapq\n\n# NumPy for numerical computing\nimport numpy as np\n\n# pandas for creating DataFrames\nimport pandas as pd\n\n# matplotlib for plot\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "posts/2020-12-30-TextSummarizer-tieng-viet.html#credit-code-from-httpsgithub.comlouisteo9personal-text-summarizer",
    "href": "posts/2020-12-30-TextSummarizer-tieng-viet.html#credit-code-from-httpsgithub.comlouisteo9personal-text-summarizer",
    "title": "Text summarizer in Python, Tieng Viet",
    "section": "",
    "text": "# Natural Language Tool Kit (NLTK)\nimport nltk\nnltk.download('stopwords')\n\n# Regular Expression for text preprocessing\nimport re\n\n# Heap (priority) queue algorithm to get the top sentences\nimport heapq\n\n# NumPy for numerical computing\nimport numpy as np\n\n# pandas for creating DataFrames\nimport pandas as pd\n\n# matplotlib for plot\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\n[nltk_data] Downloading package stopwords to /home/david/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!"
  },
  {
    "objectID": "posts/2020-12-30-TextSummarizer-tieng-viet.html#load-text-data",
    "href": "posts/2020-12-30-TextSummarizer-tieng-viet.html#load-text-data",
    "title": "Text summarizer in Python, Tieng Viet",
    "section": "Load text data",
    "text": "Load text data\n\n# load text file\nwith open('viet_article1.txt', 'r') as f:\n    file_data = f.read()\n\n\n# view text data\nprint(file_data)\n\nThêm 9 người nhập cảnh nhiễm nCoV\n\nNgày 1/1, Bộ Y tế ghi nhận ca dương tính nCoV, đều là người nhập cảnh được cách ly ngay tại Khánh Hòa, Bình Dương và Long An. Tổng ca nhiễm lên 1.474.\n\n\"Bệnh nhân 1466\", nam, 35 tuổi, ở TP Hưng Yên, tỉnh Hưng Yên. \"Bệnh nhân 1467\" nam, 38, tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. \"Bệnh nhân 1468\", nam, 51 tuổi, ở huyện Thanh Chương, tỉnh Nghệ An. Ba người này từ Nga nhập cảnh sân bay Cam Ranh trên chuyến bay VN5062 ngày 25/12, cách ly tại tỉnh Khánh Hòa. Kết quả xét nghiệm lần một ngày 25/12 âm tính; lấy mẫu lần hai ngày 30/12 kết quả xét nghiệm tại Trung tâm Kiểm soát Bệnh tật tỉnh Khánh Hòa dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Bệnh nhiệt đới tỉnh Khánh Hòa.\n\nTrước đó trên chuyến bay này đã ghi nhận 9 trường hợp dương tính với nCoV.\n\n\"Bệnh nhân 1469\", nam, 32 tuổi, ở quận Hải Châu, TP Đà Nẵng. \"Bệnh nhân 1470\", nữ, 40 tuổi, ở huyện Ba Vì, và \"bệnh nhân 1471\", nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội. \"Bệnh nhân 1472\", nam, 29 tuổi, ở huyện Kim Thành, và \"bệnh nhân 1473\", nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. Họ từ Đức nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN30 ngày 30/12 được cách ly tại tỉnh Bình Dương. Kết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương.\n\n\"Bệnh nhân 1474\", nam, 50 tuổi, ở Quận 7, TP HCM. Ông từ Canada quá cảnh sân bay Incheon (Hàn Quốc), sau đó nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN405 ngày 29/12, được cách ly tại tỉnh Long An. Lấy mẫu ngày 30/12, kết quả xét nghiệm tại Trung tâm Kiểm soát bệnh tật tỉnh Long An và Viện Pasteur TP HCM đều dương tính với nCoV, bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Long An.\n\nNhư vậy, tổng ca nhiễm lên 1.474, tổng số khỏi 1.325. Số người tử vong do Covid-19 là 35, bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính. Các bệnh nhân còn lại đa số sức khỏe ổn định, trong đó 7 người xét nghiệm âm tính nCoV lần một, 11 người âm tính lần hai và 10 người âm tính lần ba.\n\nTổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn 17.000. Trong đó, cách ly tập trung tại bệnh viện 150; cách ly tập trung hơn 15.000, còn lại ở nhà hoặc nơi lưu trú.\n\nViệt Nam đã 30 ngày không ghi nhận ca nhiễm mới trong cộng đồng. Song, nguy cơ dịch xâm nhập luôn thường trực, đặc biệt khi tăng số lượng chuyến bay đưa công dân Việt Nam, chuyên gia về nước. Nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép.\n\nBộ Y tế khuyến cáo người dân tiếp tục thực hiện tốt \"Thông điệp 5K\", nhất là đeo khẩu trang và rửa tay bằng xà phòng, dung dịch sát khuẩn.\n\nThế giới ghi nhận hơn 1,7 triệu người chết vì nCoV trong hơn 83 triệu người nhiễm. Mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch, tiếp theo là Ấn Độ và Brazil.\n\n\n\n\ntext = file_data\ntext = re.sub(r'\\[[0-9]*\\]',' ',text) # replace reference number i.e. [1], [10], [20] with empty space, if any..\ntext = re.sub(r'\\s+',' ',text) # replace one or more spaces with single space\nprint(text)\n\nThêm 9 người nhập cảnh nhiễm nCoV Ngày 1/1, Bộ Y tế ghi nhận ca dương tính nCoV, đều là người nhập cảnh được cách ly ngay tại Khánh Hòa, Bình Dương và Long An. Tổng ca nhiễm lên 1.474. \"Bệnh nhân 1466\", nam, 35 tuổi, ở TP Hưng Yên, tỉnh Hưng Yên. \"Bệnh nhân 1467\" nam, 38, tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. \"Bệnh nhân 1468\", nam, 51 tuổi, ở huyện Thanh Chương, tỉnh Nghệ An. Ba người này từ Nga nhập cảnh sân bay Cam Ranh trên chuyến bay VN5062 ngày 25/12, cách ly tại tỉnh Khánh Hòa. Kết quả xét nghiệm lần một ngày 25/12 âm tính; lấy mẫu lần hai ngày 30/12 kết quả xét nghiệm tại Trung tâm Kiểm soát Bệnh tật tỉnh Khánh Hòa dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Bệnh nhiệt đới tỉnh Khánh Hòa. Trước đó trên chuyến bay này đã ghi nhận 9 trường hợp dương tính với nCoV. \"Bệnh nhân 1469\", nam, 32 tuổi, ở quận Hải Châu, TP Đà Nẵng. \"Bệnh nhân 1470\", nữ, 40 tuổi, ở huyện Ba Vì, và \"bệnh nhân 1471\", nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội. \"Bệnh nhân 1472\", nam, 29 tuổi, ở huyện Kim Thành, và \"bệnh nhân 1473\", nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương. Họ từ Đức nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN30 ngày 30/12 được cách ly tại tỉnh Bình Dương. Kết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương. \"Bệnh nhân 1474\", nam, 50 tuổi, ở Quận 7, TP HCM. Ông từ Canada quá cảnh sân bay Incheon (Hàn Quốc), sau đó nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN405 ngày 29/12, được cách ly tại tỉnh Long An. Lấy mẫu ngày 30/12, kết quả xét nghiệm tại Trung tâm Kiểm soát bệnh tật tỉnh Long An và Viện Pasteur TP HCM đều dương tính với nCoV, bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Long An. Như vậy, tổng ca nhiễm lên 1.474, tổng số khỏi 1.325. Số người tử vong do Covid-19 là 35, bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính. Các bệnh nhân còn lại đa số sức khỏe ổn định, trong đó 7 người xét nghiệm âm tính nCoV lần một, 11 người âm tính lần hai và 10 người âm tính lần ba. Tổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn 17.000. Trong đó, cách ly tập trung tại bệnh viện 150; cách ly tập trung hơn 15.000, còn lại ở nhà hoặc nơi lưu trú. Việt Nam đã 30 ngày không ghi nhận ca nhiễm mới trong cộng đồng. Song, nguy cơ dịch xâm nhập luôn thường trực, đặc biệt khi tăng số lượng chuyến bay đưa công dân Việt Nam, chuyên gia về nước. Nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép. Bộ Y tế khuyến cáo người dân tiếp tục thực hiện tốt \"Thông điệp 5K\", nhất là đeo khẩu trang và rửa tay bằng xà phòng, dung dịch sát khuẩn. Thế giới ghi nhận hơn 1,7 triệu người chết vì nCoV trong hơn 83 triệu người nhiễm. Mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch, tiếp theo là Ấn Độ và Brazil. \n\n\n\n# generate clean text\nclean_text = text.lower() # convert all uppercase characters into lowercase characters\n\n# replace characters other than [a-zA-Z0-9], digits & one or more spaces with single space\nregex_patterns = [r'\\W',r'\\d',r'\\s+']\nfor regex in regex_patterns:\n    clean_text = re.sub(regex,' ',clean_text)\n\nprint(clean_text)\n\nthêm người nhập cảnh nhiễm ncov ngày bộ y tế ghi nhận ca dương tính ncov đều là người nhập cảnh được cách ly ngay tại khánh hòa bình dương và long an tổng ca nhiễm lên bệnh nhân nam tuổi ở tp hưng yên tỉnh hưng yên bệnh nhân nam tuổi ở huyện thanh hà tỉnh hải dương bệnh nhân nam tuổi ở huyện thanh chương tỉnh nghệ an ba người này từ nga nhập cảnh sân bay cam ranh trên chuyến bay vn ngày cách ly tại tỉnh khánh hòa kết quả xét nghiệm lần một ngày âm tính lấy mẫu lần hai ngày kết quả xét nghiệm tại trung tâm kiểm soát bệnh tật tỉnh khánh hòa dương tính với ncov các bệnh nhân điều trị tại bệnh viện bệnh nhiệt đới tỉnh khánh hòa trước đó trên chuyến bay này đã ghi nhận trường hợp dương tính với ncov bệnh nhân nam tuổi ở quận hải châu tp đà nẵng bệnh nhân nữ tuổi ở huyện ba vì và bệnh nhân nam tuổi ở huyện chương mỹ tp hà nội bệnh nhân nam tuổi ở huyện kim thành và bệnh nhân nam tuổi ở huyện thanh hà tỉnh hải dương họ từ đức nhập cảnh sân bay tân sơn nhất trên chuyến bay vn ngày được cách ly tại tỉnh bình dương kết quả xét nghiệm ngày tại viện pasteur tp hcm dương tính với ncov các bệnh nhân điều trị tại bệnh viện đa khoa tỉnh bình dương bệnh nhân nam tuổi ở quận tp hcm ông từ canada quá cảnh sân bay incheon hàn quốc sau đó nhập cảnh sân bay tân sơn nhất trên chuyến bay vn ngày được cách ly tại tỉnh long an lấy mẫu ngày kết quả xét nghiệm tại trung tâm kiểm soát bệnh tật tỉnh long an và viện pasteur tp hcm đều dương tính với ncov bệnh nhân điều trị tại bệnh viện đa khoa tỉnh long an như vậy tổng ca nhiễm lên tổng số khỏi số người tử vong do covid là bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính các bệnh nhân còn lại đa số sức khỏe ổn định trong đó người xét nghiệm âm tính ncov lần một người âm tính lần hai và người âm tính lần ba tổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn trong đó cách ly tập trung tại bệnh viện cách ly tập trung hơn còn lại ở nhà hoặc nơi lưu trú việt nam đã ngày không ghi nhận ca nhiễm mới trong cộng đồng song nguy cơ dịch xâm nhập luôn thường trực đặc biệt khi tăng số lượng chuyến bay đưa công dân việt nam chuyên gia về nước nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép bộ y tế khuyến cáo người dân tiếp tục thực hiện tốt thông điệp k nhất là đeo khẩu trang và rửa tay bằng xà phòng dung dịch sát khuẩn thế giới ghi nhận hơn triệu người chết vì ncov trong hơn triệu người nhiễm mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch tiếp theo là ấn độ và brazil \n\n\n\n# split (tokenize) the sentences\nsentences = nltk.sent_tokenize(text)\nprint(sentences)\n\n['Thêm 9 người nhập cảnh nhiễm nCoV Ngày 1/1, Bộ Y tế ghi nhận ca dương tính nCoV, đều là người nhập cảnh được cách ly ngay tại Khánh Hòa, Bình Dương và Long An.', 'Tổng ca nhiễm lên 1.474.', '\"Bệnh nhân 1466\", nam, 35 tuổi, ở TP Hưng Yên, tỉnh Hưng Yên.', '\"Bệnh nhân 1467\" nam, 38, tuổi, ở huyện Thanh Hà, tỉnh Hải Dương.', '\"Bệnh nhân 1468\", nam, 51 tuổi, ở huyện Thanh Chương, tỉnh Nghệ An.', 'Ba người này từ Nga nhập cảnh sân bay Cam Ranh trên chuyến bay VN5062 ngày 25/12, cách ly tại tỉnh Khánh Hòa.', 'Kết quả xét nghiệm lần một ngày 25/12 âm tính; lấy mẫu lần hai ngày 30/12 kết quả xét nghiệm tại Trung tâm Kiểm soát Bệnh tật tỉnh Khánh Hòa dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Bệnh nhiệt đới tỉnh Khánh Hòa.', 'Trước đó trên chuyến bay này đã ghi nhận 9 trường hợp dương tính với nCoV.', '\"Bệnh nhân 1469\", nam, 32 tuổi, ở quận Hải Châu, TP Đà Nẵng.', '\"Bệnh nhân 1470\", nữ, 40 tuổi, ở huyện Ba Vì, và \"bệnh nhân 1471\", nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội.', '\"Bệnh nhân 1472\", nam, 29 tuổi, ở huyện Kim Thành, và \"bệnh nhân 1473\", nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương.', 'Họ từ Đức nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN30 ngày 30/12 được cách ly tại tỉnh Bình Dương.', 'Kết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương.', '\"Bệnh nhân 1474\", nam, 50 tuổi, ở Quận 7, TP HCM.', 'Ông từ Canada quá cảnh sân bay Incheon (Hàn Quốc), sau đó nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN405 ngày 29/12, được cách ly tại tỉnh Long An.', 'Lấy mẫu ngày 30/12, kết quả xét nghiệm tại Trung tâm Kiểm soát bệnh tật tỉnh Long An và Viện Pasteur TP HCM đều dương tính với nCoV, bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Long An.', 'Như vậy, tổng ca nhiễm lên 1.474, tổng số khỏi 1.325.', 'Số người tử vong do Covid-19 là 35, bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính.', 'Các bệnh nhân còn lại đa số sức khỏe ổn định, trong đó 7 người xét nghiệm âm tính nCoV lần một, 11 người âm tính lần hai và 10 người âm tính lần ba.', 'Tổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn 17.000.', 'Trong đó, cách ly tập trung tại bệnh viện 150; cách ly tập trung hơn 15.000, còn lại ở nhà hoặc nơi lưu trú.', 'Việt Nam đã 30 ngày không ghi nhận ca nhiễm mới trong cộng đồng.', 'Song, nguy cơ dịch xâm nhập luôn thường trực, đặc biệt khi tăng số lượng chuyến bay đưa công dân Việt Nam, chuyên gia về nước.', 'Nhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép.', 'Bộ Y tế khuyến cáo người dân tiếp tục thực hiện tốt \"Thông điệp 5K\", nhất là đeo khẩu trang và rửa tay bằng xà phòng, dung dịch sát khuẩn.', 'Thế giới ghi nhận hơn 1,7 triệu người chết vì nCoV trong hơn 83 triệu người nhiễm.', 'Mỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch, tiếp theo là Ấn Độ và Brazil.']\n\n\n\n# # get stop words list\n# stop_words = nltk.corpus.stopwords.words('Vietnamese')\n# print(stop_words)\n\n\n# create an empty dictionary to house the word count\nword_count = {}\n\n# loop through tokenized words, remove stop words and save word count to dictionary\nfor word in nltk.word_tokenize(clean_text):\n        # save word count to dictionary\n        if word not in word_count.keys():\n            word_count[word] = 1\n        else:\n            word_count[word] += 1\n\n\nplt.figure(figsize=(16,10))\nplt.xticks(rotation = 90)\nplt.bar(word_count.keys(), word_count.values())\nplt.show()\n\n\n\n\n\n# helper function for plotting the top words.\ndef plot_top_words(word_count_dict, show_top_n=20):\n    word_count_table = pd.DataFrame.from_dict(word_count_dict, orient = 'index').rename(columns={0: 'score'})\n    word_count_table.sort_values(by='score').tail(show_top_n).plot(kind='barh', figsize=(10,10))\n    plt.show()\n\n\nplot_top_words(word_count, 20)\n\n\n\n\n\n# create empty dictionary to house sentence score    \nsentence_score = {}\n\n# loop through tokenized sentence, only take sentences that have less than 30 words, then add word score to form sentence score\nfor sentence in sentences:\n    # check if word in sentence is in word_count dictionary\n    for word in nltk.word_tokenize(sentence.lower()):\n        if word in word_count.keys():\n            # only take sentence that has less than 30 words\n            if len(sentence.split(' ')) &lt; 30:\n                # add word score to sentence score\n                if sentence not in sentence_score.keys():\n                    sentence_score[sentence] = word_count[word]\n                else:\n                    sentence_score[sentence] += word_count[word]\n\n\ndf_sentence_score = pd.DataFrame.from_dict(sentence_score, orient = 'index').rename(columns={0: 'score'})\ndf_sentence_score.sort_values(by='score', ascending = False)\n\n\n\n\n\n\n\n\nscore\n\n\n\n\nKết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương.\n198\n\n\n\"Bệnh nhân 1472\", nam, 29 tuổi, ở huyện Kim Thành, và \"bệnh nhân 1473\", nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương.\n177\n\n\n\"Bệnh nhân 1470\", nữ, 40 tuổi, ở huyện Ba Vì, và \"bệnh nhân 1471\", nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội.\n155\n\n\nBa người này từ Nga nhập cảnh sân bay Cam Ranh trên chuyến bay VN5062 ngày 25/12, cách ly tại tỉnh Khánh Hòa.\n130\n\n\nHọ từ Đức nhập cảnh sân bay Tân Sơn Nhất trên chuyến bay VN30 ngày 30/12 được cách ly tại tỉnh Bình Dương.\n127\n\n\nTrong đó, cách ly tập trung tại bệnh viện 150; cách ly tập trung hơn 15.000, còn lại ở nhà hoặc nơi lưu trú.\n109\n\n\n\"Bệnh nhân 1467\" nam, 38, tuổi, ở huyện Thanh Hà, tỉnh Hải Dương.\n99\n\n\n\"Bệnh nhân 1468\", nam, 51 tuổi, ở huyện Thanh Chương, tỉnh Nghệ An.\n91\n\n\nSố người tử vong do Covid-19 là 35, bốn người tử vong sau ba đến bốn lần xét nghiệm âm tính.\n89\n\n\n\"Bệnh nhân 1466\", nam, 35 tuổi, ở TP Hưng Yên, tỉnh Hưng Yên.\n88\n\n\nTổng số người tiếp xúc gần và nhập cảnh từ vùng dịch đang được cách ly hơn 17.000.\n87\n\n\n\"Bệnh nhân 1469\", nam, 32 tuổi, ở quận Hải Châu, TP Đà Nẵng.\n76\n\n\nNhà chức trách luôn cảnh báo nguy cơ dịch xâm nhập từ người nhập cảnh trái phép.\n74\n\n\n\"Bệnh nhân 1474\", nam, 50 tuổi, ở Quận 7, TP HCM.\n73\n\n\nSong, nguy cơ dịch xâm nhập luôn thường trực, đặc biệt khi tăng số lượng chuyến bay đưa công dân Việt Nam, chuyên gia về nước.\n70\n\n\nTrước đó trên chuyến bay này đã ghi nhận 9 trường hợp dương tính với nCoV.\n69\n\n\nThế giới ghi nhận hơn 1,7 triệu người chết vì nCoV trong hơn 83 triệu người nhiễm.\n68\n\n\nViệt Nam đã 30 ngày không ghi nhận ca nhiễm mới trong cộng đồng.\n48\n\n\nMỹ là quốc gia chịu ảnh hưởng nghiêm trọng nhất của dịch, tiếp theo là Ấn Độ và Brazil.\n46\n\n\nNhư vậy, tổng ca nhiễm lên 1.474, tổng số khỏi 1.325.\n27\n\n\nTổng ca nhiễm lên 1.474.\n15\n\n\n\n\n\n\n\n\n# get the best 3 sentences for summary             \nbest_sentences = heapq.nlargest(3, sentence_score, key=sentence_score.get)\n\n\nprint('SUMMARY')\nprint('------------------------')\n\n# display top sentences based on their sentence sequence in the original text\nfor sentence in sentences:\n    if sentence in best_sentences:\n        print (sentence)\n\nSUMMARY\n------------------------\n\"Bệnh nhân 1470\", nữ, 40 tuổi, ở huyện Ba Vì, và \"bệnh nhân 1471\", nam, 37 tuổi, ở huyện Chương Mỹ, TP Hà Nội.\n\"Bệnh nhân 1472\", nam, 29 tuổi, ở huyện Kim Thành, và \"bệnh nhân 1473\", nam, 37 tuổi, ở huyện Thanh Hà, tỉnh Hải Dương.\nKết quả xét nghiệm ngày 31/12 tại Viện Pasteur TP HCM dương tính với nCoV, các bệnh nhân điều trị tại Bệnh viện Đa khoa tỉnh Bình Dương.\n\n\n\nimport wordcloud\n\n# Wordcloud of training set\ncloud = np.array(file_data).flatten()\nplt.figure(figsize=(20,10))\nword_cloud = wordcloud.WordCloud(\n    max_words=200,background_color =\"black\",\n    width=2000,height=1000,mode=\"RGB\"\n).generate(str(cloud))\nplt.axis(\"off\")\nplt.imshow(word_cloud)\n\n&lt;matplotlib.image.AxesImage at 0x7f3cc9e372b0&gt;\n\n\n\n\n\n\nimport wordcloud\n\n# Wordcloud of training set\ncloud = np.array(best_sentences).flatten()\nplt.figure(figsize=(20,10))\nword_cloud = wordcloud.WordCloud(\n    max_words=200,background_color =\"black\",\n    width=2000,height=1000,mode=\"RGB\"\n).generate(str(cloud))\nplt.axis(\"off\")\nplt.imshow(word_cloud)\n\n&lt;matplotlib.image.AxesImage at 0x7f3d09c0b190&gt;"
  },
  {
    "objectID": "posts/2021-01-15-Folium-Maps.html",
    "href": "posts/2021-01-15-Folium-Maps.html",
    "title": "Maps and Folium",
    "section": "",
    "text": "import folium\nfrom IPython.core.display import display, HTML\nimport tempfile\n\ndef folium_ipython_show(m):\n    tmp_output_filename = tempfile.NamedTemporaryFile(suffix='.html').name\n    m.save(tmp_output_filename)\n\n    f = open(tmp_output_filename, \"r\")\n    data = f.read()\n    data_fixed_width = data.replace('width: 100%;height: 100%', 'width: 100%').replace('height: 100.0%;', 'height: 609px;')\n    display(HTML(data_fixed_width))\n\n\nfolium_ipython_show(folium.Map(location=[42.079391, -87.815622], zoom_start=13))\n\n\n    \n    \n    \n        \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n            \n            \n        \n\n    \n    \n            \n        \n\n\n\n\n\nm3 = folium.Map(location=[42.079391, -87.815622], zoom_start=12, tiles=\"Stamen Terrain\")\n\nfolium_ipython_show(m3)\n\n\n    \n    \n    \n        \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n            \n            \n        \n\n    \n    \n            \n        \n\n\n\n\n\nm4 = folium.Map(location=[42.079391, -87.815622], zoom_start=12, tiles=\"Stamen Terrain\")\n\nfolium_ipython_show(m4)\n\n\n    \n    \n    \n        \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n    \n            \n            \n        \n\n    \n    \n            \n        \n\n\n\n\n\nm5 = folium.Map(location=[42.079391, -87.815622], tiles=\"Stamen Toner\", zoom_start=13)\n\nfolium_ipython_show(m5)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Kearney Data Science",
    "section": "",
    "text": "Bootstrapping in Python\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEDA on Healthcare Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nsklearn-pipelines-example\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nNLP with Pyspark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCausalML Uplift Tree Visualization\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEDA with Sklearn examples\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTensorflow and Keras\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGroup By and Aggregation with Pyspark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWorking with sqlite databases in Jupyter for European Soccer Match Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing Dask with dask.bag and regex to parse The Brothers Karamazov from project gutenberg\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nText summarizer in Python, A Tale of Two Cities\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStock Market and Portfolio Anaylsis of the S&P 500 various metrics with pandas and quandl\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nNLP Heatmaps with Seaborn\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing sqlalchemy and pandas to read and write from and to a local sqlite database for NLP\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCausalML uplift with tree-based algorithms\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nModel Evaluation\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing Prophet for S&P 500 forcasting\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nVisualizing Operations with Dask Dataframes on Fiscal Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMoving fiscal data from a pandas dataframe to a sqlite local database\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMoving Dask XGboost with Fiscal Data, saving and loading Dask XGboost models\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nKaggle Submission Example\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWorking with dask data frames. Reading Fiscal Data from a sqlite db to a dask dataframe. Computing, visualizing and groupby with dask dataframes. Using dask.distributed locally.\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nXgboost for Health Data (Pipeline Step 3)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAdam optimizer\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEvaluating Distributions and generating experimental Crosstabs for the evaluation of Experiments\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSharpe Ratio and Portfolio Values\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWorking with sqlite databases in Jupyter for Visualizing European Soccer Match Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nModel Inspection\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPortfolio Optimization\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPytorch\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWebscraping Text and Images with BeautifulSoup example\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing Dask with dask.bag and regex to parse Notes from the Underground from project gutenberg\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing quandl zillow api for Chicago and Evanston Home Sale Prices using ARIMA and EWMAs\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDask API for analytics\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDask for Predicting Onset/Diagnosis of Chronic Conditions, Diabetes\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAnalyzing US Real Interest Rate From 1959 - 2009 with statsmodels\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPreprocessing example in Sklearn\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStock Market and Portfolio Anaylsis with pandas_datareader and quandl\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEvaluating Distributions and generating Experimental Crosstabs for the Evaluation of Experiments as well as Experimental Comparisons with T-Tests, Purpose Assumptions, Corrections and Implementations in Python\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStock Market Analysis of the S&P 500 Index using ARIMA and Seasonal ARIMA for forecasting\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRegression and Classification with Pyspark ML\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing Dask and dask-sql\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPandas profiling and Shap values for European Soccer Match Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020 with pandas_datareader and writing to at sqlite database\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRDDs and Schemas and Data Types with Pyspark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nText Generation - LSTM\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCross-Validation in scikit-learn example\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRegression using Fiscal Data with PyCaret\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nNLP Example bằng tiếng Việt using StackNetClassifier\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nA timer for ML functions\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStock Market and Portfolio Anaylsis of the S&P 500 with pandas and quandl\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStock Market Analysis of the S&P 500 Index\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMoving Fiscal Data from a sqlite db to a dask dataframe\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWorld Cup prediction example\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nGroupby and Pivot Tables in Python\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTimeseries, Stocks and Altair\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDataframes, Formatting, Casting Data Type and Correlation with Pyspark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAutoML, Xgboost and H2O\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAnalyzing Size of Armed Forces From 1947 - 1963 with statsmodels\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nHandling Missing Data with Pyspark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPython Email Example\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nParameter tuning\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing Dask for Arrays\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing dask_ml.preprocessing and OneHotEncoder for categorical encoding with Dask\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAnalyzing US Unemployment From 1959 - 2009 with statsmodels\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClustering with Pyspark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nModel Calibration\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCreating and accessing SQL databases with python using sqlalchemy and sqlite\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nModeling Health Care Data App\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing Pandas and Statsmodels for Regression with Fiscal Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDaily and Cumulative Returns, CAPM\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCreating E-Books (.epub) in python using ebooklib\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing the Quandl API and Pandas Datareader API to call Microsoft, Apple, Zoom, Snowflake stocks and other finance data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nReview of scikit-learn\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMissing values in scikit-learn\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nCreating and accessing SQL databases with python using sqlalchemy and sqlite3\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWorking with sqlite databases in Jupyter for Visualizing European Soccer Match Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nTimeseries\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nEDA for Health Data (Pipeline Step 2)\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nSpacy in Python for Natural Language Processing (NLP) Example\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nForecasting running data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMCMC algorithms, sampling problems/sampling techniques, Bayesian data analysis\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStock Market and Optimal Portfolio Anaylsis scipy and quandl\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClassification example 1 using Health Data with PyCaret\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPoisson regression\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nRegression using Health Data with PyCaret\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nImbalanced data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nAnalyzing US Inflation From 1959 - 2009 with statsmodels\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing NewsPlease an open source, news crawler that extracts structured information\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nLinear Regression using Dask Data Frames\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nClassification example 2 using Health Data with PyCaret\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPredicting Onset/Diagnosis of Chronic Conditions, Diabetes\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nStock Market Analysis of the S&P 500 Index using Simple Moving Averages and Exponentially-weighted moving averages\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nModeling Health Care Data App\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPyspark Regression with Fiscal Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nNLP ngrams With Python\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing sqlalchemy and pandas to read and write from and to a local sqlite database for Stock Market and Portfolio Anaylsis Tech Stocks and the S&P 500 in 2020\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nModeling Health Care Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nReading CSV from Datasette SQL Database\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nWindow functions and Pivot Tables with Pyspark\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nDataframe Filitering and Operations with Pyspark\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPandas Interoperability\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nUsing sqlalchemy and pandas to read and write from and to a local sqlite database for Fiscal Data\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nText summarizer in Python, Notes from underground\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPrincipal component analysis with sklearn\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nText summarizer in Python, Tieng Viet\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nMaps and Folium\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2023\n\n\ntest\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nApr 20, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Core data science contributor of high-performing revenue team delivering over a billion dollars in revenue annually for a top-grossing global app.\nConducted pricing, payment, and revenue growth experiments across multiple markets, including the USA, EU, Japan, UK, Canada, and Australia, resulting in annual revenue growth of over $30 million.\nImplemented machine learning-based fraud detection and prevention experiments, leading to the elimination of over $1 million in fees and mitigating more than $100 million in business risk. Deployed ML models to identify high-propensity subscribers and ALC product purchasers.\nDesigned and implemented automated dashboards, KPI tracking and reporting features, and experiment and company-level revenue metrics, streamlining data analysis and providing crucial insights for business decision-making."
  },
  {
    "objectID": "about.html#tinder",
    "href": "about.html#tinder",
    "title": "About",
    "section": "",
    "text": "Core data science contributor of high-performing revenue team delivering over a billion dollars in revenue annually for a top-grossing global app.\nConducted pricing, payment, and revenue growth experiments across multiple markets, including the USA, EU, Japan, UK, Canada, and Australia, resulting in annual revenue growth of over $30 million.\nImplemented machine learning-based fraud detection and prevention experiments, leading to the elimination of over $1 million in fees and mitigating more than $100 million in business risk. Deployed ML models to identify high-propensity subscribers and ALC product purchasers.\nDesigned and implemented automated dashboards, KPI tracking and reporting features, and experiment and company-level revenue metrics, streamlining data analysis and providing crucial insights for business decision-making."
  },
  {
    "objectID": "about.html#cvs-health",
    "href": "about.html#cvs-health",
    "title": "About",
    "section": "CVS Health:",
    "text": "CVS Health:\n\nCreated automated member and provider engagement campaigns that leverage e-mail, IVR, SMS and live calls to increase medication adherence.\nDesigned experiments and employ randomized control trials to improve campaigns by identifying the most effective campaign variants, and productionlize machine learning models to identify members at risk of behaving in a way that contributes to poor health outcomes and most likely to benefit from campaigns."
  },
  {
    "objectID": "about.html#duke-university",
    "href": "about.html#duke-university",
    "title": "About",
    "section": "Duke University:",
    "text": "Duke University:\n\nRecieved Ph.D. where research focused on applying econometric and statistical techniques to explain and predict the allocation of fiscal transfers."
  }
]